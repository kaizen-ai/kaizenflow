{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hk_xa2snRpoL",
        "outputId": "6b13c9e4-ed87-4949-e0f8-cdec064d64f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pprint import pprint\n",
        "import time\n"
      ],
      "metadata": {
        "id": "w5M5fggPR5_G"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize SparkContext\n",
        "sc = SparkContext()\n",
        "# Create a StreamingContext with batch interval of 10 seconds\n",
        "ssc = StreamingContext(sc, 10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwXmm3rBR8lI",
        "outputId": "d66eea89-cf92-4ae0-f2f0-cfe239930ebf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/streaming/context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```ssc.queueStream(rdd_queue)```: Creates a DStream from rdd_queue.\n",
        "\n",
        "```.map(lambda x: (x % 10, 1))```: Applies a transformation to each element of the DStream, mapping it to a tuple (x % 10, 1). This groups elements by their remainder when divided by 10, converting each element into a key-value pair with the key as the remainder and the value as 1.\n",
        "\n",
        "```.window(10)```: Creates a sliding window of 10 seconds on the DStream, collecting elements over this time frame.\n",
        "\n",
        "```.reduceByKey(lambda v1, v2: v1 + v2```: Reduces elements with the same key by adding their values together. In this case, it counts the occurrences of each remainder within the 10-second window."
      ],
      "metadata": {
        "id": "uHB9-nEZEImj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty list to hold RDDs\n",
        "rdd_queue = []\n",
        "\n",
        "# Create 5 batches of data\n",
        "for i in range(5):\n",
        "    # Generate data ranging from 0 to 999\n",
        "    rdd_data = range(1000)\n",
        "\n",
        "    # Create an RDD from the generated data\n",
        "    rdd = ssc.sparkContext.parallelize(rdd_data)\n",
        "\n",
        "    # Append the RDD to the list\n",
        "    rdd_queue.append(rdd)\n",
        "\n",
        "# Print the list of RDDs\n",
        "pprint(rdd_queue)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xLQWdXCWCT6",
        "outputId": "1ea2f999-c70a-4fa5-c03c-05a89423d447"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PythonRDD[5] at RDD at PythonRDD.scala:53,\n",
            " PythonRDD[6] at RDD at PythonRDD.scala:53,\n",
            " PythonRDD[7] at RDD at PythonRDD.scala:53,\n",
            " PythonRDD[8] at RDD at PythonRDD.scala:53,\n",
            " PythonRDD[9] at RDD at PythonRDD.scala:53]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "`ssc.queueStream(rdd_queue)`: Creates a DStream from rdd_queue.\n",
        "\n",
        "`.map(lambda x: (x % 10, 1))`: Applies a transformation to each element of the DStream, mapping it to a tuple `(x % 10, 1)`. This groups elements by their remainder when divided by 10, converting each element into a key-value pair with the key as the remainder and the value as 1.\n",
        "\n",
        "`.window(10)`: Creates a sliding window of 10 seconds on the DStream, collecting elements over this time frame.\n",
        "\n",
        "`.reduceByKey(lambda v1, v2: v1 + v2)`: Reduces elements with the same key by adding their values together. In this case, it counts the occurrences of each remainder within the 10-second window."
      ],
      "metadata": {
        "id": "QzHCiEZ-a-4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DStream from a queue of RDDs, mapping each element to (remainder, 1), apply a 10-second window, and count occurrences.\n",
        "dataset1 = ssc.queueStream(rdd_queue).map(lambda x: (x % 10, 1)).window(10).reduceByKey(lambda v1, v2: v1 + v2)\n",
        "# Print the result of dataset1 DStream\n",
        "dataset1.pprint()"
      ],
      "metadata": {
        "id": "gPIMX0ap256B"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This following code constructs a streaming computation pipeline similar to the previous one but with a 20 seconds window duration and grouping based on the remainder when dividing by 5."
      ],
      "metadata": {
        "id": "-dnYH-t3b868"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DStream from a queue of RDDs, mapping each element to (remainder, 1), apply a 20-second window, and count occurrences.\n",
        "dataset2 = ssc.queueStream(rdd_queue).map(lambda x: (x % 5, 1)).window(windowDuration=20).reduceByKey(lambda v1, v2: v1 + v2)\n",
        "# Print the result of dataset2 DStream\n",
        "dataset2.pprint()"
      ],
      "metadata": {
        "id": "LL2GW4Dw2dxl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "The code performs an inner join operation between two DStreams.\n",
        "\n",
        "In both cases, the key is computed as the remainder when dividing each element `x` by either 10 (for `dataset1`) or 5 (for `dataset2`). Therefore, the common key between the two datasets is the remainder of dividing each element by `5`.\n"
      ],
      "metadata": {
        "id": "3EH-s53lcnXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform an inner join operation between dataset1 and dataset2\n",
        "joinedStream = dataset1.join(dataset2)\n",
        "\n",
        "# Print the output of the joinedStream\n",
        "joinedStream.pprint()"
      ],
      "metadata": {
        "id": "atjgooHJ2rN1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform left outer join operation between dataset1 and dataset2\n",
        "joinedStream_left_outer = dataset1.leftOuterJoin(dataset2)\n",
        "\n",
        "# Print the output of the joinedStream\n",
        "joinedStream_left_outer.pprint()"
      ],
      "metadata": {
        "id": "rhGXCGcEkhlu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the streaming context\n",
        "ssc.start()"
      ],
      "metadata": {
        "id": "QMOu7uKZTmVq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Inner Join**:\n",
        "\n",
        "The joined counts from `dataset1` and `dataset2` represent the combined results of the inner join operation performed between the two datasets. In the provided output, the joined counts are displayed as tuples where the first element corresponds to the key (remainder) and the second element is a tuple containing the count from `dataset1` and `dataset2` respectively.\n",
        "\n",
        "At the timestamp \"`Time: 2024-05-10 18:11:30`\", the joined count for key 0 is (100, 200). This means that within the specified window duration:\n",
        "\n",
        "* Key 0 appears 100 times in `dataset1`.\n",
        "\n",
        "* Key 0 appears 200 times in `dataset2`.\n",
        "\n",
        "\n",
        "**Left Outer Join**:\n",
        "\n",
        "A left outer join operation between two DStreams combines elements from the left DStream with matching elements from the right DStream, based on a common key. Left outer join returns all records from the left dataset (`dataset1`) and matching records from the right dataset (`dataset2`), if any.\n",
        "If a key exists in `dataset1` but not in `dataset2`, the corresponding value in the second element of the tuple is None.\n",
        "\n",
        "At the timestamp \"`Time: 2024-05-10 18:11:30`\", the joined count for key 6 is (100, None). This means that within the specified window duration:\n",
        "\n",
        "* Key 6 appears 100 times in `dataset1`.\n",
        "\n",
        "* Key 6 doesnot appear in `dataset2`."
      ],
      "metadata": {
        "id": "tTezXRhLfoUl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Comparison**:\n",
        "\n",
        "* Inner join focuses on the intersection of keys present in both datasets, filtering out non-matching keys.\n",
        "\n",
        "* Left outer join retains all keys from the left dataset (`dataset1`), including those that do not have a match in the right dataset (`dataset2`), represented by None in the output."
      ],
      "metadata": {
        "id": "9X5AhrsEmdof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Await termination for a specified duration (30 seconds)\n",
        "ssc.awaitTermination(timeout=30)\n",
        "\n",
        "# Stop the streaming context gracefully\n",
        "ssc.stop(stopSparkContext=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSZGMzl120m6",
        "outputId": "40b18ca1-1b17-47f5-f66e-5e701bd2522e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n",
            "Time: 2024-05-10 18:11:30\n",
            "-------------------------------------------\n",
            "(0, 100)\n",
            "(2, 100)\n",
            "(4, 100)\n",
            "(6, 100)\n",
            "(8, 100)\n",
            "(1, 100)\n",
            "(3, 100)\n",
            "(5, 100)\n",
            "(7, 100)\n",
            "(9, 100)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2024-05-10 18:11:30\n",
            "-------------------------------------------\n",
            "(0, 200)\n",
            "(2, 200)\n",
            "(4, 200)\n",
            "(1, 200)\n",
            "(3, 200)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2024-05-10 18:11:30\n",
            "-------------------------------------------\n",
            "(0, (100, 200))\n",
            "(2, (100, 200))\n",
            "(4, (100, 200))\n",
            "(1, (100, 200))\n",
            "(3, (100, 200))\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2024-05-10 18:11:30\n",
            "-------------------------------------------\n",
            "(0, (100, 200))\n",
            "(2, (100, 200))\n",
            "(4, (100, 200))\n",
            "(6, (100, None))\n",
            "(8, (100, None))\n",
            "(1, (100, 200))\n",
            "(3, (100, 200))\n",
            "(5, (100, None))\n",
            "(7, (100, None))\n",
            "(9, (100, None))\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2024-05-10 18:11:40\n",
            "-------------------------------------------\n",
            "(0, 100)\n",
            "(2, 100)\n",
            "(4, 100)\n",
            "(6, 100)\n",
            "(8, 100)\n",
            "(1, 100)\n",
            "(3, 100)\n",
            "(5, 100)\n",
            "(7, 100)\n",
            "(9, 100)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2024-05-10 18:11:40\n",
            "-------------------------------------------\n",
            "(0, 400)\n",
            "(2, 400)\n",
            "(4, 400)\n",
            "(1, 400)\n",
            "(3, 400)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2024-05-10 18:11:40\n",
            "-------------------------------------------\n",
            "(0, (100, 400))\n",
            "(2, (100, 400))\n",
            "(4, (100, 400))\n",
            "(1, (100, 400))\n",
            "(3, (100, 400))\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2024-05-10 18:11:40\n",
            "-------------------------------------------\n",
            "(0, (100, 400))\n",
            "(2, (100, 400))\n",
            "(4, (100, 400))\n",
            "(6, (100, None))\n",
            "(8, (100, None))\n",
            "(1, (100, 400))\n",
            "(3, (100, 400))\n",
            "(5, (100, None))\n",
            "(7, (100, None))\n",
            "(9, (100, None))\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2024-05-10 18:11:50\n",
            "-------------------------------------------\n",
            "(0, 100)\n",
            "(2, 100)\n",
            "(4, 100)\n",
            "(6, 100)\n",
            "(8, 100)\n",
            "(1, 100)\n",
            "(3, 100)\n",
            "(5, 100)\n",
            "(7, 100)\n",
            "(9, 100)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}