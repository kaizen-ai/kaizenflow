{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Spring2024_Streaming_Word_Count_with_Apache_Spark_Streaming.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1kfEMsZrlHged3z4gbn9yznFHDvi5QGMI\n",
    "\"\"\"\n",
    "### Streaming Word Count with Apache Spark Streaming\n",
    "**This project focuses on real-time word counting using Apache Spark Streaming, a powerful framework for processing and analyzing streaming data. In this scenario, we aim to continuously process incoming text data streams, perform word counting, and visualize the results in real-time. Here's an overview of the functionalities and testing scenarios for this project:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By testing these functionalities and scenarios, we can gain valuable insights into the behavior and performance of the streaming word count application, identify potential bottlenecks or limitations, and fine-tune its configuration to optimize performance and reliability in real-world deployment scenarios.\n",
    "\n",
    "**In this section, to run the streaming word count application with Apache Spark and visualize the results, we need to install the required Python libraries. This section of the code installs two essential dependencies:**\n",
    "\n",
    "1. **pyspark**: This library provides the Python API for Apache Spark, allowing us to interact with Spark's distributed computing engine and perform data processing tasks.\n",
    "\n",
    "2. **matplotlib**: This library is used for data visualization in Python. We'll use it to create visualizations of the word counts generated by the streaming application.\n",
    "\n",
    "pip install pyspark\n",
    "pip install matplotlib\n",
    "\n",
    "**In the below section of the code, we import a collection of Python libraries essential for the execution of the streaming word count application using Apache Spark. These libraries provide various functionalities crucial for data processing, manipulation, visualization, and interaction with the Spark framework. They include \"pyspark\" for accessing Spark's distributed computing engine, \"sys\" for system-specific parameters and functionalities, \"operator\" for efficient operations on iterable data structures, \"matplotlib.pyplot\" for creating visualizations, \"pyspark.sql.functions\" for working with structured data in Spark SQL, \"SparkSession\" for initializing the connection to the Spark cluster, \"Tokenizer\" and \"StopWordsRemover\" from the \"pyspark.ml.feature\" module for text preprocessing, \"SparkContext\" for creating Resilient Distributed Datasets (RDDs), and \"StreamingContext\" for handling streaming functionality in Spark. Each library serves a unique purpose in facilitating the development and execution of the streaming word count application, ensuring efficient data processing and visualization within the Spark framework.**\n",
    "import pyspark\n",
    "\n",
    "# Required imports\n",
    "import sys\n",
    "from operator import add\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "**The function below, the process_stream function is defined to handle streaming data and generate word counts in near real-time. It takes an RDD as input, extracts words from the data, counts their occurrences, sorts them, and displays the top words along with their counts. Additionally, it visualizes the top 10 words using a bar plot, providing a quick and efficient way to analyze word frequencies in the streaming data.**\n",
    "# Function to process streaming data and generate word counts\n",
    "def process_stream(rdd):\n",
    "    if not rdd.isEmpty():\n",
    "        word_counts = rdd.withColumn('word', F.explode(F.col('words_clean'))) \\\n",
    "                         .groupBy('word') \\\n",
    "                         .count() \\\n",
    "                         .sort('count', ascending=False)\n",
    "\n",
    "        # Show the result\n",
    "        word_counts.show()\n",
    "\n",
    "        # Visualize word counts\n",
    "        word_counts_pd = word_counts.limit(10).toPandas()  # Limit to top 10 words for visualization\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(word_counts_pd['word'], word_counts_pd['count'])\n",
    "        plt.xlabel('Words')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Top 10 Words in Billboard Songs')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "**The code below the SparkContext (sc) and StreamingContext (ssc) objects are initialized to set up the environment for real-time streaming processing. The SparkContext serves as the entry point to the Spark cluster, while the StreamingContext facilitates the creation of DStream objects, enabling the application to process continuous streams of data with a micro-batch interval of 5 seconds.**\n",
    "# Initialize SparkContext and StreamingContext\n",
    "sc = SparkContext(appName=\"RealTimeWordCount\")\n",
    "ssc = StreamingContext(sc, 5)  # 5-second micro-batch interval\n",
    "\n",
    "**In the section below, the SparkSession object (spark) is initialized to enable the use of Spark SQL and the DataFrame API for processing structured data. By creating a unified entry point for interacting with different data sources and executing SQL queries, SparkSession facilitates seamless data manipulation within the Spark application, with the application name set to \"PythonWordCount\".**\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"PythonWordCount\").getOrCreate()\n",
    "\n",
    "**Below, the code reads CSV data from the file 'billboard_lyrics_1964-2015.csv' into a Spark DataFrame called data and then prints the extracted data with column headers.**\n",
    "# Read CSV data\n",
    "data = spark.read.format('csv').options(header='true', inferSchema='true') \\\n",
    "    .load('billboard_lyrics_1964-2015.csv')\n",
    "print('############ CSV extract:')\n",
    "data.show()\n",
    "\n",
    "**The below function we tokenize the lyrics data from the DataFrame using the Tokenizer class from the pyspark.ml.feature module. The Tokenizer splits the text into individual words or tokens, creating a new column named \"words_token\" containing the tokenized lyrics. The resulting DataFrame, containing the original rank along with the tokenized lyrics, is then displayed to provide a glimpse of the tokenized data.**\n",
    "tokenizer = Tokenizer(inputCol=\"Lyrics\", outputCol=\"words_token\")\n",
    "tokenized = tokenizer.transform(data).select('Rank', 'words_token')\n",
    "\n",
    "print('############ Tokenized data extract:')\n",
    "tokenized.show(truncate=False)\n",
    "\n",
    "**In this section below, we remove stop words from the tokenized lyrics data using the StopWordsRemover class from the pyspark.ml.feature module. Stop words, common words like \"the\" or \"and\", are filtered out, resulting in a cleaner representation of the lyrics. The DataFrame containing the rank and cleaned words is displayed to provide insight into the cleaned data.**\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol='words_token', outputCol='words_clean')\n",
    "data_clean = remover.transform(tokenized).select('Rank', 'words_clean')\n",
    "\n",
    "print('############ Data Cleaning extract:')\n",
    "data_clean.show(truncate=False)\n",
    "\n",
    "**In this final word count section, we compute the frequency of each word in the cleaned lyrics data. The DataFrame is transformed to explode the cleaned words column into individual rows, grouped by word, and then aggregated to count the occurrences of each word. The result is sorted in descending order based on word count, providing insights into the most common words in the lyrics dataset.**\n",
    "# Final word count\n",
    "result = data_clean.withColumn('word', F.explode(F.col('words_clean'))) \\\n",
    "                   .groupBy('word') \\\n",
    "                   .count().sort('count', ascending=False)\n",
    "\n",
    "print('############ Final word count:')\n",
    "result.show()\n",
    "\n",
    "**In this section, we install the Pandas library using pip and import it as pd for easy access.**\n",
    "pip install pandas\n",
    "import pandas as pd\n",
    "\n",
    "**The below visualization is the word counts obtained from the streaming data analysis. The word counts DataFrame (result) is converted to a Pandas DataFrame (result_pd) to facilitate visualization. Using Matplotlib, we create a bar plot to display the top 10 words and their respective counts. The plot provides a visual representation of the most frequently occurring words in the streaming data, aiding in data interpretation and analysis**\n",
    "# Tokenize and remove stop words\n",
    "import matplotlib.pyplot as plt\n",
    "words = [row['word'] for row in result.limit(10).collect()]\n",
    "counts = [row['count'] for row in result.limit(10).collect()]\n",
    "\n",
    "# Create a bar plot using matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(words, counts)\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Top 10 Words in Billboard Songs')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "This below section of code is performing text preprocessing and analysis on lyrics data obtained from a CSV file. It returns these metrics as word_count, average_word_length, most_common_word, most_common_word_count, and unique_word_count.\n",
    "\n",
    "```python\n",
    "import csv\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Function to read CSV file\n",
    "def read_csv_file(file_path):\n",
    "    lyrics = []\n",
    "    with open(file_path, 'r', encoding='iso-8859-1') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        next(csv_reader)  # Skip header row\n",
    "        for row in csv_reader:\n",
    "            lyrics.append(row[4])  # Assuming lyrics are in the 5th column\n",
    "    return lyrics\n",
    "\n",
    "# Function to preprocess text (tokenize, filter, remove stop words)\n",
    "def preprocess_text(text):\n",
    "    # Tokenize\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())  # Assuming words are separated by whitespace\n",
    "    \n",
    "    # Filter out short words (length < 3 characters)\n",
    "    words = [word for word in words if len(word) >= 3]\n",
    "    \n",
    "    # Remove stop words (you can define your own list of stop words)\n",
    "    stop_words = set(['the', 'and', 'of', 'in', 'to', 'a', 'is', 'that', 'it', 'for', 'on', 'with', 'as'])\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    return words\n",
    "\n",
    "# Function to perform word count and additional functionalities\n",
    "def process_text(lyrics):\n",
    "    # Flatten the list of preprocessed lyrics\n",
    "    flattened_lyrics = [word for sublist in lyrics for word in sublist]\n",
    "    \n",
    "    # Word count\n",
    "    word_count = len(flattened_lyrics)\n",
    "    \n",
    "    # Calculate average word length\n",
    "    total_word_length = sum(len(word) for word in flattened_lyrics)\n",
    "    average_word_length = total_word_length / word_count if word_count > 0 else 0\n",
    "    \n",
    "    # Find the most common word\n",
    "    word_freq = Counter(flattened_lyrics)\n",
    "    most_common_word, most_common_word_count = word_freq.most_common(1)[0]\n",
    "    \n",
    "    # Count the number of unique words\n",
    "    unique_word_count = len(word_freq)\n",
    "    \n",
    "    return word_count, average_word_length, most_common_word, most_common_word_count, unique_word_count\n",
    "\n",
    "# Read CSV data\n",
    "lyrics = read_csv_file('billboard_lyrics_1964-2015.csv')\n",
    "\n",
    "# Process text\n",
    "preprocessed_lyrics = [preprocess_text(text) for text in lyrics]\n",
    "word_count, average_word_length, most_common_word, most_common_word_count, unique_word_count = process_text(preprocessed_lyrics)\n",
    "\n",
    "# Display results\n",
    "print(\"Total Word Count:\", word_count)\n",
    "print(\"Average Word Length:\", average_word_length)\n",
    "print(\"Most Common Word:\", most_common_word, \"(Count:\", most_common_word_count, \")\")\n",
    "print(\"Number of Unique Words:\", unique_word_count)\n",
    "```\n",
    "\n",
    "This section of the code defines a function visualize_results to create a bar plot visualizing various text analysis metrics.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to visualize the results\n",
    "def visualize_results(word_count, average_word_length, most_common_word, most_common_word_count, unique_word_count):\n",
    "    # Define the data to visualize\n",
    "    data = {\n",
    "        'Total Word Count': word_count,\n",
    "        'Average Word Length': average_word_length,\n",
    "        'Most Common Word': most_common_word_count,\n",
    "        'Unique Word Count': unique_word_count\n",
    "    }\n",
    "\n",
    "    # Create a bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(data.keys(), data.values(), color='skyblue')\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Values')\n",
    "    plt.title('Text Analysis Metrics')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the results\n",
    "visualize_results(word_count, average_word_length, most_common_word, most_common_word_count, unique_word_count)\n",
    "```",
    "\n",
    "**The below function, we create a DStream (Discretized Stream) by reading text files from a specified directory. The textFileStream method of the StreamingContext (ssc) is used to create the stream, with the directory path provided as the argument. This enables the streaming application to ingest and process data from the text files in real-time, allowing for continuous analysis of the data as it becomes available.**\n",
    "# Create DStream by reading text files from the directory\n",
    "data_dir = \"./data_chunks\"\n",
    "stream = ssc.textFileStream(data_dir)\n",
    "\n",
    "**Followed by the below code, we define a process to handle each RDD (Resilient Distributed Dataset) in the stream. The foreachRDD method is used to apply the process_stream function to each RDD in the stream. This function processes the streaming data, extracting word counts, and visualizing the results. By iterating over each RDD in the stream, we ensure that the processing logic is applied to every batch of data received, enabling real-time analysis of the streaming data.**\n",
    "# Process each RDD in the stream\n",
    "stream.foreachRDD(process_stream)\n",
    "\n",
    "**In the below process, we initiate the streaming process by calling the start method on the StreamingContext object (ssc). Additionally, we use the awaitTermination method to instruct the program to wait until the streaming process is terminated or manually stopped.**\n",
    "# Start streaming\n",
    "ssc.start()\n",
    "# Wait for streaming to finish\n",
    "ssc.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}


