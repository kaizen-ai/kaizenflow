{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "569d863e",
   "metadata": {},
   "source": [
    "TODO(Vlad): TODO(Samarth): convert the notebook into unit tests CmTask7331"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b504db",
   "metadata": {},
   "source": [
    "The notebook demonstrates current behavior of various parquet functions with respect to\n",
    "different time units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d791102e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa78c66c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T15:53:12.983426Z",
     "start_time": "2024-02-29T15:53:12.948452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3af0cd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T15:53:13.760098Z",
     "start_time": "2024-02-29T15:53:13.729798Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "\n",
    "import helpers.hdbg as hdbg\n",
    "import helpers.henv as henv\n",
    "import helpers.hio as hio\n",
    "import helpers.hparquet as hparque\n",
    "import helpers.hprint as hprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e17ec2b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T15:53:16.103573Z",
     "start_time": "2024-02-29T15:53:15.997617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[33mWARNING\u001b[0m: Logger already initialized: skipping\n",
      "\u001b[31m-----------------------------------------------------------------------------\n",
      "This code is not in sync with the container:\n",
      "code_version='1.13.0' != container_version='1.15.0'\n",
      "-----------------------------------------------------------------------------\n",
      "You need to:\n",
      "- merge origin/master into your branch with `invoke git_merge_master`\n",
      "- pull the latest container with `invoke docker_pull`\u001b[0m\n",
      "INFO  # Git\n",
      "  branch_name='CmampTask7292_DEV_TOOLS___Docker___Update_pyarrow_to_latest_version'\n",
      "  hash='56b65f870'\n",
      "  # Last commits:\n",
      "    * 56b65f870 vlady    7292_second_iter                                                  (43 minutes ago) Thu Feb 29 15:09:48 2024  (HEAD -> CmampTask7292_DEV_TOOLS___Docker___Update_pyarrow_to_latest_version, origin/CmampTask7292_DEV_TOOLS___Docker___Update_pyarrow_to_latest_version)\n",
      "    * 1dc509b73 vlady    7292_jyp_test                                                     (   2 hours ago) Thu Feb 29 13:59:03 2024           \n",
      "    *   47978e4a9 vlady    Merge branch 'CmampTask7292_DEV_TOOLS___Docker___Update_pyarrow_to_latest_version' of github.com:cryptokaizen/cmamp into CmampTask7292_DEV_TOOLS___Docker___Update_pyarrow_to_latest_version (   7 hours ago) Thu Feb 29 09:07:28 2024           \n",
      "    |\\  \n",
      "# Machine info\n",
      "  system=Linux\n",
      "  node name=78ac0bba0fe6\n",
      "  release=5.15.0-1052-aws\n",
      "  version=#57~20.04.1-Ubuntu SMP Mon Jan 15 17:04:56 UTC 2024\n",
      "  machine=x86_64\n",
      "  processor=x86_64\n",
      "  cpu count=8\n",
      "  cpu freq=scpufreq(current=2499.9959999999996, min=0.0, max=0.0)\n",
      "  memory=svmem(total=33280286720, available=19868934144, percent=40.3, used=12931612672, free=1654857728, active=10423332864, inactive=15502258176, buffers=1418117120, cached=17275699200, shared=2748416, slab=5169451008)\n",
      "  disk usage=sdiskusage(total=218506772480, used=105601765376, free=112888229888, percent=48.3)\n",
      "# Packages\n",
      "  python: 3.9.5\n",
      "  cvxopt: 1.3.2\n",
      "  cvxpy: 1.4.2\n",
      "  gluonnlp: ?\n",
      "  gluonts: ?\n",
      "  joblib: 1.3.2\n",
      "  mxnet: ?\n",
      "  numpy: 1.26.0\n",
      "  pandas: 2.1.1\n",
      "  pyarrow: 15.0.0\n",
      "  scipy: 1.11.3\n",
      "  seaborn: 0.13.0\n",
      "  sklearn: 1.3.1\n",
      "  statsmodels: 0.14.0\n"
     ]
    }
   ],
   "source": [
    "hdbg.init_logger(verbosity=logging.INFO)\n",
    "_LOG = logging.getLogger(__name__)\n",
    "_LOG.info(\"%s\", henv.get_system_signature()[0])\n",
    "hprint.config_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94ddde3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T15:53:18.737222Z",
     "start_time": "2024-02-29T15:53:18.698878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'15.0.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyarrow.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad31177",
   "metadata": {},
   "source": [
    "# Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b79d1f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T15:53:29.769720Z",
     "start_time": "2024-02-29T15:53:29.691633Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_legs</th>\n",
       "      <th>animal</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-01 00:00:00.123456-05:00</th>\n",
       "      <td>2</td>\n",
       "      <td>Flamingo</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 00:00:00.123456-05:00</th>\n",
       "      <td>2</td>\n",
       "      <td>Parrot</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 00:00:00.123456-05:00</th>\n",
       "      <td>4</td>\n",
       "      <td>Dog</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 00:00:00.123456-05:00</th>\n",
       "      <td>4</td>\n",
       "      <td>Horse</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 00:00:00.123456-05:00</th>\n",
       "      <td>5</td>\n",
       "      <td>Brittle stars</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 00:00:00.123456-05:00</th>\n",
       "      <td>100</td>\n",
       "      <td>Centipede</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  n_legs         animal  year\n",
       "2022-01-01 00:00:00.123456-05:00       2       Flamingo  2001\n",
       "2022-01-01 00:00:00.123456-05:00       2         Parrot  2002\n",
       "2022-01-01 00:00:00.123456-05:00       4            Dog  2001\n",
       "2022-01-01 00:00:00.123456-05:00       4          Horse  2003\n",
       "2022-01-01 00:00:00.123456-05:00       5  Brittle stars  2003\n",
       "2022-01-01 00:00:00.123456-05:00     100      Centipede  2001"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp_us = pd.Timestamp(\"2022-01-01 00:00:00.123456\", tz=\"America/New_York\")\n",
    "index = [timestamp_us for _ in range(6)]\n",
    "initial_df = pd.DataFrame(\n",
    "    {\n",
    "        \"n_legs\": [2, 2, 4, 4, 5, 100],\n",
    "        \"animal\": [\n",
    "            \"Flamingo\",\n",
    "            \"Parrot\",\n",
    "            \"Dog\",\n",
    "            \"Horse\",\n",
    "            \"Brittle stars\",\n",
    "            \"Centipede\",\n",
    "        ],\n",
    "        \"year\": [2001, 2002, 2001, 2003, 2003, 2001],\n",
    "    },\n",
    "    index=index,\n",
    ")\n",
    "initial_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c05170ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T15:53:31.198719Z",
     "start_time": "2024-02-29T15:53:31.168180Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ns'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_df.index.unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15f13ede",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T16:43:13.996907Z",
     "start_time": "2024-02-29T16:43:13.962102Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_write_and_read_partition_parquet_with_unit(\n",
    "    initial_df: pd.DataFrame,\n",
    "    partition_columns: List[str],\n",
    "    dst_dir: str,\n",
    "    unit: str,\n",
    "    *,\n",
    "    clean_up: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Write the provided DataFrame to partitioned Parquet files and read it back,\n",
    "    verifying the retention of time unit information in the index.\n",
    "\n",
    "    :param initial_df: dataframe to write\n",
    "    :param dst_dir: root folder to write partition parquet\n",
    "    :param partition_columns: partition columns to write\n",
    "    :param unit: initial time unit in the index\n",
    "    :param clean_up: delete parquet folder at the end\n",
    "    \"\"\"\n",
    "    current_df = initial_df.copy()\n",
    "    _LOG.info(\"Initial DF unit: %s\", current_df.index.unit)\n",
    "    _LOG.info(\"Converting DF unit from ns to: %s\", unit)\n",
    "    current_df.index = current_df.index.as_unit(unit)\n",
    "    _LOG.info(\n",
    "        \"DF Unit before writing to parquet files: %s\", current_df.index.unit\n",
    "    )\n",
    "    # The `to_partitioned_parquet` saves the given dataframe as Parquet\n",
    "    # files partitioned along the given columns.\n",
    "    hparque.to_partitioned_parquet(current_df, partition_columns, dst_dir)\n",
    "    # Generates the DF from parquet files in the `dst_dir`.\n",
    "    df = hparque.from_parquet(dst_dir)\n",
    "    print(\"\\n\")\n",
    "    print(\"DF from parquet files\")\n",
    "    print(df)\n",
    "    _LOG.info(\"DF Unit after reading from parquet files: %s\", df.index.unit)\n",
    "    if clean_up:\n",
    "        hio.delete_dir(dst_dir)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "383277ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T16:43:31.028505Z",
     "start_time": "2024-02-29T16:43:30.991279Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_write_and_read_parquet_file_with_unit(\n",
    "    initial_df: pd.DataFrame, file_name: str, unit: str, *, clean_up: bool = False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Write the provided DataFrame to Parquet file and read it back, verifying\n",
    "    the retention of time unit information in the index.\n",
    "\n",
    "    :param initial_df: dataframe to write\n",
    "    :param file_name: destination parquet file name\n",
    "    :param unit: initial time unit in the index\n",
    "    :param clean_up: delete parquet file at the end\n",
    "    \"\"\"\n",
    "    current_df = initial_df.copy()\n",
    "    _LOG.info(\"Initial DF unit: %s\", current_df.index.unit)\n",
    "    _LOG.info(\"Converting DF unit from ns to: %s\", unit)\n",
    "    current_df.index = current_df.index.as_unit(unit)\n",
    "    _LOG.info(\n",
    "        \"Unit before writing to single parquet file: %s\", current_df.index.unit\n",
    "    )\n",
    "    # The `to_parquet` function writes a DF to a single parquet file without\n",
    "    # any partition.\n",
    "    hparque.to_parquet(current_df, file_name)\n",
    "    df = hparque.from_parquet(file_name)\n",
    "    print(\"\\n\")\n",
    "    print(\"DF from single parquet file\")\n",
    "    print(df)\n",
    "    _LOG.info(\"DF Unit after reading from parquet file: %s\", df.index.unit)\n",
    "    if clean_up:\n",
    "        hio.delete_file(file_name)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9246bf7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T16:42:11.306829Z",
     "start_time": "2024-02-29T16:42:11.278233Z"
    }
   },
   "outputs": [],
   "source": [
    "# Columns to partition on.\n",
    "partition_columns = [\"year\", \"n_legs\"]\n",
    "# Testing on different time units.\n",
    "test_units = [\"ms\", \"us\", \"ns\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1727174",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The upcoming 3 sections shows the working of parquet functions with different condition described in each section.\n",
    "The behavior is different based on if we are writing/reading a partitioned parquet files from root dir or just a single parquet file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856050f0",
   "metadata": {},
   "source": [
    "# Current behavior\n",
    "\n",
    "\n",
    "This includes what we have in the current master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec29d557",
   "metadata": {},
   "source": [
    "## `hparque.to_partitioned_parquet()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8731532",
   "metadata": {},
   "source": [
    "amp/helpers/hparquet.py:885\n",
    "```python\n",
    "        pq.write_to_dataset(\n",
    "            table,\n",
    "            dst_dir,\n",
    "            partition_cols=partition_columns,\n",
    "            # partition_filename_cb=partition_filename,\n",
    "            filesystem=filesystem,\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90ab623",
   "metadata": {},
   "source": [
    "## `hparque.to_parquet()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e0d15e",
   "metadata": {},
   "source": [
    "amp/helpers/hparquet.py:266\n",
    "\n",
    "The dictionary introduced by GP\n",
    "\n",
    "```python\n",
    "        # This is needed to handle:\n",
    "        # ```\n",
    "        # pyarrow.lib.ArrowInvalid: Casting from timestamp[ns, tz=America/New_York]\n",
    "        #   to timestamp[us] would lose data: 1663595160000000030\n",
    "        # ```\n",
    "        parquet_args = {\n",
    "            \"coerce_timestamps\": \"us\",\n",
    "            \"allow_truncated_timestamps\": True,\n",
    "        }\n",
    "        pq.write_table(table, file_name, filesystem=filesystem, **parquet_args)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71229fe",
   "metadata": {},
   "source": [
    "## `hparque.from_parquet()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8bcc40",
   "metadata": {},
   "source": [
    "amp/helpers/hparquet.py:172\n",
    "\n",
    "The hacks we applied in the version 14 upgrade\n",
    "\n",
    "```python\n",
    "            # Convert timestamp columns to `ns` resolution to keep the old\n",
    "            # behaviour with pyarrow=10.0.0 as opposed to pyarrow>=14.0.0\n",
    "            # which preserves the returned resolution.\n",
    "            # See CmTask7097 for details. https://github.com/cryptokaizen/cmamp/issues/7097\n",
    "            df = table.to_pandas(coerce_temporal_nanoseconds=True)\n",
    "            # Convert timestamp indices to `ns` resolution to keep the old\n",
    "            # behaviour with pyarrow=10.0.0 as opposed to pyarrow>=14.0.0\n",
    "            # which preserves the returned resolution.\n",
    "            # See CmTask7097 for details. https://github.com/cryptokaizen/cmamp/issues/7097\n",
    "            if isinstance(df.index, pd.DatetimeIndex):\n",
    "                df.index = df.index.as_unit(\"ns\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b4b00f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T16:49:27.390236Z",
     "start_time": "2024-02-29T16:49:27.299180Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  \n",
      "################################################################################\n",
      "Testing partition write and read\n",
      "################################################################################\n",
      "INFO  \n",
      "################################################################################\n",
      "Unit: ms\n",
      "################################################################################\n",
      "INFO  Initial DF unit: ns\n",
      "INFO  Converting DF unit from ns to: ms\n",
      "INFO  DF Unit before writing to parquet files: ms\n",
      "INFO  pyarrow.Table\n",
      "animal: string\n",
      "__index_level_0__: timestamp[ms, tz=America/New_York]\n",
      "year: int32\n",
      "n_legs: int32\n",
      "----\n",
      "animal: [[\"Centipede\"],[\"Flamingo\"],...,[\"Horse\"],[\"Brittle stars\"]]\n",
      "__index_level_0__: [[2022-01-01 05:00:00.123Z],[2022-01-01 05:00:00.123Z],...,[2022-01-01 05:00:00.123Z],[2022-01-01 05:00:00.123Z]]\n",
      "year: [[2001],[2001],...,[2003],[2003]]\n",
      "n_legs: [[100],[2],...,[4],[5]]\n",
      "\n",
      "\n",
      "DF from parquet files\n",
      "                                         animal  year  n_legs\n",
      "2022-01-01 00:00:00.123000-05:00      Centipede  2001     100\n",
      "2022-01-01 00:00:00.123000-05:00       Flamingo  2001       2\n",
      "2022-01-01 00:00:00.123000-05:00            Dog  2001       4\n",
      "2022-01-01 00:00:00.123000-05:00         Parrot  2002       2\n",
      "2022-01-01 00:00:00.123000-05:00          Horse  2003       4\n",
      "2022-01-01 00:00:00.123000-05:00  Brittle stars  2003       5\n",
      "INFO  DF Unit after reading from parquet files: ns\n",
      "\n",
      "\n",
      "INFO  \n",
      "################################################################################\n",
      "Unit: us\n",
      "################################################################################\n",
      "INFO  Initial DF unit: ns\n",
      "INFO  Converting DF unit from ns to: us\n",
      "INFO  DF Unit before writing to parquet files: us\n",
      "INFO  pyarrow.Table\n",
      "animal: string\n",
      "__index_level_0__: timestamp[us, tz=America/New_York]\n",
      "year: int32\n",
      "n_legs: int32\n",
      "----\n",
      "animal: [[\"Centipede\"],[\"Flamingo\"],...,[\"Horse\"],[\"Brittle stars\"]]\n",
      "__index_level_0__: [[2022-01-01 05:00:00.123456Z],[2022-01-01 05:00:00.123456Z],...,[2022-01-01 05:00:00.123456Z],[2022-01-01 05:00:00.123456Z]]\n",
      "year: [[2001],[2001],...,[2003],[2003]]\n",
      "n_legs: [[100],[2],...,[4],[5]]\n",
      "\n",
      "\n",
      "DF from parquet files\n",
      "                                         animal  year  n_legs\n",
      "2022-01-01 00:00:00.123456-05:00      Centipede  2001     100\n",
      "2022-01-01 00:00:00.123456-05:00       Flamingo  2001       2\n",
      "2022-01-01 00:00:00.123456-05:00            Dog  2001       4\n",
      "2022-01-01 00:00:00.123456-05:00         Parrot  2002       2\n",
      "2022-01-01 00:00:00.123456-05:00          Horse  2003       4\n",
      "2022-01-01 00:00:00.123456-05:00  Brittle stars  2003       5\n",
      "INFO  DF Unit after reading from parquet files: ns\n",
      "\n",
      "\n",
      "INFO  \n",
      "################################################################################\n",
      "Unit: ns\n",
      "################################################################################\n",
      "INFO  Initial DF unit: ns\n",
      "INFO  Converting DF unit from ns to: ns\n",
      "INFO  DF Unit before writing to parquet files: ns\n",
      "INFO  pyarrow.Table\n",
      "animal: string\n",
      "__index_level_0__: timestamp[ns, tz=America/New_York]\n",
      "year: int32\n",
      "n_legs: int32\n",
      "----\n",
      "animal: [[\"Centipede\"],[\"Flamingo\"],...,[\"Horse\"],[\"Brittle stars\"]]\n",
      "__index_level_0__: [[2022-01-01 05:00:00.123456000Z],[2022-01-01 05:00:00.123456000Z],...,[2022-01-01 05:00:00.123456000Z],[2022-01-01 05:00:00.123456000Z]]\n",
      "year: [[2001],[2001],...,[2003],[2003]]\n",
      "n_legs: [[100],[2],...,[4],[5]]\n",
      "\n",
      "\n",
      "DF from parquet files\n",
      "                                         animal  year  n_legs\n",
      "2022-01-01 00:00:00.123456-05:00      Centipede  2001     100\n",
      "2022-01-01 00:00:00.123456-05:00       Flamingo  2001       2\n",
      "2022-01-01 00:00:00.123456-05:00            Dog  2001       4\n",
      "2022-01-01 00:00:00.123456-05:00         Parrot  2002       2\n",
      "2022-01-01 00:00:00.123456-05:00          Horse  2003       4\n",
      "2022-01-01 00:00:00.123456-05:00  Brittle stars  2003       5\n",
      "INFO  DF Unit after reading from parquet files: ns\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/amp/helpers/hparquet.py:157: FutureWarning: Passing 'use_legacy_dataset' is deprecated as of pyarrow 15.0.0 and will be removed in a future version.\n",
      "  dataset = pq.ParquetDataset(\n",
      "/app/amp/helpers/hparquet.py:157: FutureWarning: Passing 'use_legacy_dataset' is deprecated as of pyarrow 15.0.0 and will be removed in a future version.\n",
      "  dataset = pq.ParquetDataset(\n",
      "/app/amp/helpers/hparquet.py:157: FutureWarning: Passing 'use_legacy_dataset' is deprecated as of pyarrow 15.0.0 and will be removed in a future version.\n",
      "  dataset = pq.ParquetDataset(\n"
     ]
    }
   ],
   "source": [
    "dst_dir = \"tmp.pyarrow_current\"\n",
    "_LOG.info(\"\\n\" + hprint.frame(\"Testing partition write and read\"))\n",
    "for unit in test_units:\n",
    "    _LOG.info(\"\\n\" + hprint.frame(f\"Unit: {unit}\"))\n",
    "    # The case where DF is partitioned in multiple PQ files.\n",
    "    # Under the hood we are calling `to_partioned_parquet` function\n",
    "    # as we use `partition_columns`.\n",
    "    #\n",
    "    # While writing to parquet, the unit is preserved.\n",
    "    # While reading from the parquet, the unit is always `ns`.\n",
    "    test_write_and_read_partition_parquet_with_unit(\n",
    "        initial_df, partition_columns, dst_dir, unit, clean_up=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6ebf781b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T17:06:06.293507Z",
     "start_time": "2024-02-29T17:06:06.212835Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  \n",
      "################################################################################\n",
      "Testing file write and read\n",
      "################################################################################\n",
      "INFO  \n",
      "################################################################################\n",
      "Unit: ms\n",
      "################################################################################\n",
      "INFO  Initial DF unit: ns\n",
      "INFO  Converting DF unit from ns to: ms\n",
      "INFO  Unit before writing to single parquet file: ms\n",
      "INFO  pyarrow.Table\n",
      "n_legs: int64\n",
      "animal: string\n",
      "year: int64\n",
      "__index_level_0__: timestamp[us, tz=America/New_York]\n",
      "----\n",
      "n_legs: [[2,2,4,4,5,100]]\n",
      "animal: [[\"Flamingo\",\"Parrot\",\"Dog\",\"Horse\",\"Brittle stars\",\"Centipede\"]]\n",
      "year: [[2001,2002,2001,2003,2003,2001]]\n",
      "__index_level_0__: [[2022-01-01 05:00:00.123000Z,2022-01-01 05:00:00.123000Z,2022-01-01 05:00:00.123000Z,2022-01-01 05:00:00.123000Z,2022-01-01 05:00:00.123000Z,2022-01-01 05:00:00.123000Z]]\n",
      "\n",
      "\n",
      "DF from single parquet file\n",
      "                                  n_legs         animal  year\n",
      "2022-01-01 00:00:00.123000-05:00       2       Flamingo  2001\n",
      "2022-01-01 00:00:00.123000-05:00       2         Parrot  2002\n",
      "2022-01-01 00:00:00.123000-05:00       4            Dog  2001\n",
      "2022-01-01 00:00:00.123000-05:00       4          Horse  2003\n",
      "2022-01-01 00:00:00.123000-05:00       5  Brittle stars  2003\n",
      "2022-01-01 00:00:00.123000-05:00     100      Centipede  2001\n",
      "INFO  DF Unit after reading from parquet file: us\n",
      "\n",
      "\n",
      "INFO  \n",
      "################################################################################\n",
      "Unit: us\n",
      "################################################################################\n",
      "INFO  Initial DF unit: ns\n",
      "INFO  Converting DF unit from ns to: us\n",
      "INFO  Unit before writing to single parquet file: us\n",
      "INFO  pyarrow.Table\n",
      "n_legs: int64\n",
      "animal: string\n",
      "year: int64\n",
      "__index_level_0__: timestamp[us, tz=America/New_York]\n",
      "----\n",
      "n_legs: [[2,2,4,4,5,100]]\n",
      "animal: [[\"Flamingo\",\"Parrot\",\"Dog\",\"Horse\",\"Brittle stars\",\"Centipede\"]]\n",
      "year: [[2001,2002,2001,2003,2003,2001]]\n",
      "__index_level_0__: [[2022-01-01 05:00:00.123456Z,2022-01-01 05:00:00.123456Z,2022-01-01 05:00:00.123456Z,2022-01-01 05:00:00.123456Z,2022-01-01 05:00:00.123456Z,2022-01-01 05:00:00.123456Z]]\n",
      "\n",
      "\n",
      "DF from single parquet file\n",
      "                                  n_legs         animal  year\n",
      "2022-01-01 00:00:00.123456-05:00       2       Flamingo  2001\n",
      "2022-01-01 00:00:00.123456-05:00       2         Parrot  2002\n",
      "2022-01-01 00:00:00.123456-05:00       4            Dog  2001\n",
      "2022-01-01 00:00:00.123456-05:00       4          Horse  2003\n",
      "2022-01-01 00:00:00.123456-05:00       5  Brittle stars  2003\n",
      "2022-01-01 00:00:00.123456-05:00     100      Centipede  2001\n",
      "INFO  DF Unit after reading from parquet file: us\n",
      "\n",
      "\n",
      "INFO  \n",
      "################################################################################\n",
      "Unit: ns\n",
      "################################################################################\n",
      "INFO  Initial DF unit: ns\n",
      "INFO  Converting DF unit from ns to: ns\n",
      "INFO  Unit before writing to single parquet file: ns\n",
      "INFO  pyarrow.Table\n",
      "n_legs: int64\n",
      "animal: string\n",
      "year: int64\n",
      "__index_level_0__: timestamp[us, tz=America/New_York]\n",
      "----\n",
      "n_legs: [[2,2,4,4,5,100]]\n",
      "animal: [[\"Flamingo\",\"Parrot\",\"Dog\",\"Horse\",\"Brittle stars\",\"Centipede\"]]\n",
      "year: [[2001,2002,2001,2003,2003,2001]]\n",
      "__index_level_0__: [[2022-01-01 05:00:00.123456Z,2022-01-01 05:00:00.123456Z,2022-01-01 05:00:00.123456Z,2022-01-01 05:00:00.123456Z,2022-01-01 05:00:00.123456Z,2022-01-01 05:00:00.123456Z]]\n",
      "\n",
      "\n",
      "DF from single parquet file\n",
      "                                  n_legs         animal  year\n",
      "2022-01-01 00:00:00.123456-05:00       2       Flamingo  2001\n",
      "2022-01-01 00:00:00.123456-05:00       2         Parrot  2002\n",
      "2022-01-01 00:00:00.123456-05:00       4            Dog  2001\n",
      "2022-01-01 00:00:00.123456-05:00       4          Horse  2003\n",
      "2022-01-01 00:00:00.123456-05:00       5  Brittle stars  2003\n",
      "2022-01-01 00:00:00.123456-05:00     100      Centipede  2001\n",
      "INFO  DF Unit after reading from parquet file: us\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/amp/helpers/hparquet.py:157: FutureWarning: Passing 'use_legacy_dataset' is deprecated as of pyarrow 15.0.0 and will be removed in a future version.\n",
      "  dataset = pq.ParquetDataset(\n",
      "/app/amp/helpers/hparquet.py:157: FutureWarning: Passing 'use_legacy_dataset' is deprecated as of pyarrow 15.0.0 and will be removed in a future version.\n",
      "  dataset = pq.ParquetDataset(\n",
      "/app/amp/helpers/hparquet.py:157: FutureWarning: Passing 'use_legacy_dataset' is deprecated as of pyarrow 15.0.0 and will be removed in a future version.\n",
      "  dataset = pq.ParquetDataset(\n"
     ]
    }
   ],
   "source": [
    "file_name = \"tmp_current.parquet\"\n",
    "_LOG.info(\"\\n\" + hprint.frame(\"Testing file write and read\"))\n",
    "for unit in test_units:\n",
    "    _LOG.info(\"\\n\" + hprint.frame(f\"Unit: {unit}\"))\n",
    "    # The case where a Df is converted to single PQ file without any\n",
    "    # partition. Under the hood, we call `to_parquet` function which has\n",
    "    # GP's dictionary.\n",
    "    #\n",
    "    # While writing to parquet, the unit is always `us` because of GP's  dictionary.\n",
    "    # While reading from parquet, the unit is always ns.\n",
    "    test_write_and_read_parquet_file_with_unit(\n",
    "        initial_df, file_name, unit, clean_up=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1956fa",
   "metadata": {},
   "source": [
    "\n",
    "# Remove hacks from the `hparque.from_parquet()`\n",
    "\n",
    "This includes removing ns v/s us hacks but keeping the GP's dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457e9a76",
   "metadata": {},
   "source": [
    "## `hparque.to_partitioned_parquet()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c641d6",
   "metadata": {},
   "source": [
    "amp/helpers/hparquet.py:885\n",
    "```python\n",
    "        pq.write_to_dataset(\n",
    "            table,\n",
    "            dst_dir,\n",
    "            partition_cols=partition_columns,\n",
    "            # partition_filename_cb=partition_filename,\n",
    "            filesystem=filesystem,\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9872d6c0",
   "metadata": {},
   "source": [
    "## `hparque.to_parquet()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681e06a6",
   "metadata": {},
   "source": [
    "amp/helpers/hparquet.py:266\n",
    "```python\n",
    "        # This is needed to handle:\n",
    "        # ```\n",
    "        # pyarrow.lib.ArrowInvalid: Casting from timestamp[ns, tz=America/New_York]\n",
    "        #   to timestamp[us] would lose data: 1663595160000000030\n",
    "        # ```\n",
    "        parquet_args = {\n",
    "            \"coerce_timestamps\": \"us\",\n",
    "            \"allow_truncated_timestamps\": True,\n",
    "        }\n",
    "        pq.write_table(table, file_name, filesystem=filesystem, **parquet_args)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6881bb5a",
   "metadata": {},
   "source": [
    "## `hparque.from_parquet()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b144ff44",
   "metadata": {},
   "source": [
    "amp/helpers/hparquet.py:172\n",
    "```python\n",
    "            table = dataset.read_pandas(columns=columns)\n",
    "            # Convert timestamp columns to `ns` resolution to keep the old\n",
    "            # behaviour with pyarrow=10.0.0 as opposed to pyarrow>=14.0.0\n",
    "            # which preserves the returned resolution.\n",
    "            # See CmTask7097 for details. https://github.com/cryptokaizen/cmamp/issues/7097\n",
    "            # df = table.to_pandas(coerce_temporal_nanoseconds=True)\n",
    "            df = table.to_pandas()\n",
    "            # Convert timestamp indices to `ns` resolution to keep the old\n",
    "            # behaviour with pyarrow=10.0.0 as opposed to pyarrow>=14.0.0\n",
    "            # which preserves the returned resolution.\n",
    "            # See CmTask7097 for details. https://github.com/cryptokaizen/cmamp/issues/7097\n",
    "            # if isinstance(df.index, pd.DatetimeIndex):\n",
    "                # df.index = df.index.as_unit(\"ns\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c4a94f08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T17:04:42.740562Z",
     "start_time": "2024-02-29T17:04:42.653933Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  \n",
      "################################################################################\n",
      "Testing partition write and read\n",
      "################################################################################\n",
      "INFO  \n",
      "################################################################################\n",
      "Unit: ms\n",
      "################################################################################\n",
      "INFO  Initial DF unit: ns\n",
      "INFO  Converting DF unit from ns to: ms\n",
      "INFO  DF Unit before writing to parquet files: ms\n",
      "INFO  pyarrow.Table\n",
      "animal: string\n",
      "__index_level_0__: timestamp[ms, tz=America/New_York]\n",
      "year: int32\n",
      "n_legs: int32\n",
      "----\n",
      "animal: [[\"Centipede\"],[\"Flamingo\"],...,[\"Horse\"],[\"Brittle stars\"]]\n",
      "__index_level_0__: [[2022-01-01 05:00:00.123Z],[2022-01-01 05:00:00.123Z],...,[2022-01-01 05:00:00.123Z],[2022-01-01 05:00:00.123Z]]\n",
      "year: [[2001],[2001],...,[2003],[2003]]\n",
      "n_legs: [[100],[2],...,[4],[5]]\n",
      "\n",
      "\n",
      "DF from parquet files\n",
      "                                         animal  year  n_legs\n",
      "2022-01-01 00:00:00.123000-05:00      Centipede  2001     100\n",
      "2022-01-01 00:00:00.123000-05:00       Flamingo  2001       2\n",
      "2022-01-01 00:00:00.123000-05:00            Dog  2001       4\n",
      "2022-01-01 00:00:00.123000-05:00         Parrot  2002       2\n",
      "2022-01-01 00:00:00.123000-05:00          Horse  2003       4\n",
      "2022-01-01 00:00:00.123000-05:00  Brittle stars  2003       5\n",
      "INFO  DF Unit after reading from parquet files: ms\n",
      "\n",
      "\n",
      "INFO  \n",
      "################################################################################\n",
      "Unit: us\n",
      "################################################################################\n",
      "INFO  Initial DF unit: ns\n",
      "INFO  Converting DF unit from ns to: us\n",
      "INFO  DF Unit before writing to parquet files: us\n",
      "INFO  pyarrow.Table\n",
      "animal: string\n",
      "__index_level_0__: timestamp[us, tz=America/New_York]\n",
      "year: int32\n",
      "n_legs: int32\n",
      "----\n",
      "animal: [[\"Centipede\"],[\"Flamingo\"],...,[\"Horse\"],[\"Brittle stars\"]]\n",
      "__index_level_0__: [[2022-01-01 05:00:00.123456Z],[2022-01-01 05:00:00.123456Z],...,[2022-01-01 05:00:00.123456Z],[2022-01-01 05:00:00.123456Z]]\n",
      "year: [[2001],[2001],...,[2003],[2003]]\n",
      "n_legs: [[100],[2],...,[4],[5]]\n",
      "\n",
      "\n",
      "DF from parquet files\n",
      "                                         animal  year  n_legs\n",
      "2022-01-01 00:00:00.123456-05:00      Centipede  2001     100\n",
      "2022-01-01 00:00:00.123456-05:00       Flamingo  2001       2\n",
      "2022-01-01 00:00:00.123456-05:00            Dog  2001       4\n",
      "2022-01-01 00:00:00.123456-05:00         Parrot  2002       2\n",
      "2022-01-01 00:00:00.123456-05:00          Horse  2003       4\n",
      "2022-01-01 00:00:00.123456-05:00  Brittle stars  2003       5\n",
      "INFO  DF Unit after reading from parquet files: us\n",
      "\n",
      "\n",
      "INFO  \n",
      "################################################################################\n",
      "Unit: ns\n",
      "################################################################################\n",
      "INFO  Initial DF unit: ns\n",
      "INFO  Converting DF unit from ns to: ns\n",
      "INFO  DF Unit before writing to parquet files: ns\n",
      "INFO  pyarrow.Table\n",
      "animal: string\n",
      "__index_level_0__: timestamp[ns, tz=America/New_York]\n",
      "year: int32\n",
      "n_legs: int32\n",
      "----\n",
      "animal: [[\"Centipede\"],[\"Flamingo\"],...,[\"Horse\"],[\"Brittle stars\"]]\n",
      "__index_level_0__: [[2022-01-01 05:00:00.123456000Z],[2022-01-01 05:00:00.123456000Z],...,[2022-01-01 05:00:00.123456000Z],[2022-01-01 05:00:00.123456000Z]]\n",
      "year: [[2001],[2001],...,[2003],[2003]]\n",
      "n_legs: [[100],[2],...,[4],[5]]\n",
      "\n",
      "\n",
      "DF from parquet files\n",
      "                                         animal  year  n_legs\n",
      "2022-01-01 00:00:00.123456-05:00      Centipede  2001     100\n",
      "2022-01-01 00:00:00.123456-05:00       Flamingo  2001       2\n",
      "2022-01-01 00:00:00.123456-05:00            Dog  2001       4\n",
      "2022-01-01 00:00:00.123456-05:00         Parrot  2002       2\n",
      "2022-01-01 00:00:00.123456-05:00          Horse  2003       4\n",
      "2022-01-01 00:00:00.123456-05:00  Brittle stars  2003       5\n",
      "INFO  DF Unit after reading from parquet files: ns\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/amp/helpers/hparquet.py:157: FutureWarning: Passing 'use_legacy_dataset' is deprecated as of pyarrow 15.0.0 and will be removed in a future version.\n",
      "  dataset = pq.ParquetDataset(\n",
      "/app/amp/helpers/hparquet.py:157: FutureWarning: Passing 'use_legacy_dataset' is deprecated as of pyarrow 15.0.0 and will be removed in a future version.\n",
      "  dataset = pq.ParquetDataset(\n",
      "/app/amp/helpers/hparquet.py:157: FutureWarning: Passing 'use_legacy_dataset' is deprecated as of pyarrow 15.0.0 and will be removed in a future version.\n",
      "  dataset = pq.ParquetDataset(\n"
     ]
    }
   ],
   "source": [
    "dst_dir = \"tmp.pyarrow_current\"\n",
    "_LOG.info(\"\\n\" + hprint.frame(\"Testing partition write and read\"))\n",
    "for unit in test_units:\n",
    "    _LOG.info(\"\\n\" + hprint.frame(f\"Unit: {unit}\"))\n",
    "    # The case where DF is partitioned in multiple PQ files.\n",
    "    # Under the hood we are calling `to_partioned_parquet` function\n",
    "    # as we use `partition_columns`.\n",
    "    #\n",
    "    # While writing to parquet, the unit is preserved.\n",
    "    # While reading from the parquet, the unit is preserved.\n",
    "    test_write_and_read_partition_parquet_with_unit(\n",
    "        initial_df, partition_columns, dst_dir, unit, clean_up=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "92779742",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T17:04:56.382132Z",
     "start_time": "2024-02-29T17:04:56.297231Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  \n",
      "################################################################################\n",
      "Testing file write and read\n",
      "################################################################################\n",
      "INFO  \n",
      "################################################################################\n",
      "Unit: ms\n",
      "################################################################################\n",
      "INFO  Initial DF unit: ns\n",
      "INFO  Converting DF unit from ns to: ms\n",
      "INFO  Unit before writing to single parquet file: ms\n",
      "INFO  pyarrow.Table\n",
      "n_legs: int64\n",
      "animal: string\n",
      "year: int64\n",
      "__index_level_0__: timestamp[us, tz=America/New_York]\n",
      "----\n",
      "n_legs: [[2,2,4,4,5,100]]\n",
      "animal: [[\"Flamingo\",\"Parrot\",\"Dog\",\"Horse\",\"Brittle stars\",\"Centipede\"]]\n",
      "year: [[2001,2002,2001,2003,2003,2001]]\n",
      "__index_level_0__: [[2022-01-01 05:00:00.123000Z,2022-01-01 05:00:00.123000Z,2022-01-01 05:00:00.123000Z,2022-01-01 05:00:00.123000Z,2022-01-01 05:00:00.123000Z,2022-01-01 05:00:00.123000Z]]\n",
      "\n",
      "\n",
      "DF from single parquet file\n",
      "                                  n_legs         animal  year\n",
      "2022-01-01 00:00:00.123000-05:00       2       Flamingo  2001\n",
      "2022-01-01 00:00:00.123000-05:00       2         Parrot  2002\n",
      "2022-01-01 00:00:00.123000-05:00       4            Dog  2001\n",
      "2022-01-01 00:00:00.123000-05:00       4          Horse  2003\n",
      "2022-01-01 00:00:00.123000-05:00       5  Brittle stars  2003\n",
      "2022-01-01 00:00:00.123000-05:00     100      Centipede  2001\n",
      "INFO  DF Unit after reading from parquet file: us\n",
      "\n",
      "\n",
      "INFO  \n",
      "################################################################################\n",
      "Unit: us\n",
      "################################################################################\n",
      "INFO  Initial DF unit: ns\n",
      "INFO  Converting DF unit from ns to: us\n",
      "INFO  Unit before writing to single parquet file: us\n",
      "INFO  pyarrow.Table\n",
      "n_legs: int64\n",
      "animal: string\n",
      "year: int64\n",
      "__index_level_0__: timestamp[us, tz=America/New_York]\n",
      "----\n",
      "n_legs: [[2,2,4,4,5,100]]\n",
      "animal: [[\"Flamingo\",\"Parrot\",\"Dog\",\"Horse\",\"Brittle stars\",\"Centipede\"]]\n",
      "year: [[2001,2002,2001,2003,2003,2001]]\n",
      "__index_level_0__: [[2022-01-01 05:00:00.123456Z,2022-01-01 05:00:00.123456Z,2022-01-01 05:00:00.123456Z,2022-01-01 05:00:00.123456Z,2022-01-01 05:00:00.123456Z,2022-01-01 05:00:00.123456Z]]\n",
      "\n",
      "\n",
      "DF from single parquet file\n",
      "                                  n_legs         animal  year\n",
      "2022-01-01 00:00:00.123456-05:00       2       Flamingo  2001\n",
      "2022-01-01 00:00:00.123456-05:00       2         Parrot  2002\n",
      "2022-01-01 00:00:00.123456-05:00       4            Dog  2001\n",
      "2022-01-01 00:00:00.123456-05:00       4          Horse  2003\n",
      "2022-01-01 00:00:00.123456-05:00       5  Brittle stars  2003\n",
      "2022-01-01 00:00:00.123456-05:00     100      Centipede  2001\n",
      "INFO  DF Unit after reading from parquet file: us\n",
      "\n",
      "\n",
      "INFO  \n",
      "################################################################################\n",
      "Unit: ns\n",
      "################################################################################\n",
      "INFO  Initial DF unit: ns\n",
      "INFO  Converting DF unit from ns to: ns\n",
      "INFO  Unit before writing to single parquet file: ns\n",
      "INFO  pyarrow.Table\n",
      "n_legs: int64\n",
      "animal: string\n",
      "year: int64\n",
      "__index_level_0__: timestamp[us, tz=America/New_York]\n",
      "----\n",
      "n_legs: [[2,2,4,4,5,100]]\n",
      "animal: [[\"Flamingo\",\"Parrot\",\"Dog\",\"Horse\",\"Brittle stars\",\"Centipede\"]]\n",
      "year: [[2001,2002,2001,2003,2003,2001]]\n",
      "__index_level_0__: [[2022-01-01 05:00:00.123456Z,2022-01-01 05:00:00.123456Z,2022-01-01 05:00:00.123456Z,2022-01-01 05:00:00.123456Z,2022-01-01 05:00:00.123456Z,2022-01-01 05:00:00.123456Z]]\n",
      "\n",
      "\n",
      "DF from single parquet file\n",
      "                                  n_legs         animal  year\n",
      "2022-01-01 00:00:00.123456-05:00       2       Flamingo  2001\n",
      "2022-01-01 00:00:00.123456-05:00       2         Parrot  2002\n",
      "2022-01-01 00:00:00.123456-05:00       4            Dog  2001\n",
      "2022-01-01 00:00:00.123456-05:00       4          Horse  2003\n",
      "2022-01-01 00:00:00.123456-05:00       5  Brittle stars  2003\n",
      "2022-01-01 00:00:00.123456-05:00     100      Centipede  2001\n",
      "INFO  DF Unit after reading from parquet file: us\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/amp/helpers/hparquet.py:157: FutureWarning: Passing 'use_legacy_dataset' is deprecated as of pyarrow 15.0.0 and will be removed in a future version.\n",
      "  dataset = pq.ParquetDataset(\n",
      "/app/amp/helpers/hparquet.py:157: FutureWarning: Passing 'use_legacy_dataset' is deprecated as of pyarrow 15.0.0 and will be removed in a future version.\n",
      "  dataset = pq.ParquetDataset(\n",
      "/app/amp/helpers/hparquet.py:157: FutureWarning: Passing 'use_legacy_dataset' is deprecated as of pyarrow 15.0.0 and will be removed in a future version.\n",
      "  dataset = pq.ParquetDataset(\n"
     ]
    }
   ],
   "source": [
    "file_name = \"tmp_current.parquet\"\n",
    "_LOG.info(\"\\n\" + hprint.frame(\"Testing file write and read\"))\n",
    "for unit in test_units:\n",
    "    _LOG.info(\"\\n\" + hprint.frame(f\"Unit: {unit}\"))\n",
    "    # The case where a Df is converted to single PQ file without any\n",
    "    # partition. Under the hood, we call `to_parquet` function which has\n",
    "    # GP's dictionary.\n",
    "    #\n",
    "    # While writing to parquet, the unit is always `us` because of GP's  dictionary.\n",
    "    # While reading from parquet, the unit is preserved. In this case it will be `us` only.\n",
    "    test_write_and_read_parquet_file_with_unit(\n",
    "        initial_df, file_name, unit, clean_up=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9480379d",
   "metadata": {},
   "source": [
    "# Remove both hacks\n",
    "\n",
    "\n",
    "This includes removing GP's dictionary and ns v/s us hacks introduced in the verison 14 upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bf0e1f",
   "metadata": {},
   "source": [
    "## `hparque.to_partitioned_parquet()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787a8c1e",
   "metadata": {},
   "source": [
    "amp/helpers/hparquet.py:885\n",
    "```python\n",
    "        pq.write_to_dataset(\n",
    "            table,\n",
    "            dst_dir,\n",
    "            partition_cols=partition_columns,\n",
    "            # partition_filename_cb=partition_filename,\n",
    "            filesystem=filesystem,\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba25f095",
   "metadata": {},
   "source": [
    "## `hparque.to_parquet()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efe4796",
   "metadata": {},
   "source": [
    "amp/helpers/hparquet.py:266\n",
    "```python\n",
    "        table = pa.Table.from_pandas(df)\n",
    "        # This is needed to handle:\n",
    "        # ```\n",
    "        # pyarrow.lib.ArrowInvalid: Casting from timestamp[ns, tz=America/New_York]\n",
    "        #   to timestamp[us] would lose data: 1663595160000000030\n",
    "        # ```\n",
    "        # parquet_args = {\n",
    "        #     \"coerce_timestamps\": \"us\",\n",
    "        #     \"allow_truncated_timestamps\": True,\n",
    "        # }\n",
    "        pq.write_table(table, file_name, filesystem=filesystem)\n",
    "        # pq.write_table(table, file_name, filesystem=filesystem, **parquet_args)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963d5344",
   "metadata": {},
   "source": [
    "## `hparque.from_parquet()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d786185",
   "metadata": {},
   "source": [
    "amp/helpers/hparquet.py:172\n",
    "```python\n",
    "            table = dataset.read_pandas(columns=columns)\n",
    "            # Convert timestamp columns to `ns` resolution to keep the old\n",
    "            # behaviour with pyarrow=10.0.0 as opposed to pyarrow>=14.0.0\n",
    "            # which preserves the returned resolution.\n",
    "            # See CmTask7097 for details. https://github.com/cryptokaizen/cmamp/issues/7097\n",
    "            # df = table.to_pandas(coerce_temporal_nanoseconds=True)\n",
    "            df = table.to_pandas()\n",
    "            # Convert timestamp indices to `ns` resolution to keep the old\n",
    "            # behaviour with pyarrow=10.0.0 as opposed to pyarrow>=14.0.0\n",
    "            # which preserves the returned resolution.\n",
    "            # See CmTask7097 for details. https://github.com/cryptokaizen/cmamp/issues/7097\n",
    "            # if isinstance(df.index, pd.DatetimeIndex):\n",
    "                # df.index = df.index.as_unit(\"ns\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ac4a8ee9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T17:07:55.032807Z",
     "start_time": "2024-02-29T17:07:54.939948Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  \n",
      "################################################################################\n",
      "Testing partition write and read\n",
      "################################################################################\n",
      "INFO  \n",
      "################################################################################\n",
      "Unit: ms\n",
      "################################################################################\n",
      "INFO  Initial DF unit: ns\n",
      "INFO  Converting DF unit from ns to: ms\n",
      "INFO  DF Unit before writing to parquet files: ms\n",
      "INFO  pyarrow.Table\n",
      "animal: string\n",
      "__index_level_0__: timestamp[ms, tz=America/New_York]\n",
      "year: int32\n",
      "n_legs: int32\n",
      "----\n",
      "animal: [[\"Centipede\"],[\"Flamingo\"],...,[\"Horse\"],[\"Brittle stars\"]]\n",
      "__index_level_0__: [[2022-01-01 05:00:00.123Z],[2022-01-01 05:00:00.123Z],...,[2022-01-01 05:00:00.123Z],[2022-01-01 05:00:00.123Z]]\n",
      "year: [[2001],[2001],...,[2003],[2003]]\n",
      "n_legs: [[100],[2],...,[4],[5]]\n",
      "\n",
      "\n",
      "DF from parquet files\n",
      "                                         animal  year  n_legs\n",
      "2022-01-01 00:00:00.123000-05:00      Centipede  2001     100\n",
      "2022-01-01 00:00:00.123000-05:00       Flamingo  2001       2\n",
      "2022-01-01 00:00:00.123000-05:00            Dog  2001       4\n",
      "2022-01-01 00:00:00.123000-05:00         Parrot  2002       2\n",
      "2022-01-01 00:00:00.123000-05:00          Horse  2003       4\n",
      "2022-01-01 00:00:00.123000-05:00  Brittle stars  2003       5\n",
      "INFO  DF Unit after reading from parquet files: ms\n",
      "\n",
      "\n",
      "INFO  \n",
      "################################################################################\n",
      "Unit: us\n",
      "################################################################################\n",
      "INFO  Initial DF unit: ns\n",
      "INFO  Converting DF unit from ns to: us\n",
      "INFO  DF Unit before writing to parquet files: us\n",
      "INFO  pyarrow.Table\n",
      "animal: string\n",
      "__index_level_0__: timestamp[us, tz=America/New_York]\n",
      "year: int32\n",
      "n_legs: int32\n",
      "----\n",
      "animal: [[\"Centipede\"],[\"Flamingo\"],...,[\"Horse\"],[\"Brittle stars\"]]\n",
      "__index_level_0__: [[2022-01-01 05:00:00.123456Z],[2022-01-01 05:00:00.123456Z],...,[2022-01-01 05:00:00.123456Z],[2022-01-01 05:00:00.123456Z]]\n",
      "year: [[2001],[2001],...,[2003],[2003]]\n",
      "n_legs: [[100],[2],...,[4],[5]]\n",
      "\n",
      "\n",
      "DF from parquet files\n",
      "                                         animal  year  n_legs\n",
      "2022-01-01 00:00:00.123456-05:00      Centipede  2001     100\n",
      "2022-01-01 00:00:00.123456-05:00       Flamingo  2001       2\n",
      "2022-01-01 00:00:00.123456-05:00            Dog  2001       4\n",
      "2022-01-01 00:00:00.123456-05:00         Parrot  2002       2\n",
      "2022-01-01 00:00:00.123456-05:00          Horse  2003       4\n",
      "2022-01-01 00:00:00.123456-05:00  Brittle stars  2003       5\n",
      "INFO  DF Unit after reading from parquet files: us\n",
      "\n",
      "\n",
      "INFO  \n",
      "################################################################################\n",
      "Unit: ns\n",
      "################################################################################\n",
      "INFO  Initial DF unit: ns\n",
      "INFO  Converting DF unit from ns to: ns\n",
      "INFO  DF Unit before writing to parquet files: ns\n",
      "INFO  pyarrow.Table\n",
      "animal: string\n",
      "__index_level_0__: timestamp[ns, tz=America/New_York]\n",
      "year: int32\n",
      "n_legs: int32\n",
      "----\n",
      "animal: [[\"Centipede\"],[\"Flamingo\"],...,[\"Horse\"],[\"Brittle stars\"]]\n",
      "__index_level_0__: [[2022-01-01 05:00:00.123456000Z],[2022-01-01 05:00:00.123456000Z],...,[2022-01-01 05:00:00.123456000Z],[2022-01-01 05:00:00.123456000Z]]\n",
      "year: [[2001],[2001],...,[2003],[2003]]\n",
      "n_legs: [[100],[2],...,[4],[5]]\n",
      "\n",
      "\n",
      "DF from parquet files\n",
      "                                         animal  year  n_legs\n",
      "2022-01-01 00:00:00.123456-05:00      Centipede  2001     100\n",
      "2022-01-01 00:00:00.123456-05:00       Flamingo  2001       2\n",
      "2022-01-01 00:00:00.123456-05:00            Dog  2001       4\n",
      "2022-01-01 00:00:00.123456-05:00         Parrot  2002       2\n",
      "2022-01-01 00:00:00.123456-05:00          Horse  2003       4\n",
      "2022-01-01 00:00:00.123456-05:00  Brittle stars  2003       5\n",
      "INFO  DF Unit after reading from parquet files: ns\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/amp/helpers/hparquet.py:157: FutureWarning: Passing 'use_legacy_dataset' is deprecated as of pyarrow 15.0.0 and will be removed in a future version.\n",
      "  dataset = pq.ParquetDataset(\n",
      "/app/amp/helpers/hparquet.py:157: FutureWarning: Passing 'use_legacy_dataset' is deprecated as of pyarrow 15.0.0 and will be removed in a future version.\n",
      "  dataset = pq.ParquetDataset(\n",
      "/app/amp/helpers/hparquet.py:157: FutureWarning: Passing 'use_legacy_dataset' is deprecated as of pyarrow 15.0.0 and will be removed in a future version.\n",
      "  dataset = pq.ParquetDataset(\n"
     ]
    }
   ],
   "source": [
    "dst_dir = \"tmp.pyarrow_current\"\n",
    "_LOG.info(\"\\n\" + hprint.frame(\"Testing partition write and read\"))\n",
    "for unit in test_units:\n",
    "    _LOG.info(\"\\n\" + hprint.frame(f\"Unit: {unit}\"))\n",
    "    # The case where DF is partitioned in multiple PQ files.\n",
    "    # Under the hood we are calling `to_partioned_parquet` function\n",
    "    # as we use `partition_columns`.\n",
    "    #\n",
    "    # While writing to parquet, the unit is preserved.\n",
    "    # While reading from the parquet, the unit is preserved.\n",
    "    test_write_and_read_partition_parquet_with_unit(\n",
    "        initial_df, partition_columns, dst_dir, unit, clean_up=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ccab4ee5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T17:09:03.656419Z",
     "start_time": "2024-02-29T17:09:03.579541Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  \n",
      "################################################################################\n",
      "Testing file write and read\n",
      "################################################################################\n",
      "INFO  \n",
      "################################################################################\n",
      "Unit: ms\n",
      "################################################################################\n",
      "INFO  Initial DF unit: ns\n",
      "INFO  Converting DF unit from ns to: ms\n",
      "INFO  Unit before writing to single parquet file: ms\n",
      "INFO  pyarrow.Table\n",
      "n_legs: int64\n",
      "animal: string\n",
      "year: int64\n",
      "__index_level_0__: timestamp[ms, tz=America/New_York]\n",
      "----\n",
      "n_legs: [[2,2,4,4,5,100]]\n",
      "animal: [[\"Flamingo\",\"Parrot\",\"Dog\",\"Horse\",\"Brittle stars\",\"Centipede\"]]\n",
      "year: [[2001,2002,2001,2003,2003,2001]]\n",
      "__index_level_0__: [[2022-01-01 05:00:00.123Z,2022-01-01 05:00:00.123Z,2022-01-01 05:00:00.123Z,2022-01-01 05:00:00.123Z,2022-01-01 05:00:00.123Z,2022-01-01 05:00:00.123Z]]\n",
      "\n",
      "\n",
      "DF from single parquet file\n",
      "                                  n_legs         animal  year\n",
      "2022-01-01 00:00:00.123000-05:00       2       Flamingo  2001\n",
      "2022-01-01 00:00:00.123000-05:00       2         Parrot  2002\n",
      "2022-01-01 00:00:00.123000-05:00       4            Dog  2001\n",
      "2022-01-01 00:00:00.123000-05:00       4          Horse  2003\n",
      "2022-01-01 00:00:00.123000-05:00       5  Brittle stars  2003\n",
      "2022-01-01 00:00:00.123000-05:00     100      Centipede  2001\n",
      "INFO  DF Unit after reading from parquet file: ms\n",
      "\n",
      "\n",
      "INFO  \n",
      "################################################################################\n",
      "Unit: us\n",
      "################################################################################\n",
      "INFO  Initial DF unit: ns\n",
      "INFO  Converting DF unit from ns to: us\n",
      "INFO  Unit before writing to single parquet file: us\n",
      "INFO  pyarrow.Table\n",
      "n_legs: int64\n",
      "animal: string\n",
      "year: int64\n",
      "__index_level_0__: timestamp[us, tz=America/New_York]\n",
      "----\n",
      "n_legs: [[2,2,4,4,5,100]]\n",
      "animal: [[\"Flamingo\",\"Parrot\",\"Dog\",\"Horse\",\"Brittle stars\",\"Centipede\"]]\n",
      "year: [[2001,2002,2001,2003,2003,2001]]\n",
      "__index_level_0__: [[2022-01-01 05:00:00.123456Z,2022-01-01 05:00:00.123456Z,2022-01-01 05:00:00.123456Z,2022-01-01 05:00:00.123456Z,2022-01-01 05:00:00.123456Z,2022-01-01 05:00:00.123456Z]]\n",
      "\n",
      "\n",
      "DF from single parquet file\n",
      "                                  n_legs         animal  year\n",
      "2022-01-01 00:00:00.123456-05:00       2       Flamingo  2001\n",
      "2022-01-01 00:00:00.123456-05:00       2         Parrot  2002\n",
      "2022-01-01 00:00:00.123456-05:00       4            Dog  2001\n",
      "2022-01-01 00:00:00.123456-05:00       4          Horse  2003\n",
      "2022-01-01 00:00:00.123456-05:00       5  Brittle stars  2003\n",
      "2022-01-01 00:00:00.123456-05:00     100      Centipede  2001\n",
      "INFO  DF Unit after reading from parquet file: us\n",
      "\n",
      "\n",
      "INFO  \n",
      "################################################################################\n",
      "Unit: ns\n",
      "################################################################################\n",
      "INFO  Initial DF unit: ns\n",
      "INFO  Converting DF unit from ns to: ns\n",
      "INFO  Unit before writing to single parquet file: ns\n",
      "INFO  pyarrow.Table\n",
      "n_legs: int64\n",
      "animal: string\n",
      "year: int64\n",
      "__index_level_0__: timestamp[ns, tz=America/New_York]\n",
      "----\n",
      "n_legs: [[2,2,4,4,5,100]]\n",
      "animal: [[\"Flamingo\",\"Parrot\",\"Dog\",\"Horse\",\"Brittle stars\",\"Centipede\"]]\n",
      "year: [[2001,2002,2001,2003,2003,2001]]\n",
      "__index_level_0__: [[2022-01-01 05:00:00.123456000Z,2022-01-01 05:00:00.123456000Z,2022-01-01 05:00:00.123456000Z,2022-01-01 05:00:00.123456000Z,2022-01-01 05:00:00.123456000Z,2022-01-01 05:00:00.123456000Z]]\n",
      "\n",
      "\n",
      "DF from single parquet file\n",
      "                                  n_legs         animal  year\n",
      "2022-01-01 00:00:00.123456-05:00       2       Flamingo  2001\n",
      "2022-01-01 00:00:00.123456-05:00       2         Parrot  2002\n",
      "2022-01-01 00:00:00.123456-05:00       4            Dog  2001\n",
      "2022-01-01 00:00:00.123456-05:00       4          Horse  2003\n",
      "2022-01-01 00:00:00.123456-05:00       5  Brittle stars  2003\n",
      "2022-01-01 00:00:00.123456-05:00     100      Centipede  2001\n",
      "INFO  DF Unit after reading from parquet file: ns\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/amp/helpers/hparquet.py:157: FutureWarning: Passing 'use_legacy_dataset' is deprecated as of pyarrow 15.0.0 and will be removed in a future version.\n",
      "  dataset = pq.ParquetDataset(\n",
      "/app/amp/helpers/hparquet.py:157: FutureWarning: Passing 'use_legacy_dataset' is deprecated as of pyarrow 15.0.0 and will be removed in a future version.\n",
      "  dataset = pq.ParquetDataset(\n",
      "/app/amp/helpers/hparquet.py:157: FutureWarning: Passing 'use_legacy_dataset' is deprecated as of pyarrow 15.0.0 and will be removed in a future version.\n",
      "  dataset = pq.ParquetDataset(\n"
     ]
    }
   ],
   "source": [
    "file_name = \"tmp_current.parquet\"\n",
    "_LOG.info(\"\\n\" + hprint.frame(\"Testing file write and read\"))\n",
    "for unit in test_units:\n",
    "    _LOG.info(\"\\n\" + hprint.frame(f\"Unit: {unit}\"))\n",
    "    # The case where a Df is converted to single PQ file without any\n",
    "    # partition. Under the hood, we call `to_parquet` function which do not have\n",
    "    # GP's dictionary.\n",
    "    #\n",
    "    # While writing to parquet, the unit is preserved.\n",
    "    # While reading from the parquet, the unit is preserved.\n",
    "    test_write_and_read_parquet_file_with_unit(\n",
    "        initial_df, file_name, unit, clean_up=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
