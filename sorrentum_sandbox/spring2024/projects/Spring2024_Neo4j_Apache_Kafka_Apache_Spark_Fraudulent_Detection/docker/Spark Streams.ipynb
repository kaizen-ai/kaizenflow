{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109699d6-a50b-4c94-b60e-415ea8bb7f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, col, concat, lit\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.streaming import DataStreamReader\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .master(\"local\")\n",
    "         .appName(\"Transactions\")\n",
    "         .config('spark.executor.memory', '1g')\n",
    "         .config('spark.executor.cores', '1')\n",
    "         .config('spark.driver.memory','2g')\n",
    "         .getOrCreate()\n",
    "        )\n",
    "StreamReader = DataStreamReader(spark)\n",
    "\n",
    "URI_container = \"neo4j://neo4j:7687\"\n",
    "URI_local = \"bolt://localhost:7687\"\n",
    "URI = URI_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05f58d1-1faf-4323-ac5d-0e720ee0d974",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating property indexes to improve insert performance\n",
    "\n",
    "with GraphDatabase.driver(URI) as driver:\n",
    "    driver.execute_query(\"CREATE TEXT INDEX FOR (n:Person) ON n.ssn\")\n",
    "    driver.execute_query(\"CREATE TEXT INDEX FOR (n:Transaction) ON n.trans_num\")\n",
    "    driver.execute_query(\"CREATE TEXT INDEX FOR (n:Merchant) ON n.merchant\")\n",
    "    driver.execute_query(\"CREATE TEXT INDEX FOR (n:Location) ON n.city\")\n",
    "    driver.execute_query(\"CREATE TEXT INDEX FOR (n:Account) ON n.acct_num\")\n",
    "    driver.execute_query(\"CREATE TEXT INDEX FOR (n:CreditCard) on n.cc_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72dfd00-ec85-4185-9b06-87e1bac665f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_in = spark.read.csv(\"schema.csv\", sep=\"|\", header=True)\n",
    "schema = sample_in.schema\n",
    "headers = sample_in.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9402b1a4-4cbb-49f2-8bd3-f87b520b821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_fields = [\"ssn\", \"first\", \"last\", \"gender\", \"job\", \"dob\"]\n",
    "acct_fields = [\"acct_num\"]\n",
    "cc_fields = [\"cc_num\"]\n",
    "per_loc_fields = [\"street\",\"city\",\"state\",\"zip\",\"lat\",\"long\",\"city_pop\"]\n",
    "trans_fields = [\"trans_num\",\"trans_date\",\"trans_time\",\"amt\", \"trans_datetime\"]\n",
    "merch_fields = [\"category\",\"merchant\",\"merch_lat\",\"merch_long\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c62f6a-d1b9-4855-a128-fe1c795d066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Sparkov_Data_Generation/output/\"\n",
    "\n",
    "float_fields = [\"amt\",\"lat\",\"long\",\"merch_lat\",\"merch_long\"]\n",
    "date_fields = [\"trans_date\",\"dob\"]\n",
    "\n",
    "def cast_data(df, fields):\n",
    "    if \"trans_date\" in fields:\n",
    "        df = df.withColumn(\"trans_datetime\", concat(col(\"trans_date\"), lit(\" \"), col(\"trans_time\")))\n",
    "        df = df.withColumn(\"trans_datetime\",col(\"trans_datetime\").cast(TimestampType()))\n",
    "    if set(date_fields) & set(fields):\n",
    "        df = df.select([col(column).cast(\"date\") if column in date_fields else col(column) for column in fields])\n",
    "    if set(float_fields) & set(fields):\n",
    "        df = df.select([col(column).cast(\"double\") if column in float_fields else col(column) for column in fields])\n",
    "    \n",
    "    return df\n",
    "\n",
    "class StreamWriter(pyspark.sql.DataFrame):\n",
    "    def __init__(self, spark_read):\n",
    "        self.read_stream = spark_read\n",
    "\n",
    "    def insert_nodes(self, fields, checkpoint_path, label, key):\n",
    "        self.read_stream = cast_data(self.read_stream, fields)\n",
    "        self.read_stream.select(fields).filter(\"first != 'first'\").dropna(how=\"any\").writeStream \\\n",
    "          .format(\"org.neo4j.spark.DataSource\") \\\n",
    "          .option(\"url\", URI) \\\n",
    "          .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "          .option(\"labels\", label) \\\n",
    "          .option(\"node.keys\", key) \\\n",
    "          .option(\"SaveMode\", \"Overwrite\") \\\n",
    "          .start()\n",
    "\n",
    "    def insert_relationships(self, fields, checkpoint_path, relationship, labels_keys):\n",
    "        self.read_stream.select(fields).dropna(how=\"any\").writeStream \\\n",
    "          .format(\"org.neo4j.spark.DataSource\") \\\n",
    "          .option(\"relationship\", relationship) \\\n",
    "          .option(\"url\", URI) \\\n",
    "          .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "          .option(\"relationship.save.strategy\", \"keys\") \\\n",
    "          .option(\"relationship.source.labels\", labels_keys.get(\"s_label\")) \\\n",
    "          .option(\"relationship.source.save.mode\", \"Overwrite\") \\\n",
    "          .option(\"relationship.source.node.keys\", labels_keys.get(\"s_key\")) \\\n",
    "          .option(\"relationship.target.labels\", labels_keys.get(\"t_label\")) \\\n",
    "          .option(\"relationship.target.save.mode\", \"Overwrite\") \\\n",
    "          .option(\"relationship.target.node.keys\", labels_keys.get(\"t_key\")) \\\n",
    "          .start()\n",
    "\n",
    "# Reading the csv for the single read stream, which splits to multiple writer streams to control the node writes\n",
    "csv_reader = StreamReader.csv(path=path, sep=\"|\", schema=schema)\n",
    "\n",
    "per_stream = StreamWriter(csv_reader)\n",
    "tran_stream = StreamWriter(csv_reader)\n",
    "merchant_stream = StreamWriter(csv_reader)\n",
    "loc_stream = StreamWriter(csv_reader)\n",
    "acct_stream = StreamWriter(csv_reader)\n",
    "cc_stream = StreamWriter(csv_reader)\n",
    "\n",
    "sent_stream = StreamWriter(csv_reader)\n",
    "received_stream = StreamWriter(csv_reader)\n",
    "resides_stream = StreamWriter(csv_reader)\n",
    "used_stream = StreamWriter(csv_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb74d03b-e3df-43ec-9660-da4b0a36e35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_stream.insert_nodes(person_fields, \"/tmp/chpt1\", \":Person\", \"ssn\")\n",
    "tran_stream.insert_nodes(trans_fields, \"/tmp/chpt2\", \":Transaction\", \"trans_num\")\n",
    "merchant_stream.insert_nodes(merch_fields, \"/tmp/chpt3\", \":Merchant\", \"merchant\")\n",
    "loc_stream.insert_nodes(per_loc_fields, \"/tmp/chpt4\", \":Location\", \"city\")\n",
    "acct_stream.insert_nodes(acct_fields, \"/tmp/chpt5\", \":Account\", \"acct_num\")\n",
    "cc_stream.insert_nodes(cc_fields, \"/tmp/chpt6\", \":CreditCard\", \"cc_num\")\n",
    "\n",
    "\n",
    "used_stream.insert_relationships([\"ssn\", \"cc_num\"],\n",
    "                                 \"/tmp/chpt7\",\n",
    "                                 \"USED\",\n",
    "                                 {\n",
    "                                    \"s_label\": \":Person\",\n",
    "                                    \"s_key\": \"ssn\",\n",
    "                                    \"t_label\":\":CreditCard\",\n",
    "                                    \"t_key\": \"cc_num\"\n",
    "                                })\n",
    "sent_stream.insert_relationships([\"cc_num\",\"trans_num\"],\n",
    "                                \"/tmp/chpt8\",\n",
    "                                \"SENT\",\n",
    "                                {\n",
    "                                    \"s_label\": \":CreditCard\",\n",
    "                                    \"s_key\": \"cc_num\",\n",
    "                                    \"t_label\":\":Transaction\",\n",
    "                                    \"t_key\": \"trans_num\"\n",
    "                                })\n",
    "received_stream.insert_relationships([\"trans_num\", \"merchant\"],\n",
    "                                \"/tmp/chpt9\",\n",
    "                                \"RECEIVED\",\n",
    "                                {\n",
    "                                    \"s_label\": \":Transaction\",\n",
    "                                    \"s_key\": \"trans_num\",\n",
    "                                    \"t_label\":\":Merchant\",\n",
    "                                    \"t_key\": \"merchant\"\n",
    "                                })\n",
    "resides_stream.insert_relationships([\"ssn\",\"city\"],\n",
    "                                \"/tmp/chpt10\",\n",
    "                                \"RESIDES_IN\",\n",
    "                                {\n",
    "                                    \"s_label\": \":Person\",\n",
    "                                    \"s_key\": \"ssn\",\n",
    "                                    \"t_label\":\":Location\",\n",
    "                                    \"t_key\": \"city\"\n",
    "                                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ad7128-358c-4b5c-8da1-bfb6e43ed96b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
