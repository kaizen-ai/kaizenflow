# Adapted from https://github.com/resdevd/docker-compose-airflow/blob/main/docker-compose.yml
version: '3.9'

services:
  # Postgres
 
  postgres:
    # Use prebuilt image.
    image: postgres:14.0
    container_name: postgres_cont
    environment:
      # Pass env vars to control credentials.
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      # We reuse the Airflow DB also for client data.
      # TODO(gp): Ideally we should separate infra from application.
      - POSTGRES_DB=airflow
      - POSTGRES_PORT=5432
    volumes:
      # Map volume backing the DB to a local dir.
      - airflow-database-data:/var/lib/postgresql/data/
    ports:
      # Access PostgreSQL from outside the container on port 5532.
      - "5532:5432"
        
  # Mongo.

  mongo:
    # Use prebuilt image.
    image: mongo
    container_name: mongo_cont
    environment:
      # Pass env vars to control credentials.
      - MONGO_INITDB_ROOT_USERNAME=mongo
      - MONGO_INITDB_ROOT_PASSWORD=mongo
    volumes:
      - ./mongo_data:/data/db
    ports:
      # Access Mongo from outside the container on port 27017.
      - "27017:27017"

  # Airflow.

  airflow:
    # This is a container built locally and storing the current app.
    image: sorrentum/sorrentum:latest
    container_name: airflow_cont
    depends_on:
      - postgres
    env_file:
      - .env
    restart: always
    command: webserver
    ports:
      - "8091:8080"
    volumes:
      - ./airflow_data/dags:/opt/airflow/dags
      - ./airflow_data/plugins:/opt/airflow/plugins
      - ./airflow_data/log_volume:/opt/airflow/log
      - ../..:/cmamp
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@host.docker.internal:5532/airflow
    extra_hosts:
      - "host.docker.internal:host-gateway"

  airflow-scheduler:
    # Force to use a pre-built image, by commenting the previous line.
    # build: .
    image: sorrentum/sorrentum:latest
    container_name: airflow_scheduler_cont
    env_file:
      - .env
    restart: always
    command: scheduler
    depends_on:
      - postgres
    volumes:
      - ./airflow_data/dags:/opt/airflow/dags
      - ./airflow_data/plugins:/opt/airflow/plugins
      - ./airflow_data/log_volume:/opt/airflow/log
      - ../..:/cmamp
    environment:
      # TODO(Juraj): this is a quick hack to get through
      # `hserver._dassert_setup_consistency()`.
      - CK_IN_PROD_CMAMP_CONTAINER=True
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@host.docker.internal:5532/airflow
    extra_hosts:
      - "host.docker.internal:host-gateway"

  #airflow-worker1:
  #  build: .
  #  image: resdev-airflow:latest
  #  container_name: airflow_worker1_cont
  #  env_file:
  #    - .env
  #  restart: always
  #  command: celery worker
  #  depends_on:
  #    - postgres
  #  volumes:
  #    - ./airflow_data/dags:/opt/airflow/dags
  #    - ./airflow_data/plugins:/opt/airflow/plugins
  #    - ./airflow_data/log_volume:/opt/airflow/log

  #airflow-worker2:
  #  build: .
  #  image: resdev-airflow:latest
  #  container_name: airflow_worker2_cont
  #  env_file:
  #    - .env
  #  restart: always
  #  command: celery worker
  #  depends_on:
  #    - postgres
  #  volumes:
  #    - ./airflow_data/dags:/opt/airflow/dags
  #    - ./airflow_data/plugins:/opt/airflow/plugins
  #    - ./airflow_data/log_volume:/opt/airflow/log

  #airflow-worker3:
  #  build: .
  #  image: resdev-airflow:latest
  #  container_name: airflow_worker3_cont
  #  env_file:
  #    - .env
  #  restart: always
  #  depends_on:
  #    - postgres
  #  command: celery worker
  #  volumes:
  #    - ./airflow_data/dags:/opt/airflow/dags
  #    - ./airflow_data/plugins:/opt/airflow/plugins
  #    - ./airflow_data/log_volume:/opt/airflow/log

  #airflow-flower:
  #  build: .
  #  image: resdev-airflow:latest
  #  container_name: airflow_flower_cont
  #  depends_on:
  #    - postgres
  #  volumes:
  #    - ./airflow_data/dags:/opt/airflow/dags
  #    - ./airflow_data/plugins:/opt/airflow/plugins
  #    - ./airflow_data/log_volume:/opt/airflow/log
  #  env_file:
  #    - .env
  #  restart: always
  #  command: celery flower
  #  ports:
  #    - "5555:5555"

  # Redis.

  #redis:
  #  image: redis:latest
  #  container_name: redis_cont
  #  restart: always
  #  ports:
  #    - "6379:6379"

volumes:
  airflow-database-data:
  airflow-log-volume:
