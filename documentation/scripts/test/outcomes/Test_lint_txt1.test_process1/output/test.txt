* Gradient descent for logistic regression
- The typical implementations of gradient descent (basic or advanced) need two
  inputs:
  - The cost function $E_{in}(\vw)$ (to monitor convergence)
  - The gradient of the cost function
    $\frac{\partial E}{w_j} \text{ for all } j$ (to optimize)
- The cost function is:

  $$
  E_{in} = \frac{1}{N} \sum_i e(h(\vx_i), y_i)
  $$

- In case of general probabilistic model $h(\vx)$ in \{0, 1\}):

  $$
  E_{in}(\vw) = \frac{1}{N} \sum_i \big(
  -y_i \log(\Pr(h(\vx) = 1|\vx)) - (1 - y_i) \log(1 - \Pr(h(\vx)=1|\vx))
  \big)
  $$

- In case of logistic regression in \{+1, -1\}:

  $$
  E_{in}(\vw) = \frac{1}{N} \sum_i \log(1 + \exp(-y_i \vw^T \vx_i))
  $$

- It can be proven that the function $E_{in}(\vw)$ to minimize is convex in
  $\vw$ (sum of exponentials and flipped exponentials is convex and log is
  monotone)