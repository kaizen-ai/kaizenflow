<!--ts-->
   * [Definitions](#definitions)
         * [Passing Config across processes](#passing-config-across-processes)
      * [run_experiment.py](#run_experimentpy)
      * [run_notebook.py](#run_notebookpy)



<!--te-->

# Definitions

- An experiment in this context is simply code that is parametrized by `Config`s
  - The code contained in an experiment can be anything
  - In this current context, a typical experiment is running a `Dag` through a
    `DagRunner`, where a `Dag` is built from a `Config` and a `DagBuilder`

- There are two flows to run experiments:
  - `run_experiment.py`
  - `run_notebook.py`

- These flows:
  - Run experiments which are represented by a notebook or by a piece of Python
    code, respectively
  - Have similar options
  - Both have the experiment manager (i.e., `run_experiment.py` and
    `run_notebook.py`) and the actual experiments run in different processes, so
    that experiments can be parallelized and isolated from each other

### Passing `Config` across processes

- The experiment manager needs to pass the config in order to execute each
  experiment process

- The problem is that it's not always possible to serialize/deserialize a
  `Config`, e.g., a `Config` that contains functions and lambda is not
  pickle-able by `pickle`

- To work around this issue we pass various information among processes that
  allow to build the same `Config` multiple times in different processes
  - The needed information are:
    1. The name of a config builder function that can be executed through an
       `eval` statement
    2. The index of the specific `Config` in the list of available `Config`s to
       select and execute
    3. A destination dir representing the scratch space for the experiment
       artifacts

- This information is passed through the "ExperimentRunner" part of a `Config`
  - TODO(gp): Separate `meta` from `run_notebook` since we don't want this
    information to collide.

- We then keep the `Config` in sync between the experiment manager and the
  specific experiment runner by executing the same code to generate the
  `Config`s on both processes

- There are two interfaces to materialize `Config`s:
  - One on the command line side
  - One on the run_experiment, notebook side
    - The params to reconstruct the configs are passed through env vars or
      params of the script

## run_experiment.py

- `run_experiment.py` is the experiment manager that:
  - Parallelizes the experiments
  - Runs all the experiments
  - Saves config for each experiment
  - Implements the retry logic
  - Etc.

- `run_experiment_stub.py` is a wrapper that adapts the `run_experiment.py`
  protocol and:
  - Runs a single experiment
  - Has a similar "interface" as to the notebook flow from `run_notebook.py` to
    pass:
    - `config_builder`
    - `experiment_builder`
    - `config_index` of the Config to run among the ones generated by
      `config_builder`
    - `dst_dir` of the entire experiments (i.e., the one passed to `--dst_dir`),
      since the code infers the name of the dir for each experiment from the
      `config_idx`

- An example of running an experiment is:

  ```bash
  > run_experiment.py \
    --experiment_builder "core.dataflow_model.master_experiment.run_experiment" \
    --config_builder "dataflow_lm.RH1E.task89_config_builder.build_15min_ar_model_configs()" \
    --dst_dir experiment_results \
    --num_threads 2
  ```

- Note that `--experiment_builder` needs the name of a function, while
  `--config_builder` needs a function invocation
  - This is because `run_experiment` function has a fixed signature, while a
    config builder function can accept parameters

- This command line:
  - Runs the experiment in
    `core.dataflow_model.master_experiment.run_experiment`
  - The experiment is parametrized through the `Config`s generated by the
    function
    `dataflow_lm.RH1E.task89_config_builder.build_15min_ar_model_configs()`
  - Saves the results of each experiment into a separate dir under
    `experiment_results`
    - Each experiment dir contains a copy of the `Config` and any artifact
      generated by the experiment

- Run `run_experiment.py -h` to see the available options

## `run_notebook.py`

- It is a generic flow for running the same notebook parametrized through
  several `Config`s
- `run_notebook.py`
  - Injects the `Config` through env vars into the notebook
  - Parallelizes the execution of notebooks for different `Config`s using
    `joblib`
  - Takes care of book-keeping by saving the configs that are run by each
    notebook
  - Handles:
    - Retries (`--num_retries`)
    - Aborting on error (`--abort_on_error`)
    - Skipping running `Config` already run (`--incremental`)
    - Etc.

- Each notebook:
  - Calls `get_config_from_env()` to materialize the config
  - Runs top-to-bottom
  - Is saved as `ipynb` and published as HTML, if needed

- An invocation is like:

  ```bash
  > run_notebook.py \
    --notebook nlp/notebooks/NLP_RP_pipeline.ipynb \
    --config_builder "nlp.build_configs.build_Task1088_configs()" \
    --dst_dir notebook_results \
    --num_threads 2
  ```

- This command line:
  - Runs the notebook `nlp/notebooks/NLP_RP_pipeline.ipynb`
  - The notebook is parametrized through the `Config`s generated by the function
    `nlp.build_configs.build_Task1088_configs()`
  - Saves the results of each notebook into a separate dir under
    `notebook_results`
