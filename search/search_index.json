{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Kaizen Technologies internal documentation","text":"<p>Welcome to the internal documentation pages of Kaizen Technologies.</p> <p>To start exploring the repository refer to workflows reference.</p>"},{"location":"all.code_organization.reference.html","title":"Code Organization","text":""},{"location":"all.code_organization.reference.html#code-organization-of-amp","title":"Code organization of <code>amp</code>","text":""},{"location":"all.code_organization.reference.html#conventions","title":"Conventions","text":"<ul> <li>In this code organization files we use the following conventions:</li> <li>Comments: <code>\"\"\"foobar is ...\"\"\"</code></li> <li>Dirs and subdirs: <code>/foobar</code></li> <li>Files: <code>foobar.py</code></li> <li>Objects: <code>FooBar</code></li> <li> <p>Markdown files: <code>foobar.md</code></p> </li> <li> <p>The directories, subdirectory, objects are listed in order of their   dependencies (from innermost to outermost)</p> </li> <li> <p>When there is a dir in one repo that has the same role of a dir in an included   repo we add the suffix from the repo to make them unique</p> </li> <li> <p>E.g., a <code>dataflow</code> dir in <code>lemonade</code> is called <code>dataflow_lem</code></p> </li> <li> <p>We assume that there is no filename repeated across different repos</p> </li> <li>This holds for notebooks, tests, and Python files</li> <li> <p>To disambiguate we add a suffix to make it unique (e.g., <code>_lem</code>)</p> </li> <li> <p>Since the code is split in different repos for access protection reason, we   assume that if the repos could be merged into a single one, then the   corresponding dirs could be collapsed (e.g., <code>//amp/dataflow</code> and   <code>//lime/dataflow_lem</code>) without violating the dependencies</p> </li> <li> <p>TODO(gp): Not sure about this</p> </li> <li> <p>E.g.,</p> </li> <li>We want to build a <code>HistoricalDataSource</code> (from     <code>//amp/dataflow/system/source_nodes.py</code>) with inside an     <code>IgReplayedTimeMarketDataInterface</code> (from     <code>//lime/market_data_lime/eg_market_data.py</code>)</li> <li>The object could be called <code>IgHistoricalDataSource</code> since it's a     specialization of an <code>HistoricalDataSource</code> using IG data</li> <li>The file:<ul> <li>Can't go in <code>//lime/market_data</code> since <code>dataflow_lime</code> depends on   <code>market_data</code></li> <li>Needs to go in <code>//lime/dataflow_lime/system</code></li> <li>Can be called <code>eg_historical_data_source.py</code></li> </ul> </li> </ul>"},{"location":"all.code_organization.reference.html#finding-deps","title":"Finding deps","text":""},{"location":"all.code_organization.reference.html#using-invoke-find_dependency","title":"Using <code>invoke find_dependency</code>","text":"<pre><code>&gt; i find_dependency --module-name \"amp.dataflow.model\" --mode \"find_lev2_deps\" --ignore-helpers --only-module dataflow\n</code></pre>"},{"location":"all.code_organization.reference.html#using-grep","title":"Using grep","text":"<ul> <li>To check for dependencies between one module (e.g., <code>dataflow/model</code>) and   another (e.g., <code>dataflow/system</code>):   ``` <p>(cd dataflow/model/; jackpy \"import \") | grep -v notebooks | grep -v test | grep -v init | grep \"import dataflow.system\" | sort   ```</p> </li> </ul>"},{"location":"all.code_organization.reference.html#using-pydeps","title":"Using Pydeps","text":"<ul> <li> <p>Install   ```</p> <p>pip install pydeps pip install dot   ```</p> </li> <li> <p>Test on a small part of the repo:   ```</p> <p>pydeps . --only helpers -v --show-dot -o deps.dot   ```</p> </li> <li> <p>Run on helpers   ```</p> <p>pydeps --only helpers -x helpers.test -x helpers.old -x helpers.telegram_notify -vv --show-dot -o deps.html --max-bacon 2 --reverse   ```</p> </li> </ul>"},{"location":"all.code_organization.reference.html#component-dirs","title":"Component dirs","text":"<p>Useful utilities are:</p> <pre><code>&gt; dev_scripts/vi_all_py.sh im\n\n&gt; tree.sh -d 1 -p im_v2/ccxt/data\nim_v2/ccxt/data/\n|-- client/\n|-- extract/\n|-- qa/\n`-- __init__.py\n\n4 directories, 1 file\n</code></pre>"},{"location":"all.code_organization.reference.html#helpers","title":"<code>helpers/</code>","text":"<ul> <li><code>helpers/</code></li> <li>\"\"\"Low-level helpers that are general and not specific of this project\"\"\"</li> </ul>"},{"location":"all.code_organization.reference.html#core","title":"<code>core/</code>","text":"<ul> <li><code>core/</code></li> <li>\"\"\"Low-level helpers that are specific of this project\"\"\"</li> <li><code>/config</code><ul> <li><code>Config</code></li> <li>\"\"\"A dict-like object that allows to configure workflows\"\"\"</li> </ul> </li> <li><code>event_study/</code></li> <li><code>artificial_signal_generators.py</code></li> <li><code>features.py</code></li> <li><code>finance.py</code></li> <li><code>signal_processing.py</code></li> <li><code>statistics.py</code></li> </ul>"},{"location":"all.code_organization.reference.html#devops","title":"<code>devops/</code>","text":"<ul> <li><code>devops/</code></li> <li>TODO(gp): reorg</li> </ul>"},{"location":"all.code_organization.reference.html#dev_scripts","title":"<code>dev_scripts/</code>","text":"<ul> <li><code>/dev_scripts</code></li> <li>TODO(gp): reorg</li> </ul>"},{"location":"all.code_organization.reference.html#im","title":"<code>im/</code>","text":"<ul> <li><code>sorrentum_sandbox/</code></li> <li> <p><code>common/</code></p> <ul> <li><code>validate.py</code>: implement <code>QaCheck</code> and <code>DatasetValidator</code></li> <li>TODO(gp): Move this to im_v2</li> </ul> </li> <li> <p><code>im/</code></p> </li> <li> <p>TODO(gp): Merge with im_v2</p> </li> <li> <p><code>im_v2/</code></p> </li> <li>TODO(gp): Rename <code>im_v2</code> to <code>datapull</code></li> <li><code>airflow/</code><ul> <li>\"\"\"Airflow DAGs for downloading, archiving, QA, reconciliation\"\"\"</li> <li><code>airflow_utils/</code></li> </ul> </li> <li><code>aws/</code><ul> <li>\"\"\"Deploy Airflow DAGs definition\"\"\"</li> <li><code>aws_update_task_definition.py</code></li> </ul> </li> <li><code>binance/</code><ul> <li>\"\"\"ETL for Binance data\"\"\"</li> </ul> </li> <li><code>bloomberg/</code><ul> <li>\"\"\"ETL for Bloomberg data\"\"\"</li> </ul> </li> <li><code>ccxt/</code><ul> <li>\"\"\"ETL for CCXT\"\"\"</li> <li><code>data/</code></li> <li><code>client</code></li> <li><code>extract</code><ul> <li><code>extractor.py</code></li> </ul> </li> <li><code>qa</code></li> <li><code>db/</code></li> <li><code>universe/</code></li> <li><code>utils.py</code></li> </ul> </li> <li><code>common/</code><ul> <li><code>data/</code></li> <li><code>client/</code><ul> <li>\"\"\"Objects to read data from ETL pipeline\"\"\"</li> </ul> </li> <li><code>extract/</code><ul> <li>\"\"\"Scripts and utils to perform various extract functions\"\"\"</li> <li><code>extract_utils.py</code>: library functions</li> </ul> </li> <li><code>qa/</code><ul> <li>\"\"\"Utils to perform QA checks\"\"\"</li> </ul> </li> <li><code>transform/</code><ul> <li>\"\"\"Scripts and utils to transform data\"\"\"</li> <li>E.g., convert PQ to CSV, resample data</li> <li><code>transform_utils.py</code></li> </ul> </li> <li><code>data_snapshot/</code></li> <li><code>db/</code></li> <li><code>db_utils.py</code>: manage IM Postgres DB</li> <li><code>extract/</code></li> <li><code>extract_utils.py</code></li> <li><code>universe/</code></li> <li><code>full_symbol.py</code>: implement full symbol (e.g., <code>binance:BTC_USDT</code>)</li> <li><code>universe.py</code>: get universe versions and components</li> <li><code>universe_utils.py</code></li> </ul> </li> <li><code>crypto_chassis/</code><ul> <li>\"\"\"ETL for CryptoChassis\"\"\"</li> </ul> </li> <li><code>devops/</code></li> <li><code>ig/</code></li> <li><code>kibot/</code></li> <li><code>mock1/</code></li> </ul>"},{"location":"all.code_organization.reference.html#market_data","title":"<code>market_data/</code>","text":"<ul> <li><code>market_data/</code></li> <li>\"\"\"Interface to read price data\"\"\"</li> <li><code>MarketData</code></li> <li><code>ImClientMarketData</code></li> <li><code>RealTimeMarketData</code></li> <li><code>ReplayedMarketData</code></li> <li>TODO(gp): Move market_data to <code>datapull/market_data</code></li> </ul>"},{"location":"all.code_organization.reference.html#dataflow","title":"<code>dataflow/</code>","text":"<ul> <li><code>dataflow/</code></li> <li>\"\"\"DataFlow module\"\"\"</li> <li><code>core/</code><ul> <li><code>nodes/</code></li> <li>\"\"\"Implementation of DataFlow nodes that don't depend on anything     outside of this directory\"\"\"</li> <li><code>base.py</code><ul> <li><code>FitPredictNode</code></li> <li><code>DataSource</code></li> </ul> </li> <li><code>sources</code><ul> <li><code>FunctionDataSource</code></li> <li><code>DfDataSource</code></li> <li><code>ArmaDataSource</code></li> </ul> </li> <li><code>sinks.py</code><ul> <li><code>WriteCols</code></li> <li><code>WriteDf</code></li> </ul> </li> <li><code>transformers.py</code></li> <li><code>volatility_models.py</code></li> <li><code>sklearn_models.py</code></li> <li><code>unsupervided_sklearn_models.py</code></li> <li><code>supervided_sklearn_models.py</code></li> <li><code>regression_models.py</code></li> <li><code>sarimax_models.py</code></li> <li><code>gluonts_models.py</code></li> <li><code>local_level_models.py</code></li> <li><code>Dag</code></li> <li><code>DagBuilders</code></li> <li><code>DagRunners</code></li> <li><code>ResultBundle</code></li> </ul> </li> <li><code>pipelines/</code><ul> <li>\"\"\"DataFlow pipelines that use only <code>core</code> nodes\"\"\"</li> <li><code>event_study/</code></li> <li><code>features/</code></li> <li>\"\"\"General feature pipelines\"\"\"</li> <li><code>price/</code></li> <li>\"\"\"Pipelines computing prices\"\"\"</li> <li><code>real_times/</code></li> <li>TODO(gp): -&gt; dataflow/system</li> <li><code>returns/</code></li> <li>\"\"\"Pipelines computing returns\"\"\"</li> <li><code>dataflow_example.py</code></li> <li><code>NaivePipeline</code></li> </ul> </li> <li><code>system/</code><ul> <li>\"\"\"DataFlow pipelines with anything that depends on code outside of   DataFlow\"\"\"</li> <li><code>source_nodes.py</code></li> <li><code>DataSource</code></li> <li><code>HistoricalDataSource</code></li> <li><code>RealTimeDataSource</code></li> <li><code>sink_nodes.py</code></li> <li><code>ProcessForecasts</code></li> <li><code>RealTimeDagRunner</code></li> </ul> </li> <li> <p><code>model/</code></p> <ul> <li>\"\"\"Code for evaluating a DataFlow model\"\"\"</li> </ul> </li> <li> <p><code>oms/</code></p> </li> <li>\"\"\"Order management system\"\"\"</li> <li><code>architecture.md</code></li> <li><code>Broker</code></li> <li><code>Order</code></li> <li><code>OrderProcessor</code></li> <li><code>Portfolio</code></li> <li> <p><code>ForecastProcessor</code></p> </li> <li> <p><code>/optimizer</code></p> </li> <li> <p><code>/research_amp</code></p> </li> </ul>"},{"location":"all.code_organization.reference.html#dataflow-dependencies","title":"dataflow dependencies","text":"<ul> <li><code>dataflow/core</code></li> <li>Should not depend on anything in <code>dataflow</code></li> <li><code>dataflow/pipelines</code></li> <li>-&gt; <code>core</code> since it needs the nodes</li> <li><code>dataflow/model</code></li> <li>-&gt; <code>core</code></li> <li><code>dataflow/backtest</code></li> <li>\"\"\"contain all the code to run a backtest\"\"\"</li> <li>-&gt; <code>core</code></li> <li>-&gt; <code>model</code></li> <li><code>dataflow/system</code></li> <li>-&gt; <code>core</code></li> <li>-&gt; <code>backtest</code></li> <li>-&gt; <code>model</code></li> <li> <p>-&gt; <code>pipelines</code></p> </li> <li> <p>TODO(gp): Move backtest up</p> </li> </ul>"},{"location":"all.code_organization.reference.html#top-level-dirs","title":"Top level dirs","text":"<pre><code>(cd amp; tree -L 1 -d --charset=ascii -I \"*test*|*notebooks*\" 2&gt;&amp;1 | tee /tmp/tmp)\n.\n|-- core\n|-- dataflow\n|-- helpers\n|-- im\n|-- im_v2\n|-- infra\n|-- market_data\n|-- oms\n|-- optimizer\n`-- research_amp\n</code></pre>"},{"location":"all.code_organization.reference.html#invariants","title":"Invariants","text":"<ul> <li>We assume that there is no file with the same name either in the same repo or   across different repos</li> <li>In case of name collision, we prepend as many dirs as necessary to make the     filename unique</li> <li> <p>E.g., the files below should be renamed:</p> <p>```bash</p> <p>ffind.py utils.py | grep -v test ./amp/core/config/utils.py   -&gt; amp/core/config/config_utils.py</p> <p>./amp/dataflow/core/utils.py   -&gt; amp/dataflow/core_config.py</p> <p>./amp/dataflow/model/utils.py   -&gt; amp/dataflow/model/model_utils.py ```   - Note that this rule makes the naming of files depending on the history, but it minimizes churn of names</p> </li> </ul>"},{"location":"all.code_organization.reference.html#misc","title":"Misc","text":"<ul> <li>To execute a vim command, go on the line</li> </ul> <p><code>bash   :exec '!'.getline('.')   :read /tmp/tmp</code></p> <ul> <li>To inline in vim</li> </ul> <p><code>bash   !(cd amp; tree -v --charset=ascii -I \"*test*|*notebooks*\" market_data 2&gt;&amp;1 | tee /tmp/tmp)   :read /tmp/tmp</code></p> <ul> <li>Print only dirs</li> </ul> <p>```bash</p> <p>tree -d   ```</p> <ul> <li>Print only dirs up to a certain level</li> </ul> <p>```bash</p> <p>tree -L 1 -d   ```</p> <ul> <li>Sort alphanumerically</li> </ul> <p>```bash</p> <p>tree -v   ```</p> <ul> <li>Print full name so that one can also grep</li> </ul> <p>```bash</p> <p>tree -v -f --charset=ascii -I \"test|notebooks\" | grep amp | grep -v dev_scripts</p> <p><code>-- dataflow/system       |-- dataflow/system/__init__.py       |-- dataflow/system/real_time_dag_adapter.py       |-- dataflow/system/real_time_dag_runner.py       |-- dataflow/system/research_dag_adapter.py       |-- dataflow/system/sink_nodes.py</code>-- dataflow/system/source_nodes.py   ```</p>"},{"location":"all.documentation_meta.reference.html","title":"Documentation Meta","text":""},{"location":"all.documentation_meta.reference.html#how-to-organize-the-docs","title":"How to organize the docs","text":"<ul> <li>Documentation can be organized in multiple ways:</li> <li>By software component</li> <li>By functionality (e.g., infra, backtesting)</li> <li> <p>By team (e.g., trading ops)</p> </li> <li> <p>We have decided that</p> </li> <li>For each software component there should be a corresponding documentation</li> <li> <p>We have documentation for each functionality and team</p> </li> <li> <p>Processes</p> </li> <li><code>onboarding</code></li> <li><code>general_background</code></li> <li><code>work_organization</code></li> <li><code>work_tools</code></li> <li><code>coding</code></li> <li> <p>...</p> </li> <li> <p>Software components</p> </li> <li><code>build</code></li> <li><code>kaizenflow</code></li> <li><code>datapull</code></li> <li><code>dataflow</code></li> <li><code>trade_execution</code></li> <li><code>infra</code></li> <li>...</li> </ul>"},{"location":"all.documentation_meta.reference.html#dir-vs-no-dirs","title":"Dir vs no-dirs","text":"<ul> <li>Directories make it difficult to navigate the docs</li> <li>We use \u201cname spaces\u201d until we have enough objects to create a dir</li> </ul>"},{"location":"all.documentation_meta.reference.html#tracking-reviews-and-improvements","title":"Tracking reviews and improvements","text":"<ul> <li>Doc needs to be reviewed \"actively\", e.g., by making sure someone checks them   in the field</li> <li> <p>Somebody should verify that is \"executable\"</p> </li> <li> <p>There is a   Master Documentation Gdoc   that contains a list of tasks related to documentation, including what needs   to be reviewed</p> </li> <li> <p>For small action items we add a markdown TODO like we do for the code   ```   </p> </li> </ul> <p>```</p> <ul> <li>To track the last revision we use a tag at the end of the document like:   <code>markdown   Last review: GP on 2024-04-20, ...</code></li> </ul>"},{"location":"all.documentation_meta.reference.html#how-to-search-the-documentation","title":"How to search the documentation","text":"<ul> <li> <p>Be patient and assume that the documentation is there, but you can't find it   because you are not familiar with it and not because you think the   documentation is poorly done or not organized</p> </li> <li> <p>Look for files that contain words related to what you are looking for</p> </li> <li>E.g., <code>ffind.py XYZ</code></li> <li>Grep in the documentation looking for words related to what you are looking   for</li> <li>E.g., <code>jackmd trading</code></li> <li>Scan through the content of the references</li> <li>E.g., <code>all.code_organization.reference.md</code></li> <li>Grep for the name of a tool in the documentation</li> </ul>"},{"location":"all.documentation_meta.reference.html#ensure-that-all-the-docs-are-cross-referenced-in-the-indices","title":"Ensure that all the docs are cross-referenced in the indices","text":"<ul> <li>There is a script to check and update the documentation cross-referencing   files in a directory and a file with all the links to the files   <code>/Users/saggese/src/dev_tools1/linters/amp_fix_md_links.py   docs/all.amp_fix_md_links.explanation.md</code></li> </ul>"},{"location":"all.documentation_meta.reference.html#list-of-files","title":"List of files","text":"<ul> <li>The current structure of files is given by:</li> </ul> <p>```bash</p> <p>tree docs -I 'figs|test*' --dirsfirst -n -F --charset unicode | grep -v init.py   ```</p> <ul> <li>The simple list is:   ```bash <p>ls -1 docs   all.code_organization.reference.md   all.documentation_meta.reference.md   all.software_components.reference.md   all.workflow.explanation.md   build   ck.components.reference.md   coding   dash_web_apps   dataflow   datapull   deploying   dev_tools   documentation_meta   general_background   infra   kaizenflow   marketing   monitoring   oms   onboarding   trading_ops   work_organization   work_tools   ```</p> </li> </ul>"},{"location":"all.documentation_meta.reference.html#description","title":"Description","text":"<ul> <li> <p>Please keep the directory in alphabetical order</p> </li> <li> <p><code>all.documentation_meta.reference.md</code>: contains rules and conventions for all   the documentation under <code>docs</code></p> </li> <li><code>all.code_organization.reference.md</code>: describes how the code is organized in   terms of components, libraries, and directories</li> <li><code>all.software_components.reference.md</code>: lists all the software components in   the codebase</li> <li><code>all.workflow.explanation.md</code>: describes all the workflows for quants, quant   devs, and devops</li> <li><code>build</code>: information related to the build system and GitHub actions</li> <li><code>ck.components.reference.md</code>: list software components and maintainers</li> <li><code>coding</code></li> <li>Guidelines and good practices for coding and code-adjacent activities (such     as code review)</li> <li>This includes general tips and tricks that are useful for anybody writing     any code (e.g., how to use type hints) as well as in-depth descriptions of     specific functions and libraries</li> <li><code>dash_web_apps</code></li> <li><code>dataflow</code>: docs related to the framework of implementing and running machine   learning models</li> <li><code>datapull</code>: docs related to dataset handling: downloading, onboarding,   interpretation, etc.</li> <li><code>deploying</code></li> <li><code>dev_tools</code></li> <li><code>documentation_meta</code>: how to write documentation for code and workflows</li> <li><code>general_background</code>: documents that provide general reference information,   often across different topics</li> <li>E.g., glossaries, reading lists</li> <li><code>infra</code>: docs related to the company\u2019s infrastructure</li> <li>E.g., AWS services, code deployment, monitoring, server administration, etc.</li> <li><code>kaizenflow</code>: docs related to high-level packages that are used across the   codebase , as well as overall codebase organization.</li> <li>E.g., <code>helpers</code>, <code>config</code></li> <li><code>marketing</code></li> <li><code>monitoring</code></li> <li><code>oms</code></li> <li><code>onboarding</code>: practicalities of on-boarding new team members</li> <li>E.g., things typically done only once at the beginning of joining the team</li> <li><code>trading_ops</code>: docs related to placing and monitoring trading orders to market   or broker</li> <li><code>work_organization</code>: how the work is organized on a general level</li> <li>E.g., the company's adopted practices spanning coding and development</li> <li><code>work_tools</code>: how to set up, run and use various software needed for   development</li> <li>E.g., IDE</li> </ul> <p>Last review: GP on 2024-08-11</p>"},{"location":"all.software_components.reference.html","title":"Software Components","text":"<p>{width=\"6.5in\" height=\"1.875in\"}</p> <pre><code>C4Component\ntitle Something\nContainer(Data, \"Data\", \"\", \"\")\nRel(Data, Forecaster, \"Uses\", \"Market and alt data\")\nContainer(Forecaster, \"Forecaster\", \"\", \"\")\nRel(Forecaster, TargetPosAndOrderGen, \"\", \"Forecasts\")\nContainer(TargetPosAndOrderGen, \"TargetPosAndOrderGen\", \"\", \"\")\nRel(TargetPosAndOrderGen, Broker, \"\", \"OMS.Orders\")\nContainer(Broker, \"Broker\", \"\", \"\")\nRel(Broker, Portfolio, \"\", \"OMS.Fills\")\nRel(Data, Portfolio, \"\", \"Prices\")\nContainer(Portfolio, \"Portfolio\", \"\", \"\")\nRel(Portfolio, TargetPosAndOrderGen, \"\", \"Positions\")\nContainer(Exchange, \"Exchange\", \"\", \"\")\nRel(Broker, Exchange, \"\", \"Market specific orders\")\nRel(Exchange, Broker, \"\", \"Market specific fills\")\n</code></pre> <pre><code>Portfolio\n- Broker\n\nprocess_forecasts()\n  - Input:\n    - prediction_df\n    - Portfolio\n  - Instantiates:\n    - TargetPositionAndOrderGenerator\n      - Portfolio\n      - Optimizer\n\nHistoricalDataSource\n</code></pre>"},{"location":"all.software_components.reference.html#conventions","title":"Conventions","text":"<ul> <li>A dir is a C4 container (level 2)</li> <li>A subdir is a C4 component (level 3)</li> <li>A class is a C4/UML class (level 4)</li> <li>We use the same level of header for each of these C4 levels</li> </ul>"},{"location":"all.software_components.reference.html#datapull","title":"<code>DataPull</code>","text":""},{"location":"all.software_components.reference.html#extract","title":"Extract","text":""},{"location":"all.software_components.reference.html#extractor","title":"<code>Extractor</code>","text":"<ul> <li>File: im_v2/common/data/extract/extractor.py</li> <li>Responsibilities: abstract class for downloading raw data from vendors</li> <li>Interactions:</li> <li>Main methods:</li> </ul> <pre><code>classDiagram\n  Extractor &lt;|-- CcxtExtractor\n  CcxtExtractor &lt;|-- CcxtKrakenExtractor\n  Extractor &lt;|-- BinanceExtractor\n  Extractor &lt;|-- CryptoChassisExtractor\n</code></pre>"},{"location":"all.software_components.reference.html#qa","title":"QA","text":""},{"location":"all.software_components.reference.html#qacheck","title":"<code>QaCheck</code>","text":"<ul> <li>File: sorrentum_sandbox/common/validate.py</li> <li>Responsibilities: QA check on one or more datasets</li> </ul> <pre><code>classDiagram\n  class QaCheck{\n    +check()\n    +get_status()\n  }\n  QaCheck &lt;|-- GapsInTimeIntervalCheck\n  QaCheck &lt;|-- GapsInTimeIntervalBySymbolsCheck\n  QaCheck &lt;|-- NaNChecks\n  QaCheck &lt;|-- OhlcvLogicalValuesCheck\n  QaCheck &lt;|-- FullUniversePresentCheck\n  QaCheck &lt;|-- IdenticalDataFramesCheck\n  QaCheck &lt;|-- BidAskDataFramesSimilarityCheck\n  QaCheck &lt;|-- DuplicateDifferingOhlcvCheck\n</code></pre>"},{"location":"all.software_components.reference.html#datasetvalidator","title":"<code>DataSetValidator</code>","text":"<ul> <li>File: sorrentum_sandbox/common/validate.py</li> <li>Responsibilities: Apply a set of QA checks to validate one or more datasets</li> </ul> <p><code>mermaid   classDiagram     class DataSetValidator{       +List[QaCheck] qa_checks       +run_all_checks()     }     DataSetValidator &lt;|-- SingleDatasetValidator     DataSetValidator &lt;|-- DataFrameDatasetValidator</code></p>"},{"location":"all.software_components.reference.html#transform","title":"Transform","text":""},{"location":"all.software_components.reference.html#db","title":"DB","text":""},{"location":"all.software_components.reference.html#dbconnectionmanager","title":"<code>DbConnectionManager</code>","text":""},{"location":"all.software_components.reference.html#testimdbhelper","title":"<code>TestImDbHelper</code>","text":""},{"location":"all.software_components.reference.html#universe","title":"Universe","text":""},{"location":"all.software_components.reference.html#fullsymbol","title":"<code>FullSymbol</code>","text":"<ul> <li>Responsibilities: implement full symbol (e.g., <code>binance:BTC_USDT</code>)</li> </ul>"},{"location":"all.software_components.reference.html#client","title":"Client","text":""},{"location":"all.software_components.reference.html#imclient","title":"<code>ImClient</code>","text":"<ul> <li>Responsibilities: adapts the data from a vendor to the MarketData format   (i.e., a wide format with knowledge time)</li> <li>Main methods:</li> </ul> <p>```mermaid   classDiagram     ImClient &lt;|-- ImClientReadingOneSymbol     ImClient &lt;|-- ImClientReadingMultipleSymbols     ImClientReadingMultipleSymbols &lt;|-- DataFrameImClient</p> <pre><code>ImClientReadingMultipleSymbols &lt;|-- HistoricalPqByTileClient\n\nHistoricalPqByTileClient &lt;|-- HistoricalPqByCurrencyPairTileClient\n\nImClientReadingMultipleSymbols &lt;|-- HistoricalPqByTileClient\n\nImClient &lt;|-- RealTimeImClient\nRealTimeImClient &lt;|-- SqlRealTimeImClient\n</code></pre> <p>```</p>"},{"location":"all.software_components.reference.html#dataframeimclient","title":"<code>DataFrameImClient</code>","text":"<ul> <li>Responsibilities: read data from a passed dataframe</li> <li>This is used for synthetic data</li> </ul>"},{"location":"all.software_components.reference.html#historicalpqbytileclient","title":"<code>HistoricalPqByTileClient</code>","text":"<ul> <li>Responsibilities: read historical data stored as Parquet by-tile</li> </ul>"},{"location":"all.software_components.reference.html#historicalpqbycurrencypairtileclient","title":"<code>HistoricalPqByCurrencyPairTileClient</code>","text":"<ul> <li>Responsibilities: read historical data stored as Parquet by asset</li> </ul>"},{"location":"all.software_components.reference.html#historicalpqbydateclient","title":"<code>HistoricalPqByDateClient</code>","text":"<ul> <li>Responsibilities: read historical data stored as Parquet by tile</li> </ul>"},{"location":"all.software_components.reference.html#realtimeimclient","title":"<code>RealTimeImClient</code>","text":"<ul> <li>Responsibilities: type representing a real-time client</li> </ul>"},{"location":"all.software_components.reference.html#sqlrealtimeimclient","title":"<code>SqlRealTimeImClient</code>","text":"<ul> <li>Responsibilities: read data from a table of an SQL DB</li> </ul>"},{"location":"all.software_components.reference.html#imclienttestcase","title":"<code>ImClientTestCase</code>","text":"<ul> <li>Responsibilities: help test for classes derived from <code>ImClient</code></li> </ul>"},{"location":"all.software_components.reference.html#rawdatareaderread-data-from-a-table-of-an-sql-db","title":"<code>RawDataReader</code>read data from a table of an SQL DB","text":"<ul> <li>Responsibilities: read data based on a dataset signature</li> </ul>"},{"location":"all.software_components.reference.html#market_data","title":"<code>market_data</code>","text":""},{"location":"all.software_components.reference.html#marketdata","title":"<code>MarketData</code>","text":"<ul> <li>Responsibilities:</li> <li>Interactions:</li> <li>Main methods:</li> </ul>"},{"location":"all.software_components.reference.html#imclientmarketdata","title":"<code>ImClientMarketData</code>","text":""},{"location":"all.software_components.reference.html#marketdata__testcase","title":"<code>MarketData_*_TestCase</code>","text":""},{"location":"all.software_components.reference.html#realtimemarketdata","title":"<code>RealTimeMarketData</code>","text":""},{"location":"all.software_components.reference.html#realtimemarketdata2","title":"<code>RealTimeMarketData2</code>","text":""},{"location":"all.software_components.reference.html#replayedmarketdata","title":"<code>ReplayedMarketData</code>","text":""},{"location":"all.software_components.reference.html#horizontalstitchedmarketdata","title":"<code>HorizontalStitchedMarketData</code>","text":""},{"location":"all.software_components.reference.html#igstitchedmarketdata","title":"<code>IgStitchedMarketData</code>","text":""},{"location":"all.software_components.reference.html#dataflow","title":"<code>dataflow</code>","text":""},{"location":"all.software_components.reference.html#dataflowcore","title":"<code>dataflow/core</code>","text":""},{"location":"all.software_components.reference.html#node","title":"<code>Node</code>","text":"<ul> <li>Responsibilities:</li> <li>Store and retrieve its output values on a per-method (e.g., \"fit\" and     \"predict\") basis</li> <li> <p>Each node has an identifier and names for inputs/outputs</p> </li> <li> <p>Interactions:</p> </li> <li><code>DAG</code>: a DAG of <code>Nodes</code></li> <li> <p>Derived <code>Nodes</code> implementing specific features</p> </li> <li> <p>Main methods:</p> </li> </ul>"},{"location":"all.software_components.reference.html#dag","title":"<code>DAG</code>","text":"<ul> <li>Responsibilities:</li> <li>Build a DAG of <code>Nodes</code> by adding and connecting <code>Node</code>s</li> <li>Query a DAG</li> <li> <p>Manage node execution storing outputs within executed nodes</p> </li> <li> <p>Interactions:</p> </li> <li><code>DagBuilder</code>: a class to build <code>DAG</code>s</li> <li> <p><code>Node</code>: nodes of the <code>DAG</code></p> </li> <li> <p>Main methods:</p> </li> </ul>"},{"location":"all.software_components.reference.html#dagbuilder","title":"<code>DagBuilder</code>","text":"<ul> <li>Responsibilities:</li> <li>Abstract class for creating DAGs</li> <li>Interactions:</li> <li>Main methods:</li> <li><code>get_config_template()</code>: return a <code>Config</code> with the parameters that can be     customized when building the <code>DAG</code></li> <li><code>get_dag()</code>: builds the <code>DAG</code></li> </ul>"},{"location":"all.software_components.reference.html#dagrunner","title":"<code>DagRunner</code>","text":"<ul> <li>Responsibilities:</li> <li>Run a <code>DAG</code> by calling a <code>Method</code> on all the nodes</li> <li>Interactions:</li> <li>Main methods:</li> </ul>"},{"location":"all.software_components.reference.html#fitpredictdagrunner","title":"<code>FitPredictDagRunner</code>","text":"<ul> <li>Responsibilities:</li> <li>Run a <code>DAG</code> with <code>fit</code>, <code>predict</code> methods</li> <li>Interactions:</li> <li>Main methods:</li> <li><code>set_fit_intervals()</code>, <code>set_predict_intervals()</code> to set the intervals to run     on</li> <li><code>fit()</code>, <code>predict()</code> to run the corresponding methods</li> </ul>"},{"location":"all.software_components.reference.html#rollingfitpredictdagrunner","title":"<code>RollingFitPredictDagRunner</code>","text":"<ul> <li>Responsibilities:</li> <li>Run a <code>DAG</code> by periodic fitting on previous history and evaluating on new     data</li> <li>Interactions:</li> <li>Main methods:</li> </ul>"},{"location":"all.software_components.reference.html#resultbundle","title":"<code>ResultBundle</code>","text":"<ul> <li>Responsibilities:</li> <li>Store <code>DAG</code> execution results.</li> <li>Interactions:</li> <li>Main methods:</li> </ul>"},{"location":"all.software_components.reference.html#dataflowcorenodes","title":"dataflow/core/nodes","text":""},{"location":"all.software_components.reference.html#fitpredictnode","title":"<code>FitPredictNode</code>","text":"<ul> <li>Abstract node implementing <code>fit()</code> / <code>predict()</code> function</li> <li>Store and load state</li> </ul>"},{"location":"all.software_components.reference.html#datasource","title":"<code>DataSource</code>","text":"<ul> <li>DataSource &lt;|-- FitPredictNode</li> <li>Abstract</li> <li>Generate train/test data from the passed data frame</li> </ul>"},{"location":"all.software_components.reference.html#transformer","title":"<code>Transformer</code>","text":"<ul> <li>FitPredictNode &lt;|-- DataSource</li> <li>Abstract</li> <li>Single-input single-output node calling a stateless transformation</li> </ul>"},{"location":"all.software_components.reference.html#yconnector","title":"<code>YConnector</code>","text":"<ul> <li>FitPredictNode &lt;|-- YConnector</li> <li>Create an output df from two input dataframes</li> </ul>"},{"location":"all.software_components.reference.html#groupedcoldftodfcolprocessor","title":"<code>GroupedColDfToDfColProcessor</code>","text":""},{"location":"all.software_components.reference.html#crosssectionaldftodfcolprocessor","title":"<code>CrossSectionalDfToDfColProcessor</code>","text":"<ul> <li>Wrappers for cross-sectional transformations</li> </ul>"},{"location":"all.software_components.reference.html#seriestodfcolprocessor","title":"<code>SeriesToDfColProcessor</code>","text":"<ul> <li>Series-to-dataframe wrapper</li> </ul>"},{"location":"all.software_components.reference.html#seriestoseriescolprocessor","title":"<code>SeriesToSeriesColProcessor</code>","text":"<ul> <li>Series-to-series wrapper</li> </ul>"},{"location":"all.software_components.reference.html#dfstacker","title":"<code>DfStacker</code>","text":"<ul> <li>Stack and unstack dataframes with identical columns</li> </ul>"},{"location":"all.software_components.reference.html#_1","title":"Software Components","text":""},{"location":"all.software_components.reference.html#dataflowsystem","title":"dataflow/system","text":""},{"location":"all.software_components.reference.html#realtimedagrunner","title":"<code>RealTimeDagRunner</code>","text":"<ul> <li>Run a DAG in real-time</li> </ul>"},{"location":"all.software_components.reference.html#processforecastsnode","title":"<code>ProcessForecastsNode</code>","text":""},{"location":"all.software_components.reference.html#historicaldatasource","title":"<code>HistoricalDataSource</code>","text":"<ul> <li>Adapt a <code>MarketData</code> object to a DAG</li> <li>Store and load the state of the node.</li> </ul>"},{"location":"all.software_components.reference.html#realtimedatasource","title":"<code>RealTimeDataSource</code>","text":""},{"location":"all.software_components.reference.html#system","title":"<code>System</code>","text":"<ul> <li>Responsibilities: abstract class that builds a <code>System</code></li> <li>Interactions: there are several derived classes that allow to build various   types of Systems   <code>mermaid   classDiagram     System &lt;|-- ForecastSystem     System &lt;|-- Df_ForecastSystem     ForecastSystem &lt;|-- NonTime_ForecastSystem     _Time_ForecastSystem_Mixin     System &lt;|-- Time_ForecastSystem     _Time_ForecastSystem_Mixin &lt;|-- Time_ForecastSystem     ForecastSystem_with_DataFramePortfolio &lt;|-- _ForecastSystem_with_Portfolio     _Time_ForecastSystem_Mixin, ForecastSystem_with_DataFramePortfolio &lt;|-- Time_ForecastSystem_with_DataFramePortfolio</code></li> <li>Main methods:</li> </ul>"},{"location":"all.software_components.reference.html#forecastsystem","title":"<code>ForecastSystem</code>","text":""},{"location":"all.software_components.reference.html#df_forecastsystem","title":"<code>Df_ForecastSystem</code>","text":""},{"location":"all.software_components.reference.html#nontime_forecastsystem","title":"<code>NonTime_ForecastSystem</code>","text":""},{"location":"all.software_components.reference.html#time_forecastsystem","title":"<code>Time_ForecastSystem</code>","text":""},{"location":"all.software_components.reference.html#forecastsystem_with_dataframeportfolio","title":"<code>ForecastSystem_with_DataFramePortfolio</code>","text":""},{"location":"all.software_components.reference.html#time_forecastsystem_with_dataframeportfolio","title":"<code>Time_ForecastSystem_with_DataFramePortfolio</code>","text":""},{"location":"all.software_components.reference.html#time_forecastsystem_with_databaseportfolio_and_orderprocessor","title":"<code>Time_ForecastSystem_with_DatabasePortfolio_and_OrderProcessor</code>","text":""},{"location":"all.software_components.reference.html#time_forecastsystem_with_databaseportfolio","title":"<code>Time_ForecastSystem_with_DatabasePortfolio</code>","text":""},{"location":"all.software_components.reference.html#dataflowbacktest","title":"dataflow/backtest","text":""},{"location":"all.software_components.reference.html#forecaster","title":"Forecaster","text":"<ul> <li>It is a DAG system that forecasts the value of the target economic quantities   (e.g.,</li> </ul> <p>for each asset in the target</p> <ul> <li>Responsibilities:</li> <li>Interactions:</li> <li>Main methods:</li> </ul>"},{"location":"all.software_components.reference.html#marketoms","title":"<code>MarketOms</code>","text":"<p>MarketOms is the interface that allows to place orders and receive back fills to the specific target market. This is provided as-is and it's not under control of the user or of the protocol</p> <ul> <li>E.g., a specific exchange API interface</li> </ul>"},{"location":"all.software_components.reference.html#orderprocessor","title":"<code>OrderProcessor</code>","text":"<ul> <li>TODO(gp): Maybe MockedMarketOms since that's the actual function?</li> </ul>"},{"location":"all.software_components.reference.html#omsdb","title":"<code>OmsDb</code>","text":"<p>Simulation</p>"},{"location":"all.software_components.reference.html#implementedbroker","title":"<code>ImplementedBroker</code>","text":"<ul> <li><code>submit_orders()</code></li> <li>Save files in the proper location</li> <li>Wait for orders to be accepted</li> <li><code>get_fills</code></li> <li>No-op since the portfolio is updated automatically</li> </ul> <p>Mocked system</p> <ul> <li>Our implementation of the implemented system where we replace DB with a mock</li> <li>The mocked DB should be as similar as possible to the implemented DB</li> </ul>"},{"location":"all.software_components.reference.html#databasebroker","title":"<code>DatabaseBroker</code>","text":"<ul> <li><code>submit_orders()</code></li> <li>Same behavior of <code>ImplementedBroker</code> but using <code>OmsDb</code></li> </ul>"},{"location":"all.software_components.reference.html#omsdb_1","title":"<code>OmsDb</code>","text":"<ul> <li><code>submitted_orders</code> table (mocks S3)</li> <li>Contain the submitted orders</li> <li><code>accepted_orders</code> table</li> <li><code>current_position</code> table</li> </ul>"},{"location":"all.software_components.reference.html#omsfill","title":"oms/fill","text":""},{"location":"all.software_components.reference.html#fill","title":"<code>Fill</code>","text":"<ul> <li>Responsibilities:</li> <li>Represent an order fill</li> <li>Interactions:</li> <li><code>Order</code></li> <li>Main methods:</li> </ul>"},{"location":"all.software_components.reference.html#omsorder","title":"oms/order","text":""},{"location":"all.software_components.reference.html#order","title":"<code>Order</code>","text":"<ul> <li>Responsibilities:</li> <li>Represent an order to be executed over a period of time</li> </ul>"},{"location":"all.software_components.reference.html#omsbroker","title":"oms/broker","text":"<pre><code>classDiagram\nBroker &lt;|-- FakeFillsBroker : Inheritance\nFakeFillsBroker &lt;|-- DataFrameBroker : Inheritance\nDataFrameBroker &lt;|-- ReplayedFillsDataFrameBroker : Inheritance\nFakeFillsBroker &lt;|-- DatabaseBroker : Inheritance\n\nBroker &lt;|-- AbstractCcxtBroker : Inheritance\nAbstractCcxtBroker &lt;|-- CcxtBroker : Inheritance\n</code></pre>"},{"location":"all.software_components.reference.html#broker","title":"<code>Broker</code>","text":"<ul> <li>Description</li> <li>A <code>Broker</code> is an object to place orders to the market and to receive fills,     adapting <code>Order</code> and <code>Fill</code> objects to the corresponding exchange-specific     objects</li> <li> <p>In other words <code>Broker</code> adapts the internal representation of <code>Order</code> and     <code>Fill</code>s to the ones that are specific to the target exchange</p> </li> <li> <p>Responsibilities:</p> </li> <li>Submit orders to MarketOms</li> <li>Wait to ensure that orders were properly accepted by MarketOms</li> <li>Execute complex orders (e.g., TWAP, VWAP, pegged orders) interacting with     the target market</li> <li> <p>Receive fill information from the target market</p> </li> <li> <p>Interactions:</p> </li> <li><code>MarketData</code> to receive prices and other information necessary to execute     orders</li> <li><code>MarketOms</code> to place orders and receive fills<ul> <li>TODO(gp): Check this</li> </ul> </li> <li> <p><code>LimitPriceComputer</code> to compute the limit price for limit orders</p> </li> <li> <p>Main methods:</p> </li> <li><code>submit_orders()</code>: submit orders to the trading exchange</li> <li><code>get_fills()</code></li> </ul>"},{"location":"all.software_components.reference.html#fakefillsbroker","title":"<code>FakeFillsBroker</code>","text":"<ul> <li>Responsibilities:</li> <li>Interactions:</li> <li>Main methods:</li> </ul>"},{"location":"all.software_components.reference.html#dataframebroker","title":"<code>DataFrameBroker</code>","text":"<ul> <li>Responsibilities:</li> <li>Interactions:</li> <li>Main methods:</li> </ul>"},{"location":"all.software_components.reference.html#databasebroker_1","title":"<code>DatabaseBroker</code>","text":"<ul> <li>Responsibilities:</li> <li>Interactions:</li> <li>Main methods:</li> </ul>"},{"location":"all.software_components.reference.html#replayeddatareader","title":"<code>ReplayedDataReader</code>","text":"<ul> <li>Responsibilities:</li> <li>Replay data from an actual <code>RawDataReader</code></li> <li>Interactions:</li> <li>Derived from <code>DataFrameBroker</code></li> <li>Main methods:</li> </ul>"},{"location":"all.software_components.reference.html#replayedfillsdataframebroker","title":"<code>ReplayedFillsDataFrameBroker</code>","text":"<ul> <li>Responsibilities:</li> <li>Replay the fills from a Broker</li> <li>Interactions:</li> <li>Derived from <code>DataFrameBroker</code></li> <li>Main methods:</li> </ul>"},{"location":"all.software_components.reference.html#omsbrokerccxt","title":"oms/broker/ccxt","text":"<pre><code>classDiagram\n    Broker &lt;|-- AbstractCcxtBroker : Inheritance\n    AbstractCcxtBroker &lt;|-- CcxtBroker : Inheritance\n</code></pre>"},{"location":"all.software_components.reference.html#abstractccxtbroker","title":"<code>AbstractCcxtBroker</code>","text":"<ul> <li>Responsibilities:</li> <li>Retrieve broker configuration, market data (including CCXT), open positions,     fills, and trades</li> <li> <p>Update order statistics, validate child orders, handle exceptions, and     calculate TWAP waves.</p> </li> <li> <p>Interactions:</p> </li> <li> <p>Derived from <code>Broker</code></p> </li> <li> <p>Main methods:</p> </li> <li><code>get_broker_config()</code>: Retrieve broker configuration.</li> <li><code>get_bid_ask_data_for_last_period()</code>: Obtain bid-ask data for the given last     period.</li> <li><code>get_market_info()</code>: Load market information from the given exchange and map     to asset ids.</li> <li><code>get_fills()</code>: Retrieves a list of completed trades (fills) from the     previous order execution, to allow the Portfolio component to update its     state accurately.</li> <li><code>get_ccxt_trades()</code>: Retrieves CCXT trades (completed orders/fills)     corresponding to a list of provided CCXT orders, grouped by trading symbol.</li> <li><code>get_open_positions()</code>: Retrieves and caches the open positions from the     exchange, with an optional sanity check against live data, to efficiently     provide the current open positions for the trading account.</li> <li><code>cancel_open_orders_for_symbols()</code>: Cancels all open orders for a given list     of trading pairs.</li> <li><code>get_total_balance()</code>: Retrieves, validates, and logs the total available     balance from an exchange.</li> </ul>"},{"location":"all.software_components.reference.html#ccxtbroker","title":"<code>CcxtBroker</code>","text":"<ul> <li>Responsibilities:</li> <li> <p>Manage CCXT interactions, submit orders, handle cancellations, and sync with     wave start times; log results, obtain fills and trades, manage TWAP child     orders, and get CCXT order structures.</p> </li> <li> <p>Interactions:</p> </li> <li> <p>Derived from <code>AbstractCcxtBroker</code></p> </li> <li> <p>Main methods:</p> </li> <li><code>get_ccxt_fills()</code>: Get fills from submitted orders in OMS and CCXT formats.</li> <li><code>_submit_twap_orders()</code>: Execute orders using the TWAP strategy.</li> <li><code>_get_ccxt_order_structure()</code>: Get the CCXT order structure corresponding to     the submitted order.</li> </ul>"},{"location":"all.software_components.reference.html#omslimit_computer","title":"oms/limit_computer","text":"<pre><code>classDiagram\n    AbstractLimitPriceComputer &lt;|-- LimitPriceComputerUsingSpread : Inheritance\n    AbstractLimitPriceComputer &lt;|-- LimitPriceComputerUsingVolatility : Inheritance\n</code></pre>"},{"location":"all.software_components.reference.html#abstractlimitpricecomputer","title":"<code>AbstractLimitPriceComputer</code>","text":"<ul> <li>Responsibilities:</li> <li> <p>Provide methods to retrieve timestamp data, extract latest bid/ask sizes,     and validate/normalize bid/ask data.</p> </li> <li> <p>Interactions:</p> </li> <li> <p>Main methods:</p> </li> <li><code>get_latest_timestamps_from_bid_ask_data()</code>: Get timestamp data related to     the bid/ask price.</li> <li><code>get_latest_size_from_bid_ask_data()</code>: Extract latest bid/ask size data and     returns the size data dictionary.</li> <li><code>calculate_limit_price()</code>: Return limit price and price data such as     latest/mean bid/ask price.</li> <li><code>normalize_bid_ask_data()</code>: Validate and normalize the bid ask data.</li> </ul>"},{"location":"all.software_components.reference.html#limitpricecomputerusingspread","title":"<code>LimitPriceComputerUsingSpread</code>","text":"<ul> <li>Responsibilities:</li> <li> <p>Retrieve, compare latest and average bid/ask prices.</p> </li> <li> <p>Interactions:</p> </li> <li> <p>Derived from <code>AbstractLimitPriceComputer</code></p> </li> <li> <p>Main methods:</p> </li> <li><code>compare_latest_and_average_price()</code> : Retrieve and compare latest and     average bid/ask prices.</li> <li><code>calculate_limit_price()</code>:Calculate limit price based on recent bid / ask     data and uses a <code>passivity_factor</code> to adjust the limit price between the bid     and ask prices.</li> </ul>"},{"location":"all.software_components.reference.html#limitpricecomputerusingvolatility","title":"<code>LimitPriceComputerUsingVolatility</code>","text":"<ul> <li>Responsibilities:</li> <li> <p>Compute limit price based on volatility multiple</p> </li> <li> <p>Interactions:</p> </li> <li> <p>Derived from <code>AbstractLimitPriceComputer</code></p> </li> <li> <p>Main methods:</p> </li> <li><code>compute_metrics_from_price_data()</code>: Analyze bid-ask price data to compute     volume, sum of squares of difference, last price, and count.</li> <li><code>calculate_limit_price()</code>:Calculate limit price based on recent bid / ask     data and uses a <code>volatility_multiple</code> to adjust the limit price based on the     volatility of the bid and ask prices.</li> </ul>"},{"location":"all.software_components.reference.html#omschild_order_quantity_computer","title":"oms/child_order_quantity_computer","text":"<pre><code>classDiagram\n    AbstractChildOrderQuantityComputer &lt;|-- DynamicSchedulingChildOrderQuantityComputer : Inheritance\n    AbstractChildOrderQuantityComputer &lt;|-- StaticSchedulingChildOrderQuantityComputer : Inheritance\n</code></pre>"},{"location":"all.software_components.reference.html#abstractchildorderquantitycomputer","title":"<code>AbstractChildOrderQuantityComputer</code>","text":"<ul> <li>Responsibilities:</li> <li> <p>Represent strategy to decide child order quantities within a parent order</p> </li> <li> <p>Interactions:</p> </li> <li> <p>Main methods:</p> </li> <li><code>set_instance_params()</code>: Initialize instance parameters with parent order     size, market data, and execution goals.</li> <li><code>get_wave_quantities()</code>: Return the quantity for the specified wave ID from     the wave quantities.</li> <li><code>update_current_positions()</code>: Update the current positions using data from     the Broker.</li> </ul>"},{"location":"all.software_components.reference.html#dynamicschedulingchildorderquantitycomputer","title":"<code>DynamicSchedulingChildOrderQuantityComputer</code>","text":"<ul> <li>Responsibilities:</li> <li>Place each child order wave with the remaining amount to fill.</li> <li> <p>Determine the child order quantity to be placed during each wave.</p> </li> <li> <p>Interactions:</p> </li> <li> <p>Derived from <code>AbstractChildOrderQuantityComputer</code></p> </li> <li> <p>Main methods:</p> </li> <li><code>get_wave_quantities()</code>: calculates the target positions for parent orders     and get the first child order quantity as equal to the parent order quantity     for the first wave and for following waves, the child order quantities are     computed as     <code>target_position - open_position</code></li> </ul>"},{"location":"all.software_components.reference.html#staticschedulingchildorderquantitycomputer","title":"<code>StaticSchedulingChildOrderQuantityComputer</code>","text":"<ul> <li>Responsibilities:</li> <li>Generate a TWAP-like schedule for placing child orders.</li> <li> <p>Calculate child order quantities for each provided parent order.</p> </li> <li> <p>Interactions:</p> </li> <li> <p>Derived from <code>AbstractChildOrderQuantityComputer</code></p> </li> <li> <p>Main methods:</p> </li> <li><code>calculate_static_child_order_quantities()</code>: Calculate child order     quantities for each provided parent order. The quantity is static, so it is     calculated only once.</li> </ul>"},{"location":"all.software_components.reference.html#omsportfolio","title":"oms/portfolio","text":""},{"location":"all.software_components.reference.html#portfolio","title":"<code>Portfolio</code>","text":"<ul> <li> <p>A Portfolio stores information about asset and cash holdings of a System over   time.</p> </li> <li> <p>Responsibilities:</p> </li> <li> <p>Hold the holdings in terms of shares of each asset id and cash available</p> </li> <li> <p>Interactions:</p> </li> <li>MarketData to receive current prices to estimate the value of the holdings</li> <li> <p>Accumulate statistics and</p> </li> <li> <p>Main methods:</p> </li> <li> <p>Mark_to_market(): estimate the value of the current holdings using the     current market prices</p> </li> <li> <p><code>get_holdings()</code></p> </li> <li>Abstract because IS, Mocked, Simulated have a different implementations</li> <li> <p><code>mark_to_market()</code></p> </li> <li> <p>Not abstract</p> </li> <li>-&gt; <code>get_holdings()</code>, <code>PriceInterface</code></li> <li><code>update_state()</code></li> <li>Abstract</li> <li>Use abstract but make it NotImplemented (we will get some static checks and   some other dynamic checks)</li> <li>We are trying not to mix static typing and duck typing</li> <li>CASH_ID, <code>_compute_statistics()</code> goes in <code>Portolio</code></li> </ul>"},{"location":"all.software_components.reference.html#dataframeportfolio","title":"<code>DataFramePortfolio</code>","text":"<ul> <li> <p>An implementation of a Portfolio backed by a DataFrame. This is used to   simulate a system on an order-by-order basis. This should be equivalent to   using a DatabasePortfolio but without the complexity of querying a DB.</p> </li> <li> <p>This is what we call <code>Portfolio</code></p> </li> <li>In RT we can run <code>DataFramePortfolio</code> and <code>ImplementedPortfolio</code> in parallel   to collect real and simulated behavior</li> <li><code>get_holdings()</code></li> <li>Store the holdings in a df</li> <li><code>update_state()</code></li> <li>Update the holdings with fills -&gt; <code>SimulatedBroker.get_fills()</code></li> <li>To make the simulated system closer to the implemented</li> </ul>"},{"location":"all.software_components.reference.html#databaseportfolio","title":"<code>DatabasePortfolio</code>","text":"<p>an implementation of a Portfolio backed by an SQL Database to simulate systems where the Portfolio state is held in a database. This allows to simulate a system on an order-by-order basis.</p> <ul> <li><code>get_holdings()</code></li> <li>Same behavior of <code>ImplementedPortfolio</code> but using <code>OmsDb</code></li> </ul>"},{"location":"all.software_components.reference.html#implementedportfolio","title":"ImplementedPortfolio","text":"<ul> <li><code>get_holdings()</code></li> <li>Check self-consistency and assumptions</li> <li>Check that no order is in flight otherwise we should assert or log an error</li> <li>Query the DB and gets you the answer</li> <li><code>update_state()</code></li> <li>No-op since the portfolio is updated automatically</li> </ul>"},{"location":"all.software_components.reference.html#omsorder_processing","title":"oms/order_processing","text":""},{"location":"all.software_components.reference.html#orderprocessor_1","title":"<code>OrderProcessor</code>","text":"<ul> <li>Monitor <code>OmsDb.submitted_orders</code></li> <li>Update <code>OmsDb.accepted_orders</code></li> <li> <p>Update <code>OmsDb.current_position</code> using <code>Fill</code> and updating the <code>Portfolio</code></p> </li> <li> <p>TODO(gp): Unclear where it is used?</p> </li> </ul>"},{"location":"all.software_components.reference.html#process_forecasts","title":"<code>process_forecasts</code>","text":"<ul> <li>Responsibilities:</li> <li>Process all the forecasts from <code>prediction_df</code> using     <code>TargetPositionAndOrderGenerator</code> to generate orders</li> <li>Interactions:</li> <li> <p>Stores <code>TargetPositionAndOrderGenerator</code></p> </li> <li> <p>This is used as an interface to simulate the effect of given forecasts under   different optimization conditions, spread, and restrictions, without running   the Forecaster</p> </li> </ul>"},{"location":"all.software_components.reference.html#targetpositionandordergenerator","title":"<code>TargetPositionAndOrderGenerator</code>","text":"<ul> <li>Responsibilities:</li> <li>Retrieve the current holdings from <code>Portfolio</code></li> <li>Perform optimization using forecasts and current holdings to compute the     target positions</li> <li>Generate the orders needed to achieve the target positions</li> <li>Submit orders to the <code>Broker</code></li> <li>Interactions:</li> <li>Forecaster to receive the forecasts of returns for each asset</li> <li>Portfolio to recover the current holdings</li> <li>Main methods:</li> <li>Compute_target_positions_and_generate_orders(): compute the target positions     and generate the orders needed to reach</li> <li> <p><code>compute_target_holdings_shares()</code>: call the Optimizer to compute the target     holdings in shares</p> </li> <li> <p>Aka <code>place_trades()</code></p> </li> <li>Act on the forecasts by:</li> <li>Get the state of portfolio (by getting fills from previous clock)</li> <li>Updating the portfolio holdings</li> <li>Computing the optimal positions</li> <li>Submitting the corresponding orders</li> <li><code>optimize_positions()</code></li> <li>Aka <code>optimize_and_update()</code></li> <li>Calls the Optimizer</li> <li><code>compute_target_positions()</code></li> <li>Aka <code>compute_trades()</code></li> <li><code>submit_orders()</code></li> <li>Call <code>Broker</code></li> <li><code>get_fills()</code></li> <li>Call <code>Broker</code></li> <li>For IS it is different</li> <li><code>update_portfolio()</code></li> <li>Call <code>Portfolio</code></li> <li>For IS it is different</li> <li>It should not use any concrete implementation but only <code>Abstract\\*</code></li> </ul>"},{"location":"all.software_components.reference.html#locates","title":"Locates","text":""},{"location":"all.software_components.reference.html#restrictions","title":"Restrictions","text":""},{"location":"all.software_components.reference.html#omsoptimizer","title":"oms/optimizer","text":""},{"location":"all.software_components.reference.html#omsdb_2","title":"oms/db","text":""},{"location":"all.software_components.reference.html#omsccxt","title":"oms/ccxt","text":""},{"location":"all.workflow.explanation.html","title":"Workflow","text":""},{"location":"all.workflow.explanation.html#kaizenflow-workflow-explanation","title":"KaizenFlow workflow explanation","text":"<p>This document is a roadmap of most activities that Quants, Quant devs, and DevOps can perform using <code>KaizenFlow</code>.</p> <p>For each activity we point to the relevant resources (e.g., documents in <code>docs</code>, notebooks) in the repo.</p> <p>A high-level description of KaizenFlow is KaizenFlow White Paper</p>"},{"location":"all.workflow.explanation.html#work-organization","title":"Work organization","text":"<ul> <li>Issues workflow explained   <code>amp/docs/work_organization/ck.issue_workflow.explanation.md</code></li> <li>GitHub and ZenHub workflows explained   <code>/docs/work_organization/all.use_github_and_zenhub.how_to_guide.md</code></li> <li>TODO(Grisha): add more from <code>/docs/work_organization/</code>.</li> </ul>"},{"location":"all.workflow.explanation.html#set-up","title":"Set-up","text":"<ul> <li>TODO(gp): Add pointers to the docs we ask to read during the on-boarding</li> </ul>"},{"location":"all.workflow.explanation.html#documentation_meta","title":"Documentation_meta","text":"<ul> <li> <p>The dir <code>docs/documentation_meta</code> contains documents about writing the   documentation</p> </li> <li> <p>Conventions and suggestions on how to create diagrams in the documentation</p> </li> <li> <p>/docs/documentation_meta/all.architecture_diagrams.explanation.md</p> </li> <li> <p>A summary of how to create how-to, tutorial, explanations, reference according   to the Diataxis framework</p> </li> <li> <p>/docs/documentation_meta/all.diataxis.explanation.md</p> </li> <li> <p>Writing documentation in Google Docs</p> </li> <li> <p>/docs/documentation_meta/all.gdocs.how_to_guide.md</p> </li> <li> <p>Writing documentation in Markdown</p> </li> <li> <p>/docs/documentation_meta/all.writing_docs.how_to_guide.md</p> </li> <li> <p>Plotting in Latex</p> </li> <li>/docs/documentation_meta/plotting_in_latex.how_to_guide.md</li> </ul>"},{"location":"all.workflow.explanation.html#quant-workflows","title":"Quant workflows","text":"<p>The life of a Quant is spent between:</p> <ul> <li>Exploring the raw data</li> <li>Computing features</li> <li>Building models to predict output given features</li> <li>Assessing models</li> </ul> <p>These activities are mapped in <code>KaizenFlow</code> as follows:</p> <ul> <li>Exploring the raw data</li> <li>This is performed by reading data using <code>DataPull</code> in a notebook and     performing exploratory analysis</li> <li>Computing features</li> <li>This is performed by reading data using <code>DataPull</code> in a notebook and     creating some <code>DataFlow</code> nodes</li> <li>Building models to predict output given features</li> <li>This is performed by connecting <code>DataFlow</code> nodes into a <code>Dag</code></li> <li>Assessing models</li> <li>This is performed by running data through a <code>Dag</code> in a notebook or in a     Python script and post-processing the results in an analysis notebook</li> <li>Comparing models</li> <li>The parameters of a model are exposed through a <code>Config</code> and then sweep over     <code>Config</code> lists</li> </ul>"},{"location":"all.workflow.explanation.html#datapull","title":"<code>DataPull</code>","text":"<ul> <li>General intro to <code>DataPull</code></li> <li>/docs/datapull/ck.datapull.explanation.md</li> <li>/docs/datapull/all.datapull_qa_flow.explanation.md</li> <li>/docs/datapull/all.datapull_client_stack.explanation.md</li> <li>/docs/datapull/all.datapull_sandbox.explanation.md</li> <li>/docs/datapull/ck.ccxt_exchange_timestamp_interpretation.reference.md</li> </ul>"},{"location":"all.workflow.explanation.html#universe","title":"Universe","text":"<ul> <li>Universe explanation</li> <li> <p>/docs/datapull/ck.universe.explanation.md</p> </li> <li> <p>Analyze universe metadata</p> </li> <li>/im_v2/common/universe/notebooks/Master_universe_analysis.ipynb</li> <li>/im_v2/ccxt/notebooks/Master_universe.ipynb</li> </ul>"},{"location":"all.workflow.explanation.html#dataset-signature","title":"Dataset signature","text":"<ul> <li>Organize and label datasets</li> <li>Helps to uniquely identify datasets across different sources, types,     attributes etc.</li> <li>/docs/datapull/all.data_schema.explanation.md</li> <li> <p>/docs/datapull/ck.handle_datasets.how_to_guide.md</p> </li> <li> <p>Inspect RawData</p> </li> <li>/im_v2/common/notebooks/Master_raw_data_gallery.ipynb</li> <li> <p>/im_v2/common/data/client/im_raw_data_client.py</p> </li> <li> <p>Convert data types</p> </li> <li>/im_v2/common/data/transform/convert_csv_to_pq.py</li> <li> <p>/im_v2/common/data/transform/convert_pq_to_csv.py</p> </li> <li> <p>Data download pipelines explanation</p> </li> <li>/docs/datapull/ck.binance_bid_ask_data_pipeline.explanation.md</li> <li> <p>/docs/datapull/ck.binance_ohlcv_data_pipeline.explanation.md</p> </li> <li> <p>Download data in bulk</p> </li> <li>/im_v2/common/data/extract/download_bulk.py</li> <li>/im_v2/ccxt/data/extract/download_exchange_data_to_db.py</li> <li> <p>TODO(Juraj): technically this could be joined into one script and also     generalized for more sources</p> </li> <li> <p>Download data in real time over a given time interval</p> </li> <li> <p>/im_v2/common/data/extract/periodic_download_exchange_data_to_db.py</p> </li> <li> <p>Archive data</p> </li> <li>Helps with optimizing data storage performance/costs by transferring older     data from a storage like postgres to S3</li> <li>Suitable to apply to high frequency high volume realtime orderbook data</li> <li> <p>/im_v2/ccxt/db/archive_db_data_to_s3.py</p> </li> <li> <p>Resampling data</p> </li> <li>/docs/datapull/all.datapull_derived_data.explanation.md</li> <li> <p>/im_v2/common/data/transform/resample_daily_bid_ask_data.py</p> </li> <li> <p>ImClient</p> </li> <li> <p>/docs/datapull/all.im_client.reference.ipynb</p> </li> <li> <p>MarketData</p> </li> <li> <p>/docs/datapull/all.market_data.reference.ipynb</p> </li> <li> <p>How to QA data</p> </li> <li>/docs/datapull/ck.datapull_data_quality_assurance.reference.md</li> <li>/im_v2/ccxt/data/qa/notebooks/data_qa_bid_ask.ipynb</li> <li>/im_v2/ccxt/data/qa/notebooks/data_qa_ohlcv.ipynb</li> <li>/im_v2/common/data/qa/notebooks/cross_dataset_qa_ohlcv.ipynb</li> <li>/im_v2/common/data/qa/notebooks/cross_dataset_qa_bid_ask.ipynb</li> <li>/research_amp/cc/notebooks/Master_single_vendor_qa.ipynb</li> <li>/research_amp/cc/notebooks/Master_cross_vendor_qa.ipynb</li> <li> <p>/research_amp/cc/notebooks/compare_qa.periodic.airflow.downloaded_websocket_EOD.all.bid_ask.futures.all.ccxt_cryptochassis.all.v1_0_0.ipynb</p> </li> <li> <p>How to load <code>Bloomberg</code> data</p> </li> <li>/im_v2/common/notebooks/CmTask5424_market_data.ipynb</li> <li> <p>TODO: Generalize the name and make it Master_</p> </li> <li> <p>Kibot guide</p> </li> <li>/docs/datapull/ck.kibot_data.explanation.md</li> <li> <p>/docs/datapull/ck.kibot_timing.reference.md</p> </li> <li> <p>Interactive broker guide</p> </li> <li>/docs/datapull/ck.run_ib_connect.how_to_guide.md</li> <li> <p>/docs/datapull/ck.use_ib_metadata_crawler.how_to_guide.md</p> </li> <li> <p>How to run IM app   /docs/datapull/ck.run_im_app.how_to_guide.md</p> </li> <li> <p>TODO(gp): Reorg   /research_amp/cc/notebooks/Master_single_vendor_qa.ipynb /research_amp/cc/notebooks/Master_model_performance_analyser.old.ipynb /research_amp/cc/notebooks/Master_machine_learning.ipynb /research_amp/cc/notebooks/Master_cross_vendor_qa.ipynb /research_amp/cc/notebooks/Master_model_performance_analyser.ipynb /research_amp/cc/notebooks/Master_crypto_analysis.ipynb /research_amp/cc/notebooks/Master_model_prediction_analyzer.ipynb /research_amp/cc/notebooks/Master_Analysis_CrossSectionalLearning.ipynb /im/app/notebooks/Master_IM_DB.ipynb /im/ib/metadata/extract/notebooks/Master_analyze_ib_metadata_crawler.ipynb</p> </li> </ul>"},{"location":"all.workflow.explanation.html#dataflow","title":"<code>DataFlow</code>","text":""},{"location":"all.workflow.explanation.html#meta","title":"Meta","text":"<ul> <li>Best practices for Quant research</li> <li>/docs/dataflow/ck.research_methodology.explanation.md</li> <li> <p>TODO(Grisha): <code>ck.*</code> -&gt; <code>all.*</code>?</p> </li> <li> <p>A description of all the available generic notebooks with a short description</p> </li> <li>/docs/dataflow/ck.master_notebooks.reference.md</li> <li>TODO(Grisha): does this belong to <code>DataFlow</code>?</li> <li>TODO(Grisha): <code>ck.master_notebooks...</code> -&gt; <code>all.master_notebooks</code>?</li> </ul>"},{"location":"all.workflow.explanation.html#dag","title":"DAG","text":"<ul> <li>General concepts of <code>DataFlow</code></li> <li>Introduction to KaizenFlow, DAG nodes, DataFrame as unit of computation, DAG     execution<ul> <li>/docs/dataflow/all.computation_as_graphs.explanation.md</li> </ul> </li> <li>DataFlow data format<ul> <li>/docs/dataflow/all.dataflow_data_format.explanation.md</li> </ul> </li> <li>Different views of System components, Architecture<ul> <li>/docs/dataflow/all.dataflow.explanation.md</li> </ul> </li> <li>Conventions for representing time series<ul> <li>/docs/dataflow/all.time_series.explanation.md</li> </ul> </li> <li> <p>Explanation of how to debug a DAG</p> <ul> <li>/docs/dataflow/all.dag.explanation.md</li> </ul> </li> <li> <p>Learn how to build a <code>DAG</code></p> </li> <li>Build a <code>DAG</code> with two nodes<ul> <li>/docs/dataflow/all.build_first_dag.tutorial.ipynb</li> </ul> </li> <li>Build a more complex <code>DAG</code> implementing a simple risk model<ul> <li>/docs/dataflow/all.build_simple_risk_model_dag.tutorial.ipynb</li> </ul> </li> <li> <p>Best practices to follow while building <code>DAG</code></p> <ul> <li>/docs/dataflow/all.best_practice_for_building_dags.explanation.md</li> </ul> </li> <li> <p>Learn how to run a <code>DAG</code></p> </li> <li>Overview, DagBuilder, Dag, DagRunner<ul> <li>/docs/dataflow/ck.run_batch_computation_dag.explanation.md</li> </ul> </li> <li>Configure a simple risk model, build a DAG, generate data and connect data     source to the DAG, run the DAG<ul> <li>/docs/dataflow/ck.run_batch_computation_dag.tutorial.ipynb</li> </ul> </li> <li> <p>Build a DAG from a Mock2 DagBuilder and run it</p> <ul> <li>/docs/kaizenflow/all.run_Mock2_pipeline_in_notebook.how_to_guide.ipynb</li> </ul> </li> <li> <p>General intro about model simulation</p> </li> <li>Property of tilability, batch vs streaming<ul> <li>/docs/dataflow/all.batch_and_streaming_mode_using_tiling.explanation.md</li> </ul> </li> <li>Time semantics, How clock is handled, Flows<ul> <li>/docs/dataflow/all.timing_semantic_and_clocks.md</li> </ul> </li> <li>Phases of evaluation of <code>Dag</code>s<ul> <li>/docs/dataflow/all.train_and_predict_phases.explanation.md</li> </ul> </li> <li> <p>Event study explanation</p> <ul> <li>/docs/dataflow/ck.event_study.explanation.md</li> </ul> </li> <li> <p>Run a simulation of a <code>DataFlow</code> system</p> </li> <li>Overview, Basic concepts, Implementation details<ul> <li>/docs/dataflow/ck.run_backtest.explanation.md</li> </ul> </li> <li>How to build a system, run research backtesting, Process results of     backtesting, How to run replayed time simulation, Running experiments<ul> <li>/docs/dataflow/ck.run_backtest.how_to_guide.md</li> </ul> </li> <li> <p>Simulation output explanation</p> <ul> <li>/docs/dataflow/all.simulation_output.reference.md</li> </ul> </li> <li> <p>Run a simulation sweep using a list of <code>Config</code> parameters</p> </li> <li>/docs/dataflow/ck.run_backtest.how_to_guide.md</li> <li>TODO(gp): @grisha do we have anything here? It's like the stuff that Dan     does</li> <li> <p>TODO(Grisha): @Dan, add a link to the doc here once it is ready</p> </li> <li> <p>Post-process the results of a simulation</p> </li> <li>Build the Config dict, Load tile results, Compute portfolio bar metrics,     Compute aggregate portfolio stats</li> <li>/dataflow/model/notebooks/Master_research_backtest_analyzer.ipynb</li> <li> <p>TODO(Grisha): is showcasing an example with fake data enough? We could use     Mock2 output</p> </li> <li> <p>Analyze a <code>DataFlow</code> model in details</p> </li> <li>Build Config, Initialize ModelEvaluator and ModelPlotter</li> <li>/dataflow/model/notebooks/Master_model_analyzer.ipynb</li> <li>TODO(gp): @grisha what is the difference with the other?</li> <li> <p>TODO(Grisha): ask Paul about the notebook</p> </li> <li> <p>Analyze features computed with <code>DataFlow</code></p> </li> <li>Read features from a Parquet file and perform some analysis<ul> <li>/dataflow/model/notebooks/Master_feature_analyzer.ipynb</li> </ul> </li> <li>TODO(gp): Grisha do we have a notebook that reads data from     ImClient/MarketData and performs some analysis?</li> <li> <p>TODO(Grisha): create a tutorial notebook for analyzing features using some     real (or close to real) data</p> </li> <li> <p>Mix multiple <code>DataFlow</code> models</p> </li> <li>/dataflow/model/notebooks/Master_model_mixer.ipynb</li> <li> <p>TODO(gp): add more comments</p> </li> <li> <p>Exporting PnL and trades</p> </li> <li>/dataflow/model/notebooks/Master_save_pnl_and_trades.ipynb</li> <li>/docs/dataflow/ck.export_alpha_data.explanation.md</li> <li>/docs/dataflow/ck.export_alpha_data.how_to_guide.md</li> <li>/docs/dataflow/ck.load_alpha_and_trades.tutorial.ipynb</li> <li>/docs/dataflow/ck.load_alpha_and_trades.tutorial.py</li> <li>TODO(gp): add more comments</li> </ul>"},{"location":"all.workflow.explanation.html#system","title":"System","text":"<ul> <li>Learn how to build <code>System</code></li> <li>TODO(gp): @grisha what do we have for this?</li> <li> <p>TODO(Grisha): add a tutorial notebook that builds a System and explain the     flow step-by-step</p> </li> <li> <p>Configure a full system using a <code>Config</code></p> </li> <li>Fill the <code>SystemConfig</code>, build all the components and run the <code>System</code></li> <li> <p>/docs/dataflow/system/all.use_system_config.tutorial.ipynb</p> </li> <li> <p>Create an ETL batch process using a <code>System</code></p> </li> <li>/dataflow_amp/system/risk_model_estimation/run_rme_historical_simulation.py</li> <li> <p>TODO(Grisha): add an explanation doc and consider converting into a Jupyter     notebook.</p> </li> <li> <p>Create an ETL real-time process</p> </li> <li>DagBuilder, Dag, DagRunner<ul> <li>/docs/dataflow/system/ck.build_real_time_dag.explanation.md</li> </ul> </li> <li>Build a DAG that runs in real time<ul> <li>/dataflow_amp/system/realtime_etl_data_observer/scripts/run_realtime_etl_data_observer.py</li> <li>TODO(Grisha): consider converting into a Jupyter notebook.</li> </ul> </li> <li> <p>Build a <code>System</code> that runs in real time</p> <ul> <li>/dataflow_amp/system/realtime_etl_data_observer/scripts/DataObserver_template.run_data_observer_simulation.py</li> <li>TODO(Grisha): consider converting into a Jupyter notebook.</li> </ul> </li> <li> <p>Batch simulation a Mock2 <code>System</code></p> </li> <li>Description of the forecast system, Description of the System, Run a     backtest, Explanation of the backtesting script, Analyze the results</li> <li>/docs/kaizenflow/all.run_Mock2_in_batch_mode.how_to_guide.md</li> <li>Build the config, Load tiled results, Compute portfolio bar metrics, Compute     aggregate portfolio stats</li> <li> <p>/docs/kaizenflow/all.analyze_Mock2_pipeline_simulation.how_to_guide.ipynb</p> </li> <li> <p>Run an end-to-end timed simulation of <code>Mock2</code> <code>System</code></p> </li> <li>/docs/kaizenflow/all.run_end_to_end_Mock2_system.tutorial.md</li> <li> <p>/dataflow_amp/system/mock2/scripts/run_end_to_end_Mock2_system.py</p> </li> <li> <p>TODO(gp): reorg the following files   /oms/notebooks/Master_PnL_real_time_observer.ipynb /oms/notebooks/Master_bid_ask_execution_analysis.ipynb /oms/notebooks/Master_broker_debugging.ipynb /oms/notebooks/Master_broker_portfolio_reconciliation.ipynb /oms/notebooks/Master_c1b_portfolio_vs_portfolio_reconciliation.ipynb /oms/notebooks/Master_dagger_reconciliation.ipynb /oms/notebooks/Master_execution_analysis.ipynb /oms/notebooks/Master_model_qualifier.ipynb /oms/notebooks/Master_multiday_system_reconciliation.ipynb /oms/notebooks/Master_portfolio_vs_portfolio_reconciliation.ipynb /oms/notebooks/Master_portfolio_vs_research_stats.ipynb /oms/notebooks/Master_system_reconciliation_fast.ipynb /oms/notebooks/Master_system_reconciliation_slow.ipynb /oms/notebooks/Master_system_run_debugger.ipynb</p> </li> </ul>"},{"location":"all.workflow.explanation.html#quant-dev-workflows","title":"Quant dev workflows","text":""},{"location":"all.workflow.explanation.html#datapull_1","title":"DataPull","text":"<ul> <li>Learn how to create a <code>DataPull</code> adapter for a new data source</li> <li>/docs/datapull/all.dataset_onboarding_checklist.reference.md</li> <li> <p>/docs/datapull/ck.add_new_data_source.how_to_guide.md</p> </li> <li> <p>How to update CCXT version</p> </li> <li> <p>/docs/datapull/all.update_CCXT_version.how_to_guide.md</p> </li> <li> <p>Download <code>DataPull</code> historical data</p> </li> <li> <p>?</p> </li> <li> <p>Onboard new exchange</p> </li> <li> <p>/docs/datapull/ck.onboarding_new_exchange.md</p> </li> <li> <p>Put a <code>DataPull</code> source in production with Airflow</p> </li> <li>/docs/datapull/ck.create_airflow_dag.tutorial.md<ul> <li>TODO(gp): This file is missing</li> </ul> </li> <li> <p>/docs/datapull/ck.develop_an_airflow_dag_for_production.explanation.md</p> <ul> <li>TODO(Juraj): See https://github.com/cryptokaizen/cmamp/issues/6444</li> </ul> </li> <li> <p>Add QA for a <code>DataPull</code> source</p> </li> <li> <p>Compare OHLCV bars</p> </li> <li>/im_v2/ccxt/data/client/notebooks/CmTask6537_One_off_comparison_of_Parquet_and_DB_OHLCV_data.ipynb</li> <li> <p>TODO(Grisha): review and generalize</p> </li> <li> <p>How to import <code>Bloomberg</code> historical data</p> </li> <li> <p>/docs/datapull/ck.process_historical_data_without_dataflow.tutorial.ipynb</p> </li> <li> <p>How to import <code>Bloomberg</code> real-time data</p> </li> <li> <p>TODO(*): add doc.</p> </li> <li> <p>TODO(gp): Add docs   /docs/datapull/ck.binance_trades_data_pipeline.explanation.md /docs/datapull/ck.database_schema_update.how_to_guide.md /docs/datapull/ck.datapull.explanation.md /docs/datapull/ck.relational_database.explanation.md</p> </li> </ul>"},{"location":"all.workflow.explanation.html#dataflow_1","title":"DataFlow","text":"<ul> <li>All software components</li> <li>/docs/dataflow/ck.data_pipeline_architecture.reference.md</li> </ul>"},{"location":"all.workflow.explanation.html#tradingops-workflows","title":"TradingOps workflows","text":""},{"location":"all.workflow.explanation.html#trading-execution","title":"Trading execution","text":""},{"location":"all.workflow.explanation.html#intro","title":"Intro","text":"<ul> <li>Binance trading terms</li> <li>/docs/oms/broker/ck.binance_terms.reference.md</li> </ul>"},{"location":"all.workflow.explanation.html#components","title":"Components","text":"<ul> <li>OMS explanation</li> <li>/docs/oms/ck.oms.explanation.md</li> <li>CCXT log structure</li> <li>/docs/oms/broker/ck.ccxt_broker_logs_schema.reference.md</li> </ul>"},{"location":"all.workflow.explanation.html#testing","title":"Testing","text":"<ul> <li>Replayed CCXT exchange explanation</li> <li>/docs/oms/broker/ck.replayed_ccxt_exchange.explanation.md</li> <li>How to generate broker test data</li> <li>/docs/oms/broker/ck.generate_broker_test_data.how_to_guide.md</li> </ul>"},{"location":"all.workflow.explanation.html#procedures","title":"Procedures","text":"<ul> <li>Trading procedures (e.g., trading account information)</li> <li>/docs/trading_ops/ck.trading.how_to_guide.md</li> <li>How to run broker only/full system experiments</li> <li>/docs/trading_ops/ck.trade_execution_experiment.how_to_guide.md</li> <li>Execution notebooks explanation</li> <li>/docs/oms/broker/ck.execution_notebooks.explanation.md</li> </ul>"},{"location":"all.workflow.explanation.html#mlops-workflows","title":"MLOps workflows","text":"<ul> <li>Encrypt a model</li> <li>/docs/dataflow/ck.release_encrypted_models.explanation.md</li> <li>/docs/dataflow/ck.release_encrypted_models.how_to_guide.md</li> </ul>"},{"location":"all.workflow.explanation.html#deploying","title":"Deploying","text":"<ul> <li>Model deployment in production</li> <li>/docs/deploying/all.model_deployment.how_to_guide.md</li> <li>Run production system</li> <li>/docs/deploying/ck.run_production_system.how_to_guide.md</li> <li>Model references</li> <li>/docs/deploying/ck.supported_models.reference.md</li> </ul>"},{"location":"all.workflow.explanation.html#monitoring","title":"Monitoring","text":"<ul> <li>Monitor system</li> <li>/docs/monitoring/ck.monitor_system.how_to_guide.md</li> <li>System reconciliation explanation</li> <li>/docs/monitoring/ck.system_reconciliation.explanation.md</li> <li>System Reconciliation How to guide</li> <li>/docs/monitoring/ck.system_reconciliation.how_to_guide.md</li> </ul>"},{"location":"all.workflow.explanation.html#devops-workflows","title":"DevOps workflows","text":"<p>The documentation outlines the architecture and deployment processes for the Kaizen Infrastructure, leveraging a blend of AWS services, Kubernetes for container orchestration, and traditional EC2 for virtualized computing. Emphasizing Infrastructure as Code (IaC), the project employs Terraform for provisioning and Ansible for configuration, ensuring a maintainable and replicable environment.</p>"},{"location":"all.workflow.explanation.html#overview","title":"Overview","text":"<ul> <li>Development and deployment stages</li> <li> <p>/docs/infra/ck.development_stages.explanation.md</p> </li> <li> <p>S3 Buckets overview</p> </li> <li>/docs/infra/ck.s3_buckets.explanation.md</li> <li>This document provides an overview of the S3 buckets utilized by Kaizen     Technologies.</li> </ul>"},{"location":"all.workflow.explanation.html#current-set-up-description","title":"Current set-up description","text":"<ul> <li>Document details steps for setting up Kaizen infrastructure</li> <li> <p>/docs/infra/ck.kaizen_infrastructure.reference.md</p> </li> <li> <p>EC2 servers overview</p> </li> <li>/docs/infra/ck.ec2_servers.explanation.md</li> </ul>"},{"location":"all.workflow.explanation.html#set-up-infra","title":"Set up infra","text":"<ul> <li>Document the implementation of Auto Scaling in the Kubernetes setup, focusing   on the Cluster Autoscaler (CA), Horizontal Pod Autoscaler (HPA), and Auto   Scaling Groups (ASG)</li> <li> <p>/docs/infra/all.auto_scaling.explanation.md</p> </li> <li> <p>Compare AWS RDS instance types and storage performance</p> </li> <li> <p>/docs/infra/all.rds.comparison.md</p> </li> <li> <p>Setup S3 buckets with Terraform</p> </li> <li> <p>/docs/infra/ck.set_up_s3_buckets.how_to_guide.md</p> </li> <li> <p>AWS API Key rotation guide</p> </li> <li> <p>/docs/infra/ck.aws_api_key_rotation.how_to_guide.md</p> </li> <li> <p>Amazon Elastic File System (EFS) overview</p> </li> <li> <p>/docs/infra/ck.aws_elastic_file_system.explanation.md</p> </li> <li> <p>Client VPN endpoint creation with Terraform</p> </li> <li> <p>/docs/infra/ck.create_client_vpn_endpoint.how_to_guide.md</p> </li> <li> <p>Set-up AWS Client VPN</p> </li> <li> <p>/docs/infra/ck.set_up_aws_client_vpn.how_to_guide.md</p> </li> <li> <p>Utility server application set-up overview</p> </li> <li> <p>/docs/infra/ck.set_up_utility_server_app.how_to_guide.md</p> </li> <li> <p>Storing secret information (API keys, login credentials, access tokens etc.)</p> </li> <li>/docs/infra/ck.storing_secrets.explanation.md</li> </ul>"},{"location":"meta.html","title":"Files","text":"<ul> <li><code>docs/meta.md</code></li> <li> <p>This file contains rules and conventions for all the documentation under     <code>docs</code>.</p> </li> <li> <p><code>docs/all.code_organization.reference.md</code></p> </li> <li> <p>Describe how the code is organized in terms of components, libraries, and     directories</p> </li> <li> <p><code>docs/all.software_components.reference.md</code></p> </li> <li> <p>List of all the software components in the codebase</p> </li> <li> <p><code>docs/all.workflow.explanation.md</code></p> </li> <li>Describe all the workflows for quants, quant devs, and devops</li> </ul>"},{"location":"meta.html#current-dir-structure","title":"Current dir structure","text":"<p>Please keep the directory in a conceptual order.</p> <p>The current dir structure of <code>docs</code> is:</p>"},{"location":"meta.html#meta","title":"Meta","text":"<ul> <li><code>documentation_meta</code></li> <li>How to write documentation for code and workflows.</li> </ul>"},{"location":"meta.html#processes","title":"Processes","text":"<ul> <li><code>onboarding</code></li> <li>Practicalities of on-boarding new team members.</li> <li> <p>E.g., things typically done only once at the beginning of joining the team.</p> </li> <li> <p><code>work_organization</code></p> </li> <li>How the work is organized on a general level</li> <li> <p>E.g., the company's adopted practices spanning coding and development</p> </li> <li> <p><code>work_tools</code></p> </li> <li>How to set up, run and use various software needed for development</li> <li> <p>E.g., IDE</p> </li> <li> <p><code>general_background</code></p> </li> <li> <p>Documents that provide general reference information, often across different     topics (e.g., glossaries, reading lists).</p> </li> <li> <p><code>coding</code></p> </li> <li>Guidelines and good practices for coding and code-adjacent activities (such     as code review)</li> <li>This includes general tips and tricks that are useful for anybody writing     any code (e.g., how to use type hints) as well as in-depth descriptions of     specific functions and libraries.</li> </ul>"},{"location":"meta.html#software-components","title":"Software components","text":"<ul> <li><code>kaizenflow</code></li> <li> <p>Docs related to high-level packages that are used across the codebase (e.g.,     <code>helpers</code>, <code>config</code>), as well as overall codebase organization.</p> </li> <li> <p><code>datapull</code></p> </li> <li> <p>Docs related to dataset handling: downloading, onboarding, interpretation,     etc.</p> </li> <li> <p><code>dataflow</code></p> </li> <li> <p>Docs related to the framework of implementing and running machine learning     models.</p> </li> <li> <p><code>trade_execution</code></p> </li> <li> <p>Docs related to placing and monitoring trading orders to market or broker.</p> </li> <li> <p><code>infra</code></p> </li> <li>Docs related to the company\u2019s infrastructure: AWS services, code deployment,     monitoring, server administration, etc.</li> </ul>"},{"location":"meta.html#papers","title":"Papers","text":"<ul> <li><code>papers</code></li> <li>Papers written by the team members about the company's products and     know-how.</li> </ul>"},{"location":"meta.html#how-to-organize-the-docs","title":"How to organize the docs","text":"<ul> <li>Documentation can be organized in multiple ways:</li> <li>By software component</li> <li>By functionality (e.g., infra, backtesting)</li> <li> <p>By team (e.g., trading ops)</p> </li> <li> <p>We have decided that</p> </li> <li>For each software component there should be a corresponding documentation</li> <li>We have documentation for each functionality and team</li> </ul>"},{"location":"meta.html#dir-vs-no-dirs","title":"Dir vs no-dirs","text":"<ul> <li>Directories make it difficult to navigate the docs</li> <li>We use \u201cname spaces\u201d until we have enough objects to create a dir</li> </ul>"},{"location":"meta.html#_1","title":"Files","text":"<ul> <li>Doc needs to be reviewed \"actively\", e.g., by making sure someone checks them   in the field</li> <li>Somebody should verify that is \"executable\"</li> </ul>"},{"location":"meta.html#tracking-reviews-and-improvements","title":"Tracking reviews and improvements","text":"<ul> <li> <p>There is a   Master Documentation Gdoc   that contains a list of tasks related to documentation, including what needs   to be reviewed</p> </li> <li> <p>For small action items we add a markdown TODO like we do for the code   ```   </p> </li> </ul> <p>```</p> <ul> <li>To track the last revision we use a tag at the end of the document like:   <code>markdown   Last review: GP on 2024-04-20, ...</code></li> </ul>"},{"location":"meta.html#how-to-search-the-documentation","title":"How to search the documentation","text":"<ul> <li> <p>Be patient and assume that the documentation is there, but you can't find it   because you are not familiar with it and not because you think the   documentation is poorly done or not organized</p> </li> <li> <p>Look for files that contain words related to what you are looking for</p> </li> <li>E.g., <code>ffind.py XYZ</code></li> <li>Grep in the documentation looking for words related to what you are looking   for</li> <li>E.g., <code>jackmd trading</code></li> <li>Scan through the content of the references</li> <li>E.g., <code>all.code_organization.reference.md</code></li> <li>Grep for the name of a tool in the documentation</li> </ul>"},{"location":"meta.html#how-to-ensure-that-all-the-docs-are-cross-referenced-in-the-indices","title":"How to ensure that all the docs are cross-referenced in the indices?","text":"<ul> <li>There is a script to check and update the documentation cross-referencing   files in a directory and a file with all the links to the files</li> </ul> <p>/Users/saggese/src/dev_tools1/linters/amp_fix_md_links.py docs/all.amp_fix_md_links.explanation.md</p> <ul> <li>How to point mistakes in a md file?</li> </ul>"},{"location":"meta.html#list-of-files","title":"List of files","text":"<pre><code>&gt; tree docs -I '*figs*|test*' --dirsfirst -n -F --charset unicode | grep -v __init__.py\n</code></pre> <p>The current structure of files are:</p> <pre><code>docs/\n|-- build/\n|   |-- all.linter_gh_workflow.explanation.md\n|   |-- all.pytest_allure.explanation.md\n|   |-- all.pytest_allure.how_to_guide.md\n|   |-- all.semgrep_workflow.explanation.md\n|   |-- ck.gh_workflows.explanation.md\n|   `-- ck.gh_workflows.reference.md\n|-- coding/\n|   |-- all.asyncio.explanation.md\n|   |-- all.code_design.how_to_guide.md\n|   |-- all.code_like_pragmatic_programmer.how_to_guide.md\n|   |-- all.code_review.how_to_guide.md\n|   |-- all.coding_style.how_to_guide.md\n|   |-- all.gsheet_into_pandas.how_to_guide.md\n|   |-- all.hcache.explanation.md\n|   |-- all.hgoogle_file_api.explanation.md\n|   |-- all.hplayback.how_to_guide.md\n|   |-- all.imports_and_packages.how_to_guide.md\n|   |-- all.integrate_repos.how_to_guide.md\n|   |-- all.jupyter_notebook.how_to_guide.md\n|   |-- all.plotting.how_to_guide.md\n|   |-- all.profiling.how_to_guide.md\n|   |-- all.publish_notebook.how_to_guide.md\n|   |-- all.qgrid.how_to_guide.md\n|   |-- all.reading_other_people_code.how_to_guide.md\n|   |-- all.run_unit_tests.how_to_guide.md\n|   |-- all.str_to_df.how_to_guide.md\n|   |-- all.submit_code_for_review.how_to_guide.md\n|   |-- all.type_hints.how_to_guide.md\n|   `-- all.write_unit_tests.how_to_guide.md\n|-- dataflow/\n|   |-- system/\n|   |   |-- all.use_system_config.tutorial.ipynb\n|   |   |-- all.use_system_config.tutorial.py\n|   |   |-- ck.build_real_time_dag.explanation.md\n|   |   |-- ck.build_real_time_dag.tutorial.ipynb\n|   |   `-- ck.build_real_time_dag.tutorial.py\n|   |-- all.batch_and_streaming_mode_using_tiling.explanation.md\n|   |-- all.best_practice_for_building_dags.explanation.md\n|   |-- all.build_first_dag.tutorial.ipynb\n|   |-- all.build_first_dag.tutorial.py\n|   |-- all.build_simple_risk_model_dag.tutorial.ipynb\n|   |-- all.build_simple_risk_model_dag.tutorial.py\n|   |-- all.computation_as_graphs.explanation.md\n|   |-- all.dag.explanation.md\n|   |-- all.dataflow.explanation.md\n|   |-- all.dataflow_data_format.explanation.md\n|   |-- all.simulation_output.reference.md\n|   |-- all.time_series.explanation.md\n|   |-- all.timing_semantic_and_clocks.md\n|   |-- all.train_and_predict_phases.explanation.md\n|   |-- ck.data_pipeline_architecture.reference.md\n|   |-- ck.event_study.explanation.md\n|   |-- ck.export_alpha_data.explanation.md\n|   |-- ck.export_alpha_data.how_to_guide.md\n|   |-- ck.load_alpha_and_trades.tutorial.ipynb\n|   |-- ck.load_alpha_and_trades.tutorial.py\n|   |-- ck.master_notebooks.reference.md\n|   |-- ck.release_encrypted_models.explanation.md\n|   |-- ck.release_encrypted_models.how_to_guide.md\n|   |-- ck.research_methodology.explanation.md\n|   |-- ck.run_backtest.explanation.md\n|   |-- ck.run_backtest.how_to_guide.md\n|   |-- ck.run_batch_computation_dag.explanation.md\n|   |-- ck.run_batch_computation_dag.tutorial.ipynb\n|   |-- ck.run_batch_computation_dag.tutorial.py\n|   `-- ck.system_design.explanation.md\n|-- datapull/\n|   |-- all.datapull_client_stack.explanation.md\n|   |-- all.datapull_derived_data.explanation.md\n|   |-- all.datapull_qa_flow.explanation.md\n|   |-- all.datapull_sandbox.explanation.md\n|   |-- all.dataset_onboarding_checklist.reference.md\n|   |-- all.im_client.reference.ipynb\n|   |-- all.im_client.reference.py\n|   |-- all.market_data.reference.ipynb\n|   |-- all.market_data.reference.py\n|   |-- all.update_CCXT_version.how_to_guide.md\n|   |-- ck.add_new_data_source.how_to_guide.md\n|   |-- ck.binance_bid_ask_data_pipeline.explanation.md\n|   |-- ck.binance_ohlcv_data_pipeline.explanation.md\n|   |-- ck.binance_trades_data_pipeline.explanation.md\n|   |-- ck.ccxt_exchange_timestamp_interpretation.reference.md\n|   |-- ck.create_airflow_dag.tutorial.md\n|   |-- ck.datapull.explanation.md\n|   |-- ck.datapull_data_quality_assurance.reference.md\n|   |-- ck.develop_an_airflow_dag_for_production.explanation.md\n|   |-- ck.handle_datasets.how_to_guide.md\n|   |-- ck.kibot_data.explanation.md\n|   |-- ck.kibot_timing.reference.md\n|   |-- ck.onboarding_new_exchnage.md\n|   |-- ck.process_historical_data_without_dataflow.tutorial.ipynb\n|   |-- ck.process_historical_data_without_dataflow.tutorial.py\n|   |-- ck.run_ib_connect.how_to_guide.md\n|   |-- ck.run_im_app.how_to_guide.md\n|   |-- ck.universe.explanation.md\n|   `-- ck.use_ib_metadata_crawler.how_to_guide.md\n|-- deploying/\n|   |-- all.model_deployment.how_to_guide.md\n|   |-- ck.run_production_system.how_to_guide.md\n|   `-- ck.supported_models.reference.md\n|-- dev_tools/\n|   `-- thin_env/\n|       `-- all.gh_and_thin_env_requirements.reference.md\n|-- documentation_meta/\n|   |-- all.architecture_diagrams.explanation.md\n|   |-- all.diataxis.explanation.md\n|   |-- all.writing_docs.how_to_guide.md\n|   `-- plotting_in_latex.how_to_guide.md\n|-- general_background/\n|   |-- all.common_abbreviations.reference.md\n|   |-- all.glossary.reference.md\n|   |-- all.literature_review.reference.md\n|   |-- all.reading_list.reference.md\n|   `-- ck.mailing_groups.reference.md\n|-- infra/\n|   |-- all.auto_scaling.explanation.md\n|   |-- all.aws_scripts.reference.md\n|   |-- all.rds.comparison.md\n|   |-- ck.aws_api_key_rotation.how_to_guide.md\n|   |-- ck.aws_elastic_file_system.explanation.md\n|   |-- ck.create_client_vpn_endpoint.how_to_guide.md\n|   |-- ck.development_stages.explanation.md\n|   |-- ck.ec2_servers.explanation.md\n|   |-- ck.kaizen_infrastructure.reference.md\n|   |-- ck.s3_buckets.explanation.md\n|   |-- ck.services.reference.md\n|   |-- ck.set_up_aws_client_vpn.how_to_guide.md\n|   |-- ck.set_up_s3_buckets.how_to_guide.md\n|   |-- ck.set_up_utility_server_app.how_to_guide.md\n|   |-- ck.sorrentum_experiments_aws_infrastructure.explanation.md\n|   |-- ck.storing_secrets.explanation.md\n|   `-- ck.use_htmlcov_server.how_to_guide.md\n|-- kaizenflow/\n|   |-- all.analyze_Mock2_pipeline_simulation.how_to_guide.ipynb\n|   |-- all.analyze_Mock2_pipeline_simulation.how_to_guide.py\n|   |-- all.dev_scripts_catalogue.reference.md\n|   |-- all.devops_code_organization.reference.md\n|   |-- all.install_helpers.how_to_guide.md\n|   |-- all.run_Mock2_in_batch_mode.how_to_guide.md\n|   |-- all.run_Mock2_pipeline_in_notebook.how_to_guide.ipynb\n|   |-- all.run_Mock2_pipeline_in_notebook.how_to_guide.py\n|   |-- all.run_end_to_end_Mock2_system.tutorial.md\n|   |-- ck.config.explanation.md\n|   `-- ck.system_config.explanation.md\n|-- monitoring/\n|   |-- ck.monitor_system.how_to_guide.md\n|   |-- ck.system_reconciliation.explanation.md\n|   `-- ck.system_reconciliation.how_to_guide.md\n|-- onboarding/\n|   |-- admin.onboarding_process.md\n|   |-- all.communicate_in_telegram.how_to_guide.md\n|   |-- all.development_documents.reference.md\n|   |-- all.organize_email.how_to_guide.md*\n|   |-- all.receive_crypto_payment.how_to_guide.md\n|   |-- all.track_time_with_hubstaff.how_to_guide.md\n|   |-- ck.development_setup.how_to_guide.md\n|   |-- ck.setup_vpn_and_dev_server_access.how_to_guide.md\n|   |-- sorrentum.hiring_meister.how_to_guide.md\n|   |-- sorrentum.set_up_development_environment.how_to_guide.md\n|   `-- sorrentum.signing_up.how_to_guide.md\n|-- oms/\n|   |-- broker/\n|   |   |-- ck.ccxt_broker_logs_schema.reference.md\n|   |   |-- ck.generate_broker_test_data.how_to_guide.md\n|   |   |-- ck.replayed_ccxt_exchange.explanation.md\n|   |   |-- ck.binance_terms.reference.md\n|   |-- child_order_quantity_computer/\n|   |-- limit_price_computer/\n|   |-- order/\n|   |-- ck.oms.explanation.md\n|   |-- ck.execution_notebooks.explanation.md\n|-- trade_execution/\n|   |-- ccxt_notes.txt\n|   |-- ck.git_branches.reference.md\n|   |-- ck.model_configurations.reference.md\n|   |-- ck.trade_execution_experiment.how_to_guide.md\n|   |-- ck.trading.how_to_guide.md\n|   |-- ck.trading.onboarding_new_binance_trading_account.how_to_guide.ipynb\n|   `-- ck.trading.onboarding_new_binance_trading_account.how_to_guide.py\n|-- work_organization/\n|   |-- all.buildmeister.how_to_guide.md\n|   |-- all.contributor_scoring.how_to_guide.md\n|   |-- all.epicmeister.how_to_guide.md\n|   |-- all.rollout.how_to_guide.md\n|   |-- all.scrum.explanation.md\n|   |-- all.team_collaboration.how_to_guide.md\n|   |-- all.use_github_and_zenhub.how_to_guide.md\n|   |-- ck.roles_and_responsibilities.md\n|   `-- sorrentum.organize_your_work.how_to_guide.md\n|-- work_tools/\n|   |-- all.add_toc_to_notebook.how_to_guide.md\n|   |-- all.bfg_repo_cleaner.how_to_guide.md\n|   |-- all.chatgpt_api.how_to_guide.md\n|   |-- all.codebase_clean_up.how_to_guide.md\n|   |-- all.conda_environment_obsolete.how_to_guide.md\n|   |-- all.development.how_to_guide.md\n|   |-- all.docker.how_to_guide.md\n|   |-- all.dockerhub.how_to_guide.md\n|   |-- all.git.how_to_guide.md\n|   |-- all.invoke_workflows.how_to_guide.md\n|   |-- all.jupytext.how_to_guide.md\n|   |-- all.latex.how_to_guide.md\n|   |-- all.parquet.explanation.md\n|   |-- all.pycharm.how_to_guide.md\n|   |-- all.ssh.how_to_guide.md\n|   |-- all.telegram_notify_bot.how_to_guide.md\n|   `-- all.visual_studio_code.how_to_guide.md\n|-- all.code_organization.reference.md\n|-- all.components.reference.md\n|-- all.software_components.reference.md\n|-- all.workflow.explanation.md\n`-- meta.md\n\n18 directories, 204 files\n</code></pre> <p>Last review: GP on 2024-04-29</p>"},{"location":"build/all.linter_gh_workflow.explanation.html","title":"Linter Gh Workflow","text":""},{"location":"build/all.linter_gh_workflow.explanation.html#linter-github-action-workflow-explanation","title":"Linter Github Action Workflow Explanation","text":""},{"location":"build/all.linter_gh_workflow.explanation.html#overview","title":"Overview","text":"<ul> <li>We want to use linter for all the new code that needs to be merged into the   <code>master</code> branch</li> <li>This is implemented by running the GitHub Actions workflow, <code>linter.yml</code>,   which is executed against the changed or added files in the branch</li> <li>If the linter detects changes in the new files, it indicates that the linter   did not run before.</li> <li>In this case, the workflow will fail, and will not allow the PR to be merged</li> </ul>"},{"location":"build/all.linter_gh_workflow.explanation.html#how-it-works","title":"How it works","text":""},{"location":"build/all.linter_gh_workflow.explanation.html#fetch-master-branch","title":"Fetch master branch","text":"<p>In order to compare the changed files in the PR with the latest master branch, fetch the latest master, e.g.,</p> <pre><code>invoke git_fetch_master\n</code></pre>"},{"location":"build/all.linter_gh_workflow.explanation.html#run-the-linter-and-check-the-linter-results","title":"Run the linter and check the linter results","text":"<ul> <li>Run the linter against the changed files in the PR branch</li> </ul> <pre><code>invoke lint --branch\n</code></pre> <ul> <li>Check if the git client is clean</li> </ul> <pre><code>git status\n</code></pre> <ul> <li>If the git client is not clean, abort the execution and the workflow will fail</li> <li>If the git client is clean, the workflow will exit successfully</li> </ul> <p>Invoke task for this action is:</p> <pre><code>invoke lint_check_if_it_was_run\n</code></pre>"},{"location":"build/all.pytest_allure.explanation.html","title":"Pytest Allure","text":""},{"location":"build/all.pytest_allure.explanation.html#pytest-allure-explanantion","title":"Pytest Allure Explanantion","text":""},{"location":"build/all.pytest_allure.explanation.html#overview","title":"Overview","text":"<ul> <li>Allure Report boosts collaboration and project quality by providing clear,   detailed test reports that aid issue resolution for different team members</li> <li>The goal is to set up all our tests to generate and publish this report</li> <li>Here is the demo report   from allure</li> <li>The simplified flow for the Allure report is:</li> </ul> <pre><code>flowchart TD\n\nsubgraph A1[\" \"]\ndirection LR\n    A[Run tests and generate\\nthe Allure output] --&gt; B[(Backup the Allure output\\nto the AWS S3 bucket)]\n    B --&gt; C(Copy last history\\nfrom the S3 bucket\\nto the Allure output folder)\nend\nsubgraph B1[\" \"]\ndirection LR\n    D[Generate the HTML-report\\nfrom the Allure output] --&gt; E[(Publish the HTML-report\\nto the S3 HTML bucket)]\nend\nA1 ==&gt; B1 ==&gt; C1[View the report in the browser]\n</code></pre>"},{"location":"build/all.pytest_allure.explanation.html#core-features","title":"Core features","text":""},{"location":"build/all.pytest_allure.explanation.html#rich-and-interactive-reports","title":"Rich and Interactive Reports:","text":"<ul> <li>Allure generates visually appealing and interactive HTML reports, making it   easy to analyze test results</li> <li>Reports include detailed information about test cases, steps, attachments, and   more</li> </ul>"},{"location":"build/all.pytest_allure.explanation.html#annotations-and-labels","title":"Annotations and Labels:","text":"<ul> <li>Allure uses annotations and labels to provide additional information about   test methods, making it easier to understand and categorize test results</li> <li>Annotations are used to mark and describe test methods, and labels help in   categorizing and filtering tests</li> </ul>"},{"location":"build/all.pytest_allure.explanation.html#test-history-and-trends","title":"Test History and Trends","text":"<ul> <li>Allure maintains a history of test runs, allowing you to track changes in test   results over time</li> <li>Trends and statistics help identify patterns, improvements, or regressions in   the application's behavior</li> </ul>"},{"location":"build/all.pytest_allure.explanation.html#key-components","title":"Key Components","text":""},{"location":"build/all.pytest_allure.explanation.html#pytest-plugin-for-the-allure-output-generation","title":"Pytest plugin for the Allure output generation","text":"<p>In order to generate the Allure output, we need to install the <code>allure-pytest</code> plugin. For the time and efforts saving reasons, we will install it on-the-fly in the container where we will run the tests. This feature is introduced as the <code>allure_dir</code> parameter to the <code>run_&lt;type&gt;_tests</code> tasks. If specified the <code>allure-pytest</code> plugin will be installed and the results will be stored in the specified directory.</p>"},{"location":"build/all.pytest_allure.explanation.html#allure-reporting-tool","title":"Allure reporting tool","text":"<p>The CLI utility for generating Allure reports creates an HTML report from the Allure output. We'll install this utility using GitHub Actions workflow to generate the HTML report.</p>"},{"location":"build/all.pytest_allure.explanation.html#how-it-works","title":"How it works","text":"<ul> <li>Allure Report is composed of a framework(pytest) adapter and the allure   command-line utility</li> <li>Run the tests with pytest options <code>--alluredir=allure-results</code> to store the   results</li> <li>The generated files include:</li> <li>Test result files     to describe execution of tests</li> <li>Container files     to describe test fixtures</li> <li>If required, add additional files to the <code>allure-results</code> to display   additional info in the report</li> <li>Additional files includes:</li> <li>Environment file     to store some global data for all the tests in the report</li> <li>History files     to enable trend graphs and other history-related features</li> <li>Categories file     to group test results into custom categories</li> <li>Use <code>allure generate</code> to generate the test report into the specified   directory. For e.g.: <code>allure generate allure-report</code></li> </ul>"},{"location":"build/all.pytest_allure.explanation.html#historical-trends","title":"Historical Trends","text":"<p>A test report generated by Allure can not only display data about the latest test launch, but also help you compare it with the data from previous reports. To do so, Allure can keep a history or previous reports.</p> <p>In a tests report with the history included, you can:</p> <ul> <li>See what statuses did a test have previously (see Details panel \u2192 History tab)</li> <li>Find tests that changed status since last report (see Sorting and filtering \u2192   Filter tests by marks)</li> <li>See how specific values change over time (see Graphs)</li> </ul>"},{"location":"build/all.pytest_allure.how_to_guide.html","title":"Pytest Allure","text":""},{"location":"build/all.pytest_allure.how_to_guide.html#pytest-allure-how-to-guide","title":"Pytest Allure How to Guide","text":""},{"location":"build/all.pytest_allure.how_to_guide.html#how-to-run-the-flow-end-to-end-via-gh-actions","title":"How to run the flow end-to-end via GH actions","text":"<p>Considering that we run the tests on the <code>cmamp</code> repo with the fast <code>tests</code> group</p> <p>Important note: Unlike usual test run, we don't stop the execution on failure. For this we use the <code>continue-on-error: true</code> in the GitHub action step.</p> <p>Here is the link to the GitHub action file.</p>"},{"location":"build/all.pytest_allure.how_to_guide.html#how-to-generate-allure-pytest-results","title":"How to generate allure-pytest results","text":"<p>To save Allure results after a test run, append <code>--allure-dir</code> parameter to a <code>pytest</code> cmd, e.g.,</p> <pre><code>i run_fast_tests ... --allure-dir ./allure_results\n</code></pre> <p>where <code>allure-dir</code> is the directory where the Allure results will be stored.</p>"},{"location":"build/all.pytest_allure.how_to_guide.html#how-to-backup-the-allure-results","title":"How to backup the Allure results","text":"<p>To backup the Allure results, copy the <code>allure_results</code> directory to a AWS S3 bucket, e.g.,</p> <pre><code>aws s3 cp allure_results s3://cryptokaizen-unit-test/allure_test/cmamp/fast/results.20231120_102030 --recursive\n</code></pre> <p>where:</p> <ul> <li><code>allure_results</code> is the directory where the Allure results are stored</li> <li><code>20231120_102030</code> is the date and time with the mask <code>%Y%m%d_%H%M%S</code></li> <li><code>cmamp</code> is the name of the GitHub repo</li> <li><code>fast</code> is the name of the tests group</li> </ul>"},{"location":"build/all.pytest_allure.how_to_guide.html#how-to-generate-allure-html-report","title":"How to generate Allure HTML-report","text":"<ul> <li>Whenever Allure generates a test report in a specified directory i.e.   <code>allure-report</code> (refer to the   \"How it works\" section), it   concurrently produces data inside the history subdirectory</li> <li>The JSON files within the history subdirectory preserve all the necessary   information for Allure to use the data from this test report into the next   test report, see   History files</li> </ul> <p>To install the Allure CLI utility, refer to the official docs.</p> <p>In order to generate the HTML report, run the following command:</p> <pre><code>allure generate allure_results -o allure_report\n</code></pre> <p>where:</p> <ul> <li><code>allure_results</code> is the directory where the Allure results are stored</li> <li><code>allure_report</code> is the directory where the Allure HTML-report will be stored</li> </ul> <p>TODO(Vlad): Come up with a clean-up strategy for the S3 bucket.</p>"},{"location":"build/all.pytest_allure.how_to_guide.html#keep-the-history-of-test-runs","title":"Keep the history of test runs","text":"<ul> <li>To activate the features related to history, copy the history subdirectory   from the previous report into the latest test results directory before   generating the subsequent test report</li> <li>Here is an example of how to do it, assuming that your project is configured   to use <code>allure-results</code> and <code>allure-report</code> directories:</li> <li>Make sure you have the previous report generated in the <code>allure-report</code>     directory</li> <li>Remove the <code>allure-results</code> directory</li> <li>Run the tests with the option <code>--allure-dir allure_results</code></li> <li>Copy the <code>allure-report/history</code> subdirectory to <code>allure-results/history</code></li> <li>Generate the new report</li> </ul> <p>To copy the history subdirectory from the previous run to the <code>allure_results</code>:</p> <pre><code>aws s3 cp s3://cryptokaizen-html/allure_reports/cmamp/fast/report.20231120_102030/history allure_results/history --recursive\n</code></pre>"},{"location":"build/all.pytest_allure.how_to_guide.html#how-to-publish-the-allure-html-report","title":"How to publish the Allure-HTML report","text":"<p>To publish the Allure report, copy the <code>allure_report</code> directory to a AWS S3 bucket, e.g.,</p> <pre><code>aws s3 cp allure_report s3://cryptokaizen-html/allure_reports/cmamp/fast/report.20231120_102030 --recursive\n</code></pre> <p>where:</p> <ul> <li><code>allure_report</code> is the directory where the Allure HTML-report will be stored</li> <li><code>20231120_102030</code> is the date and time with the mask <code>%Y%m%d_%H%M%S</code></li> <li><code>cmamp</code> is the name of the GitHub repo</li> <li><code>fast</code> is the name of the tests group</li> </ul> <p>For e.g., to access the HTML-report open this link on a browser: http://172.30.2.44/allure_reports/cmamp/fast/report.20231120_102030</p>"},{"location":"build/all.semgrep_workflow.explanation.html","title":"Semgrep Integration in GitHub Actions","text":""},{"location":"build/all.semgrep_workflow.explanation.html#table-of-contents","title":"Table of Contents","text":""},{"location":"build/all.semgrep_workflow.explanation.html#overview","title":"Overview","text":"<p>This document provides an overview and guidance on the integration of Semgrep, a static analysis tool, into the GitHub Actions workflow. Semgrep is used for identifying issues and vulnerabilities in the codebase automatically during the development process.</p>"},{"location":"build/all.semgrep_workflow.explanation.html#features","title":"Features","text":"<ul> <li>Automatic Scanning: Semgrep runs automatically on every pull request to   the <code>master</code> branch and for every push to the <code>master</code> branch. This ensures   that new code is checked before merging.</li> <li>Scheduled Scans: Additionally, Semgrep scans are scheduled to run once a   day, ensuring regular codebase checks even without new commits.</li> <li>Workflow Dispatch: The integration allows for manual triggering of the   Semgrep scan, providing flexibility for ad-hoc code analysis.</li> </ul>"},{"location":"build/all.semgrep_workflow.explanation.html#setup","title":"Setup","text":"<ul> <li>GitHub Action Workflow: The Semgrep integration is set up as a part of the   GitHub Actions workflow in the <code>semgrep.yml</code>   file.</li> <li>Running Environment: The workflow runs on <code>ubuntu-latest</code> and uses the   <code>returntocorp/semgrep</code> container.</li> <li>Semgrep Rules: The current configuration uses the <code>p/secrets</code> rule pack   from Semgrep, focusing on detecting secrets and sensitive information   inadvertently committed to the repository.</li> </ul>"},{"location":"build/all.semgrep_workflow.explanation.html#exclusions","title":"Exclusions","text":""},{"location":"build/all.semgrep_workflow.explanation.html#semgrepignore-file","title":"<code>.semgrepignore</code> File","text":"<ul> <li>To optimize the scanning process, a <code>.semgrepignore</code> file   is placed in the root directory. This file functions similarly to   <code>.gitignore</code>, specifying files and paths that Semgrep should ignore during   scans. This is useful for excluding third-party libraries, or any other   non-relevant parts of the codebase from the scan.</li> </ul>"},{"location":"build/all.semgrep_workflow.explanation.html#ignoring-specific-lines","title":"Ignoring Specific Lines","text":"<ul> <li>To exclude a specific line of code from Semgrep analysis, append the comment   <code># nosemgrep</code> to the end of the line. This directive instructs Semgrep to   bypass that particular line, allowing developers to suppress false positives   or exclude non-relevant code snippets from analysis.</li> </ul>"},{"location":"build/all.semgrep_workflow.explanation.html#notifications","title":"Notifications","text":"<ul> <li>Failure Notifications: In case of scan failures on the <code>master</code> branch, a   Telegram notification is sent. This includes details such as the workflow   name, repository, branch, event type, and a link to the GitHub Actions run.</li> </ul>"},{"location":"build/all.semgrep_workflow.explanation.html#running-semgrep-locally","title":"Running Semgrep Locally","text":"<p>To run Semgrep locally for testing before pushing your changes:</p> <ol> <li>Install Semgrep on your local machine. Instructions can be found at    Semgrep Installation Guide.</li> <li>Run <code>semgrep --config=p/secrets</code> in your project directory to execute the    same rules as the CI/CD pipeline.</li> </ol>"},{"location":"build/all.semgrep_workflow.explanation.html#additional-resources","title":"Additional Resources","text":"<ul> <li>For more details on Semgrep rules and usage, visit the   Semgrep Official Documentation.</li> <li>To understand GitHub Actions configuration, refer to the   GitHub Actions Documentation.</li> </ul>"},{"location":"build/all.semgrep_workflow.explanation.html#conclusion","title":"Conclusion","text":"<p>The Semgrep integration into the GitHub Actions workflow provides an essential layer of code quality and security checks, aligning with the commitment to maintaining a robust and secure codebase.</p>"},{"location":"coding/all.asyncio.explanation.html","title":"Asyncio Best Practices","text":"<p>Asynchronous programming is a powerful paradigm in Python that allows you to write non-blocking, concurrent code that can greatly improve the efficiency of your applications when I/O and computation can be overlapped.</p> <p>The official documentation is https://docs.python.org/3/library/asyncio.html This document provides an overview of how asyncio works and offers best practices to avoid common pitfalls.</p>"},{"location":"coding/all.asyncio.explanation.html#nomenclature-in-asyncio","title":"Nomenclature in asyncio","text":"<ul> <li>Event Loop</li> <li>Asyncio operates on an event loop that manages the execution of asynchronous     tasks. Whenever one wants to execute the asynchronous tasks we do     <code>asyncio.run()</code> or <code>asyncio.run_until_complete()</code>: both of these methods     will start the event loop and it will be set to running yielding control to     it.</li> <li> <p>In asyncio, you can obtain the event loop using either     <code>asyncio.get_event_loop()</code> or <code>asyncio.get_running_loop()</code>. In the latest     version of asyncio, both methods now serve the same purpose. However, it is     recommended to use <code>get_running_loop</code> because <code>get_event_loop</code> has been     deprecated since version 3.12. This change ensures consistency and future     compatibility with asyncio.</p> </li> <li> <p>Coroutines (aka \"async functions\" defined with <code>async def</code>)</p> </li> <li> <p>These functions can be paused and resumed without blocking other tasks.</p> </li> <li> <p><code>await</code></p> </li> <li>The <code>await</code> keyword is used only within coroutines to pause execution until     an asynchronous operation (e.g., I/O) is completed. While waiting, the event     loop can execute other tasks.</li> </ul>"},{"location":"coding/all.asyncio.explanation.html#common-pitfalls-and-solutions","title":"Common Pitfalls and Solutions","text":""},{"location":"coding/all.asyncio.explanation.html#1-avoid-running-multiple-event-loops","title":"1. Avoid Running Multiple Event Loops","text":"<p>One common error is <code>Event loop is already running</code>. When you use <code>asyncio.run()</code> or <code>run_until_complete()</code>, the event loop is started. Attempting to start a new event loop while one is already running will result in this error.</p> <p>To avoid this:</p> <ul> <li>Solution 1: use <code>nest_asyncio</code></li> <li><code>nest_asyncio</code> is a library that allows you to create nested event loops.     While this may seem like a solution but may lead to complex issues. This was     mainly developed to run <code>asyncio</code> in Jupyter/ipython which already runs an     event loop in backend. This library also does not support     <code>asyncio_solipsism</code> so there is another trade-off.</li> <li> <p>Here's how nest_asyncio works:</p> <ul> <li>It saves the current event loop, if any, that is running in the   environment.</li> <li>It sets up a new event loop specifically for running asyncio code.</li> <li>You can run your asyncio code within this nested event loop.</li> <li>When you're done running asyncio code, <code>nest_asyncio</code> restores the   original event loop, ensuring compatibility with the environment.</li> </ul> </li> <li> <p>Solution 2: use threads</p> </li> <li> <p>Instead of starting a new event loop, run that specific part of your code in     a separate thread to prevent conflicts. This solves the issue but using     thread has its own complications such as race conditions which can be     difficult to debug</p> </li> <li> <p>Solution 3: embrace \"async all the way up\" approach</p> </li> <li>Use <code>await</code> instead of nested call to <code>asyncio.run</code> and make your methods     asynchronous using <code>async def</code> all the way</li> </ul>"},{"location":"coding/all.asyncio.explanation.html#example-code","title":"Example Code","text":"<p>Consider the following coroutines</p> <ul> <li><code>A</code> that sleeps and then calls <code>B</code></li> <li><code>B</code> which calls <code>C</code></li> <li><code>C</code> sleeps</li> </ul> <p>```mermaid   graph TD       A[async def A] --&gt; B[def B]       B --&gt;C[ async def C]</p> <pre><code>  style A fill:#FFA07A, stroke:#FF6347\n  style B fill:#98FB98, stroke:#2E8B57\n  style C fill:#ADD8E6, stroke:#4682B4\n</code></pre> <p><code></code>   import asyncio   import helpers.hasyncio as hasynci</p> <p># Corresponds to <code>submit twap</code> in CmampTask5842   async def A():       print(\"IN A\")       await asyncio.sleep(2)       print(\"ENTER B\")       B()</p> <p># Corresponds to <code>get_fill_per_order</code>   async def C():       print(\"IN C\")       await asyncio.sleep(2)       print(\"EXIT C\")</p> <p># get_fill   def B():       print(\"IN B\")       cor = C()       asyncio.get_running_loop().run_until_complete(cor)       print(\"EXIT B\")</p> <p># Call A.   hasynci.run(A(), asyncio.get_event_loop(), close_event_loop=False)   ```</p> <ul> <li>The code above won't work and will give   <code>Error: \"Event loop is already running\"</code></li> </ul> <p>This error arises because, when <code>run()</code> is invoked, it initializes the event   loop on the current thread. Subsequently, if <code>run()</code> is called within the   function <code>B()</code>, the system checks for the running state of the event loop. If   the event loop is already in progress, the system raises the 'Event loop is   already running' error.\"</p> <ul> <li>Adding   ```   import nest_asyncio</li> </ul> <p>nest_asyncio.apply()   ```</p> <p>the code above will work</p> <ul> <li>If <code>nest_asyncio</code> is present the following code does not work   <code>with hasynci.solipsism_context() as event_loop:     hasynci.run(A(), event_loop, close_event_loop=False)</code>   failing with the error   <code>Error: \"Event loop is already running\"</code></li> </ul>"},{"location":"coding/all.asyncio.explanation.html#2-timesleep-vs-asynciosleep","title":"2. time.sleep() vs asyncio.sleep()","text":"<p>One common error is to use <code>time.sleep()</code> with asynchronous methods.</p> <p>This blocks the execution of that method and the event loop cannot proceed to task until the sleep period is over. This negates the primary purpose of the asyncio and doesn't allow us to simulate systems with <code>solipsism</code>.</p> <p>We should almost never use this, and use <code>asyncio.sleep</code> instead.</p>"},{"location":"coding/all.code_design.how_to_guide.html","title":"Code Design","text":""},{"location":"coding/all.code_design.how_to_guide.html#design-philosophy","title":"Design Philosophy","text":""},{"location":"coding/all.code_design.how_to_guide.html#measure-seven-times-cut-once-russian-proverb","title":"Measure seven times, cut once (Russian proverb)","text":"<ul> <li>Before doing any work, sit down and plan</li> <li>Describe somewhere in writing your high-level plan. Put it in a Google doc     to make it easier to collaborate and review.<ul> <li>What should the code do?</li> <li>What are the functionalities you want to implement?</li> <li>What are the functionalities you don\u2019t want to implement? (what are you   explicitly considering to be out-of-scope?)</li> <li>What is more important, what is less important? E.g., in terms of P0, P1,   P2</li> <li>What are the requirements/invariants?</li> <li>What are the semantics of the entities involved?</li> <li>What are the analyses, the comparisons, and the plots?</li> <li>What are the ideas (expressed without any code!)?</li> <li>ETA: Spend quality time thinking about it (e.g., 30 mins, 1 hr)</li> </ul> </li> <li>Review the plan<ul> <li>Look at the plan again with fresh eyes (e.g., go for a 5-min walk)</li> <li>Does the plan make sense?</li> <li>What can you remove?</li> <li>Can you make things simpler?</li> <li>What is not elegant?</li> <li>What entity is a special case of another?</li> <li>What is similar to what?</li> <li>ETA: Spend 30 mins thinking</li> </ul> </li> <li>Ask for someone to review the plan<ul> <li>Don\u2019t be ashamed of asking for advice</li> </ul> </li> <li>Implement a design<ul> <li>Transform the plan into high-level code, e.g.,</li> <li>What are the objects / functions involved?</li> <li>What are the responsibilities of each class / function?</li> <li>What are the code invariants?</li> <li>What are the data structures?</li> <li>Write the interfaces</li> <li>Only the interfaces! Refrain from implementing the logic</li> <li>Comment the interfaces clearly</li> <li>Think of how the objects / functions interact (who does what, what is the   data passed around)</li> <li>Sprinkle TODOs with ideas about potential problems, simpler approaches</li> <li>ETA: Spend 1/2 day, 1 day</li> </ul> </li> <li>Do a PR of the design</li> <li> <p>Once we converge on the design:</p> <ul> <li>Implement the functions</li> <li>Unit test</li> <li>PR</li> </ul> </li> <li> <p>Remember:</p> </li> <li>We want to do quick interactions: every day there is communication, update     and discussion</li> <li>Do not disappear for one week and come back with something that makes sense     only to you, or that you didn\u2019t get buy-in from others on</li> </ul>"},{"location":"coding/all.code_design.how_to_guide.html#hacker-laws","title":"Hacker laws","text":"<ul> <li>A list of interesting \"laws\" (some are more rule of thumbs / heuristics)   related to computing:</li> <li>hacker-laws</li> </ul>"},{"location":"coding/all.code_design.how_to_guide.html#keep-it-simple","title":"Keep it simple","text":"<ul> <li>Follow the KISS principle.</li> <li>Pursue simple, elegant solutions. Some things are inherently complex, but even   complex systems can (and should) be broken down into simple pieces.</li> <li>Designs that are simple are easier to</li> <li>Understand</li> <li>Modify</li> <li>Debug</li> </ul>"},{"location":"coding/all.code_design.how_to_guide.html#tips-from-a-pro","title":"Tips from a pro","text":"<ul> <li>Adapted from   these slides   from a Stanford talk given by   Jeff Dean   (the Chuck Norris of SWE)</li> </ul>"},{"location":"coding/all.code_design.how_to_guide.html#designing-software-systems-is-tricky","title":"Designing software systems is tricky","text":"<ul> <li>Need to balance:</li> <li>Simplicity [note that this comes first!]</li> <li>Scalability</li> <li>Performance</li> <li>Reliability</li> <li>Generality</li> <li>Features [note that this comes last!]</li> </ul>"},{"location":"coding/all.code_design.how_to_guide.html#get-advice-early","title":"Get Advice Early!","text":"<ul> <li>Get advice</li> <li>Before you write any code</li> <li>Before you write any lengthy design documents [notice the implicit     assumption that there is a design documented!]</li> <li>Before writing a doc or code</li> <li>Jot down some rough ideas (a few paragraphs)</li> <li>Chat about the design with colleagues</li> <li>Consider discussing multiple potential designs</li> </ul>"},{"location":"coding/all.code_design.how_to_guide.html#interfaces","title":"Interfaces","text":"<ul> <li>Think carefully about interfaces in your system!</li> <li>Imagine other hypothetical clients trying to use your interface</li> <li>Document precisely, but avoid constraining the implementation</li> <li>Get feedback on your interfaces before implementing!</li> <li>The best way to learn is to look at well-designed interfaces</li> </ul>"},{"location":"coding/all.code_design.how_to_guide.html#architecture","title":"Architecture","text":""},{"location":"coding/all.code_design.how_to_guide.html#use-design-patterns","title":"Use design patterns","text":"<ul> <li>Design patterns are   idioms or recipes for solving problems that commonly appear in software   engineering across projects and even languages. The classical introduction to   design patterns is the so-called \"Gang of Four\"   book.</li> <li>A free python-focused reference is available   here.</li> <li>Expanding your knowledge of design patterns is a worthwhile investment,   because design patterns</li> <li>Capture elegant solutions that have been developed by many experienced     programmers over a long period of time</li> <li>Provide a framework and reference point for software architecture</li> <li>Are widely used and well-known and therefore quickly recognized by skilled     programmers</li> <li>In other words, by using design patterns, you</li> <li>Don\u2019t have to re-invent the wheel</li> <li>Simplify the high-level picture of your code</li> <li>Make it easier for other people to understand your code</li> </ul>"},{"location":"coding/all.code_design.how_to_guide.html#functions","title":"Functions","text":""},{"location":"coding/all.code_design.how_to_guide.html#avoid-modifying-the-function-input","title":"Avoid modifying the function input","text":"<ul> <li>If, for example, a function <code>f</code> accepts a dataframe <code>df</code> as its (sole)   argument, then, ideally, <code>f(df)</code> will not modify <code>df</code>. If modifications are   desired, then instead one can do:</li> </ul> <p><code>python   def f(df):     df = df.copy()     ...     return df</code></p> <p>in the function so that <code>f(df)</code> returns the desired new dataframe without   modifying the dataframe that was passed in to the function.</p> <ul> <li> <p>In some cases the memory costs associated with the copy are prohibitive, and   so modifying in-place is appropriate. If such is the case, state it explicitly   in the docstring.</p> </li> <li> <p>Functions that do not modify the input are especially convenient to have in   notebook settings. In particular, using them makes it easy to write blocks of   code in a notebook that will return the same results when re-executed out of   order.</p> </li> </ul>"},{"location":"coding/all.code_design.how_to_guide.html#prefer-pure-functions-by-default","title":"Prefer pure functions by default","text":"<ul> <li>Pure functions have two key   properties:</li> <li>If the function arguments do not change, then the return value returned does     not change (in contrast to, e.g., functions that rely upon global state)</li> <li>Function evaluation does not have     side effects</li> <li>Some nice properties enjoyed by pure functions are:</li> <li>They are easy to understand and easy to test</li> <li>Using pure functions makes refactoring easier</li> <li>They allow chaining in an     elegant way</li> <li>They are often a natural choice for data manipulation and analysis</li> <li>They are convenient in notebooks</li> <li>Though it is good to develop an appreciation for   functional programming,   and we like to adopt that style when appropriate, we recognize that it is not   pragmatic to dogmatically insist upon a functional style (especially in our   domain and when using Python).</li> </ul>"},{"location":"coding/all.code_design.how_to_guide.html#invariants","title":"Invariants","text":"<p>From ./oms/architecture.md</p> <p>Invariants and conventions</p> <ul> <li> <p>In this doc we use the new names for concepts and use \"aka\" to refer to the   old name, if needed</p> </li> <li> <p>We refer to:</p> </li> <li>The as-of-date for a query as <code>as_of_timestamp</code></li> <li>The actual time from <code>get_wall_clock_time()</code> as <code>wall_clock_timestamp</code></li> <li>Objects need to use <code>get_wall_clock_time()</code> to get the \"actual\" time</li> <li>We don't want to pass <code>wall_clock_timestamp</code> because this is dangerous</li> <li> <p>It is difficult to enforce that there is no future peeking when one object   tells another what time it is, since there is no way for the second object to   double check that the wall clock time is accurate</p> </li> <li> <p>We pass <code>wall_clock_timestamp</code> only when one \"action\" happens atomically but   it is split in multiple functions that need to all share this information.   This approach should be the exception to the rule of calling</p> </li> </ul> <p><code>get_wall_clock_time()</code></p> <ul> <li> <p>It's ok to ask for a view of the world as of <code>as_of_timestamp</code>, but then the   queried object needs to check that there is no future peeking by using   <code>get_wall_clock_time()</code></p> </li> <li> <p>Objects might need to get <code>event_loop</code></p> </li> <li> <p>TODO(gp): Clean it up so that we pass event loop all the times and event loop   has a reference to the global <code>get_wall_clock_time()</code></p> </li> <li> <p>The Optimizer only thinks in terms of dollar</p> </li> </ul>"},{"location":"coding/all.code_design.how_to_guide.html#our-approach-to-doing-things","title":"Our approach to doing things","text":""},{"location":"coding/all.code_design.how_to_guide.html#roles-and-responsibilities","title":"Roles and responsibilities","text":"<ul> <li>How to communicate</li> <li>Telegram for urgent stuff or interactive things (ideally on a small group     chat)</li> <li> <p>All the action happens in the GH issues</p> <ul> <li>Update the Issues multiple times a day</li> </ul> </li> <li> <p>RPs (responsible party), aka \"Tech leads\"</p> </li> <li>Review / clean up the board once a week (before Mon meeting)</li> <li>Distribute / coordinate work in the teams</li> <li>Do first PR review</li> <li>Merge PRs (trivial or agreed upon)</li> <li> <p>First line of defense for IT / dev issues</p> <ul> <li>The best answer to any problem is a link to gdoc with the solution</li> </ul> </li> <li> <p>Everybody to keep their Issues up to date in ZH and GH</p> </li> <li>In the <code>In progress</code> and <code>Sprint Backlog</code></li> <li>Each issue has typically:<ul> <li>A single assignee</li> <li>An EPIC</li> <li>Maybe a label</li> </ul> </li> <li>Each Git branch should go with the corresponding Issue<ul> <li>E.g., <code>AmpTask2163_Implement_tiled_backtesting</code></li> <li><code>i git_issue_number</code>, <code>i git_create_branch -i XYZ</code></li> </ul> </li> <li>When you create a (draft) branch always create an associated PR<ul> <li>E.g., <code>i gh_create_pr</code></li> </ul> </li> <li>We like draft PRs to discuss architecture before unit testing</li> </ul>"},{"location":"coding/all.code_design.how_to_guide.html#good-practices","title":"Good practices","text":"<ul> <li>Good Issue reports</li> <li>What are you trying to achieve</li> <li>What are you doing (cmd line, setup)</li> <li>What is the error</li> <li> <p>Why do you think things should work differently?</p> </li> <li> <p>\"One PR a day keeps the doctor away\"</p> </li> <li>\"\u2026 keeps investors at bay\"</li> <li>At least 1 PR a day</li> <li>Draft architectural PR when in doubt     </li> <li> <p>Break work in smaller chunks (multiple PRs per bug)</p> </li> <li> <p>Go for a skateboard</p> </li> <li> <p>= something that works end-to-end     </p> </li> <li> <p>1 PR per day</p> </li> <li>Even if it's not complete (do a draft)</li> <li>Update the bugs (at least) every day</li> <li>Ask questions if you have any doubt</li> <li>Always sync</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html","title":"Code Like Pragmatic Programmer","text":""},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#a-pragmatic-philosophy","title":"A pragmatic philosophy","text":""},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-1-care-about-your-craft","title":"PP_Tip 1: Care about your craft","text":"<ul> <li>Why spending your life developing software unless you care doing it well?</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-2-think-about-your-work","title":"PP_Tip 2: Think! About your work","text":"<ul> <li>Turn off the autopilot and take control</li> <li>Constantly critique and evaluate your work</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-3-provide-options-dont-make-lame-excuses","title":"PP_Tip 3: Provide options, don't make lame excuses","text":"<ul> <li>Things go wrong:</li> <li>Deliverables are late</li> <li>Unforeseen technical problems come up</li> <li>Specs are misunderstood</li> <li> <p>...</p> </li> <li> <p>Don't be afraid to admit ignorance or error</p> </li> <li>Before saying that something cannot be done, consider what the other people   are likely to say</li> <li>\"Have you tried this?\"</li> <li>\"Did you consider that?\"</li> <li>...</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#broken-windows","title":"Broken windows","text":"<ul> <li>= bad designs, wrong decisions, poor code, rotten software</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-4-dont-live-with-broken-windows","title":"PP_Tip 4: Don't live with broken windows","text":"<ul> <li>A broken window left un-repaired instills a sense of abandonment</li> <li>Don't live with broken windows un-repaired</li> <li>If there is no time, board it up</li> <li>Take action to show that you are on top of the situation</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-5-be-a-catalyst-for-change","title":"PP_Tip 5: Be a catalyst for change","text":"<ul> <li>Sometimes you know what is right, but if you ask for permissions you will be   slowed down</li> <li>Request for feedback</li> <li>Committees, budgets, ...</li> <li> <p>It's the so-called \"startup fatigue\"</p> </li> <li> <p>It's easier to ask forgiveness, than it is to get permission</p> </li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-6-remember-the-big-picture","title":"PP_Tip 6: Remember the big picture","text":"<ul> <li>Projects slowly and inexorably get totally out of hand</li> <li>Missing a deadline happens one day at a time</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-7-make-quality-a-requirement-issue","title":"PP_Tip 7: Make quality a requirement issue","text":"<ul> <li>One of the requirements from the user should be \"how good do you want the   software to be?\"</li> <li>Good software today is better than perfect software tomorrow</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-8-invest-regularly-in-your-knowledge-portfolio","title":"PP_Tip 8: Invest regularly in your knowledge portfolio","text":"<ul> <li>Your knowledge and experience are your most important professional assets</li> <li> <p>Unfortunately they are expiring assets</p> </li> <li> <p>It's like investing for retirement</p> </li> <li> <p>Invest as a habit</p> <ul> <li>Study every day</li> </ul> </li> <li>Diversify<ul> <li>The more different things you know, the more valuable you are</li> </ul> </li> <li>Balance conservative and high-risk / high-reward investments<ul> <li>Don't put all the technical eggs in one basket</li> </ul> </li> <li>Review and rebalance periodically</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-9-critically-analyze-what-you-read-and-hear","title":"PP_Tip 9: Critically analyze what you read and hear","text":"<ul> <li>Beware of media hype</li> <li>Beware of zealots who insist that their dogma provides the only answer</li> <li>A best seller book is not necessarily a good book</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-10-its-both-what-you-say-and-the-way-you-say-it","title":"PP_Tip 10: It's both what you say and the way you say it","text":"<ul> <li>Plan what you want to say: write an outline</li> <li>Choose the right moment</li> <li>Choose a style; if in doubt, ask)</li> <li>Make it look good</li> <li>Involve your audience in early drafts</li> <li>Be a listener</li> <li>Get back to people</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-10-remember-the-wisdom-acrostic","title":"PP_Tip 10: Remember the WISDOM acrostic","text":"<ul> <li>What do you Want them to learn?</li> <li>What is their Interest in what you've got to say?</li> <li>How Sophisticated are they?</li> <li>How much Detail do they want?</li> <li>Whom do you want to Own the information?</li> <li>How can you Motivate them to listen?</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#a-pragmatic-approach","title":"A pragmatic approach","text":""},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-11-dry-dont-repeat-yourself","title":"PP_Tip 11: DRY - Don't repeat yourself","text":"<ul> <li>Every piece of information must have a:</li> <li>Single</li> <li>Unambiguous</li> <li> <p>Authoritative representation within a system</p> </li> <li> <p>Information is duplicated in multiple places -&gt; maintenance nightmare</p> </li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-11-programs-knowledge","title":"PP_Tip 11: Programs = knowledge","text":"<ul> <li>Programs are made of knowledge (e.g., requirements, specifications, code,   internal and external documentation)</li> <li>Knowledge is unstable</li> <li>Over time one gets better understanding of requirements</li> <li>Requirements change</li> <li>The solution to a problem changes over time<ul> <li>E.g., tests show that an algorithm is not general or does not work</li> </ul> </li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-11-how-does-duplication-arise","title":"PP_Tip 11: How does duplication arise?","text":"<ul> <li> <p>There are 4 I's of duplication:</p> </li> <li> <p>Imposed</p> </li> <li>The environment seems to require duplication</li> <li>Inadvertent</li> <li>A developer does not realize that he/she is duplicating information</li> <li>Impatient</li> <li>A developer gets lazy and duplicates code because it is easy</li> <li>Inter-developer</li> <li>Multiple developers duplicate a piece of info</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-11-imposed-duplication","title":"PP_Tip 11: Imposed duplication","text":"<ul> <li>Multiple representations of the same piece of info</li> <li>E.g., same info in two pieces of code written in different languages</li> <li> <p>Solution: write a filter or code generator</p> </li> <li> <p>Internal documentation and code</p> </li> <li>Solution: comments should not duplicate info present in the code</li> <li> <p>Comments should be for why, code for how</p> </li> <li> <p>External documentation and code</p> </li> <li> <p>Solution: be careful, generate it automatically</p> </li> <li> <p>Language issues</p> </li> <li>Some languages require duplication (e.g., C/C++ header vs cpp)</li> <li>Solution: at least warnings and errors are reported</li> <li>Do not duplicate comments in headers and code</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-11-inadvertent-duplication","title":"PP_Tip 11: Inadvertent duplication","text":"<ul> <li>Un-normalized data structures</li> <li>E.g.,</li> <li>A <code>Truck</code> has a type, maker, a license plate, and a driver</li> <li>A <code>DeliveryRoute</code> has a route, a truck, and a driver</li> <li>Now if the driver of a truck is changed, this info needs to be changed in     two objects</li> <li> <p>Solution: use a 3rd object to bind <code>DeliveryRoute</code>, <code>Truck</code>, <code>Driver</code></p> </li> <li> <p>Data elements that are mutually dependent for perf reason</p> </li> <li>E.g., a line has <code>startPoint</code> and <code>endPoint</code>, and a field <code>length</code> is     redundant</li> <li>Solution: use accessors to read/write object attributes</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-11-impatient-duplication","title":"PP_Tip 11: Impatient duplication","text":"<ul> <li>The temptation is always to cut-and-paste code and then modify it</li> <li>It increases the technical debt: shortcuts end up in long delays</li> <li>Solution: re-factor and then change it</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-11-inter-developer-duplication","title":"PP_Tip 11: Inter-developer duplication","text":"<ul> <li> <p>It is hard to detect</p> </li> <li> <p>Solutions:</p> </li> <li>Encourage communication</li> <li>Do code reviews</li> <li>Project libraries: central place for utility and scripts (so one knows where     to look before writing a routine)</li> <li>Project librarian to supervise the reuse</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-12-make-it-easy-to-reuse","title":"PP_Tip 12: Make it easy to reuse","text":"<ul> <li>If something is not easy to find, use, reuse, people won't reuse</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#orthogonality","title":"Orthogonality","text":"<ul> <li>= independence, decoupling</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-13-eliminate-effects-between-unrelated-things","title":"PP_Tip 13: Eliminate effects between unrelated things","text":"<ul> <li>Systems are orthogonal when changes in one sub-system don't affect other   sub-systems</li> <li>E.g., helicopter controls are not orthogonal</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-13-orthogonality-pros","title":"PP_Tip 13: Orthogonality: pros","text":"<ol> <li>Easier to change</li> <li>Changes are localized</li> <li>Changes are easier to be unit tested</li> <li>Gain productivity</li> <li>Less overlap means more useful function per unit of code</li> <li>Promote reuse</li> <li>Reduce risk</li> <li>Easier to change design</li> <li>Not being tied to a particular vendor / product / platform</li> </ol>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-13-how-to-maintain-orthogonality","title":"PP_Tip 13: How to maintain orthogonality","text":"<ol> <li>Avoid global state (e.g., singletons)</li> <li>Write shy code</li> <li>Modules / objects that don't reveal anything unnecessary</li> <li>Pass required context to modules, objects, functions</li> <li>Avoid coupling (e.g., law of Demeter)</li> <li>Always look for opportunities to refactor</li> <li>Reorganize</li> <li>Improve structure</li> <li>Increase orthogonality</li> </ol>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-14-there-are-no-final-decisions","title":"PP_Tip 14: There are no final decisions","text":"<ul> <li>Do not carve decisions into stone</li> <li>Think of them as being written on the sand</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-14-there-are-no-final-decisions-why","title":"PP_Tip 14: There are no final decisions: why","text":"<ol> <li>Requirements can change on us</li> <li>The first decision is not always the best one</li> <li>Change 3rd party components</li> </ol>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-15-use-tracer-bullets-to-find-the-target","title":"PP_Tip 15: Use tracer bullets to find the target","text":"<ul> <li>Ready, fire, aim!</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#analogy-between-shooting-bullets-in-the-dark-and-software-engineering","title":"Analogy between shooting bullets in the dark and software engineering","text":"<ul> <li> <p>How to shoot a bullet to a target in the dark?</p> </li> <li> <p>Open loop approach</p> <ul> <li>Compute all the interesting quantities (wind, angles, distance)</li> <li>Hope that the conditions don't change at all</li> </ul> </li> <li> <p>Closed loop approach</p> <ul> <li>Use tracer bullets</li> <li>They leave trace in the dark so one can see if they are hitting the    target</li> <li>If not, adjust the aim</li> <li>If they hit the target, other bullets will also hit the target</li> </ul> </li> <li> <p>Software engineering:</p> </li> <li> <p>Open loop approach</p> <ul> <li>Specify the system to death</li> </ul> </li> <li>Closed loop approach<ul> <li>Achieve end-to-end connection among components with minimal functionality</li> <li>Then adjust, re-aim, ... until you are on target</li> </ul> </li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-15-tracer-bullets-pros","title":"PP_Tip 15: Tracer bullets: pros","text":"<ul> <li>Users see something working early</li> <li>You have an integration platform, instead of big-bang integration</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-15-tracer-bullets-cons","title":"PP_Tip 15: Tracer bullets: cons","text":"<ul> <li>Tracer bullets don't always hit their target</li> <li>Still imagine the result using a waterfall approach</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-16-prototype-to-learn","title":"PP_Tip 16: Prototype to learn","text":"<ul> <li>The value of prototype lies not in the code produced, but in the lessons   learned</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-16-prototype-to-learn-cons","title":"PP_Tip 16: Prototype to learn: cons","text":"<ul> <li>Set the right expectations</li> <li>Make sure everyone understands that you are writing disposable code</li> <li>Otherwise management might insist on deploying the prototype or a cleaned up   version of it</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-16-tracer-code-vs-prototyping","title":"PP_Tip 16: Tracer code vs prototyping","text":"<ul> <li>Prototype:</li> <li>When experimentation is done, the prototype is thrown away and it is     reimplemented using the lessons learned</li> <li> <p>E.g., write prototype in Python and then code the production system in C++</p> </li> <li> <p>Tracer code:</p> </li> <li>Build architectural skeleton</li> <li>Components have minimal implementation</li> <li>Over time stubbed routines are completed</li> <li>The framework stays intact</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-17-program-close-to-the-problem-domain","title":"PP_Tip 17: Program close to the problem domain","text":"<ul> <li>Always try to write code using the vocabulary of the application domain</li> <li> <p>So you are free to concentrate on solving domain problems and ignore petty   implementation details</p> </li> <li> <p>Use mini-languages</p> </li> <li>Other people can implement business logic</li> <li>Code can issue domain specific errors</li> <li>Create metadata compiled or read-in by the main application</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-18-estimate-to-avoid-surprises","title":"PP_Tip 18: Estimate to avoid surprises","text":"<ul> <li> <p>All answers are estimates: some are more accurate than others</p> </li> <li> <p>Do we need a high accuracy or a ballpark figure?</p> </li> <li>The unit of measurement of the estimate conveys a message about accuracy</li> <li>E.g., 130 working days vs 6 months</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-18-how-to-make-estimates","title":"PP_Tip 18: How to make estimates","text":"<ol> <li>Ask people that have done a similar project before</li> <li>Specify what's the scope of an estimate: \"under assumptions X and Y, the    estimate is Z\"</li> <li>Break the system in pieces, understand which parameters they take, assign a    value to each parameter</li> <li>Understand which are the critical parameters and try to estimate them    properly</li> <li>When estimates are wrong, understand why</li> </ol>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-19-iterate-the-schedule-with-the-code","title":"PP_Tip 19: Iterate the schedule with the code","text":"<ul> <li> <p>Management often wants a single estimate for the schedule before the project   starts</p> </li> <li> <p>In reality:</p> </li> <li>Team</li> <li>Productivity</li> <li>Environment will determine the schedule</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-19-estimates-at-the-coffee-machine","title":"PP_Tip 19: Estimates at the coffee machine","text":"<ul> <li>Estimates given at the coffee machine will come back to haunt you</li> <li>When asked for an estimate, answer \"I'll get back to you\"</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#the-basic-tools","title":"The basic tools","text":""},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-20-keep-knowledge-in-plain-text","title":"PP_Tip 20: Keep knowledge in plain text","text":"<ul> <li> <p>The base material for a programmer is knowledge</p> </li> <li> <p>Knowledge is made of:</p> </li> <li>Requirements</li> <li>Specifications</li> <li>Documentation</li> <li>Design</li> <li>Implementation</li> <li> <p>Test</p> </li> <li> <p>The best format to store knowledge is plain text</p> </li> <li>Plain text can be manipulated manually and programmatically using virtually   any tool (e.g., source code control, editors, stand-alone filters)</li> <li>Human-readable self-describing data will outlive</li> <li>All other forms of data; and</li> <li>The application that created it</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-21-use-the-power-of-command-shells","title":"PP_Tip 21: Use the power of command shells","text":"<ul> <li>GUIs</li> <li>Are great</li> <li> <p>Allow to do only what they were designed for</p> </li> <li> <p>In life one does not know what exactly will be needed</p> </li> <li>Shells allow to automate and combine tools in ways that one didn't intended or   planned for</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-22-use-a-single-editor-well","title":"PP_Tip 22: Use a single editor well","text":"<ul> <li>Better to know one editor very well than knowing many editors superficially</li> <li>Use the same editor for all editing tasks</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-23-always-use-source-code-control","title":"PP_Tip 23: Always use source code control","text":"<ul> <li>Source control is like an undo key, a time machine</li> <li>Use it always: even if you are a single person team, even if you are working   on a throw-away prototype</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-23-advantages-of-source-code-control","title":"PP_Tip 23: Advantages of source code control","text":"<ul> <li>It allows to answer questions like:</li> <li>Who made changes to this line of code?</li> <li>What are the difference between this release and the previous one?</li> <li>It allows to work on a new release, while fixing bugs in the previous (i.e.,   branches)</li> <li>It can connect to automatic repeatable builds and regressions</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-24-fix-the-problem-not-the-blame","title":"PP_Tip 24: Fix the problem, not the blame","text":"<ul> <li>No one writes perfect software so debugging will take up a major portion of   your day</li> <li>Attack debugging as a puzzle to be solved</li> <li>Avoid denial, finger pointing, lame excuses</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-25-dont-panic","title":"PP_Tip 25: Don't panic","text":"<ul> <li>Before you start debugging, adopt the right mindset:</li> <li>Turn off defenses that protect your ego</li> <li>Tune out project pressure</li> <li>Get comfortable</li> <li>Don't panic while you debug, even if you have your nervous boss or your client   breathing on your neck</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-25-how-to-debug","title":"PP_Tip 25: How to debug","text":"<ul> <li>Don't waste time thinking \"this cannot happen\"</li> <li> <p>Obviously it is happening</p> </li> <li> <p>Make the bug reproducible with a single command</p> </li> <li>So you know you are fixing the right problem</li> <li>So you know when it is fixed</li> <li> <p>You can easily add a unit test for that bug</p> </li> <li> <p>What could be causing the symptoms?</p> </li> <li>Check for warnings from the compiler</li> <li>Always set all the compiler warnings so the compiler can find issues for you</li> <li>Step through the code with a debugger</li> <li>Add tracing statements</li> <li>Corrupt variables?</li> <li>Check their neighborhood variables, use <code>valgrind</code></li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-25-rubber-ducking","title":"PP_Tip 25: Rubber ducking","text":"<ul> <li>= explain the issue step-by-step to someone else, even to a yellow rubber duck</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-25-why-rubber-ducking-works","title":"PP_Tip 25: Why rubber ducking works?","text":"<ul> <li>The simple act of explaining the issue often uncovers the problem</li> <li>You state things that you may take for granted</li> <li>Verbalizing your assumptions lets you gain new insights into the problem</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-26-select-is-not-broken","title":"PP_Tip 26: <code>select</code> is not broken","text":"<ul> <li>Do not assume that the library is broken</li> <li>Assume that you are calling the library in the wrong way</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-27-dont-assume-it-prove-it","title":"PP_Tip 27: Don't assume it: prove it","text":"<ul> <li>Don't assume that a piece of code works in any condition</li> <li>Avoid statements like \"that piece of code has been used for years: it cannot     be wrong!\"</li> <li>Prove that the code works:</li> <li>In this context</li> <li>With this data</li> <li> <p>With these boundary conditions</p> </li> <li> <p>After you find a surprising error, are there any other places in the code that   may be susceptible to the same bug?</p> </li> <li>Make sure that whatever happened, never happens again</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-28-learn-a-text-manipulation-language","title":"PP_Tip 28: Learn a text manipulation language","text":"<ul> <li>Spending 30 mins trying out a crazy idea is better than spending 5 hours</li> <li>With scripting languages (e.g., Python, perl) you can quickly prototype ideas   instead of using a production language (e.g., C, C++)</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-29-write-code-that-writes-code","title":"PP_Tip 29: Write code that writes code","text":"<ul> <li>Passive code generators</li> <li>They are run once and the origin of the code is forgotten</li> <li>E.g., parameterized templates, boilerplate</li> <li> <p>They can be 90\\% accurate and the rest is done by hand</p> </li> <li> <p>Active code generators</p> </li> <li>They convert a single representation of knowledge into all the forms that are   needed</li> <li>This is not duplication, it is the DRY principle in action</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pragmatic-paranoia","title":"Pragmatic paranoia","text":""},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-30-you-cannot-write-perfect-software","title":"PP_Tip 30: You cannot write perfect software","text":"<ul> <li>Accept it and celebrate it</li> <li>Unless you accept it, you'll end up wasting time chasing an impossible dream</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#know-when-to-stop","title":"Know when to stop","text":"<ul> <li>A painter needs to know when to stop adding layers of paint</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#dont-trust-others","title":"Don't trust others","text":"<ul> <li>Code defensively</li> <li>Anticipate the unexpected</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#dont-trust-yourself","title":"Don't trust yourself","text":"<ul> <li>Code defensively against your own mistakes</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#preconditions","title":"Preconditions","text":"<ul> <li>= what must be true in order for the routine to be called</li> <li>It is caller's responsibility to pass good data</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#postconditions","title":"Postconditions","text":"<ul> <li>= what the routine is guaranteed to do</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#class-invariants","title":"Class invariants","text":"<ul> <li>= a class ensures that some conditions are always true</li> <li>E.g., between invocations to public methods</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#contracts","title":"Contracts","text":"<ul> <li>If all routines' preconditions are met</li> <li>=&gt; the routine guarantees that all postconditions and invariants will be     true when it completes</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-31-design-with-contracts","title":"PP_Tip 31: Design with contracts","text":"<ul> <li>Some languages support design by contract:</li> <li>In the compiler (e.g., static assertion, type system)</li> <li>In the runtime systems (e.g., assertions)</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-32-crash-early","title":"PP_Tip 32: Crash early","text":"<ul> <li>Better to crash than to thrash (=corrupting the state of the system)</li> <li>When something unexpected happens throw a runtime exception</li> <li>The exception, if not caught, will percolate up to the top level halting the     program</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-33-if-it-cannot-happen-use-assertions-to-ensure-that-it-wont","title":"PP_Tip 33: If it cannot happen, use assertions to ensure that it won't","text":"<ul> <li>Assertions check for things that should never happen</li> <li>E.g., at the end of a sorting routine the data is not sorted</li> <li>Don't use assertions in place of real error handling</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#leave-assertions-enabled","title":"Leave assertions enabled","text":"<ul> <li>Assertions should be left on even after the system is tested and shipped</li> <li>The assumption that testing will find all the bugs is wrong</li> <li>Testing tests a miniscule percentage of possible real-world conditions</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-34-use-exceptions-for-exceptional-problems","title":"PP_Tip 34: Use exceptions for exceptional problems","text":"<ul> <li>Interleaving normal control flow code and error handling code leads to ugly   code</li> <li>With exceptions one can split the code neatly into two parts</li> <li>Exceptions are like <code>goto</code></li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#exceptions-are-for-unexpected-events","title":"Exceptions are for unexpected events","text":"<ul> <li>Use exceptions only for truly unexpected events</li> <li>The code should still run in normal conditions if one removes all the   exception handlers</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-35-finish-what-you-start","title":"PP_Tip 35: Finish what you start","text":"<ul> <li>Resources (e.g., memory, DB transactions, threads, files, timers) follow a   pattern:</li> <li>Allocate</li> <li>Use</li> <li>Deallocate</li> <li>Routine or objects that allocate resources are responsible for deallocating   them</li> <li>To avoid deadlocks always deallocate resources in the opposite order to that   in which you allocate them</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#bend-or-break","title":"Bend or break","text":""},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-36-minimize-coupling-between-modules","title":"PP_Tip 36: Minimize coupling between modules","text":"<ul> <li>We want to limit the interaction between modules</li> <li>If one modules has to be replaced / is broken, the other modules can carry     on</li> <li>Traversing relationships between objects can quickly lead to a combinatorial   explosion of dependencies</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-36-law-of-demeter-for-functions","title":"PP_Tip 36: Law of Demeter for functions","text":"<ul> <li>Any method <code>O.m(A, B, C)</code> of an object <code>O</code> should call only methods belonging   to:</li> <li>Object <code>O</code> itself</li> <li>Objects through parameters passed to the method (e.g., <code>A</code>)</li> <li> <p>Objects instantiated within <code>m</code></p> </li> <li> <p>A rule of thumb in OOP is to use a single dot, e.g., <code>a.m()</code> and avoid   multiple dots, e.g., <code>a.b.m()</code></p> </li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-36-intuition-of-the-law-of-demeter","title":"PP_Tip 36: Intuition of the Law of Demeter","text":"<ul> <li>Object <code>A</code> can call a method of <code>B</code></li> <li><code>A</code> cannot reach through <code>B</code> to access an object <code>C</code></li> <li>Otherwise <code>A</code> knows about the internal structure of <code>B</code></li> <li><code>B</code> needs to be changed to expose the interface of <code>C</code></li> <li>Cons: lots of wrapper methods to forward requests to delegates</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-36-law-of-demeter-as-general-contractor","title":"PP_Tip 36: Law of Demeter as general contractor","text":"<ul> <li> <p>It's like using a general contractor</p> </li> <li> <p>Pros</p> </li> <li>You ask a job to be done, but you don't deal with subcontractors directly</li> <li>Cons</li> <li>The client needs to go through the general contractor all the times</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-37-configure-dont-integrate","title":"PP_Tip 37: Configure, don't integrate","text":"<ul> <li> <p>Every time we change the code to accommodate a change in business logic we   risk to break the system or to introduce a new bug</p> </li> <li> <p>Make the system highly configurable</p> </li> <li>Always use metadata, i.e., data about data (e.g., choice of algorithm,   database, middleware, user-interface style, ...)</li> <li>Use <code>.ini</code> files</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-38-put-abstractions-in-code-details-in-metadata","title":"PP_Tip 38: Put abstractions in code, details in metadata","text":"<ul> <li>The goal is to think declaratively: specify what to do, not how to do it</li> <li>We want to configure and drive the application via metadata as much as   possible</li> <li>So we program for the general case and put the specifics outside the compiled   code</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#mechanisms-vs-policies","title":"Mechanisms vs policies","text":"<ul> <li>Mechanisms = primitives, what can be done</li> <li>Policies = how to put together primitives</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-38-advantages-of-splitting-mechanisms-and-policies","title":"PP_Tip 38: Advantages of splitting mechanisms and policies","text":"<ol> <li>Decouple components in the design, resulting in more flexible and adaptable    programs</li> <li>Customize the application without recompiling it</li> <li>Metadata can be expressed in a form closer to problem domain</li> <li>Anybody can change the behavior without understanding the code</li> </ol>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-38-put-business-logic-in-metadata","title":"PP_Tip 38: Put business logic in metadata","text":"<ul> <li>Business logic and rules are the parts that are most likely to change</li> <li>So we want to maintain them in a flexible format, e.g., metadata</li> <li>Metadata should be encoded in plain text</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-39-analyze-workflow-to-improve-concurrency","title":"PP_Tip 39: Analyze workflow to improve concurrency","text":"<ul> <li>Avoid temporal coupling</li> <li>Use activity diagrams to identify activities that could be performed in   parallel</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-40-temporal-coupling","title":"PP_Tip 40: Temporal coupling","text":"<ul> <li>Time is often ignored when designing a software architecture</li> <li>We tend to think in a linear sequential fashion: \"do this, then do that\"</li> <li> <p>This leads to temporal coupling, i.e., coupling in time</p> </li> <li> <p>When designing an architecture, we need to think about:</p> </li> <li>Concurrency = things happening at the same time</li> <li>Ordering = <code>A</code> must occur before <code>B</code></li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-40-uml-activity-diagram","title":"PP_Tip 40: UML activity diagram","text":"<ul> <li>Actions are represented by rounded boxes</li> <li>Arrows between actions mean \"temporal ordering\"</li> <li>Actions with no incoming arrows can be started at any time</li> <li>Synchronization points are represented by a thick bar:</li> <li>Once all the actions leading to a barrier are done, one can proceed with the     arrows leaving the synchronization point</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#service","title":"Service","text":"<ul> <li>= independent, concurrent objects behind well-defined, consistent interfaces</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-40-design-using-services","title":"PP_Tip 40: Design using services","text":"<ul> <li>Using services allows to avoid temporal coupling</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-41-hungry-consumer-model","title":"PP_Tip 41: Hungry consumer model","text":"<ul> <li>There are multiple independent consumers and a centralized work queue</li> <li>Each consumer grabs a task from the work queue and processes it</li> <li> <p>Load balancing: if a consumer task gets bogged down or it is slower, the   others can pick up the slack</p> </li> <li> <p>PP_Tip 41: Example of 3-tier architecture with hungry consumer model</p> </li> <li>Goal</li> <li> <p>Read a request from multiple lines and process the transactions against the   database</p> </li> <li> <p>Constraints</p> </li> <li>DB operations are slow</li> <li>We need to keep accepting service requests even when waiting on DB</li> <li> <p>DB performance suffers with too many concurrent sessions</p> </li> <li> <p>Solution</p> </li> <li>3 tier, multi-processing distributed application</li> <li>Tier 1: input tasks monitor the input lines and add requests to application   queue</li> <li>Application queue</li> <li>Tier 2: read application queue, apply business logic, add transaction to DB   queue</li> <li>DB queue</li> <li> <p>Tier 3: read transactions from DB queue and apply it to DB</p> </li> <li> <p>Each component is an independent entity, running concurrently (on the same   machine or on multiple machines)</p> </li> <li>Actions are asynchronous: as soon as a request is handled by a process and put   on next queue, the process goes back to monitor inputs</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-41-always-design-for-concurrency","title":"PP_Tip 41: Always design for concurrency","text":"<ul> <li>Programming with threads imposes some design constraints</li> <li>Concurrency forces you to think things more carefully</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#examples-of-problems-with-concurrency","title":"Examples of problems with concurrency","text":"<ul> <li>Global or static variables must be protected from concurrent access</li> <li>Do you really need a global variable?</li> <li>Objects must always be in a valid state when called</li> <li>Classes with separate constructor and initialization routines are     problematic</li> <li>Interfaces should not keep state</li> <li>Make services stateless</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#event","title":"Event","text":"<ul> <li>= special message that says \"something interesting just happened\"</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-42-objects-communicating-through-events","title":"PP_Tip 42: Objects communicating through events","text":"<ul> <li>We know that we need to separate a program in modules / classes</li> <li> <p>A module / class has a single, well-defined responsibility</p> </li> <li> <p>At run-time how do objects talk to each other?</p> </li> <li>We don't want objects to know too much about each other</li> <li>E.g., how does object <code>A</code> know which objects to talk to?</li> <li>By using events one can minimize coupling between objects: objects are   interested in events and not in other objects</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-42-all-events-through-a-single-routine-approach","title":"PP_Tip 42: All events through a single routine approach","text":"<ul> <li>One approach is to send all events to a single routine that dispatches them to   the objects</li> <li>This is bad!</li> <li>A single routing needs to know all interactions among objects</li> <li>It's like a huge case statement</li> <li>It violates encapsulation, increases coupling</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-42-publish-subscribe-model","title":"PP_Tip 42: Publish / subscribe model","text":"<ul> <li>Objects should only receive events they want (do no spam objects!)</li> <li>Subscribers register themselves with publisher objects for interesting     events</li> <li>Publisher calls all subscribers when a corresponding event occurs</li> <li>Organization can be:</li> <li>Peer-to-peer</li> <li>Software bus (a centralized object maintains DB of listeners and dispatches     messages)</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-42-sequence-diagram","title":"PP_Tip 42: Sequence diagram","text":"<ul> <li>It shows the flow of messages among several objects</li> <li>Objects are arranged in columns</li> <li>Each message is an arrow from sender's column to receiver's column</li> <li>Timing relationship between messages is captured by vertical ordering</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-42-push-pull-model-for-event-services","title":"PP_Tip 42: Push / pull model for event services","text":"<ul> <li>Push mode</li> <li>Event producers inform the event channel that event has occurred</li> <li> <p>Event channel distributes event to all client objects registered for that   event</p> </li> <li> <p>Pull mode</p> </li> <li>Event consumers poll the event channel periodically</li> <li>Event channel polls possible suppliers and report interesting events</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-42-separate-views-from-models","title":"PP_Tip 42: Separate views from models","text":"<ul> <li> <p>Model-View-Controller design pattern</p> </li> <li> <p>Model:</p> </li> <li>It stores the data</li> <li>It has methods to change its state, to report changes (through events), and     to report data</li> <li>It is like a DB</li> <li>View:</li> <li>A way to interpret the Model</li> <li>It subscribes to changes in the Model</li> <li>It subscribes to events from the Controller to change representation</li> <li>Controller:</li> <li>It controls the View</li> <li> <p>It provides new data to the Model</p> </li> <li> <p>One can even have a network of MVCs: e.g., a View for one Model can be the   Model for another View</p> </li> <li> <p>PP_Tip 42: Examples of MVC</p> </li> <li>Almost any GUI</li> <li>Tree widget = a clickable, traversable tree</li> <li>Spreadsheet with a graph attached to represent data as bar charts, running   totals, ...; controllers allow to zoom in / out, enter new data, ...</li> <li>System reporting interesting information about a baseball game</li> <li>Model is a DB with all possible information</li> <li>New data from the Controller: a pitcher is changed, a player strikes out, it     starts raining</li> <li>A View reports the score</li> <li>A View reports info about current batter</li> <li>A View looks for new world records</li> <li>A View posts information on the web</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-43-use-blackboards-to-coordinate-workflow","title":"PP_Tip 43: Use blackboards to coordinate workflow","text":"<ul> <li>A blackboard system lets decouple objects from each other completely</li> <li>There is even less coupling that publish / subscribe model</li> <li>Consumers and producers exchange data anonymously and asynchronously</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-43-example-of-blackboard-implementation","title":"PP_Tip 43: Example of blackboard implementation","text":"<ul> <li> <p>Blackboard is like a DB providing atomic and distributed storage of objects</p> </li> <li> <p>One can store any object, not just data</p> </li> <li>Objects can be retrieved by partial matching fields (templates and wildcards)   or by subtypes</li> <li>Operations can be:</li> <li>Read = search and retrieve item from blackboard</li> <li>Write = put an item on the blackboard</li> <li>Take = read + remove from blackboard</li> <li>Notify = set up notification when an object matching a template is written</li> <li>Advantage is a single and consistent interface to blackboard, instead of   different APIs for every transaction / interaction in the system</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-43-example-of-blackboard-application","title":"PP_Tip 43: Example of blackboard application","text":"<ul> <li> <p>Program that accepts and process loan applications</p> </li> <li> <p>Constrains</p> </li> <li>The laws are very complex</li> <li>There is no guarantee on the order in which data arrives</li> <li>Data is gathered by different people</li> <li>Certain data is dependent on other data</li> <li>Arrival of new data may raise request for more data and policies</li> <li> <p>As regulation change, the workflow must be re-organized</p> </li> <li> <p>Solutions</p> </li> <li>Encoding all the workflow is a nightmare</li> <li>A better solution is a blackboard + rules engine</li> <li>Any time new data arrives, a new rule is triggered</li> <li>Rules can output more data on the blackboard, triggering more rules and so on</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#while-you-are-coding","title":"While you are coding","text":""},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-44-dont-program-by-coincidence","title":"PP_Tip 44: Don't program by coincidence","text":"<ul> <li>Do not program by coincidence (= relying on luck and accidental successes)</li> <li>Program deliberately</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-44-how-to-program-by-coincidence","title":"PP_Tip 44: How to program by coincidence","text":"<ul> <li>You type some code, try it, and it seems to work</li> <li>Type more code and still works</li> <li>After few weeks the program stops to work</li> <li>You don't know why the code is failing, because you didn't know why it worked   in the first place</li> <li>The code seemed to work, given the (limited) testing did, but it was just a   coincidence</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-44-why-program-by-coincidence-seems-to-work","title":"PP_Tip 44: Why program by coincidence seems to work?","text":"<ul> <li>One ends up relying on undocumented boundary conditions and when the code is   fixed / changed, our code breaks</li> <li>One tries until something works, then it does not wonder why it works: \"it   works now, better leave it alone\"</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-44-how-to-program-deliberately","title":"PP_Tip 44: How to program deliberately","text":"<ul> <li>Rely only on reliable things</li> <li>Document your assumptions (e.g., design by contract)</li> <li>Add assertions to test assumptions</li> <li>Don't just test your code, test your assumptions as well (e.g., with small   unit tests)</li> <li>Don't let existing code (even your own code) dictate future code</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-45-estimate-the-order-of-your-algorithms","title":"PP_Tip 45: Estimate the order of your algorithms","text":"<ul> <li>We always want to estimate the resources (e.g., time, memory) required by   algorithms</li> <li>E.g., \"Would the algorithm scale up from 1k records to 1M records?\"</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#big-oh-notation","title":"Big-Oh notation","text":"<ul> <li>Big-Oh notation represents the worst-case time taken by an algorithm</li> <li>Simple loops: $O(n)$</li> <li>Nested loops: $O(n^k)$</li> <li>Binary chop (e.g., binary search): $O(\\log(n))$</li> <li>Divide and conquer (split, recurse, combine): $O(n \\log(n))$</li> <li>Combinatoric: $O(2^n)$</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#estimating-big-oh","title":"Estimating Big-Oh","text":"<ul> <li>If you are not sure about Big Oh, vary input record size and plot the resource   needed (e.g., time, memory) against the input size</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#big-oh-in-the-real-world","title":"Big-Oh in the real world","text":"<ul> <li>It is possible that a $O(n^2)$ algorithm is faster than a $O(n \\log(n))$ for   small inputs</li> <li>Even if runtime looks linear, the machine might start trashing for lack of   memory and thus not scale linearly in the real world</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-46-test-your-estimate","title":"PP_Tip 46: Test your estimate","text":"<ul> <li>It's tricky to get accurate execution times</li> <li>Use code profilers to count the number of times different steps of your   algorithm get executed and plot against the input size</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#be-wary-of-premature-optimization","title":"Be wary of premature optimization","text":"<ul> <li>Make sure an algorithm is really the bottleneck before investing precious time   trying to improve it</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#refactoring","title":"Refactoring","text":"<ul> <li>= re-writing, re-working, re-architecting code</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-47-refactor-early-refactor-often","title":"PP_Tip 47: Refactor early, refactor often","text":"<ul> <li>If you cannot refactor immediately</li> <li>Make space in the schedule</li> <li>File a bug</li> <li>Limit the spread of the virus</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#how-to-refactor","title":"How to refactor","text":"<ul> <li>Refactoring needs to be undertaken slowly, deliberately, and carefully</li> <li>Don't refactor and add functionality at the same time</li> <li>Make sure you have good tests before refactoring</li> <li>Take baby steps to avoid prolonged debugging</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-47-software-development-is-more-gardening-than-building","title":"PP_Tip 47: Software development is more gardening than building","text":"<ul> <li> <p>Unfortunately the most common metaphor for software development is building   construction: blueprints, build, release, maintenance, ...</p> </li> <li> <p>As a program evolves, it is necessary to rethink earlier decisions and rework   portions of the code</p> </li> <li>Software is more like gardening:</li> <li>Start with a plan</li> <li>Some plants die</li> <li>Move plants to get in the sun / shade</li> <li>Pull weeds</li> <li>Monitor health of the plants</li> <li>Make adjustments</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-47-when-to-refactor","title":"PP_Tip 47: When to refactor?","text":"<ul> <li>It is time to refactor when you notice:</li> <li>Duplication</li> <li>Non-orthogonal design</li> <li>Outdated knowledge</li> <li>Bad performance</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-47-management-and-refactoring","title":"PP_Tip 47: Management and refactoring","text":"<ul> <li>How do you explain to your boss that \"the code works, but it needs to be   refactored?\"</li> <li>Fail to refactor now and the investment to fix the problem later on will be   larger</li> <li>It's like accumulating debt: at some point it will need to be repaid, with   interests!</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-48-design-to-test","title":"PP_Tip 48: Design to test","text":"<ul> <li>Chips are designed to be tested</li> <li>At the factory</li> <li>When they are installed</li> <li> <p>When they are deployed in the field</p> </li> <li> <p>BIST (Built-In Self Test) features</p> </li> <li> <p>TAM (Test Access Mechanism) = test harness to provide stimuli and collect   responses</p> </li> <li> <p>We need to build testability into the software from the very beginning</p> </li> <li>Test each piece thoroughly (unit testing) before wiring them together   (integration testing)</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-48-testing-against-contract","title":"PP_Tip 48: Testing against contract","text":"<ul> <li>Write test cases that ensure that a unit honors its contract</li> <li> <p>This also checks whether the contract means what we think it means</p> </li> <li> <p>We need to check over a wide range of test cases and boundary conditions</p> </li> <li>There's no better way to fix errors than by avoiding them in the first place</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#test-driven-development","title":"Test-driven development","text":"<ul> <li>By building the tests before you implement the code, you try out the interface   before you commit to it</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-48-where-to-put-unit-tests","title":"PP_Tip 48: Where to put unit tests?","text":"<ul> <li>Unit tests should be somehow close to the code they test</li> <li>E.g., in a parallel directory</li> <li> <p>If something is not easy to find and use, it won't be used</p> </li> <li> <p>By making the test code readily accessible, one provides:</p> </li> <li>Examples of how to use a module</li> <li>A means to validate any future changes to code</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-48-provide-a-test-harness","title":"PP_Tip 48: Provide a test harness","text":"<ul> <li>It's good to have a way to:</li> <li>Select tests to run</li> <li>Pass arguments on the command line to control the testing</li> <li>Analyze the test output</li> <li>Automatically find all the unit tests (instead of having a list of tests, so     that we can honor the DRY principle)</li> <li>Test harness can be built using OOP</li> <li>E.g., objects provide setup and cleanup methods, standard form of failure     report, ...</li> <li>There are standard test harness (e.g., <code>unittest</code> for Python, <code>cppunit</code> for   C++)</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-48-build-a-test-backdoor","title":"PP_Tip 48: Build a test backdoor","text":"<ul> <li>No piece of software is perfect and bugs show up in real world</li> <li>Have log files containing trace messages</li> <li>Log messages should be in a regular, consistent format</li> <li>An interesting technique is to have a built-in web server in the application</li> <li>Pointing to a certain port of the machine one can see internal status, log     entries, a debug control panel</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-49-test-your-software-or-your-users-will","title":"PP_Tip 49: Test your software or your users will","text":"<ul> <li>All software you write will be tested</li> <li>If not by you, then by the eventual users</li> <li>It is better to test it thoroughly than being swamped in help desk calls</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-50-dont-use-wizard-code-you-dont-understand","title":"PP_Tip 50: Don't use wizard code you don't understand","text":"<ul> <li>Applications are getting harder and harder to write</li> <li>User interfaces are becoming increasingly sophisticated</li> <li>The underlying applications are getting more complex</li> <li> <p>One can use wizards to generate code to perform most functions, but one must   understand all of it (otherwise one is programming by coincidence)</p> </li> <li> <p>One could argue that we routinely rely on things we don't understand</p> </li> <li>E.g., quantum mechanics in integrated circuits</li> <li>What we don't understand is behind a tidy interface</li> <li>Wizard code is interwoven with our application</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#before-the-project","title":"Before the project","text":""},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#requirement","title":"Requirement","text":"<ul> <li>= a statement about something that needs to be accomplished</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-51-dont-gather-requirements-dig-for-them","title":"PP_Tip 51: Don't gather requirements: dig for them","text":"<ul> <li>Gathering requirements implies that the requirements are already there</li> <li>In reality they are buried deep beneath layers of</li> <li>Assumptions</li> <li>Misconceptions</li> <li>Politics</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-51-example-of-tricky-requirements","title":"PP_Tip 51: Example of tricky requirements","text":"<ul> <li>A requirements can sound like: \"Only an employee's supervisors and HR may view   an employee's records\"</li> <li>This requirement has a policy embedded in it</li> <li>We don't want to hard-wire policies into requirements, since policies change     all the time</li> <li>Requirements should be a general statement: \"An employee record may be viewed   only by a nominated group of people\"</li> <li>Give the policy as an example of what should be supported</li> <li>Policies should eventually go in the metadata of the application</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-52-work-with-a-user-to-think-like-a-user","title":"PP_Tip 52: Work with a user to think like a user","text":"<ul> <li>We need to discover the underlying reason why users do a thing, rather than   just the way they currently do it</li> <li>We want to solve a business problem, not just check off requirements</li> <li>You can ask the user to sit with him for a week while he does his job</li> <li>See how the system will be really used, not how management intended to be     used</li> <li>Build trust and establish communication with your users</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#use-cases","title":"Use cases","text":"<ul> <li>= capture requirements through a particular use of the system</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-53-use-cases","title":"PP_Tip 53: Use cases","text":"<ul> <li>While sitting with the user, you see a few interesting scenarios that describe   what the application needs to do</li> <li>Write the scenarios in a document that everyone (developers, end users,   project sponsor, management) can discuss</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-53-abstractions-live-longer-than-details","title":"PP_Tip 53: Abstractions live longer than details","text":"<ul> <li>Good requirement docs should remain abstract</li> <li>They reflect the business needs, not architecture, not design, not user   interface, ...</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-53-feature-itis","title":"PP_Tip 53: Feature-itis","text":"<ul> <li>Aka feature bloat</li> <li>The issue is that by adding \"just one more feature\", the scope of the project   keeps growing</li> <li>One should track the number of bugs reported and fixed, the number of   additional features requested and who approved them</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#project-glossary","title":"Project glossary","text":"<ul> <li>= one place that defines all the specific terms and vocabulary used in a   project</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-54-use-a-project-glossary","title":"PP_Tip 54: Use a project glossary","text":"<ul> <li>It's hard to succeed if users and developers</li> <li>Refer to the same thing with different names, or</li> <li>Refer to different things with the same name</li> <li>Create and maintain a project glossary</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-55-dont-think-outside-the-box-find-the-box","title":"PP_Tip 55: Don't think outside the box: find the box","text":"<ul> <li>The secret to solve a puzzle is to identify the real (not imagined)   constraints and find a solution therein</li> <li>Some constraints are preconceived notions</li> <li> <p>E.g., \"the Gordian knot\"</p> </li> <li> <p>E.g., the Trojan horse:</p> </li> <li>How do you get troops into a walled city?</li> <li> <p>Would you have dismissed the idea of getting troops \"through the front     door\"?</p> </li> <li> <p>To solve a problem enumerate all the possible avenues</p> </li> <li>Don't dismiss anything, then explain why a certain path cannot be taken. Can     you prove it?</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-55-impossible-problems","title":"PP_Tip 55: Impossible problems","text":"<ul> <li>You are stuck on a problem that seems \"impossible\"</li> <li>You are late on the schedule</li> <li>Step back and ask yourself:</li> <li>Is there an easier way?</li> <li>What is that makes this problem so hard to solve?</li> <li>Does it have to be done this way?</li> <li>Does it have to be done at all?</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-56-listen-to-nagging-doubts-start-when-you-are-ready","title":"PP_Tip 56: Listen to nagging doubts: start when you are ready","text":"<ul> <li>When you experience some reluctance when faced with a task, take notice</li> <li>Sometimes your instinct is right on the spot, although you cannot put a finger   on it</li> <li>How can you tell that is not just procrastination?</li> <li> <p>You can start prototyping and verify if some basic premises were wrong</p> </li> <li> <p>Program specification</p> </li> <li>= process of reducing requirements to the point where coding can start</li> <li>The goal is to remove major ambiguities</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-57-some-things-are-better-done-than-described","title":"PP_Tip 57: Some things are better done than described","text":"<ul> <li>Program specification is an agreement with the user</li> <li>It's important to stop increasing level of detail and start coding,   prototyping, tracer bullet, because:</li> <li>It is naive to assume that a specification will ever capture every detail</li> <li>Once the system is running, users will ask for changes</li> <li>Natural language is not expressive enough to clarify everything</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-58-dont-be-a-slave-to-formal-methods","title":"PP_Tip 58: Don't be a slave to formal methods","text":"<ul> <li>Many methods have been developed to make programming more like engineering   (e.g., waterfall development, UML, ...)</li> <li>These formal methods use a combination of diagrams and supporting words</li> <li>The user typically does not understand them and cannot provide feedback</li> <li>It's better to give the users a prototype and let them play with it</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-59-expensive-tools-do-not-produce-better-designs","title":"PP_Tip 59: Expensive tools do not produce better designs","text":"<ul> <li>Never give in into a methodology just because it is the hot new fad</li> <li>Do not think about how much a tool costs when you look at its output</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pragmatic-projects","title":"Pragmatic projects","text":""},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-60-pragmatic-teams","title":"PP_Tip 60: Pragmatic teams","text":"<ul> <li> <p>Most of the pragmatic programming principles apply to teams, as much as they   apply to an individual</p> </li> <li> <p>No broken windows</p> </li> <li>Quality is a team issue</li> <li> <p>Team as a whole should not tolerate broken windows (= small imperfections      that no one fixes)</p> </li> <li> <p>Boiled frogs</p> </li> <li>People assume that someone is handling an issue or that someone must have      OK-ed a change request from the user</li> <li> <p>Fight this: everyone must actively monitor the environment for changes</p> </li> <li> <p>Communicate</p> </li> <li> <p>Great teams speak with one voice and are always prepared</p> </li> <li> <p>Don't repeat yourself (DRY)</p> </li> <li> <p>Some teams appoint a member as the project librarian, responsible for      checking on duplication, coordinating documentation, ...</p> </li> <li> <p>Orthogonality</p> </li> <li>Traditional team organization is based on the old-fashioned waterfall      method of software construction<ul> <li>Individuals are assigned roles based on their job function, e.g.,</li> <li>Business analysts</li> <li>Architects</li> <li>Designers</li> <li>Programmers</li> <li>Testers</li> </ul> </li> <li>Different activities of a project (analysis, design, coding, testing) can't      happen in isolation</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-60-organize-around-functionality-not-job-functions","title":"PP_Tip 60: Organize around functionality, not job functions","text":"<ul> <li>Organize people in the same way one organizes code</li> <li>Design by contract</li> <li>Decoupling</li> <li> <p>Orthogonality</p> </li> <li> <p>Split teams by functionality not by job function (e.g., architect, programmer,   tester)</p> </li> <li> <p>We look for cohesive, self-contained teams of people</p> </li> <li> <p>Let each team organize themselves internally</p> </li> <li>Each team has agreed upon responsibilities and commitments to the other     teams</li> <li>Of course this approach works only with responsible developers and strong     project management</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-61-dont-use-manual-procedures","title":"PP_Tip 61: Don't use manual procedures","text":"<ul> <li>We want to ensure consistency and repeatability in the project</li> <li>Manual procedures leave consistency up to chance</li> <li> <p>Let the computer do the mundane jobs: it will do a better job than we do</p> </li> <li> <p>Compiling the project should be reliable and repeatable</p> </li> <li>We want to check out, build, test, ship with a single command</li> <li><code>make</code> and <code>cronjobs</code> are the solutions</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-62-test-early","title":"PP_Tip 62: Test early","text":"<ul> <li>Look for your bugs now, so you don't have to endure the shame of others   finding your bugs later</li> <li>Start testing as soon as you have code</li> <li>Code a little, test a little</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-62-test-often","title":"PP_Tip 62: Test often","text":"<ul> <li>The earlier a bug is found, the cheaper it is to remedy</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-62-test-automatically","title":"PP_Tip 62: Test automatically","text":"<ul> <li>Tests that run with every build are better than test plans that sit on a shelf</li> <li>A good project may well have more test code than production code</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp-top-63-coding-aint-done-til-all-the-tests-run","title":"PP Top 63: Coding ain't done 'til all the tests run","text":"<ul> <li>Just because you finished coding, you cannot tell that the code is done</li> <li>You cannot claim that the code is done until it passes all the available tests</li> <li>Code is never really done</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-63-what-to-test","title":"PP_Tip 63: What to test","text":"<ul> <li>There are multiple types of tests:</li> <li>Unit</li> <li>Integration</li> <li>Validation</li> <li>Performance</li> <li>Usability tests</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#unit-test","title":"Unit test","text":"<ul> <li>= exercise a module</li> <li>If the parts don't work by themselves, they won't work together</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#integration-test","title":"Integration test","text":"<ul> <li>= show that the major subsystems work well together</li> <li>Integration is the largest source of bugs in the system</li> <li>Test that the entire system honors its contract</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#validation-and-verification-test","title":"Validation and verification test","text":"<ul> <li>= the users told you what they wanted, but is it what they really need?</li> <li>A bug-free system that answers the wrong question is not useful</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#resource-exhaustion-errors-and-recovery-test","title":"Resource exhaustion, errors, and recovery test","text":"<ul> <li>Resources are limited in the real world, e.g.:</li> <li>Memory</li> <li>Disk space / bandwidth</li> <li>CPU</li> <li>Network bandwidth</li> <li>Video resolution</li> <li>How will the system behave under real-world conditions?</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#performance-test","title":"Performance test","text":"<ul> <li>= testing under a given load (e.g., expected number of users, connections,   transactions per second)</li> <li>Does the system scale?</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#usability-test","title":"Usability test","text":"<ul> <li>= performed with real users, under real environmental conditions</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-63-how-to-test","title":"PP_Tip 63: How to test","text":"<ul> <li>Run regression tests for all types of tests</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#regression-testing","title":"Regression testing","text":"<ul> <li>= compare output of current test with previous known values</li> <li>It ensures that fixes for today's bugs don't break things that were working   yesterday</li> <li>All types of tests can be run as regression tests</li> <li>Unit</li> <li>Integration</li> <li>Validation</li> <li>Performance</li> <li>Usability tests</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#test-data","title":"Test data","text":"<ul> <li>Test data is either real-world data or synthetic data</li> <li>One needs to use both, since they expose different kind of bugs</li> <li>Synthetic data</li> <li>Stresses boundary conditions</li> <li>Can have certain statistical properties (e.g., data to sort is already     sorted or inversely sorted)</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#exercising-gui-systems","title":"Exercising GUI systems","text":"<ul> <li>Often specialized testing tools are required, e.g.,</li> <li>Event capture / playback model</li> <li>A data processing application with GUI front end should be decoupled so that     one can test each component by itself</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#testing-the-tests","title":"Testing the tests","text":"<ul> <li>We cannot write perfect software</li> <li>We need to test the tests and the test infrastructure</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#testing-thoroughly","title":"Testing thoroughly","text":"<ul> <li>Use coverage analysis tools to keep track of which lines of the code have been   / not been executed</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-64-use-saboteurs-to-test-your-testing","title":"PP_Tip 64: Use saboteurs to test your testing","text":"<ul> <li>If the system is a security system, test the system by trying to break in</li> <li>After you have written a test to detect a particular bug, cause the bug   deliberately and make sure the tests catch it</li> <li>Write test for both positive and negative cases</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-65-test-state-coverage-not-code-coverage","title":"PP_Tip 65: Test state coverage, not code coverage","text":"<ul> <li>Knowing that you executed all code does not tell you if you tested all states</li> <li>This is a combinatorial problem</li> <li>You can tame the complexity thinking about:</li> <li>Boundary conditions</li> <li>Structure of the code</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-65-when-to-test","title":"PP_Tip 65: When to test","text":"<ul> <li>As soon as any code exists, it must be tested</li> <li>Testing should be done automatically as often as we can (e.g., before code is   committed to the repository)</li> <li>Also results should be easy to interpret: ideally the outcome is binary <code>ok</code> /   <code>not_ok</code></li> <li>Expensive / special tests can be run less frequently, but on a regular basis</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-66-find-bugs-once","title":"PP_Tip 66: Find bugs once","text":"<ul> <li>Once a human tester finds a bug, a new test should be created to check for   that bug every time</li> <li>You don't want to keep chasing the same bugs that the automated tests could   find for you</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-67-treat-english-as-just-another-programming-language","title":"PP_Tip 67: Treat English as just another programming language","text":"<ul> <li>Embrace documentation as an integral part of software development</li> <li>Keep the documentation in the code itself as much as possible</li> <li>Treat code and documentation as two views of the same model</li> <li>Apply all the principles learned for coding (DRY principle, orthogonality,   ...) to English as well</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-68-internal-documentation","title":"PP_Tip 68: Internal documentation","text":"<ul> <li>Source code comments</li> <li>Design documents</li> <li>Test documents</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#external-documentation","title":"External documentation","text":"<ul> <li>= anything that is shipped or published to the outside world together with the   software product (e.g., user manuals)</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#documentation-vs-code","title":"Documentation vs code","text":"<ul> <li>Documentation and code are different views of the same underlying model</li> <li>If there is a discrepancy, the code is what matters</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-68-comments","title":"PP_Tip 68: Comments","text":"<ul> <li>Code should have comments, but too many comments are as bad as too few</li> <li>Comments should discuss \\textit{why} something is done (e.g, engineering     trade-offs, why decisions were made)</li> <li>The code already shows \\textit{how} it is done</li> <li>Modules, class, methods should have comments describing what is not obvious</li> <li>Javadoc notation is useful (<code>\\@param</code>, <code>\\@return</code>, ...) to extract information   from the code automatically</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#naming-concepts","title":"Naming concepts","text":"<ul> <li>Variable names should be meaningful</li> <li>Remember that you will be writing the code once, but reading it hundreds of   time: avoid write-only code</li> <li>Misleading names are worse than meaningless names</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-68-automatically-generated-documentation","title":"PP_Tip 68: Automatically generated documentation","text":"<ul> <li>Also for documentation we want to use pragmatic principles</li> <li>DRY principle</li> <li>Orthogonality</li> <li>Model-view-controller</li> <li> <p>Automation</p> </li> <li> <p>Printed material is out of date as soon as it is printed</p> </li> <li>Publish documents on-line</li> <li>There should be a single command to generate and publish the documents on-line</li> <li>Use a timestamp or review number for each page</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-69-gently-exceed-your-users-expectations","title":"PP_Tip 69: Gently exceed your users' expectations","text":"<ul> <li>Success of a project is measured by how well it meets the expectations of its   users</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#examples-of-difference-between-actual-and-expected-results","title":"Examples of difference between actual and expected results","text":"<ul> <li>A company announces record profits, and its share price drops 20\\%</li> <li>It didn't meet analysts' expectations</li> <li>A child opens an expensive present and bursts into tears</li> <li>It wasn't the cheap doll that she wanted</li> <li>A team works miracles to implement a complex application</li> <li>The users don't like it because it does not have an help system</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-69-communicating-expectations","title":"PP_Tip 69: Communicating expectations","text":"<ul> <li>Users come to you with some vision of what they want</li> <li>It may be</li> <li>Incomplete</li> <li>Inconsistent</li> <li>Impossible</li> <li>They are invested in it: you cannot ignore this</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#manage-expectations","title":"Manage expectations","text":"<ul> <li>Work with your users so that they understand what you are delivering</li> <li>Never lose sight of the business problems your application is intended to   solve</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#go-the-extra-mile","title":"Go the extra mile","text":"<ul> <li>Surprise and delight your users</li> <li>E.g., balloon help, colorization, automatic installation, splash screen   customized for their organization, ...</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-70-sign-your-work","title":"PP_Tip 70: Sign your work","text":"<ul> <li>Craftsmen of earlier ages were proud to sign their work</li> <li>Your signature should come to be recognized as an indicator of quality</li> </ul>"},{"location":"coding/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip-70-code-ownership-vs-anonymity","title":"PP_Tip 70: Code ownership vs anonymity","text":"<ul> <li>Code ownership can cause cooperation problems: people become territorial</li> <li>Anonymity can enable sloppiness, laziness</li> </ul>"},{"location":"coding/all.code_review.how_to_guide.html","title":"Code Review","text":""},{"location":"coding/all.code_review.how_to_guide.html#code-review_1","title":"Code review","text":""},{"location":"coding/all.code_review.how_to_guide.html#general-rules-about-code-review","title":"General rules about code review","text":""},{"location":"coding/all.code_review.how_to_guide.html#read-the-google-code-review-best-practices","title":"Read the Google code review best practices","text":"<ul> <li> <p>From the   developer\\'s perspective</p> </li> <li> <p>From the   reviewer\\'s perspective</p> </li> <li> <p>Where the Google guide says \"CL\", think \"PR\"</p> </li> <li> <p>Read it (several times, if you need to)</p> </li> <li> <p>Think about it</p> </li> <li> <p>Understand the rationale</p> </li> </ul>"},{"location":"coding/all.code_review.how_to_guide.html#code-review-workflows","title":"Code review workflows","text":""},{"location":"coding/all.code_review.how_to_guide.html#pull-request","title":"Pull request","text":"<ul> <li>Our usual review process is to work in a branch and create a pull request</li> <li>See the     Git     notes for details</li> <li>The name of the pull request is generated with ghi_show.py and looks like     PTask2704 make exchange contracts get contracts applicable to series</li> </ul>"},{"location":"coding/all.code_review.how_to_guide.html#from-the-code-author-point-of-view","title":"From the code author point of view","text":""},{"location":"coding/all.code_review.how_to_guide.html#why-we-review-code","title":"Why we review code","text":"<ul> <li>We spend time reviewing each other code so that we can:</li> <li>Build a better product, by letting other people look for bugs</li> <li>Propagate knowledge of the code base through the team</li> <li>Learn from each other</li> </ul>"},{"location":"coding/all.code_review.how_to_guide.html#pr-checklist","title":"PR checklist","text":"<ul> <li> <p>From   Google reviewer checklist:</p> </li> <li> <p>In asking (and doing) a code review, you should make sure that:</p> </li> <li>The code is well-designed.</li> <li>The functionality is good for the users of the code.</li> <li>The code isn't more complex than it needs to be.</li> <li>The developer isn't implementing things they might need in the future but     don't know they need now.</li> <li>Code has appropriate unit tests.</li> <li>Tests are well-designed.</li> <li>The developer used clear names for everything.</li> <li>Comments are clear and useful, and mostly explain why instead of what.</li> <li>Code is appropriately documented.</li> <li>The code conforms to our style guides.</li> </ul>"},{"location":"coding/all.code_review.how_to_guide.html#the-golden-rule-of-code-review","title":"The golden rule of code review","text":"<ul> <li>Make life easy for the reviewers</li> <li> <p>Aka \"Do not upset the reviewers, otherwise they won't let you merge your     code\"</p> </li> <li> <p>Remember that reviewing other people's code is hard and unrewarding work</p> </li> <li> <p>Do your best for not frustrating the reviewers</p> </li> <li> <p>If you are in doubt \"it's probably clear, although I am not 100% sure\", err on   giving more information and answer potential questions</p> </li> </ul>"},{"location":"coding/all.code_review.how_to_guide.html#be-clear-in-the-pr-request-about-what-you-want","title":"Be clear in the PR request about what you want","text":"<ul> <li>Summarize what was done in the PR</li> <li>Refer to the GH task, but the task alone might not be sufficient</li> <li> <p>A PR can implement only part of a complex task</p> <ul> <li>Which part is it implementing?</li> <li>Why is it doing it in a certain way?</li> </ul> </li> <li> <p>If the code is not ready for merge, but you want a \"pre-review\" convert PR to   a draft</p> </li> <li>E.g., ask for an architectural review</li> <li> <p>Draft PRs can not be merged</p> </li> <li> <p>Is it blocking?</p> </li> <li>Do not abuse asking for a quick review</li> <li>All code is important and we do our best to review code quickly and     carefully</li> <li>If it\\'s blocking a ping on IM is a good idea</li> </ul>"},{"location":"coding/all.code_review.how_to_guide.html#do-not-mix-changes-and-refactoring-shuffling-code","title":"Do not mix changes and refactoring / shuffling code","text":"<ul> <li>The job of the reviewers become frustrating when the author mixes:</li> <li>Refactoring / moving code; and</li> <li> <p>Changes</p> </li> <li> <p>It is time consuming or impossible for a reviewer to understand what happened:</p> </li> <li>What is exactly changed?</li> <li> <p>What was moved where?</p> </li> <li> <p>In those cases reviewers have the right to ask the PR to be broken in pieces</p> </li> <li> <p>One approach for the PR author is to:</p> </li> <li>Do a quick PR to move code around (e.g., refactoring) or purely cosmetic<ul> <li>You can ask the reviewer to take a quick look</li> </ul> </li> <li> <p>Do the next PRs with the actual changes</p> </li> <li> <p>Another approach is to develop in a branch and break the code into PRs as the   code firms up</p> </li> <li>In this case you need to be very organized and be fluent in using Git: both     qualities are expected of you</li> <li>E.g., develop in a branch (e.g., <code>gp_scratch</code>)</li> <li>Create a branch from it (e.g., <code>TaskXYZ_do_this_and_that</code>) or copy the files     from <code>gp_scratch</code> to <code>TaskXYZ_do_this_and_that</code></li> <li>Edit the files to make the PR self-consistent</li> <li>Do a PR for <code>TaskXYZ_do_this_and_that</code></li> <li>Keep working in gp_scratch while the review is moving forward</li> <li>Make changes to the <code>TaskXYZ_do_this_and_that</code> as requested</li> <li>Merge <code>TaskXYZ_do_this_and_that</code> to master</li> <li>Merge <code>master</code> back into <code>gp_scratch</code> and keep moving</li> </ul>"},{"location":"coding/all.code_review.how_to_guide.html#double-check-before-sending-a-pr","title":"Double check before sending a PR","text":"<ul> <li>After creating a PR take a look at it to make sure things look good, e.g.,</li> <li>Are there merge problems?</li> <li>Did you forget some file?</li> <li>Skim through the PR to make sure that people can understand what you changed</li> </ul>"},{"location":"coding/all.code_review.how_to_guide.html#reviewing-other-peoples-code-is-usually-not-fun","title":"Reviewing other people's code is usually not fun","text":"<ul> <li>Reviewing code is time-consuming and tedious</li> <li>So do everything you can to make the reviewer's job easier</li> <li> <p>Don't cut corners</p> </li> <li> <p>If a reviewer is confused about something, other readers (including you in 1   year) likely would be too</p> </li> <li>What is obvious to you as the author is often not obvious to readers</li> <li>Readability is paramount</li> <li>You should abhor write-only code</li> </ul>"},{"location":"coding/all.code_review.how_to_guide.html#the-first-reviews-are-painful","title":"The first reviews are painful","text":"<ul> <li>One needs to work on the same code over and over</li> <li> <p>Just think about the fact that the reviewer is also reading (still crappy)     code over and over</p> </li> <li> <p>Unfortunately it is needed pain to get to the quality of code we need to make   progress as a team</p> </li> </ul>"},{"location":"coding/all.code_review.how_to_guide.html#apply-review-comments-everywhere","title":"Apply review comments everywhere","text":"<ul> <li> <p>Apply a review comment everywhere, not just where the reviewer pointed out the   issue</p> </li> <li> <p>E.g., reviewer says:</p> </li> <li>\"Please replace <code>_LOG.warning(\"Hello %s\".format(name))</code> with     <code>_LOG.warning(\"Hello %s\", name)</code>\"</li> <li>You are expected to do this replacement:</li> <li>In the current review</li> <li>In all future code you write</li> <li>In old code, as you come across it in the course of your work<ul> <li>Of course don't start modifying the old code in this review, but open a   clean-up bug, if you need a reminder</li> </ul> </li> </ul>"},{"location":"coding/all.code_review.how_to_guide.html#look-at-the-code-top-to-bottom","title":"Look at the code top-to-bottom","text":"<ul> <li>E.g., if you do a search &amp; replace, make sure everything is fine</li> </ul>"},{"location":"coding/all.code_review.how_to_guide.html#answering-comments-after-a-review","title":"Answering comments after a review","text":"<ul> <li>It's better to answer comments in chunks so we don't get an email per comment</li> <li>Use \"start a review\" (not in conversation)</li> <li>If one of the comment is urgent (e.g., other comments depend on this) you can   send it as single comment</li> <li>When you answer a comment, mark it as resolved</li> </ul>"},{"location":"coding/all.code_review.how_to_guide.html#apply-changes-to-a-review-quickly","title":"Apply changes to a review quickly","text":"<ul> <li>In the same way the reviewers are expected to review PRs within 24 hours, the   author of a PR is expected to apply the requested changes quickly, ideally in   few hours</li> <li> <p>If it takes longer, then either the PR was too big or the quality of the PR     was too low</p> </li> <li> <p>If it takes too long to apply the changes:</p> </li> <li>The reviewers (and the authors) might forget what is the context of the     requested changes</li> <li>It becomes more difficult (or even impossible) to merge, since the code base     is continuously changing</li> <li>It creates dependencies among your PRs</li> <li>Remember that you should not be adding more code to the same PR, but only     fix the problems and then open a PR with new code</li> <li>Other people that rely on your code are blocked</li> </ul>"},{"location":"coding/all.code_review.how_to_guide.html#ask-for-another-review","title":"Ask for another review","text":"<ul> <li>Once you are done with resolving all the comments ask for another review</li> </ul>"},{"location":"coding/all.code_review.how_to_guide.html#workflow-of-a-review-in-terms-of-gh-labels","title":"Workflow of a review in terms of GH labels","text":"<ul> <li>The current meaning of the labels are:</li> <li>See GitHub ZenHub workflows     doc</li> </ul>"},{"location":"coding/all.code_review.how_to_guide.html#link-pr-to-gh-issue","title":"Link PR to GH issue","text":"<ul> <li>Mention the corresponding issue in the PR description to ease the navigation   E.g., see an   example</li> </ul>"},{"location":"coding/all.code_review.how_to_guide.html#fix-later","title":"Fix later","text":"<ul> <li> <p>It's ok for an author to file a follow up Issue (e.g., with a clean up), by   pointing the new Issue to the comments to address, and move on with merge</p> </li> <li> <p>The Issue needs to be addressed immediately after</p> </li> </ul>"},{"location":"coding/all.code_review.how_to_guide.html#from-the-code-reviewer-point-of-view","title":"From the code reviewer point of view","text":""},{"location":"coding/all.code_review.how_to_guide.html#post-commit-review","title":"Post-commit review","text":"<ul> <li> <p>You can comment on a PR already merged</p> </li> <li> <p>You can comment on the relevant lines in a commit straight to <code>master</code> (this   is the exception)</p> </li> </ul>"},{"location":"coding/all.code_review.how_to_guide.html#code-walk-through","title":"Code walk-through","text":"<ul> <li>It is best to create a branch with the files you want to review</li> <li>Add TODOs in the code (so that the PR will pick up those sections)</li> <li> <p>File bugs for the more involved changes</p> </li> <li> <p>Try to get a top to bottom review of a component once every N weeks (N = 2, 3)</p> </li> <li>Sometimes the structure of the</li> </ul>"},{"location":"coding/all.code_review.how_to_guide.html#close-the-pr-and-delete-the-branch","title":"Close the PR and delete the branch","text":"<ul> <li> <p>When code is merged into master by one of the reviewers through the UI one can   select the delete branch option</p> </li> <li> <p>Otherwise you can delete the branch using the procedure in   Git</p> </li> </ul>"},{"location":"coding/all.code_review.how_to_guide.html#give-priority-to-code-review","title":"Give priority to code review","text":"<ul> <li>We target to give feedback on a PR within 24hr so that the author is not   blocked for too long</li> <li>Usually we respond in few hours</li> </ul>"},{"location":"coding/all.code_review.how_to_guide.html#multiple-reviewers-problem","title":"Multiple reviewers problem","text":"<ul> <li> <p>When there are multiple reviewers for the same PR there can be some problem</p> </li> <li> <p>Ok to keep moving fast and avoid blocking</p> </li> <li> <p>Block only if it is controversial</p> </li> <li> <p>Merge when we are confident that the other is ok</p> </li> <li>The other can catch up with post-commit review</li> <li>A good approach is to monitor recently merged PRs in GH to catch up</li> </ul>"},{"location":"coding/all.code_review.how_to_guide.html#remember-small-steps-ahead","title":"Remember \"small steps ahead\"","text":"<ul> <li>Follow the Google approach of merging a PR that is a strict improvement.</li> </ul>"},{"location":"coding/all.code_review.how_to_guide.html#nothing-is-too-small","title":"Nothing is too small","text":"<ul> <li> <p>Each reviewer reviews the code pointing out everything that can be a problem</p> </li> <li> <p>Problems are highlighted even if small or controversial</p> </li> <li> <p>Not all of those comments might not be implemented by the author</p> </li> <li> <p>Of course if different approaches are really equivalent but reviewers have   their own stylistic preference, this should not be pointed, unless it's a   matter of consistency or leave the choice to the author</p> </li> </ul>"},{"location":"coding/all.code_review.how_to_guide.html#final-gh-comment","title":"Final GH comment","text":"<ul> <li>Once you are done with the detailed review of the code, you need to</li> <li>Write a short comment</li> <li> <p>Decide what is the next step for the PR, e.g.,</p> <ul> <li>Comment</li> <li>Submit general feedback without explicit approval</li> <li>Approve</li> <li>Submit feedback and approve merging these changes</li> <li>Request changes</li> <li>Submit feedback that must be addressed before merging</li> </ul> </li> <li> <p>We use an integrator / developer manager workflow, initially with Paul and GP   testing and merging most of the PRs</p> </li> <li> <p>We use the 3 possible options in the following way:</p> </li> <li>Comment<ul> <li>When reviewers want the changes to be applies and then look at the   resulting changes to decide the next steps</li> <li>In practice this means \"make the changes and then we'll discuss more\"</li> <li>E.g., this is of course the right choice for a pre-PR</li> </ul> </li> <li>Approve<ul> <li>No more changes: time to merge!</li> <li>Often it is accompanied with the comment \"LGMT\" (Looks Good To Me)</li> </ul> </li> <li>Request changes<ul> <li>This typically means \"if you address the comments we can merge\"</li> <li>In practice this is more or less equivalent to \"Comment\"</li> </ul> </li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html","title":"Coding Style","text":""},{"location":"coding/all.coding_style.how_to_guide.html#kaizenflow-python-style-guide","title":"KaizenFlow - Python Style Guide","text":""},{"location":"coding/all.coding_style.how_to_guide.html#meta","title":"Meta","text":"<ul> <li>What we call \"rules\" are actually just a convention</li> <li>The \"rules\"</li> <li>Are optimized for the common case</li> <li>Are not the absolute best way of doing something in all possible cases</li> <li>Can become cumbersome or weird to follow for some corner cases</li> <li>We prefer simple rather than optimal rules so that they can be applied in most   of the cases, without thinking or going to check the documentation</li> <li>The rules are striving to achieve consistency and robustness</li> <li>We prefer to care about consistency rather than arguing about which approach     is better in each case</li> <li>E.g., see the futile \"tab vs space\" flame-war from the 90s</li> <li>The rules are optimized for the average developer / data scientist and not for   power users</li> <li>The rules try to minimize the maintenance burden</li> <li>We want to minimize the propagation of a change</li> <li>Rules are not fixed in stone</li> <li>Rules evolve based on what we discuss through the reviews</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#disclaimer","title":"Disclaimer","text":"<ul> <li>This document was forked from   Google Python Style Guide,   therefore, the numbering of chapters sets off where the Style Guide ends. Make   sure to familiarize yourself with it before proceeding to the rest of the doc,   since it is the basis of our team\u2019s code style.</li> <li>Another important source is   The Pragmatic Programmer   by David Thomas and Andrew Hunt. While not Python-specific, it provides an   invaluable set of general principles by which any person working with code   (software developer, DevOps or data scientist) should abide. Read it on long   commutes, during lunch, and treat yourself to a physical copy on Christmas.   The book is summarized   here,   but do not deprive yourself of the engaging manner in which Thomas &amp; Hunt   elaborate on these points -- on top of it all, it is a very, very enjoyable   read.</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#references","title":"References","text":"<ul> <li>Coding</li> <li>Google Python Style Guide (GPSG)</li> <li>Code convention from PEP8</li> <li>Documentation</li> <li>Docstring convention from PEP257</li> <li>Google documentation best practices</li> <li>Commenting style</li> <li>Sphinx</li> <li>Sphinx tutorial</li> <li>Design</li> <li>Google philosophical stuff</li> <li>Unix rules (although a bit cryptic sometimes)</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#high-level-principles","title":"High-Level Principles","text":"<ul> <li>In this paragraph we summarize the high-level principles that we follow for   designing and implementing code and research. We should be careful in adding   principles here. Ideally principles should be non-overlapping and generating   all the other lower level principles we follow (like a basis for a vector   space)</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#follow-the-dry-principle","title":"Follow the DRY principle","text":""},{"location":"coding/all.coding_style.how_to_guide.html#the-writer-is-the-reader","title":"The writer is the reader","text":"<ul> <li>Make code easy to read even if it is more difficult to write</li> <li>Code is written 1x and read 100x</li> <li>Remember that even if things are perfectly clear now to the person that wrote   the code, in a couple of months the code will look foreign to whoever wrote   the code.</li> <li>So make your future-self's life easier by following the conventions and erring   on the side of documenting for the reader.</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#encapsulate-what-changes","title":"Encapsulate what changes","text":"<ul> <li>Separate what changes from what stays the same</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#least-surprise-principle","title":"Least surprise principle","text":"<ul> <li>Try to make sure that the reader is not surprised</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#pay-the-technical-debt","title":"Pay the technical debt","text":"<ul> <li>Any unpaid debt is guaranteed to bite you when you don't expect it</li> <li>Still some debt is inevitable: try to find the right trade-off</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#end-to-end-first","title":"End-to-end first","text":"<ul> <li>Always focus on implementing things end-to-end, then improve each block</li> <li>Remember the analogy of building the car through the skateboard, the bike,   etc.</li> <li>Compare this approach to building wheels, chassis, with a big-bang     integration at the end</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#unit-test-everything","title":"Unit test everything","text":"<ul> <li>Code that matters needs to be unit tested</li> <li>Code that doesn't matter should not be checked in the repo</li> <li>The logical implication is: all code checked in the repo should be unit tested</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#dont-get-attached-to-code","title":"Don't get attached to code","text":"<ul> <li>It's ok to delete, discard, retire code that is not useful any more</li> <li>Don't take it personally when people suggest changes or simplification</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#always-plan-before-writing-code","title":"Always plan before writing code","text":"<ul> <li>File a GitHub issue</li> <li>Think about what to do and how to do it</li> <li>Ask for help or for a review</li> <li>The best code is the one that we avoid to write through a clever mental   kung-fu move</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#think-hard-about-naming","title":"Think hard about naming","text":"<ul> <li>Finding a name for a code object, notebook, is extremely difficult but very   important to build a mental map</li> <li>Spend the needed time on it</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#look-for-inconsistencies","title":"Look for inconsistencies","text":"<ul> <li>Stop for a second after you have, before sending out:</li> <li>Implemented code or a notebook</li> <li>Written documentation</li> <li>Written an e-mail</li> <li>...</li> <li>Reset your mind and look at everything with fresh eyes like if it was the   first time you saw it</li> <li>Does everything make sense to someone that sees this for the first time?</li> <li>Can (and should) it be improved?</li> <li>Do you see inconsistencies, potential issues?</li> <li>It will take less and less time to become good at this</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#no-ugly-hacks","title":"No ugly hacks","text":"<ul> <li>We don't tolerate \"ugly hacks\", i.e., hacks that require lots of work to be   undone (much more than the effort to do it right in the first place)</li> <li>Especially an ugly design hack, e.g., a Singleton, or some unnecessary     dependency between distant pieces of code</li> <li>Ugly hacks spreads everywhere in the code base</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#our-coding-suggestions","title":"Our coding suggestions","text":""},{"location":"coding/all.coding_style.how_to_guide.html#being-careful-with-naming","title":"Being careful with naming","text":""},{"location":"coding/all.coding_style.how_to_guide.html#follow-the-conventions","title":"Follow the conventions","text":"<ul> <li>Name executable files (scripts) and library functions using verbs (e.g.,   <code>download.py</code>, <code>download_data()</code>)</li> <li>Name classes and (non-executable) files using nouns (e.g., <code>Downloader()</code>,   <code>downloader.py</code>)</li> <li>For decorators we don't use a verb as we do for normal functions, but rather   an adjective or a past tense verb, e.g.,   <code>python   def timed(f):       \"\"\"       Add a timer decorator around a specified function.       \"\"\"       \u2026</code></li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#follow-spelling-rules","title":"Follow spelling rules","text":"<ul> <li>We spell commands in lower-case, and programs with initial upper case:</li> <li>\"Git\" (as program), \"git\" (as the command)</li> <li>We distinguish \"research\" (not \"default\", \"base\") vs \"production\"</li> <li>We use different names for indicating the same concept, e.g., <code>dir</code>, <code>path</code>,   <code>folder</code></li> <li>Preferred term is <code>dir</code></li> <li>Name of columns</li> <li>The name of columns should be <code>..._col</code> and not <code>..._col_name</code> or <code>_column</code></li> <li>Timestamp</li> <li>We spell <code>timestamp</code>, we do not abbreviate it as <code>ts</code></li> <li>We prefer timestamp to <code>datetime</code><ul> <li>E.g., <code>start_timestamp</code> instead of <code>start_datetime</code></li> </ul> </li> <li>Abbreviations</li> <li>JSON, CSV, DB, etc., are abbreviations and thus should be capitalized in     comments and docstrings, and treated as abbreviations in code when it     doesn't conflict with other rules<ul> <li>E.g., <code>convert_to_CSV</code>, but <code>csv_file_name</code> as a variable name that is not   global</li> </ul> </li> <li>Profit-and-loss: PnL instead of pnl or PNL</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#search-good-names-avoid-bad-names","title":"Search good names, avoid bad names","text":""},{"location":"coding/all.coding_style.how_to_guide.html#general-naming-rules","title":"General naming rules","text":"<ul> <li>Naming things properly is one of the most difficult task of a programmer /   data scientist</li> <li>The name needs to be (possibly) short and memorable<ul> <li>However, don't be afraid to use long names, if needed, e.g.,   <code>process_text_with_full_pipeline_twitter_v1</code></li> <li>\u0421larity is more important than number of bytes used</li> </ul> </li> <li>The name should capture what the object represents, without reference to     things that can change or to details that are not important</li> <li>The name should refer to what objects do (i.e., mechanisms), rather than how     we use them (i.e., policies)</li> <li>The name needs to be non-controversial: people need to be able to map the     name in their mental model</li> <li>The name needs to sound good in English<ul> <li>Bad: <code>AdapterSequential</code> sounds bad</li> <li>Good: <code>SequentialAdapter</code> sounds good</li> </ul> </li> <li>Some examples of how NOT to do naming:</li> <li><code>raw_df</code> is a terrible name<ul> <li>\"raw\" with respect to what?</li> <li>Cooked?</li> <li>Read-After-Write race condition?</li> </ul> </li> <li><code>person_dict</code> is bad<ul> <li>What if we switch from a dictionary to an object?</li> <li>Then we need to change the name everywhere!</li> <li>The name should capture what the data structure represents (its semantics)   and not how it is implemented</li> </ul> </li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#do-not-be-stingy","title":"Do not be stingy","text":"<ul> <li>Why calling an object <code>TimeSeriesMinStudy</code> instead of <code>TimeSeriesMinuteStudy</code>?</li> <li>Saving 3 letters is not worth</li> <li>The reader might interpret <code>Min</code> as <code>Minimal</code> (or <code>Miniature</code>, <code>Minnie</code>,     <code>Minotaur</code>)</li> <li>If you don't like to type, we suggest you get a better keyboard, e.g.,   this</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#do-not-abbreviate-just-to-save-characters","title":"Do not abbreviate just to save characters","text":"<ul> <li>Abbreviations just to save space are rarely beneficial to the reader. E.g.,</li> <li>Fwd (forward)</li> <li>Bwd (backward)</li> <li>Act (actual)</li> <li>Exp (expected)</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#when-to-use-abbreviations","title":"When to use abbreviations","text":"<ul> <li>We could relax this rule for short lived functions and variables in order to   save some visual noise.</li> <li>Sometimes an abbreviation is so short and common that it's ok to leave it   E.g.,</li> <li>Df (dataframe)</li> <li>Srs (series)</li> <li>Idx (index)</li> <li>Id (identifier)</li> <li>Val (value)</li> <li>Var (variable)</li> <li>Args (arguments)</li> <li>Kwargs (keyword arguments)</li> <li>Col (column)</li> <li>Vol (volatility) while volume is always spelled out</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#avoid-code-stutter","title":"Avoid code stutter","text":"<ul> <li>An example of code stutter: you want to add a function that returns <code>git</code> root   path in a module <code>git</code></li> <li>Bad</li> <li> <p>Name is <code>get_git_root_path()</code></p> <p>```python import helpers.git as git</p> <p>... git.get_git_root_path() ```   - You see that the module is already specifying we are talking about Git</p> </li> <li> <p>Good</p> </li> <li> <p>Name is <code>get_root_path()</code></p> <p>```python import helpers.git as git</p> <p>... git.get_root_path() ```   - This is not only aesthetic reason but a bit related to a weak form of DRY</p> </li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#comments-and-docstrings","title":"Comments and docstrings","text":""},{"location":"coding/all.coding_style.how_to_guide.html#general-conventions","title":"General conventions","text":"<ul> <li>Code needs to be properly commented</li> <li>We follow python standard PEP 257   for commenting</li> <li>PEP 257 standardizes what comments should express and how they should do it     (e.g., use triple quotes for commenting a function), but does not specify     what markup syntax should be used to describe comments</li> <li>Different conventions have been developed for documenting interfaces</li> <li>ReST</li> <li>Google (which is cross-language, e.g., C++, python, ...)</li> <li>Epytext</li> <li>Numpydoc</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#descriptive-vs-imperative-style","title":"Descriptive vs imperative style","text":"<ul> <li>We decided to use imperative style for our comments and docstrings</li> <li>Pylint and other python QA tools favor an imperative style</li> <li>From PEP 257 <code>The docstring is a phrase ending in a period. It prescribes the function or     method's effect as a command (\"Do this\", \"Return that\"), not as a description;     e.g. don't write \"Returns the pathname ...\".</code></li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#docstrings-style","title":"Docstrings style","text":"<ul> <li>We follow ReST (aka re-Structured Text) style for docstrings which is:</li> <li>The most widely supported in the python community</li> <li>Supported by all doc generation tools (e.g., epydoc, sphinx)</li> <li>Default in Pycharm</li> <li>Default in pyment</li> <li>Supported by pydocstyle (which does not support Google style as explained     here)</li> <li>Example of a function definition with ReST styled docstring:</li> </ul> <p>```python     def my_function(param1: str) -&gt; str:         \"\"\"         A one-line description of what the function does.</p> <pre><code>    A longer description (possibly on multiple lines) with a more detailed\n    explanation of what the function does, trying to not be redundant with\n    the parameter / return description below. The focus is on the interface\n    and what the user should know to use the function and not how the\n    function is implemented.\n\n    :param param1: this is a first param\n    :return: this is a description of what is returned\n    \"\"\"\n</code></pre> <p><code>``   - We pick lowercase after</code>:param XYZ: ...` unless the first word is a proper     noun or type   - A full ReST docstring styling also requires to specify params and return     types, however type hinting makes it redundant so you should use only type     hinting</p> <ul> <li>Put docstrings in triple quotation marks</li> <li>Bad <code>python     Generate \"random returns\".</code></li> <li>Good <code>python     \"\"\"     Generate \"random returns\".     \"\"\"</code></li> <li>Sometimes functions are small enough so we just use a 1-liner docstring   without detailed params and return descriptions. Just do not put text and   docstring brackets in one line</li> <li>Bad <code>python     \"\"\"This is not our approach.\"\"\"</code></li> <li>Good <code>python     \"\"\"     This is our approach.     \"\"\"</code></li> <li>More examples of and discussions on python docstrings</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#comments-style","title":"Comments style","text":"<ul> <li>Comments follow the same style of docstrings, e.g., imperative style with   period <code>.</code> at the end</li> <li>Bad <code>python     # This comment is not imperative and has no period at the end</code></li> <li>Good <code>python     # Make comments imperative and end them with a period.</code></li> <li>Always place comments above the lines that they are referring to. Avoid   writing comments on the same line as code since they require extra maintenance   (e.g., when the line becomes too long)</li> <li>Bad <code>python     print(\"hello world\")      # Introduce yourself.</code></li> <li>Good <code>python     # Introduce yourself.     print(\"hello world\")</code></li> <li>The only exception is commenting <code>if-elif-else</code> statments: we comment them   underneath the each statement in order to explain the code that belongs to the   each statement particularly</li> <li>Bad <code>python     # Set remapping based on the run type.     if is_prod:         ...     else:         ...</code></li> <li>Good <code>python     #     if is_prod:         # Set remapping for database data used in production.         ...     else:         # Set remapping for file system data used in simulation.         ...</code><ul> <li>If you want to separate an <code>if</code> statement from a bunch of code preceeding   it, you can leave an empty comment before it</li> </ul> </li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#replace-empty-lines-in-code-with-comments","title":"Replace empty lines in code with comments","text":"<ul> <li> <p>The problem with empty lines is that they are visually confusing since one   empty line is used also to separate functions. For this reason we suggest   using a comment</p> </li> <li> <p>If you feel that you need an empty line in the code, it probably means that a   specific chunk of code is a logical piece of code performing a cohesive   function</p> </li> </ul> <p>```python   ...   end_y = end_dt.year</p> <p>paths = list()   ...   ```</p> <ul> <li>Instead of putting an empty line, you should put a comment describing at high   level what the code does.   <code>python   ...   end_y = end_dt.year   # Generate a list of file paths for Parquet dataset.   paths = list()   ...</code></li> <li>A less optimal solution is to add an empty comment, but this is not great:   <code>python   ...   end_y = end_dt.year   #   paths = list()   ...</code></li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#comment-chunk-of-codes","title":"Comment chunk of codes","text":"<ul> <li> <p>Avoid wall-of-code, by commenting chunks of code that perform a cohesive work</p> </li> <li> <p>Bad</p> </li> </ul> <p><code>python   system_log_dir = rsiprrec.get_prod_system_log_dir(mode)   prod_dir = os.path.join(target_dir, system_log_dir)   hio.create_dir(prod_dir, incremental=True)   config_dict = {       \"dag_runner_config\": {           \"wake_up_timestamp\": \"2023-11-13 08:09:00-05:00\",           \"rt_timeout_in_secs_or_time\": 86400,           \"bar_duration_in_secs\": 300,       },   }   config = cconfig.Config.from_dict(config_dict)   config_tag = \"system_config.output\"   config.save_to_file(prod_dir, config_tag)</code></p> <ul> <li> <p>The code above has obviously various chunks that can be described with   comments   <code># Create a dir to store the System Config.   ...   # Create an example Config.   ...   # Save the Config in the target dir.</code></p> </li> <li> <p>Good</p> </li> </ul> <p><code>python   # Create a dir to store the System Config.   system_log_dir = rsiprrec.get_prod_system_log_dir(mode)   prod_dir = os.path.join(target_dir, system_log_dir)   hio.create_dir(prod_dir, incremental=True)   # Create an example Config.   config_dict = {       \"dag_runner_config\": {           \"wake_up_timestamp\": \"2023-11-13 08:09:00-05:00\",           \"rt_timeout_in_secs_or_time\": 86400,           \"bar_duration_in_secs\": 300,       },   }   config = cconfig.Config.from_dict(config_dict)   config_tag = \"system_config.output\"   # Save the Config in the target dir.   config.save_to_file(prod_dir, config_tag)</code></p>"},{"location":"coding/all.coding_style.how_to_guide.html#referring-to-an-object-in-code-comments","title":"Referring to an object in code comments","text":"<ul> <li>In general, avoid this whenever possible</li> <li>Code object names (e.g., function, class, params) are often subject to change,   so we need to take care of them everywhere. It is very hard to track all of   them in comments so replace the names with their actual meaning</li> <li>Bad <code>python     # Generate a list of file paths for `ParquetDataset`.</code></li> <li>Good <code>python     # Generate a list of file paths for Parquet dataset.</code></li> <li>However, sometimes it is necessary. In this case refer to objects in the code   using Markdown. This is useful for distinguishing the object code from the   real-life object</li> <li>Bad <code>python     # The dataframe df_tmp is used for ...</code></li> <li>Good <code>python     # The dataframe `df_tmp` is used for ...</code></li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#avoid-distracting-comments","title":"Avoid distracting comments","text":"<ul> <li>Use comments to explain the high level logic / goal of a piece of code and not   the details, e.g., do not comment things that are obvious</li> <li>Bad <code>python     # Print results.     _LOG.info(\"Results are %s\", ...)</code></li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#commenting-out-code","title":"Commenting out code","text":"<ul> <li>When we comment out code, we should explain why it is no longer relevant</li> <li>Bad <code>python     is_alive = pd.Series(True, index=metadata.index)     # is_alive = kgutils.annotate_alive(metadata, self.alive_cutoff)</code></li> <li>Good <code>python     # TODO(*): As discussed in PTask5047 for now we set all timeseries to be alive.     # is_alive = kgutils.annotate_alive(metadata, self.alive_cutoff)     is_alive = pd.Series(True, index=metadata.index)</code></li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#use-type-hints","title":"Use type hints","text":"<ul> <li>We expect new code to use type hints whenever possible</li> <li>See PEP 484</li> <li>Type hints cheat sheet</li> <li>At some point we will start adding type hints to old code</li> <li>We plan to start using static analyzers (e.g., <code>mypy</code>) to check for bugs from   type mistakes and to enforce type hints at run-time, whenever possible</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#interval-notation","title":"Interval notation","text":"<ul> <li>Intervals are represented with <code>[a, b), (a, b], (a, b), [a, b]</code></li> <li>We don't use the other style <code>[a, b[</code></li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#if-you-find-a-bug-or-obsolete-docstringtodo-in-the-code","title":"If you find a bug or obsolete docstring/TODO in the code","text":"<ul> <li>The process is:</li> <li>Do a <code>git blame</code> to find who wrote the code</li> <li>If it's an easy bug, you can fix it and ask for a review from the author</li> <li>You can comment on a PR (if there is one)</li> <li>You can file a bug on Github with<ul> <li>Clear info on the problem</li> <li>How to reproduce it, ideally a unit test</li> <li>Stacktrace</li> </ul> </li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#linter","title":"Linter","text":"<ul> <li>The linter is in charge of reformatting the code according to our conventions   and reporting potential problems</li> <li>You can find instructions on how to run linter at the   First review process doc</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#remove-linter-messages","title":"Remove linter messages","text":"<ul> <li>When the linter reports a problem:</li> <li>We assume that linter messages are correct, until the linter is proven wrong</li> <li>We try to understand what is the rationale for the linter's complaints</li> <li>We then change the code to follow the linter's suggestion and remove the     lint</li> <li>If you think a message is too pedantic, please file a bug with the example and   as a team we will consider whether to exclude that message from our list of   linter suggestions</li> <li>If you think the message is a false positive, then try to change the code to   make the linter happy</li> <li>E.g., if the code depends on some run-time behavior that the linter can't     infer, then you should question whether that behavior is really needed</li> <li>A human reader would probably be as confused as the linter is</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#when-to-disable-linter-messages","title":"When to disable linter messages","text":"<ul> <li>If you really believe you should override the linter in this particular case,   then use something like:   <code>python   # pylint: disable=some-message,another-one</code></li> <li>You then need to explain in a comment why you are overriding the linter.</li> <li>Don't use linter code numbers, but the     symbolic name     whenever possible:<ul> <li>Bad <code>python   # pylint: disable=W0611   import config.logging_settings</code></li> <li>Good <code>python   # pylint: disable=unused-import   # This is needed when evaluating code at run-time that depends from   # this import.   import config.logging_settings</code></li> </ul> </li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#prefer-non-inlined-linter-comments","title":"Prefer non-inlined linter comments","text":"<ul> <li>As for the general comments, we prefer make linter comments non-inlined</li> <li>However, sometimes there is no other choice than an inlined comment to get the   linter to understand which line we are referring to, so in rare cases it is   OK:</li> <li>Bad but ok if needed     <code>python     import config.logging_settings  # pylint: disable=unused-import</code></li> <li>Good <code>python     # pylint: disable=line-too-long       expected_df_as_str = \"\"\"# df=                                   asset_id   last_price            start_datetime              timestamp_db       end_datetime       2000-01-01 09:31:00-05:00      1000   999.874540 2000-01-01 09:30:00-05:00 2000-01-01 09:31:00-05:00       2000-01-01 09:32:00-05:00      1000  1000.325254 2000-01-01 09:31:00-05:00 2000-01-01 09:32:00-05:00       2000-01-01 09:33:00-05:00      1000  1000.557248 2000-01-01 09:32:00-05:00 2000-01-01 09:33:00-05:00\"\"\"       # pylint: enable=line-too-long</code></li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#dont-mix-real-changes-with-linter-changes","title":"Don't mix real changes with linter changes","text":"<ul> <li>We don't commit changes that modify the code together with linter   reformatting, unless the linting is applied to the changes we just made</li> <li>The reason for not mixing real and linter changes is that for a PR or to     just read the code it is difficult to understand what really changed vs what     was just a cosmetic modification</li> <li>If you are worried the linter might change your code in a way you don't like,   e.g.,</li> <li>Screwing up some formatting you care about for some reason, or</li> <li>Suggesting changes that you are worried might introduce bugs you can commit     your code and then do a \"lint commit\" with a message \"CMTaskXYZ: Lint\"</li> <li>In this way you have a backup state that you can rollback to, if you want</li> <li>If you run the linter and see that the linter is reformatting / modifying   pieces of code you din't change, it means that our team mate forgot to lint   their code</li> <li><code>git blame</code> can figure out the culprit</li> <li>You can send him / her a ping to remind her to lint, so you don't have to     clean after him / her</li> <li>In this case, the suggested approach is:<ul> <li>Commit your change to a branch / stash</li> <li>Run the linter by itself on the files that need to be cleaned, without any   change</li> <li>Run the unit tests to make sure nothing is breaking</li> <li>You can fix lints or just do formatting: it's up to you</li> <li>You can make this change directly on <code>master</code> or do a PR if you want to be   extra sure: your call</li> </ul> </li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#logging","title":"Logging","text":""},{"location":"coding/all.coding_style.how_to_guide.html#always-use-logging-instead-of-prints","title":"Always use logging instead of prints","text":"<ul> <li>Always use <code>logging</code> and never <code>print()</code> to monitor the execution</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#our-logging-idiom","title":"Our logging idiom","text":"<ul> <li>In order to use our logging framework (e.g., <code>-v</code> from command lines, and much   more) use:</li> </ul> <p>```python   import helpers.hdbg as hdbg</p> <p>_LOG = logging.getLogger(name)</p> <p>hdbg.init_logger(verbosity=logging.DEBUG)</p> <p>_LOG.debug(\"I am a debug function about %s\", a)   ```</p> <ul> <li>In this way one can decide how much debug info is needed (see Unix rule of   silence)</li> <li>E.g., when there is a bug one can run with <code>-v DEBUG</code> and see what's     happening right before the bug</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#logging-level","title":"Logging level","text":"<ul> <li>Use <code>_LOG.warning</code> for messages to the final user related to something   unexpected where the code is making a decision that might be controversial</li> <li>E.g., processing a dir that is supposed to contain only <code>.csv</code> files the     code finds a non-<code>.csv</code> file and decides to skip it, instead of breaking</li> <li>Use <code>_LOG.info</code> to communicate to the final user, e.g.,</li> <li>When the script is started</li> <li>Where the script is saving its results</li> <li>A progress bar indicating the amount of work completed</li> <li>Use <code>_LOG.debug</code> to communicate information related to the internal behavior   of code</li> <li>Do not pollute the output with information a regular user does not care     about</li> <li>Make sure the script prints when the work is terminated, e.g., \"DONE\" or   \"Results written to ...\"</li> <li>This is useful to indicate that the script did not die in the middle:     sometimes this happens silently and it is reported only from the OS return     code</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#how-to-pick-the-level-for-a-logging-statement","title":"How to pick the level for a logging statement","text":"<ul> <li>If all the debug info was printed at <code>INFO</code> level, the output will be too slow   by default</li> <li>So we separate what needs to be always printed (i.e., <code>INFO</code>) and what is   needed only if there is a problem to debug (i.e., <code>DEBUG</code>)</li> <li>Only who writes the code should decide what is <code>DEBUG</code>, since they know what     is needed to debug</li> <li>In fact many loggers use multiple levels of debugging level depending of how     much detailed debugging info are needed</li> <li><code>logging</code> has ways to enable logging on a per module basis</li> <li>So in prod mode you need to know which part you want to debug, since     printing everything at <code>INFO</code> level is not possible</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#use-positional-args-when-logging","title":"Use positional args when logging","text":"<ul> <li>Bad <code>python   _LOG.debug(\"cmd=%s %s %s\" % (cmd1, cmd2, cmd3))   _LOG.debug(\"cmd=%s %s %s\".format(cmd1, cmd2, cmd3))   _LOG.debug(\"cmd={cmd1} {cmd2} {cmd3}\")</code></li> <li>Good <code>python   _LOG.debug(\"cmd=%s %s %s\", cmd1, cmd2, cmd3)</code></li> <li>All the statements are equivalent from the functional point of view</li> <li>The reason is that in the second case the string is not built unless the   logging is actually performed, which limits time overhead from logging</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#exceptions-dont-allow-positional-args","title":"Exceptions don't allow positional args","text":"<ul> <li>For some reason people tend to believe that using the <code>logging</code> / <code>dassert</code>   approach of positional param to exceptions</li> <li>Bad (use positional args)     <code>python     raise ValueError(\"Invalid server_name='%s'\", server_name)</code></li> <li>Good (use string interpolation)     <code>python     raise ValueError(\"Invalid server_name='%s'\" % server_name)</code></li> <li>Best (use string format)     <code>python     raise ValueError(f\"Invalid server_name='{server_name}'\")</code></li> <li>The constructor of an exception accepts a string</li> <li>Using the string f-format is best since</li> <li>It's more readable</li> <li>There is little time overhead since if you get to the exception probably the     code is going to terminate, and it's not in a hot loop</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#report-warnings","title":"Report warnings","text":"<ul> <li>If there is a something that is suspicious but you don't feel like it's   worthwhile to assert, report a warning with:   <code>python   _LOG.warning(...)</code></li> <li>If you know that if there is a warning then there are going to be many many   warnings</li> <li>Print the first warning</li> <li>Send the rest to warnings.log</li> <li>At the end of the run, reports \"there are warnings in warnings.log\"</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#assertions","title":"Assertions","text":""},{"location":"coding/all.coding_style.how_to_guide.html#validate-values-before-an-assignment","title":"Validate values before an assignment","text":"<ul> <li>We consider this as an extension of a pre-condition (\"only assign values that   are correct\") rather than a postcondition</li> <li>Often is more compact since it doesn't have reference to <code>self</code></li> <li>Bad <code>python     self._tau = tau     hdbg.dassert_lte(self._tau, 0)</code></li> <li>Good <code>python     hdbg.dassert_lte(tau, 0)     self._tau = tau</code></li> <li> <p>Exceptions</p> <p>When we handle a default assignment, it's more natural to implement a post-condition:</p> <p><code>python col_rename_func = col_rename_func or (lambda x: x) hdbg.dassert_isinstance(col_rename_func, collections.Callable)</code></p> </li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#encode-the-assumptions-using-assertions","title":"Encode the assumptions using assertions","text":"<ul> <li>If your code makes an assumption don\u2019t just write a comment, but implement an   assertion so the code can\u2019t be executed if the assertion is not verified   (instead of failing silently)   <code>python   hdbg.dassert_lt(start_date, end_date)</code></li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#use-positional-args-when-asserting","title":"Use positional args when asserting","text":"<ul> <li><code>dassert_*</code> is modeled after logging so for the same reasons one should use   positional args</li> <li>Bad <code>python   hdbg.dassert_eq(a, 1, \"No info for %s\" % method)</code>   Good   <code>python   hdbg.dassert_eq(a, 1, \"No info for %s\", method)</code></li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#report-as-much-information-as-possible-in-an-assertion","title":"Report as much information as possible in an assertion","text":"<ul> <li>When using a <code>dassert_*</code> you want to give to the user as much information as   possible to fix the problem</li> <li>E.g., if you get an assertion after 8 hours of computation you don't want to     have to add some logging and run for 8 hours to just know what happened</li> <li>A <code>dassert_*</code> typically prints as much info as possible, but it can't report   information that is not visible to it:</li> <li>Bad <code>python     hdbg.dassert(string.startswith(\"hello\"))</code><ul> <li>You don't know what is value of <code>string</code> is</li> </ul> </li> <li>Good <code>python     hdbg.dassert(string.startswith(\"hello\"), \"string='%s'\", string)</code><ul> <li>Note that often is useful to add <code>'</code> (single quotation mark) to fight   pesky spaces that make the value unclear, or to make the error as readable   as possible</li> </ul> </li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#imports","title":"Imports","text":""},{"location":"coding/all.coding_style.how_to_guide.html#dont-use-evil-import","title":"Don't use evil <code>import *</code>","text":"<ul> <li>Do not use in notebooks or code the evil <code>import *</code></li> <li>Bad <code>python     from helpers.sql import *</code></li> <li>Good <code>python     import helpers.sql as hsql</code></li> <li>The <code>from ... import *</code>:</li> <li>Pollutes the namespace with the symbols and spreads over everywhere, making     it painful to clean up</li> <li>Obscures where each function is coming from, removing the context that comes     with the namespace</li> <li>Is evil in many other ways</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#cleaning-up-the-evil-import","title":"Cleaning up the evil <code>import *</code>","text":"<ul> <li>To clean up the mess you can:</li> <li>For notebooks<ul> <li>Find &amp; replace (e.g., using jupytext and Pycharm)</li> <li>Change the import and run one cell at the time</li> </ul> </li> <li>For code<ul> <li>Change the import and use linter on file to find all the problematic spots</li> </ul> </li> <li>One of the few spots where the evil <code>import *</code> is ok is in the <code>__init__.py</code>   to tweak the path of symbols exported by a library</li> <li>This is an advanced topic and you should rarely use it</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#avoid-from-import","title":"Avoid <code>from ... import ...</code>","text":"<ul> <li>Import should always start from <code>import</code>:   <code>python   import library as short_name   import library.sublibrary as short_name</code></li> <li>This rule applies to imports of third party libraries and our library</li> <li>Because of this rule we have to always specify a short import of a parent lib   before every code object that does not belong to the file:</li> <li>Bad <code>python     from helpers.sql import get_connection, get_connection_from_env_vars, \\         DBConnection, wait_connection_from_db, execute_insert_query</code></li> <li>Good <code>python     import helpers.sql as hsql     ...     ... hsql.get_connection()</code></li> <li>The problem with the <code>from ... import ...</code> is that it:</li> <li>Creates lots of maintenance effort<ul> <li>E.g., anytime you want a new function you need to update the import   statement</li> </ul> </li> <li>Creates potential collisions of the same name<ul> <li>E.g., lots of modules have a <code>read_data()</code> function</li> </ul> </li> <li>Impairs debugging<ul> <li>Importing directly in the namespace loses information about the module</li> <li>E.g.,<code>read_documents()</code> is not clear: what documents?</li> <li><code>np.read_documents()</code> at least gives information of which packages is it   coming from and enables us to track it down to the code</li> </ul> </li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#exceptions-to-the-import-style","title":"Exceptions to the import style","text":"<ul> <li>We try to minimize the exceptions to this rule to avoid to keep this rule   simple, rather than discussing about</li> <li>The current agreed upon exceptions are:</li> <li>For <code>typing</code> it is ok to do:     <code>python     from typing import Iterable, List</code>     in order to avoid typing everywhere, since we want to use type hints as much     as possible</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#always-import-with-a-full-path-from-the-root-of-the-repo-submodule","title":"Always import with a full path from the root of the repo / submodule","text":"<ul> <li>Bad <code>python   import exchange_class</code></li> <li>Good <code>python   import im_v2.ccxt.data.extract.exchange_class</code></li> <li>In this way your code can run without depending upon your current dir</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#baptizing-module-import","title":"Baptizing module import","text":"<ul> <li>Each module that can be imported should have a docstring at the very beginning   (before any code) describing how it should be imported</li> </ul> <p>```python   \"\"\"   Import as:</p> <p>import im_v2.ccxt.data.client.ccxt_clients as imvcdccccl   \"\"\"   ```</p> <ul> <li>The import abbreviations are called 'short imports' and usually consist of 7-9   first letters of all the words that comprise path to file</li> <li>DO NOT simply give a random short import name</li> <li>Run linter to generate short import for a file automatically</li> <li>For some most files we specify short imorts by hand so thay may contain less   symbols, e.g., <code>hdbg</code></li> <li>The goal is to have always the same imports so it's easy to move code around,   without collisions</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#examples-of-imports","title":"Examples of imports","text":"<ul> <li>Example 1</li> <li>Bad <code>python     from im_v2.ccxt.data.client import ccxt_clients as ccxtcl</code></li> <li>Good <code>python     import im_v2.ccxt.data.client.ccxt_clients as imvcdccccl</code></li> <li>Example 2</li> <li>Bad <code>python     from edgar.shared import headers_extractor as he</code></li> <li>Good <code>python     import edgar.shared.headers_extractor as eshheext</code></li> <li>Example 3</li> <li>Bad <code>python     from helpers import hdbg</code></li> <li>Good <code>python     import helpers.hdbg as hdbg</code></li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#scripts","title":"Scripts","text":""},{"location":"coding/all.coding_style.how_to_guide.html#use-python-and-not-bash-for-scripting","title":"Use Python and not bash for scripting","text":"<ul> <li>We prefer to use python instead of bash scripts with very few exceptions</li> <li>E.g., scripts that need to modify the environment by setting env vars, like     <code>setenv.sh</code></li> <li>The problem with bash scripts is that it's too easy to put together a sequence   of commands to automate a workflow</li> <li>Quickly things always become more complicated than what you thought, e.g.,</li> <li>You might want to interrupt if one command in the script fails</li> <li>You want to use command line options</li> <li>You want to use logging to see what's going on inside the script</li> <li>You want to do a loop with a regex check inside</li> <li>Thus you need to use the more complex features of bash scripting and bash   scripting is absolutely horrible, much worse than perl (e.g., just think of   <code>if [ ... ]</code> vs <code>if [[ ... ]]</code>)</li> <li>Our approach is to make simple to create scripts in python that are equivalent   to sequencing shell commands, so that can evolve in complex scripts</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#skeleton-for-a-script","title":"Skeleton for a script","text":"<ul> <li>The ingredients are:</li> <li><code>dev_scripts/script_skeleton.py</code>: a template to write simple scripts you can     copy and modify it</li> <li><code>helpers/hsystem.py</code>: a set of utilities that make simple to run shell     commands (e.g., capturing their output, breaking on error or not, tee-ing to     file, logging, ...)</li> <li><code>helpers</code> has lots of useful libraries</li> <li>The official reference for a script is <code>dev_scripts/script_skeleton.py</code></li> <li>You can copy this file and change it</li> <li>A simple example is: <code>dev_scripts/git/gup.py</code></li> <li>A complex example is: <code>dev_scripts/replace_text.py</code></li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#some-useful-patterns","title":"Some useful patterns","text":"<ul> <li>Some useful patterns / idioms that are supported by the framework are:</li> <li>Incremental mode: you skip an action if its outcome is already present     (e.g., skipping creating a dir, if it already exists and it contains all the     results)</li> <li>Non-incremental mode: clean and execute everything from scratch</li> <li>Dry-run mode: the commands are written to screen instead of being executed</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#use-scripts-and-not-notebooks-for-long-running-jobs","title":"Use scripts and not notebooks for long-running jobs","text":"<ul> <li>We prefer to use scripts to execute code that might take long time (e.g.,   hours) to run, instead of notebooks</li> <li>Pros of script</li> <li>All the parameters are completely specified by a command line</li> <li>Reproducible and re-runnable</li> <li>Cons of notebooks</li> <li>Tend to crash / hang for long jobs</li> <li>Not easy to understand if the notebook is doing progress</li> <li>Not easy to get debug output</li> <li>Notebooks are designed for interactive computing / debugging and not batch   jobs</li> <li>You can experiment with notebooks, move the code into a library, and wrap it     in a script</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#follow-the-same-structure","title":"Follow the same structure","text":"<ul> <li> <p>All python scripts that are meant to be executed directly should:</p> </li> <li> <p>Be marked as executable files with:      <code>&gt; chmod +x foo_bar.py</code></p> </li> <li>Have the python code should start with the standard Unix shebang notation:      <code>python      #!/usr/bin/env python</code></li> <li>This line tells the shell to use the <code>python</code> defined in the environment</li> <li> <p>In this way you can execute directly without prepending with python</p> </li> <li> <p>Have a:      <code>python      if __name__ == \"__main__\":          ...</code></p> </li> <li>Ideally use <code>argparse</code> to have a minimum of customization</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#use-clear-names-for-the-scripts","title":"Use clear names for the scripts","text":"<ul> <li>In general scripts (like functions) should have a name like \"action_verb\".</li> <li>Bad<ul> <li>Examples of bad script names are <code>timestamp_extractor.py</code> and   <code>timestamp_extractor_v2.py</code></li> <li>Which timestamp data set are we talking about?</li> <li>What type of timestamps are we extracting?</li> <li>What is the difference about these two scripts?</li> </ul> </li> <li>We need to give names to scripts that help people understand what they do and   the context in which they operate</li> <li>We can add a reference to the task that originated the work (to give more   context)</li> <li>Good<ul> <li>E.g., for a script generating a dataset there should be an (umbrella) bug   for this dataset, that we refer in the bug name, e.g.,   <code>TaskXYZ_edgar_timestamp_dataset_extractor.py</code></li> </ul> </li> <li>Also where the script is located should give some clue of what is related to</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#functions","title":"Functions","text":""},{"location":"coding/all.coding_style.how_to_guide.html#avoid-using-non-exclusive-bool-arguments","title":"Avoid using non-exclusive <code>bool</code> arguments","text":"<ul> <li>While a simple <code>True</code>/<code>False</code> switch may suffice for today's needs, very often   more flexibility is eventually needed</li> <li>If more flexibility is needed for a <code>bool</code> argument, you are faced with the   choice:</li> <li>Adding another parameter (then parameter combinations grow exponentially and     may not all make sense)</li> <li>Changing the parameter type to something else</li> <li>Either way, you have to change the function interface</li> <li>To maintain flexibility from the start, opt for a <code>str</code> parameter \"mode\",   which is allowed to take a small well-defined set of values.</li> <li>If an implicit default is desirable, consider making the default value of the   parameter <code>None</code>. This is only a good route if the default operation is   non-controversial / intuitively obvious.</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#try-to-make-functions-work-on-multiple-types","title":"Try to make functions work on multiple types","text":"<ul> <li>We encourage implementing functions that can work on multiple related types:</li> <li>Bad: implement <code>demean_series()</code>, <code>demean_dataframe()</code></li> <li>Good: implement a function <code>demean(obj)</code> that can work with <code>pd.Series</code>     and <code>pd.DataFrame</code><ul> <li>One convention is to call <code>obj</code> the variable whose type is not known until   run-time</li> </ul> </li> <li>In this way we take full advantage of duck typing to achieve something similar   to C++ function overloading (actually even more expressive)</li> <li>Try to return the same type of the input, if possible</li> <li>E.g., the function called on a <code>pd.Series</code> returns a <code>pd.Series</code></li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#avoid-hard-wired-column-name-dependencies","title":"Avoid hard-wired column name dependencies","text":"<ul> <li>When working with dataframes, we often want need handle certain columns   differently, or perform an operation on a strict subset of columns</li> <li>In these cases, it is tempting to assume that the special columns will have   specific names, e.g., <code>\"datetime\"</code></li> <li>The problem is that column names are</li> <li>Rarely obvious (e.g., compare <code>\"datetime\"</code> vs <code>\"timestamp\"</code> vs <code>\"Datetime\"</code>)</li> <li>Tied to specific use cases<ul> <li>The function you are writing may be written for a specific use case today,   but what if it is more general</li> <li>If someone wants to reuse your function in a different setting where   different column names make sense, why should they have to conform to your   specific use case's needs?</li> </ul> </li> <li>May overwrite existing column names<ul> <li>For example, you may decided to call a column <code>\"output\"</code>, but what if the   dataframe already has a column with that name?</li> </ul> </li> <li>To get around this, allow the caller to communicate to the function the names   of any special columns</li> <li>Good <code>python     def func(datetime_col: str):         ...</code></li> <li>Make sure that you require column names only if they are actually used by the   function</li> <li>If you must use hard-write column names internally or for some application,   define the column name in the library file as a global variable, like   <code>python   DATETIME_COL = \"datetime\"</code></li> <li>Users of the library can now access the column name through imports</li> <li>This prevents hidden column name dependencies from spreading like a virus     throughout the codebase</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#single-exit-point-from-a-function","title":"Single exit point from a function","text":"<ul> <li>Consider the following Bad function   <code>python   def _get_zero_element(list_: List):         if not list_:             return None         else:             return list_[0]</code></li> <li>Linter message is     <code>im.kibot/utils.py:394: [R1705(no-else-return), ExpiryContractMapper.extract_contract_expiry] Unnecessary \"else\" after \"return\"     [pylint]</code></li> <li>Try to have a single exit point from a function, since this guarantees that   the return value is always the same</li> <li>In general returning different data structures from the same function (e.g., a   list in one case and a float in another) is indication of bad design</li> <li>There are exceptions like a function that works on different types (e.g.,     accepts a dataframe or a series and then returns a dataframe or a series,     but the input and output is the same)</li> <li>Returning different types (e.g., float and string) is also bad</li> <li>Returning a type or <code>None</code> is typically ok</li> <li>Try to return values that are consistent so that the client doesn't have to   switch statement, using <code>isinstance(...)</code></li> <li>E.g., return a <code>float</code> and if the value can't be computed return <code>np.nan</code>     (instead of <code>None</code>) so that the client can use the return value in a uniform     way</li> <li>Function examples with single exit point   <code>python   def _get_zero_element(list_: List):       if not list_:           ret = np.nan       else:           ret = list_[0]       return ret</code>   or   <code>python   def _get_zero_element(list_: List):       ret = np.nan if not list_ else list_[0]       return ret</code></li> <li>However in rare cases it is OK to have functions like:   <code>python   def ...(...):       # Handle simple cases.       ...       if ...:           return       # lots of code       ...       return</code></li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#order-of-function-parameters","title":"Order of function parameters","text":""},{"location":"coding/all.coding_style.how_to_guide.html#problem","title":"Problem","text":"<ul> <li>We want to have a standard, simple, and logical order for specifying the   arguments of a function</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#decision","title":"Decision","text":"<ul> <li>The preferred order is:</li> <li>Input parameters</li> <li>Output parameters</li> <li>In-out parameters</li> <li>Default parameters</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#consistency-of-ordering-of-function-parameters","title":"Consistency of ordering of function parameters","text":"<ul> <li>Try to:</li> <li>Keep related variables close to each other</li> <li>Keep the order of parameters similar across functions that have similar     interface</li> <li>Enforcing these rules is based on best effort</li> <li>Pycharm is helpful when changing order of parameters</li> <li>Use linter to check consistency of types between function definition and   invocation</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#style-for-default-parameter","title":"Style for default parameter","text":""},{"location":"coding/all.coding_style.how_to_guide.html#problem_1","title":"Problem","text":"<ul> <li>How to assign default parameters in a function to make them clear and   distinguishable?</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#decision_1","title":"Decision","text":"<ul> <li>We make all the default parameters keyword-only</li> <li>This means that we should always specify default parameters using a keyword</li> <li>When building a function, always put default parameters after <code>*</code></li> <li>It's ok to use a default parameter in the interface as long as it is a Python   scalar (which is immutable by definition)</li> <li>Good <code>python     def function(       value: int = 5,       *,       dir_name: str = \"hello_world\",     ):</code></li> <li>You should not use list, maps, objects, etc. as the default value but pass   <code>None</code> and then initialize the default param inside the function</li> <li>Bad <code>python     def function(       *,       obj: Object = Object(),       list_: List[int] = [],     ):</code></li> <li>Good <code>python     def function(       *,       obj: Optional[Object] = None,       list_: Optional[List[int]] = None,     ):       if obj is None:         obj = Object()       if list_ is None:         list_ = []</code></li> <li>We use a <code>None</code> default value when a function needs to be wrapped and the   default parameter needs to be propagated</li> <li> <p>Good</p> <p>```python def function1(   ...,   *,   dir_name: Optional[str] = None, ):   dir_name = dir_name or \"/very_long_path\"</p> <p>def function2(   ...,   *,   dir_name: Optional[str] = None, ):   function1(..., dir_name=dir_name) ```</p> </li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#rationale","title":"Rationale","text":"<ul> <li>Pros of the Good vs Bad style</li> <li>When you wrap multiple functions, each function needs to propagate the     default parameters, which:<ul> <li>Violates DRY; and</li> <li>Adds maintenance burden (if you change the innermost default parameter,   you need to change all of them!)</li> <li>With the proposed approach, all the functions use <code>None</code>, until the   innermost function resolves the parameters to the default values</li> </ul> </li> <li>The interface is cleaner</li> <li>Implementation details are hidden (e.g., why should the caller know what is     the default path?)</li> <li>Mutable parameters can not be passed through (see     here))</li> <li>Cons:</li> <li>One needs to add <code>Optional</code> to the type hint</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#calling-functions-with-default-parameters","title":"Calling functions with default parameters","text":""},{"location":"coding/all.coding_style.how_to_guide.html#problem_2","title":"Problem","text":"<ul> <li>You have a function   <code>python   def func(     task_name : str,     dataset_dir : str,     *,     clobber : bool = clobber,   ):   ...</code></li> <li>How should it be invoked?</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#decision_2","title":"Decision","text":"<ul> <li>We prefer to</li> <li>Assign directly the positional parameters</li> <li>Bind explicitly the parameters with a default value using their name</li> <li>Do not put actual parameter values to the function call but specify them     right before</li> <li>Bad <code>python     func(\"some_task_name\", \"/dir/subdir\", clobber=False)</code></li> <li>Good <code>python     task_name = \"some_task_name\"     dataset_dir = \"/dir/subdir\"     clobber = False     func(task_name, dataset_dir, clobber=clobber)</code></li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#rationale_1","title":"Rationale","text":"<ul> <li>Pros of Good vs Bad style</li> <li>If a new parameter with a default value is added to the function <code>func</code>     before <code>clobber</code>:<ul> <li>The Good idiom doesn't need to be changed</li> <li>All instances of the Bad idiom need to be updated</li> <li>The Bad idiom might keep working but with silent failures</li> <li>Of course <code>mypy</code> and <code>Pycharm</code> might point this out</li> </ul> </li> <li>The Good style highlights which default parameters are being overwritten,     by using the name of the parameter<ul> <li>Overwriting a default parameter is an exceptional situation that should be   explicitly commented</li> </ul> </li> <li>Cons:</li> <li>None</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#dont-repeat-non-default-parameters","title":"Don't repeat non-default parameters","text":""},{"location":"coding/all.coding_style.how_to_guide.html#problem_3","title":"Problem","text":"<ul> <li>Given a function with the following interface:   <code>python   def mult_and_sum(multiplier_1, multiplier_2, sum_):       return multiplier_1 * multiplier_2 + sum_</code>   how to invoke it?</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#decision_3","title":"Decision","text":"<ul> <li>Positional arguments are not default, so not keyword-only for consistency</li> <li>Bad <code>python     a = 1     b = 2     c = 3     mult_and_sum(multiplier_1=a,                  multiplier_2=b,                  sum_=c)</code></li> <li>Good <code>python     a = 1     b = 2     c = 3     mult_and_sum(a, b, c)</code></li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#rationale_2","title":"Rationale","text":"<ul> <li>Pros of Good vs Bad</li> <li>Non-default parameters in Python require all the successive parameters to be     name-assigned<ul> <li>This causes maintenance burden</li> </ul> </li> <li>The Bad approach is in contrast with our rule for the default parameters<ul> <li>We want to highlight which parameters are overriding the default</li> </ul> </li> <li>The Bad approach in practice requires all positional parameters to be     assigned explicitly causing:<ul> <li>Repetition in violation of DRY (e.g., you need to repeat the same   parameter everywhere); and</li> <li>Maintainance burden (e.g., if you change the name of a function parameter   you need to change all the invocations)</li> </ul> </li> <li>The Bad style is a convention used in no language (e.g., C, C++, Java)<ul> <li>All languages allow binding by parameter position</li> <li>Only some languages allow binding by parameter name</li> </ul> </li> <li>The Bad makes the code very wide, creating problems with our 80 columns     rule</li> <li>Cons of Good vs Bad</li> <li>One could argue that the Bad form is clearer<ul> <li>IMO the problem is in the names of the variables, which are uninformative,   e.g., a better naming achieves the same goal of clarity   <code>python   mul1 = 1   mul2 = 2   sum_ = 3   mult_and_sum(mul1, mul2, sum_)</code></li> </ul> </li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#writing-clear-beautiful-code","title":"Writing clear beautiful code","text":""},{"location":"coding/all.coding_style.how_to_guide.html#keep-related-code-close","title":"Keep related code close","text":"<ul> <li>E.g., keep code that computes data close to the code that uses it.</li> <li>This holds also for notebooks: do not compute all the data structure and then   analyze them.</li> <li>It\u2019s better to keep the section that \u201creads data\u201d close to the section that   \u201cprocesses it\u201d. In this way it\u2019s easier to see \u201cblocks\u201d of code that are   dependent from each other, and run only a cluster of cells.</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#order-functions-in-topological-order","title":"Order functions in topological order","text":"<ul> <li>Order functions / classes in topological order so that the ones at the top of   the files are the \"innermost\" and the ones at the end of the files are the   \"outermost\"</li> <li>In this way, reading the code top to bottom one should not find a forward     reference that requires skipping back and forth</li> <li>Linter reorders functions and classes in the topological order so make sure   you run it after adding new ones</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#distinguish-public-and-private-functions","title":"Distinguish public and private functions","text":"<ul> <li>The public functions <code>foo_bar()</code> (not starting with <code>_</code>) are the ones that   make up the interface of a module and that are called from other modules and   from notebooks</li> <li>Use private functions like <code>_foo_bar()</code> when a function is a helper of another   private or public function</li> <li>Also follow the \u201ckeep related code close\u201d close by keeping the private   functions close to the functions (private or public) that are using them</li> <li>Some references:</li> <li>StackOverflow</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#keep-public-functions-organized-in-a-logical-order","title":"Keep public functions organized in a logical order","text":"<ul> <li>Keep the public functions in an order related to the use representing the   typical flow of use, e.g.,</li> <li>Common functions, used by all other functions</li> <li>Read data</li> <li>Process data</li> <li>Save data</li> <li>You can use banners to separate layers of the code. Use the banner long 80   cols (e.g., I have a vim macro to create banners that always look the same)   and be consistent with empty lines before / empty and so on.</li> <li>The banner is a way of saying \u201call these functions belong together\u201d.</li> </ul> <p>```python   # #############################################################################   # Read data.   # #############################################################################</p> <p>def _helper1_to_func1():   ...   def _helper2_to_func1():   ...   def func1_read_data1():       _helper1_to_func1()       ...       _helper2_to_func2()</p> <p># #############################################################################   # Process data.   # #############################################################################   ...   # #############################################################################   # Save data.   # #############################################################################   ...   ```</p> <ul> <li>Ideally each section of code should use only sections above, and be used by   sections below (aka \u201cUnix layer approach\u201d).</li> <li>If you find yourself using too many banners this is in indication that code   might need to be split into different classes or files</li> <li>Although we don\u2019t have agreed upon rules, it might be ok to have large files     as long as they are well organized. E.g., in pandas code base, all the code     for DataFrame is in a single file long many thousands of lines (!), but it     is nicely separated in sections that make easy to navigate the code</li> <li>Too many files can become problematic, since one needs to start jumping     across many files: in other words it is possible to organize the code too     much (e.g. what if each function is in a single module?)</li> <li>Let\u2019s try to find the right balance.</li> <li>It might be a good idea to use classes to split the code, but also OOP can   have a dark side</li> <li>E.g., using OOP only to reorganize the code instead of introducing     \u201cconcepts\u201d</li> <li>IMO the worst issue is that they don\u2019t play super-well with Jupyter     autoreload</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#do-not-make-tiny-wrappers","title":"Do not make tiny wrappers","text":"<ul> <li>Examples of horrible functions:</li> <li>How many characters do we really saved? If typing is a problem, learn to     touch type.     <code>python     def is_exists(path: str) -&gt; None:         return os.path.exists(path)</code>     or     <code>python     def make_dirs(path: str) -&gt; List[str]:         os.makedirs(path)</code></li> <li>This one can be simply replaced by <code>os.path.dirname</code> <code>python     def folder_name(f_name: str) -&gt; str:         if f_name[-1] != \"/\":             return f_name + \"/\"         return f_name</code></li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#regex","title":"Regex","text":"<ul> <li>The rule of thumb is to compile a regex expression, e.g.,   <code>python   backslash_regex = re.compile(r\"\\\\\")</code>   only if it's called more than once, otherwise the overhead of compilation and   creating another var is not justified</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#do-not-introduce-another-concept-unless-really-needed","title":"Do not introduce another \u201cconcept\u201d unless really needed","text":"<ul> <li>We want to introduce degrees of freedom and indirection only when we think   this can be useful to make the code easy to maintain, read, and expand.</li> <li>If we add degrees of freedom everywhere just because we think that at some   point in the future this might be useful, then there is very little advantage   and large overhead.</li> <li>Introducing a new variable, function, class introduces a new concept that one   needs to keep in mind. People that read the code, needs to go back and forth   in the code to see what each concept means.</li> <li>Think about the trade-offs and be consistent.</li> <li>Example 1   <code>python   def fancy_print(txt):       print \"fancy: \", txt</code></li> <li>Then people that change the code need to be aware that there is a function     that prints in a special way. The only reason to add this shallow wrapper is     that, in the future, we believe we want to change all these calls in the     code.</li> <li>Example 2   <code>python   SNAPSHOT_ID = \"SnapshotId\"</code></li> <li>Another example is parametrizing a value used in a single function.</li> <li>If multiple functions need to use the same value, then this practice can be     a good idea. If there is a single function using this, one should at least     keep it local to the function.</li> <li>Still note that introducing a new concept can also create confusion. What if     we need to change the code to:     <code>python     SNAPSHOT_ID = \"TigerId\"</code>     then the variable and its value are in contrast.</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#return-none-or-keep-one-type","title":"Return <code>None</code> or keep one type","text":"<ul> <li>Functions that return different types can make things complicated downstream,   since the callers need to be aware of all of it and handle different cases.   This also complicates the docstring, since one needs to explicitly explain   what the special values mean, all the types and so on.</li> <li>In general returning multiple types is an indication that there is a problem.</li> <li>Of course this is a trade-off between flexibility and making the code robust   and easy to understand, e.g.,</li> <li>In the following example it is better to either return <code>None</code> (to clarify that   something special happened) or an empty dataframe <code>pd.DataFrame(None)</code> to   allow the caller code being indifferent to what is returned.</li> <li>Bad <code>python     if \"Tags\" not in df.columns:         df[\"Name\"] = np.nan     else:         df[\"Name\"] = df[\"Tags\"].apply(extract_name)</code></li> <li>Good <code>python     if \"Tags\" not in df.columns:         df[\"Name\"] = None     else:         df[\"Name\"] = df[\"Tags\"].apply(extract_name)</code></li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#avoid-wall-of-text-functions","title":"Avoid wall-of-text functions","text":"<ul> <li>Bad <code>python   def get_timestamp_data(raw_df: pd.DataFrame) -&gt; pd.DataFrame:       timestamp_df = get_raw_timestamp(raw_df)       documents_series = raw_df.progress_apply(he.extract_documents_v2, axis=1)       documents_df = pd.concat(documents_series.values.tolist())       documents_df.set_index(api.cfg.INDEX_COLUMN, drop=True, inplace=True)       documents_df = documents_df[           documents_df[api.cfg.DOCUMENTS_DOC_TYPE_COL] != \"\"]       types = documents_df.groupby(           api.cfg.DOCUMENTS_IDX_COL)[api.cfg.DOCUMENTS_DOC_TYPE_COL].unique()       timestamp_df[api.cfg.TIMESTAMP_DOC_TYPES_COL] = types       is_xbrl_series = raw_df[api.cfg.DATA_COLUMN].apply(he.check_xbrl)       timestamp_df[api.cfg.TIMESTAMP_DOC_ISXBRL_COL] = is_xbrl_series       timestamp_df[api.cfg.TIMESTAMP_DOC_EX99_COL] = timestamp_df[           api.cfg.TIMESTAMP_DOC_TYPES_COL].apply(               lambda x: any(['ex-99' in t.lower() for t in x]))       timestamp_df = timestamp_df.rename(columns=api.cfg.TIMESTAMP_COLUMN_RENAMES)       return timestamp_df</code></li> <li>This function is correct but it has few problems (e.g., lack of a docstring,     lots of unclear concepts, abuse of constants).</li> <li>Good</li> </ul> <p>```python   def get_timestamp_data(raw_df: pd.DataFrame) -&gt; pd.DataFrame:       \"\"\"       Get data containing timestamp information.</p> <pre><code>  :param raw_df: input non-processed data\n  :return: timestamp data\n  \"\"\"\n  # Get data containing raw timestamp information.\n  timestamp_df = get_raw_timestamp(raw_df)\n  # Extract the documents with data type information.\n  documents_series = raw_df.progress_apply(he.extract_documents_v2, axis=1)\n  documents_df = pd.concat(documents_series.values.tolist())\n  documents_df.set_index(api.cfg.INDEX_COLUMN, drop=True, inplace=True)\n  documents_df = documents_df[\n      documents_df[api.cfg.DOCUMENTS_DOC_TYPE_COL] != \"\"\n  ]\n  types = documents_df.groupby(\n      api.cfg.DOCUMENTS_IDX_COL\n  )[api.cfg.DOCUMENTS_DOC_TYPE_COL].unique()\n  # Set columns about types of information contained.\n  timestamp_df[api.cfg.TIMESTAMP_DOC_TYPES_COL] = types\n  is_xbrl_series = raw_df[api.cfg.DATA_COLUMN].apply(he.check_xbrl)\n  timestamp_df[api.cfg.TIMESTAMP_DOC_ISXBRL_COL] = is_xbrl_series\n  timestamp_df[api.cfg.TIMESTAMP_DOC_EX99_COL] = timestamp_df[\n      api.cfg.TIMESTAMP_DOC_TYPES_COL\n  ].apply(lambda x: any([\"ex-99\" in t.lower() for t in x]))\n  # Rename columns to canonical representation.\n  timestamp_df = timestamp_df.rename(columns=api.cfg.TIMESTAMP_COLUMN_RENAMES)\n  return timestamp_df\n</code></pre> <p><code>``   - You should at least split the functions in chunks using</code>#` or even better     comment what each chunk of code does.</p>"},{"location":"coding/all.coding_style.how_to_guide.html#writing-robust-code","title":"Writing robust code","text":""},{"location":"coding/all.coding_style.how_to_guide.html#dont-let-your-functions-catch-the-default-itis","title":"Don\u2019t let your functions catch the default-itis","text":"<ul> <li>Default-itis is a disease of a function that manifests itself by getting too   many default parameters.</li> <li>Default params should be used only for parameters that 99% of the time are   constant.</li> <li>In general we require the caller to be clear and specify all the params.</li> <li>Functions catch defaultitis when the programmer is lazy and wants to change   the behavior of a function without changing all the callers and unit tests.   Resist this urge! <code>grep</code> is friend. Pycharm does this refactoring   automatically.</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#explicitly-bind-default-parameters","title":"Explicitly bind default parameters","text":"<ul> <li>It\u2019s best to explicitly bind functions with the default params so that if the   function signature changes, your functions doesn\u2019t confuse a default param was   a positional one.</li> <li>Bad <code>python     hdbg.dassert(         args.form or args.form_list,         \"You must specify one of the parameters: --form or --form_list\",     )</code></li> <li>Good <code>python     hdbg.dassert(         args.form or args.form_list,         msg=\"You must specify one of the parameters: --form or --form_list\",     )</code></li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#dont-hardwire-params-in-a-function-call","title":"Don\u2019t hardwire params in a function call","text":"<ul> <li>Bad <code>python   esa_df = universe.get_esa_universe_mapped(False, True)</code></li> <li>It is difficult to read and understand without looking for the invoked     function (aka write-only code) and it\u2019s brittle since a change in the     function params goes unnoticed.</li> <li>Good <code>python   gvkey = False   cik = True   esa_df = universe.get_esa_universe_mapped(gvkey, cik)</code></li> <li>It\u2019s better to be explicit (as usual)</li> <li>This solution is robust since it will work as long as gvkey and cik are the     only needed params, which is as much as we can require from the called     function.</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#make-if-elif-else-complete","title":"Make <code>if-elif-else</code> complete","text":"<ul> <li>In general all the <code>if-elif-else</code> statements should to be complete, so that   the code is robust.</li> <li>Bad <code>python   hdbg.dassert_in(       frequency,       [\"D\", \"T\"]       \"Only daily ('D') and minutely ('T') frequencies are supported.\",   )   if frequency == \"T\":       ...   if frequency == \"D\":       ...</code></li> <li>Good <code>python   if frequency == \"T\":       ...   elif frequency == \"D\":       ...   else:       raise ValueError(\"The %s frequency is not supported\" % frequency)</code></li> <li>This code is robust and correct</li> <li>Still the <code>if-elif-else</code> is enough and the assertion is not needed<ul> <li>DRY here wins: you don't want to have to keep two pieces of code in sync</li> <li>The last line is a catch-all that makes sure even if we modify the   previous</li> </ul> </li> <li>It makes sense to check early only when you want to fail before doing more     work</li> <li>E.g., sanity checking the parameters of a long running function, so that it     doesn't run for 1 hr and then crash because the name of the file is     incorrect</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#add-todos-when-needed","title":"Add TODOs when needed","text":"<ul> <li>When there is something that you know you should have done, but you didn\u2019t   have time to do, add a TODO, possibly using your github name e.g.,   <code>python   # TODO(gp): \u2026</code></li> <li>In this way it\u2019s easy to grep for your TODOs, which becomes complicated when     using different names.</li> <li>Be clear on the meaning of TODO</li> <li>A <code>TODO(Batman): clean this up</code> can be interpreted as<ol> <li>\"Batman suggested to clean this up\"</li> <li>\"Batman should clean this up\"</li> <li>\"Batman has the most context to explain this problem or fix it\"</li> </ol> </li> <li>On the one hand, <code>git blame</code> will report who created the TODO, so the first     meaning is redundant.</li> <li>On the other hand, since we follow a shared ownership of the code, the     second meaning should be quite infrequent. In fact the code has mostly     <code>TODO(*)</code> todos, where <code>*</code> relates to all the team members</li> <li>Given pros and cons, the proposal is to use the first meaning.</li> <li>This is also what Google style guide suggests     here</li> <li>If the TODO is associated with a Github issue, you can simply put the issue   number and description inside the TODO, e.g.,   <code>python   # TODO(Grisha): \"Handle missing tiles\" CmTask #1775.</code></li> <li>You can create a TODO for somebody else, or you can create a Upsource comment   / review or Github bug, depending on how important the issue is</li> <li>If the TODO is general, e.g., anybody can fix it, then you can avoid to put a   name. This should not be abused since it creates a culture when people don\u2019t   take responsibility for their mistakes.</li> <li>You can use P1, P2 to indicate if the issue is critical or not. E.g., P0 is   the default for saying this is important, P1 is more of a \u201cnice to have\u201d.   <code>python   # TODO(Sergey): P1 This can be implemented in pandas using a range generation.</code></li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#common-python-mistakes","title":"Common Python mistakes","text":""},{"location":"coding/all.coding_style.how_to_guide.html#vs-is","title":"<code>==</code> vs <code>is</code>","text":"<ul> <li><code>is</code> checks whether two variables point to the same object (aka reference   equality), while <code>==</code> checks if the two pointed objects are equivalent (value   equality).</li> <li>For checking against types like <code>None</code> we want to use <code>is</code>, <code>is not</code></li> <li>Bad <code>python     if var == None:</code></li> <li>_Good__     <code>python     if var is None:</code></li> <li>For checking against values we want to use <code>==</code></li> <li>Bad <code>python     if unit is \"minute\":</code></li> <li>_Good__     <code>python     if unit == \"minute\":</code></li> <li>For more info checks   here</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#type-vs-isinstance","title":"<code>type()</code> vs <code>isinstance()</code>","text":"<ul> <li><code>type(obj) == list</code> is worse since we want to test for reference equality (the   type of object is a list) and not the type of obj is equivalent to a list.</li> <li><code>isinstance</code> caters for inheritance (an instance of a derived class is an   instance of a base class, too), while checking for equality of type does not   (it demands identity of types and rejects instances of subtypes, AKA   subclasses).</li> <li>Bad <code>python     if type(obj) is list:</code></li> <li>_Good__     <code>python     if isinstance(obj, list):</code></li> <li>For more info check   here</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#unit-tests","title":"Unit tests","text":"<ul> <li>Provide a minimal end-to-end unit testing (which creates a conda environment   and then run a few unit tests)</li> <li>Use</li> <li>Pytest https://docs.pytest.org/en/latest/</li> <li><code>unittest</code> library</li> <li>Usually we are happy with</li> <li>Lightly testing the tricky functions</li> <li>Some end-to-end test to make sure the code is working</li> <li>Use your common sense</li> <li>E.g., no reason to test code that will be used only once</li> <li>To run unit tests in a single file   ``` <p>pytest datetime_utils_test.py -x -s   ```</p> </li> <li>For more information on our testing conventions and guidelines, see   <code>docs/coding/all.unit_tests.how_to_guide.md</code></li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#refactoring","title":"Refactoring","text":""},{"location":"coding/all.coding_style.how_to_guide.html#when-moving-refactoring-code","title":"When moving / refactoring code","text":"<ul> <li>If you move files, refactor code, move functions around make sure that:</li> <li>Code and notebook work (e.g., imports and caller of the functions)</li> <li>Documentation is updated (this is difficult, so best effort is enough)</li> <li>For code find all the places that have been modified   ``` <p>grep -r \"create_dataframe\" *   edgar/form_4/notebooks/Task252_EDG4_Coverage_of_our_universe_from_Forms4.ipynb:    \"documents, transactions = edu.create_dataframes(\\n\",   edgar/form_4/notebooks/Task313_EDG4_Understand_Form_4_amendments.ipynb:    \"documents, transactions = edu.create_dataframes(\\n\",   edgar/form_4/notebooks/Task193_EDG4_Compare_form4_against_Whale_Wisdom_and_TR.ipynb:    \"documents, transactions, owners, footnotes = edu.create_dataframes(\\n\",   ```</p> </li> <li>Or if you use mighty Pycharm, Ctrl + Mouse Left Click (Shows you all places   where this function or variable was used) and try to fix them, at least to   give your best shot at making things work</li> <li>You can edit directly the notebooks without opening, or open and fix it.</li> <li>Good examples how you can safely rename anything for Pycharm users:   https://www.jetbrains.com/help/Pycharm/rename-refactorings.html</li> <li>But remember, you must know how to do it without fancy IDE like Pycharm.</li> <li>If it\u2019s important code:</li> <li>Run unit tests</li> <li>Run notebooks (see     here)</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#write-script-for-renamings","title":"Write script for renamings","text":"<ul> <li>When you need to rename any code object that is being used in many files, use   <code>dev_scripts/replace_text.py</code> to write a script that will implement your task</li> <li>Read the script docstring for detailed information about how to use it</li> <li>You DO NOT use <code>replace_text.py</code> directly. Instead, create an executable <code>.sh</code>   script that uses <code>replace_text.py</code></li> <li>Look for examples at <code>dev_scripts/cleanup_scripts</code></li> <li>Commit the created script to the mentioned folder so then your team members     can use it to implement renaming in other libs</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#architectural-and-design-pattern","title":"Architectural and design pattern","text":""},{"location":"coding/all.coding_style.how_to_guide.html#research-quality-vs-production-quality","title":"Research quality vs production quality","text":"<ul> <li>Code belonging to top level libraries (e.g., <code>//amp/core</code>, <code>//amp/helpers</code>)   and production (e.g., <code>//.../db</code>, <code>vendors</code>) needs to meet high quality   standards, e.g.,</li> <li>Well commented</li> <li>Following our style guide</li> <li>Thoroughly reviewed</li> <li>Good design</li> <li>Comprehensive unit tests</li> <li>Research code in notebook and python can follow slightly looser standards,   e.g.,</li> <li>Sprinkled with some TODOs</li> <li>Not perfectly general</li> <li>The reason is that:</li> <li>Research code is still evolving and we want to keep the structure flexible</li> <li>We don't want to invest the time in making it perfect if the research     doesn't pan out</li> <li>Note that research code still needs to be:</li> <li>Understandable / usable by not authors</li> <li>Well commented</li> <li>Follow the style guide</li> <li>Somehow unit tested</li> <li>We should be able to raise the quality of a piece of research code to   production quality when that research goes into production</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#always-separate-what-changes-from-what-stays-the-same","title":"Always separate what changes from what stays the same","text":"<ul> <li>In both main code and unit test it's not a good idea to repeat the same code</li> <li>Bad</li> <li>Copy-paste-modify</li> <li>Good</li> <li>Refactor the common part in a function and then change the parameters used     to call the function</li> <li>Example:</li> <li>What code is clearer to you, VersionA or VersionB?</li> <li> <p>Can you spot the difference between the 2 pieces of code?</p> <ul> <li>Version A</li> </ul> <p>```python   stopwords = nlp_ut.read_stopwords_json(_STOPWORDS_PATH)   texts = [\"a\", \"an\", \"the\"]   stop_words = nlp_ut.get_stopwords(       categories=[\"articles\"], stopwords=stopwords   )   actual_result = nlp_ut.remove_tokens(texts, stop_words=stop_words)   expected_result = []   self.assertEqual(actual_result, expected_result)</p> <p>...   texts = [\"do\", \"does\", \"is\", \"am\", \"are\", \"be\", \"been\", \"'s\", \"'m\", \"'re\"]   stop_words = nlp_ut.get_stopwords(       categories=[\"auxiliary_verbs\"], stopwords=stopwords,   )   actual_result = nlp_ut.remove_tokens(texts, stop_words=stop_words)   expected_result = []   self.assertEqual(actual_result, expected_result)   ``` - Version B</p> <p>```python   def _helper(texts, categories, expected_result):       stopwords = nlp_ut.read_stopwords_json(_STOPWORDS_PATH)       stop_words = nlp_ut.get_stopwords(           categories=categories, stopwords=stopwords       )       actual_result = nlp_ut.remove_tokens(texts, stop_words=stop_words)       expected_result = []       self.assertEqual(actual_result, expected_result)</p> <p>texts = [\"a\", \"an\", \"the\"]   categories = [\"articles\"]   expected_result = []   _helper(texts, categories, expected_result)   ...</p> <p>texts = [\"do\", \"does\", \"is\", \"am\", \"are\", \"be\", \"been\", \"'s\", \"'m\", \"'re\"]   categories = [\"auxiliary_verbs\"]   expected_result = []   helper(texts, categories, expected_result)   ``` - Yes, Version A is _Bad and Version B is Good</p> </li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#organize-scripts-as-pipelines","title":"Organize scripts as pipelines","text":"<ul> <li>One can organize complex computations in stages of a pipeline</li> <li>E.g., to parse EDGAR forms<ul> <li>Download -&gt; (raw data) -&gt; header parser -&gt; (pq data) -&gt; XBLR / XML / XLS   parser -&gt; (pq data) -&gt; custom transformation</li> </ul> </li> <li>One should be able to run the entire pipeline or just a piece</li> <li>E.g., one can run the header parser from the raw data, save the result to     file, then read this file back, and run the XBLR parser</li> <li>Ideally one would always prefer to run the pipeline from scratch, but   sometimes the stages are too expensive to compute over and over, so using   chunks of the pipeline is better</li> <li>This can also mixed with the \u201cincremental mode\u201d, so that if one stage has   already been run and the intermediate data has been generated, that stage is   skipped</li> <li>Each stage can save files in a <code>tmp_dir/stage_name</code></li> <li>The code should be organized to allow these different modes of operations, but   there is not always need to be super exhaustive in terms of command line   options</li> <li>E.g., I implement the various chunks of the pipeline in a library,     separating functions that read / save data after a stage and then assemble     the pieces into a throw-away script where I hardwire the file names and so     on</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#make-filename-unique","title":"Make filename unique","text":"<ul> <li>Problem</li> <li>We have a lot of structure / boilerplate in our project around RH     hypotheses.<ul> <li>E.g., there are corresponding files for all the RH like:</li> <li><code>RHxyz/configs.py</code></li> <li><code>RHxyz/pipeline.py</code></li> </ul> </li> <li>It is not clear if it's better to make filenames completely unique by     repeating the <code>RH</code>, e.g., <code>RH1E_configs.py</code>, or let the directories     disambiguate.</li> <li>Note that we are not referring to other common files like <code>utils.py</code>, which     are made unique by their position in the file system and by the automatic     shortening of the imports.</li> <li>Decision</li> <li>Invoking the principle of 'explicit is better than implicit', the proposal     is to repeat the prefix.<ul> <li>Bad: <code>RH1E/configs.py</code></li> <li>Good: <code>RH1E/RH1E_configs.py</code></li> </ul> </li> <li>Rationale</li> <li>Pros of the repetition (e.g., <code>RH1E/RH1E_configs.py</code>):<ul> <li>The filename is unique so there is no dependency on where you are</li> <li>Since pytest requires all files to be unique, we need to repeat the prefix   for the test names and the rule is \"always make the names of the files   unique\"</li> <li>We are going to have lots of these files and we want to minimize the risk   of making mistakes</li> </ul> </li> <li>Cons of the repetition:<ul> <li>Stuttering</li> <li>What happens if there are multiple nested dirs? Do we repeat all the   prefixes?</li> <li>This seems to be an infrequent case</li> </ul> </li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#incremental-behavior","title":"Incremental behavior","text":"<ul> <li>Often we need to run the same code over and over</li> <li>E.g., because the code fails on an unexpected point and then we need to     re-run from the beginning</li> <li>We use options like:   <code>--incremental   --force   --start_date   --end_date   --output_file</code></li> <li>Check existence output file before start function (or a thread when using   parallelism) which handle data of the corresponding period</li> <li>If <code>--incremental</code> is set and output file already exists then skip the     computation and report<ul> <li>Log.info(\u201cSkipping processing file %s as requested\u201d, \u2026)</li> </ul> </li> <li>If <code>--incremental</code> is not set<ul> <li>If output file exists then we issue a log.warn and abort the process</li> <li>If output file exists and param <code>--force</code>, then report a log.warn and   rewrite output file</li> </ul> </li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#run-end-to-end","title":"Run end-to-end","text":"<ul> <li>Try to run things end-to-end (and from scratch) so we can catch these   unexpected issues and code defensively</li> <li>E.g., we found out that TR data is malformed sometimes and only running     end-to-end we can catch all the weird cases</li> <li>This also helps with scalability issues, since if takes 1 hr for 1 month of     data and we have 10 years of data is going to take 120 hours (=5 days) to     run on the entire data set</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#think-about-scalability","title":"Think about scalability","text":"<ul> <li>Do experiments to try to understand if a code solution can scale to the   dimension of the data we have to deal with</li> <li>E.g., inserting data by doing SQL inserts of single rows are not scalable     for pushing 100GB of data</li> <li>Remember that typically we need to run the same scripts multiple times (e.g.,   for debug and / or production)</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#use-command-line-for-reproducibility","title":"Use command line for reproducibility","text":"<ul> <li>Try to pass params through command line options when possible</li> <li>In this way a command line contains all the set-up to run an experiment</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#structure-the-code-in-terms-of-filters","title":"Structure the code in terms of filters","text":"<ul> <li>Focus on build a set of \"filters\" split into different functions, rather than   a monolithic flow</li> <li>Organize the code in terms of a sequence of transformations that can be run in   sequence, e.g.,</li> <li>Create SQL tables</li> <li>Convert json data to csv</li> <li>Normalize tables</li> <li>Load csv files into SQL</li> <li>Sanity check the SQL (e.g., mismatching TR codes, missing dates)</li> <li>Patch up SQL (e.g., inserting missing TR codes and reporting them to us so      we can check with TR)</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#code-style-for-different-languages","title":"Code style for different languages","text":""},{"location":"coding/all.coding_style.how_to_guide.html#sql","title":"SQL","text":"<ul> <li>You can use the package https://github.com/andialbrecht/sqlparse to format SQL   queries</li> <li>There is also an on-line version of the same formatter at   https://sqlformat.org</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#conventions-addendum","title":"Conventions (Addendum)","text":""},{"location":"coding/all.coding_style.how_to_guide.html#be-patient","title":"Be patient","text":"<ul> <li>For some reason talking about conventions makes people defensive and   uncomfortable, sometimes.</li> <li>Conventions are not a matter of being right or wrong, but to consider pros and   cons of different approaches, and make the decision only once instead of   discussing the same problem every time. In this way we can focus on achieving   the Ultimate Goal.</li> <li>If you are unsure or indifferent to a choice, be flexible and let other   persons that seem to be less flexible decide.</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#goal","title":"Goal","text":"<ul> <li>The goal of the conventions is to simplify our job by removing ambiguity</li> <li>There is no right or wrong: that's why it's a convention and not a law of   nature</li> <li>On the flip-side, if there is a right and wrong, then what we are discussing     probably shouldn\u2019t be considered as a convention</li> <li>We don't want to spend time discussing inconsequential points</li> <li>We don't want reviewers to pick lints instead of focusing on architectural   issues and potential bugs</li> <li>Remove cognitive burden of being distracted by \"this is an annoying lint\" (or   at least perceived lint)</li> <li>Once a convention is stable, we would like to automate enforcing it by the   linter</li> <li>Ideally the linter should fix our mistakes so we don't even have to think     about them, and reviewers don't have to be distracted with pointing out the     lints</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#keep-the-rules-simple","title":"Keep the rules simple","text":"<ul> <li>E.g., assume that we accepted the following rules:</li> <li>Git is capitalized if it refers to the tool and it's not capitalized when it     refers to the command (this is what Git documentation suggests)</li> <li>Python is written capitalized (this is what Python documentation suggests)</li> <li><code>pandas</code> is written lowercase, unless it is a beginning of the line in which     case it's capitalized, but it's better to try to avoid to start a sentence     with it (this is what pandas + English convention seems to suggest)</li> <li>Any other library could suggest a different convention based on the     preference of its author, who tries to finally force people to follow his /     her convention \u2026)</li> <li>All these rules require mental energy to be followed and readers will spend   time checking that these rules are enforced, rather than focusing on bugs and   architecture.</li> <li>In this case we want to leverage the ambiguity of \"it's unclear what is the   correct approach\" by simplifying the rule</li> <li>E.g., every name of tools or library is always capitalized</li> <li>This is simple to remember and automatically enforce</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#allow-turning-off-the-automatic-tools","title":"Allow turning off the automatic tools","text":"<ul> <li>We understand that tools can't always understand the context and the   subtleties of human thoughts, and therefore they yield inevitably to false   positives.</li> <li>Then we always want to permit disabling the automatic checks / fixes e.g., by   using directives in comments or special syntax (e.g., anything in a <code>...</code> or   <code>\u2026</code> block should be leaved untouched)</li> <li>It can be tricky determining when an exception is really needed and when   overriding the tool becomes a slippery slope for ignoring the rules.</li> <li>Patience and flexibility is advised here.</li> </ul>"},{"location":"coding/all.coding_style.how_to_guide.html#make-the-spell-checker-happy","title":"Make the spell-checker happy","text":"<ul> <li>The spell-checker is not always right: false positives are often very annoying</li> <li>We prefer to find a way to make the spell-checker happy rather than argue that   the spell-checker is wrong and ignore it</li> <li>The risk with overriding the spell-checker (and any other tool) is that the   decision is not binary anymore correct / not-correct and can't be automated   and requires mental energy to see if the flagged error is real or not.</li> <li>E.g., <code>insample</code> is flagged as erroneous, so we convert it into <code>in-sample</code>.</li> <li>The solution for the obvious cases of missing a word (e.g., a technical word)   is to add words to the vocabulary. This still needs to be done by everyone,   until we find a way to centralize the vocabulary.</li> <li>E.g., untradable is a valid English word, but Pycharm's spell-checker     doesn't recognize it.</li> <li>TODO(*): Should we add it to the dictionary or write it as \"un-tradable\"?</li> <li>Still we don't want to override the spell-checker when an alternative   lower-cost solution is available. E.g.,</li> <li><code>in-sample</code> instead of <code>insample</code></li> <li><code>out-of-sample</code> instead of <code>oos</code></li> <li>We decided that <code>hyper-parameter</code> can be written without hyphen:   <code>hyperparameter</code></li> </ul>"},{"location":"coding/all.gsheet_into_pandas.how_to_guide.html","title":"Gsheet Into Pandas","text":""},{"location":"coding/all.gsheet_into_pandas.how_to_guide.html#connecting-google-sheets-to-pandas","title":"Connecting Google Sheets to Pandas","text":"<ul> <li>There are two layers of the API</li> <li>gspread<ul> <li>This allows to connect to Google Sheets API</li> </ul> </li> <li>gspread-pandas<ul> <li>This allows to interact with Google Sheets through Pandas DataFrames,   using <code>gspread</code></li> </ul> </li> </ul>"},{"location":"coding/all.gsheet_into_pandas.how_to_guide.html#installing-libraries","title":"Installing libraries","text":"<ul> <li> <p>The library should be automatically installed in the Dev container</p> </li> <li> <p>If not, you can install it in the notebook with   <code>bash   notebook&gt; !pip install gspread-pandas</code></p> </li> <li>Or in the Docker container with:</li> </ul> <p><code>bash   docker&gt; sudo /bin/bash -c \"(source /venv/bin/activate; pip install gspread)\"</code></p>"},{"location":"coding/all.gsheet_into_pandas.how_to_guide.html#check-installation","title":"Check installation","text":"<ul> <li>To check that the library is installed</li> <li> <p>In a notebook</p> <p>```bash import gspread print(gspread.version) 5.11.3</p> <p>import gspread_pandas print(gspread_pandas.version) 3.2.3 <code>- In the dev container</code>bash docker&gt; python -c \"import gspread; print(gspread.version)\" 5.10.0 ```</p> </li> </ul>"},{"location":"coding/all.gsheet_into_pandas.how_to_guide.html#authentication","title":"Authentication","text":"<ul> <li>It's best to access Google API using a \"Service Account\", which is used for a   bots</li> <li> <p>Since <code>gspread-pandas</code> leverages <code>gspread</code>, you can follow the instructions   for gspread https://docs.gspread.org/en/v6.0.0/oauth2.html</p> </li> <li> <p>There are two ways to authenticate</p> </li> <li>OAuth Client ID</li> <li> <p>Service account key (preferred)</p> </li> <li> <p>More details are in</p> </li> <li><code>gspread</code>: https://docs.gspread.org/en/latest/oauth2.html</li> <li><code>gspread-pandas</code>:     https://gspread-pandas.readthedocs.io/en/latest/configuration.html</li> </ul>"},{"location":"coding/all.gsheet_into_pandas.how_to_guide.html#in-short","title":"In short","text":"<ul> <li>Go to Google Developers Console and create a new project or select one you   already have</li> <li>E.g., name \"gp-gspread\", and ID \"gp-gspread-426713\"</li> <li>Search for \"Google Drive API\" and click on Enable API</li> <li>Search for \"Google Sheets API\" and click on Enable API</li> <li>Go to Credentials</li> <li>Create credentials -&gt; Service account key</li> <li>Service account details</li> <li>Service account name: gspread</li> <li>Service account ID: gspread</li> <li>Email address: gspread@gp-gspread-426713.iam.gserviceaccount.com</li> <li>Role: owner</li> <li>Click on <code>gspread</code></li> <li>Keys -&gt; Create new key -&gt; JSON</li> <li>A file is downloaded</li> </ul> <p>more ~/Downloads/gspread-gp-94afb83adb02.json   <code>{     \"type\": \"service_account\",     \"project_id\": \"gspread-gp\",     \"private_key_id\": \"94afb...5258ac\",     \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvg...FtmcXiHuZ46EMouxnQCEqrT5\\n-----END PRIVATE KEY-----\\n\",     \"client_email\": \"gp-gspread@gspread-gp.iam.gserviceaccount.com\",     \"client_id\": \"101087234904396404157\",     \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",     \"token_uri\": \"https://oauth2.googleapis.com/token\",     \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",     \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/gp-gspread%40gspread-gp.iam.gserviceaccount.com\",     \"universe_domain\": \"googleapis.com\"   }</code></p> <ul> <li> <p>Move the key in the right place   ```</p> <p>mv ~/Downloads/gspread-gp-94afb83adb02.json ~/.config/gspread_pandas/google_secret.json   ```</p> </li> <li> <p>Check that the key is visible In the Docker container   ```</p> <p>user_501@d533075e6ade:/app$ more ~/.config/gspread_pandas/google_secret.json   ```</p> </li> <li> <p>Go to your spreadsheet and share it with a client_email from the step above.   If you don\u2019t do this, you\u2019ll get a <code>gspread.exceptions.SpreadsheetNotFound</code>   exception when trying to access this spreadsheet from your application or a   script.</p> </li> </ul>"},{"location":"coding/all.gsheet_into_pandas.how_to_guide.html#testing-gspread-pandas","title":"Testing gspread-pandas","text":"<ul> <li> <p>The notebook with the usage example is located at   <code>amp/core/notebooks/gsheet_into_pandas_example.ipynb</code>.</p> </li> <li> <p>Don't feel stupid if you need multiple iterations to get this stuff   working</p> </li> <li>Clicking on GUI is always a recipe for low productivity</li> <li>Go command line and vim!</li> </ul>"},{"location":"coding/all.hcache.explanation.html","title":"Cache","text":""},{"location":"coding/all.hcache.explanation.html#how-to-use-cache","title":"How to use <code>Cache</code>","text":"<ul> <li><code>Cache</code> is typically used as a decorator function <code>@cache</code> around functions or   regular class methods</li> <li><code>Cache</code> works in code and in Python notebooks with <code>%autoreload</code></li> </ul>"},{"location":"coding/all.hcache.explanation.html#how-the-cache-works","title":"How the <code>Cache</code> works","text":"<ul> <li><code>Cache</code> tracks changes in the source code of the wrapped function</li> <li> <p>For performance reasons, it checks the code only one time unless the pointer     to the function is changed, e.g. in notebooks</p> </li> <li> <p>By default, it uses two levels of caching:</p> </li> <li><code>Memory</code> level</li> <li> <p><code>Disk</code> level</p> </li> <li> <p>When a call is made to the wrapped function:</p> </li> <li>Firstly the <code>Memory</code> level is being checked</li> <li>If there's no hit in the <code>Memory</code>, the <code>Disk</code> level is checked</li> <li>If there's no hit in <code>Disk</code> level, the wrapped function is called</li> <li> <p>The result is then stored in both <code>Disk</code> and <code>Memory</code> levels</p> </li> <li> <p><code>Cache</code> is equipped with a <code>get_last_cache_accessed()</code> method to understand if   the call hit the cache and on which level</p> </li> </ul>"},{"location":"coding/all.hcache.explanation.html#disk-level","title":"Disk level","text":"<ul> <li><code>Disk</code> level is implemented via   joblib.Memory</li> </ul>"},{"location":"coding/all.hcache.explanation.html#memory-level","title":"Memory level","text":"<ul> <li> <p>Initially, the idea was to use   functools.lru_cache   for memory cache</p> </li> <li> <p>Pros:</p> </li> <li> <p>Standard library implementation</p> </li> <li> <p>Quietly fast in-memory implementation</p> </li> <li> <p>Cons:</p> </li> <li> <p>Only hashable arguments are supported</p> </li> <li>No access to cache, it's not possible to check if an item is in cache or      not</li> <li> <p>It does not work properly in notebooks</p> </li> <li> <p>Because Cons outweighed Pros, we decided to implement <code>Memory</code> level as   joblib.Memory   over <code>tmpfs</code></p> </li> <li>In this way we reuse the same code for <code>Disk</code> level cache but over a RAM-based   disk</li> <li>This implementation overcomes the Cons listed above, although it is slightly     slower than the pure <code>functools.lru_cache</code> approach</li> </ul>"},{"location":"coding/all.hcache.explanation.html#global-cache","title":"Global cache","text":"<ul> <li>By default, all cached functions save their cached values in the default   global cache</li> <li>The cache is \"global\" in the sense that:</li> <li>It is unique per-user and per Git client</li> <li>It serves all the functions of a Git client</li> <li>The cached data stored in a folder <code>$GIT_ROOT/tmp.cache.{mem,disk}.[tag]</code></li> <li> <p>This global cache is being managed via global functions named   <code>*_global_cache</code>, e.g., <code>set_global_cache()</code></p> </li> <li> <p>TODO(gp): maybe a better name is</p> </li> <li>Global -&gt; local_cache, client_cache</li> <li>Function_specific -&gt; global or shared</li> </ul>"},{"location":"coding/all.hcache.explanation.html#tagged-global-cache","title":"Tagged global cache","text":"<ul> <li>A global cache can be specific of different applications (e.g., for unit tests   vs normal code)</li> <li>It is controlled through the <code>tag</code> parameter</li> <li>The global cache corresponds to <code>tag = None</code></li> </ul>"},{"location":"coding/all.hcache.explanation.html#function-specific-cache","title":"Function-specific cache","text":"<ul> <li>It is possible to create function-specific caches, e.g., to share the result   of a function across clients and users</li> <li>In this case the client needs to set <code>disk_cache_directory</code> and / or   <code>mem_cache_directory</code> parameters in the decorator or in the <code>Cached</code> class   constructor</li> <li>If cache is set for the function, it can be managed with   <code>.set_cache_directory()</code>, <code>.get_cache_directory()</code>, <code>.destroy_cache()</code> and   <code>.clear_function_cache()</code> methods.</li> </ul>"},{"location":"coding/all.hplayback.how_to_guide.html","title":"Playback","text":"<ul> <li> <p><code>Playback</code> is a way to automatically generate a unit test for a given function   by capturing the inputs applied to the function by the external world</p> </li> <li> <p>The working principle is:</p> </li> <li>Instrument the target function <code>f()</code> to test with a <code>Playback</code> object or      with a decorator <code>@playback</code></li> <li>Run the function <code>f()</code> using the external code to drive its inputs<ul> <li>E.g., while the function is executed as part of a more complex system, or    in a notebook</li> </ul> </li> <li>The playback framework:<ul> <li>Captures the inputs and the output of the function <code>f()</code></li> <li>Generates Python code to apply the stimuli to <code>f()</code> and to check its    output against the expected output</li> </ul> </li> <li>Modify the code automatically generated by <code>Playback</code> to create handcrafted      unit tests</li> </ul>"},{"location":"coding/all.hplayback.how_to_guide.html#code-and-tests","title":"Code and tests","text":"<ul> <li>The code for <code>Playback</code> is located at <code>helpers/hplayback.py</code></li> <li>Unit tests for <code>Playback</code> with useful usage examples are located at   <code>helpers/test/test_playback.py</code></li> </ul>"},{"location":"coding/all.hplayback.how_to_guide.html#using-playback","title":"Using playback","text":""},{"location":"coding/all.hplayback.how_to_guide.html#quick-start","title":"Quick start","text":"<ul> <li>Given a function to test like:</li> </ul> <pre><code>def function_under_test(...) -&gt; ...:\n    ...\n    &lt;code&gt;\n    ...\n    res = ...\n    return res\n</code></pre> <pre><code>def function_under_test(...) -&gt; ...:\n    import helpers.hplayback as hplayb\n    playback = hplayb.Playback(\"assert_equal\")\n\n    ...\n    &lt;code&gt;\n    ...\n\n    res = ...\n    code = playback.run(res)\n    print(code)\n    return res\n</code></pre>"},{"location":"coding/all.hplayback.how_to_guide.html#example-1-testing-get_sum","title":"Example 1: testing <code>get_sum()</code>","text":"<ul> <li>Assume that we want unit test a function <code>get_sum()</code></li> </ul> <p><code>python   def get_sum(a: List[int], b: List[int]) -&gt; Any:       c = a + b       return c</code></p> <ul> <li>Assume that typically <code>get_sum()</code> gets its inputs from a complex pipeline</li> </ul> <p><code>python   def complex_data_pipeline() -&gt; Tuple[List[int], List[int]]:      # Incredibly complex pipeline generating:      a = [1, 2, 3]      b = [4, 5, 6]      return a, b</code></p> <ul> <li>The function is called with:</li> </ul> <p><code>python   a, b = complex_data_pipeline()   c = get_sum(a, b)</code></p> <ul> <li> <p>We don't want to compute by hand the inputs <code>a, b</code>, but we can reuse   <code>complex_data_pipeline</code> to create a realistic workload for the function under   test</p> </li> <li> <p>Instrument the code with <code>Playback</code>:</p> </li> </ul> <p>```python   import helpers.playback as hplayb</p> <p>def get_sum(a: List[int], b: List[int]) -&gt; Any:       playback = hplayb.Playback(\"assert_equal\")       c = a + b       code = playback.run(res)       print(code)       return c   ```</p> <ul> <li>Create the playback object</li> </ul> <p><code>python   playback = hplayb.Playback(\"assert_equal\")</code></p> <p>which specifies:   - The unit test mode: \"check_string\" or \"assert_equal\"   - The function name that is being tested: in our case, \"get_sum\"   - The function parameters that were created earlier</p> <ul> <li>Run it with:</li> </ul> <p><code>python    a, b = complex_data_pipeline()    c = get_sum(a, b)</code></p> <ul> <li>Run the playback passing the expected outcome as a parameter</li> </ul> <p><code>python   code = playback.run(res)</code></p> <ul> <li>The output <code>code</code> will contain a string with the unit test for <code>get_sum()</code></li> </ul> <p>```python   import helpers.unit_test as hut</p> <p>class TestGetSum(hut.TestCase):       def test1(self) -&gt; None:           # Initialize function parameters.           a = [1, 2, 3]           b = [4, 5, 6]           # Get the actual function output.           act = get_sum(a, b)           # Create the expected function output.           exp = [1, 2, 3, 4, 5, 6]           # Check whether the expected value equals the actual value.           self.assertEqual(act, exp)   ```</p>"},{"location":"coding/all.hplayback.how_to_guide.html#example-2-testing-_render_plantuml-from-render_mdpy","title":"Example 2: testing <code>_render_plantuml()</code> from <code>render_md.py</code>","text":"<ul> <li> <p>Copy real <code>im_architecture.md</code> to a test location</p> </li> <li> <p>Add playback into the code:</p> </li> </ul> <p><code>python   ...   import helpers.playback as hplayb   ...   def _render_plantuml(       in_txt: List[str], out_file: str, extension: str, dry_run: bool   ) -&gt; List[str]:       # Generate test.       playback = hplayb.Playback(\"check_string\")       print(prnt.frame(playback.run(None)))       ...   ...</code></p> <ul> <li> <p>Run <code>render_md.py -i im_architecture.md</code></p> </li> <li> <p>The following output is prompted:</p> </li> </ul> <p>```python   # Test created for main._render_plantuml   import helpers.unit_test as hut   import jsonpickle   import pandas as pd</p> <p>class TestRenderPlantuml(hut.TestCase):       def test1(self) -&gt; None:           # Define input variables           in_txt = [\"\", ..., \"\", \"&gt; GP:: Not urgent\", \"\"]           out_file = \"im_architecture.md\"           extension = \"png\"           dry_run = False           # Call function to test           act = _render_plantuml(in_txt=in_txt, out_file=out_file, extension=extension, dry_run=dry_run)           act = str(act)           # Check output           self.check_string(act)   ```</p> <ul> <li><code>in_txt</code> value is too long to keep it in test - needed to be replaced with   previously generated file. Also some cosmetic changes are needed and code is   ready to paste to the existing test:   <code>python   def test_render_plantuml_playback1(self) -&gt; None:       \"\"\"Test real usage for im_architecture.md.test\"\"\"       # Define input variables       file_name = \"im_architecture.md.test\"       in_file = os.path.join(self.get_input_dir(), file_name)       in_txt = io_.from_file(in_file).split(\"\\n\")       out_file = os.path.join(self.get_scratch_space(), file_name)       extension = \"png\"       dry_run = True       # Call function to test       act = rmd._render_plantuml(           in_txt=in_txt, out_file=out_file, extension=extension, dry_run=dry_run       )       act = \"\\n\".join(act)       # Check output       self.check_string(act)</code></li> </ul>"},{"location":"coding/all.imports_and_packages.how_to_guide.html","title":"Imports And Packages","text":""},{"location":"coding/all.imports_and_packages.how_to_guide.html#imports-and-packages_1","title":"Imports and packages","text":"<ul> <li>TODO(gp): Consolidate here any other rule from other gdoc</li> </ul>"},{"location":"coding/all.imports_and_packages.how_to_guide.html#goals-of-packages","title":"Goals of packages","text":"<ul> <li>The goal of creating packages is to:</li> <li>Simplify the import from clients</li> <li>Hide in which file the actual code is, so that we can reorganize the code     without having to change all the client code</li> <li>Organize the code in related units</li> <li> <p>Make it simpler to avoid import loops by enforcing that there are no import     loops in any module and no import loops among modules</p> </li> <li> <p>E.g., referring to package from a different package looks like   <code>python   import dataflow.core as dtfcore   dtfcore.ArmaGenerator(...)</code></p> </li> <li>Importing the specific file:   <code>python   import dataflow.system.source_nodes as dtfsysonod   dtfsysonod.ArmaGenerator(...)</code></li> </ul>"},{"location":"coding/all.imports_and_packages.how_to_guide.html#circular-dependency-aka-import-cycle-import-loop","title":"Circular dependency (aka import cycle, import loop)","text":"<ul> <li>The simplest case of circular import is a situation when in lib <code>A</code> we have   <code>import B</code>, and in lib B we have <code>import A</code></li> <li>The presence of circular imports can be checked with an invoke   <code>i lint_detect_cycles</code>. By default, it will run on the whole repo, which takes   a couple of minutes, but it will provide the most reliable and thorough check   for circular imports</li> </ul>"},{"location":"coding/all.imports_and_packages.how_to_guide.html#rules-for-imports","title":"Rules for imports","text":"<ul> <li>We follow rules to avoid import loops:</li> <li> <p>Code inside a package should import directly a file in the same package and     not use the package</p> <ul> <li>E.g., <code>im_v2/common/data/client/data_frame_im_clients.py</code></li> <li> <p>Good</p> <p><code>python import im_v2.common.data.client.abstract_im_clients as imvcdcaimcl</code>       - Bad <code>python import im_v2.common.data.client as icdc</code>   - Code from a package should import other packages, instead of importing     directly the file   - We don't allow any import loop that can be detected statically (i.e., by     inspecting the code without executing it)     - This guarantees that there are no dynamic import loops, which are even       more difficult to detect and disruptive   - We allow only imports at the module level and not inside functions     - We don't accept using local imports to break import loops, unless it's       temporary to solve a more important problem   - We allow nested packages     - TODO(gp): Clarify the rules here   - We don't want to abuse packaging by creating too many of them     - Rationale:       - There is overhead in organizing and maintaining code in packages and we want to pay the overhead only if we get enough benefit from this   - We specify a short import in the <code>__init__.py</code> file for a package manually     because the linter cannot do it automatically yet     - We use the first letters to build a short import and try to keep it less       than 8 chars long, e.g., <code>im_v2.talos.data.client</code> -&gt; <code>itdcl</code>     - We insert an import docstring in the <code>__init__.py</code> file manually and then       we use the specified short import everywhere in the codebase. E.g.,</p> </li> </ul> <p>```python   Import as:</p> <p>import im_v2.talos.data.client as itdcl   ```</p> </li> </ul>"},{"location":"coding/all.imports_and_packages.how_to_guide.html#how-to-import-code-from-unit-tests","title":"How to import code from unit tests","text":"<ul> <li> <p>To avoid churning client code when code is moved among files, we allow unit   tests to both:</p> </li> <li> <p>Import the package when testing code exported from the package</p> <ul> <li>E.g., in <code>market_data/test/market_data_test_case.py</code> you can import the    package even if it's included    <code>python    import market_data as mdata    \u2026 mdata.AbstractMarketData \u2026</code></li> </ul> </li> <li> <p>Import the files directly with the code and not the package</p> <ul> <li>E.g.,    <code>python    import market_data.abstract_market_data as mdabmada    \u2026 mdabmada.AbstractMarketData \u2026</code></li> </ul> </li> <li> <p>To justify, one can argue that unit tests are clients of the code and should   import packages like any other client</p> </li> <li> <p>To justify, one can interpret that unit tests are tied to specific files, so   they should be kept in sync with the low-level code and not with the public   interface. In fact, we already allow unit tests to call private functions,   acknowledging that unit tests are not regular clients</p> </li> <li> <p>Given that both explanations are valid, we allow both styles</p> </li> </ul>"},{"location":"coding/all.imports_and_packages.how_to_guide.html#common-unit-test-code","title":"Common unit test code","text":"<ul> <li>Unit tests should not import from each other</li> <li>If there is common code, it should go in libraries inside or outside <code>test</code>     directories<ul> <li>E.g., we use <code>foobar_example.py</code> files containing builders for mocks and   examples of objects to be used by tests</li> <li>E.g., we use <code>test/foobar_test_case.py</code> or <code>test/foobar_utils.py</code></li> </ul> </li> <li>In other terms, test files are always leaves of the import graph</li> </ul>"},{"location":"coding/all.imports_and_packages.how_to_guide.html#packagelib-hierarchy-and-cycle-prevention","title":"Package/lib hierarchy and cycle prevention","text":"<ul> <li>Static import cycles can be detected by the invoke <code>lint_detect_cycles</code></li> <li>To prevent import cycles, we want to enforce that certain packages don't   depend on other packages</li> <li>E.g., <code>helpers</code> should not depend on any other package, besides external     libraries</li> <li><code>core</code> should only depend on <code>helpers</code></li> <li><code>dataflow</code> should only depend on <code>core</code> and <code>helpers</code></li> <li>These constraints can be expressed in terms of \"certain nodes of the import     graph are sources\" or \"certain edges in the import graph are forbidden\"</li> <li>We also want to enforce that certain libs don't import others within a single   package. For example, in <code>helpers</code>, the following hierarchy should be   respected:</li> <li><code>hwarnings</code>, <code>hserver</code>, <code>hlogging</code></li> <li><code>hdbg</code></li> <li><code>hintrospection</code>, <code>hprint</code></li> <li><code>henv</code>, <code>hsystem</code>, <code>hio</code>, <code>hversio</code> (this is the base layer to access env      vars and execute commands)</li> <li><code>hgit</code> (Git requires accessing env vars and system calls)</li> <li>A library can only import libs that precede it or are on the same level in the   hierarchy above.</li> <li>E.g., <code>henv</code> can import <code>hdbg</code>, <code>hprint</code>, and <code>hio</code>, but it cannot import     <code>hgit</code></li> <li>While importing a lib on the same level, make sure you are not creating an     import cycle</li> <li>In addition, keep in mind the following rules to prevent import cycles:</li> <li>Any import inside a function is just a temporary hack waiting to create     problems</li> <li>Any time we can break a file into smaller pieces, we should do that since     this helps control the dependencies</li> </ul>"},{"location":"coding/all.imports_and_packages.how_to_guide.html#anatomy-of-a-package","title":"Anatomy of a package","text":"<ul> <li>TODO(gp): Let's use <code>dataflow</code> as a running example</li> <li>A package has a special <code>__init__.py</code> exporting public methods</li> </ul>"},{"location":"coding/all.integrate_repos.how_to_guide.html","title":"Integrate Repos","text":""},{"location":"coding/all.integrate_repos.how_to_guide.html#how-to-integrate-repos","title":"How to integrate repos","text":""},{"location":"coding/all.integrate_repos.how_to_guide.html#concepts","title":"Concepts","text":"<ul> <li>We have two dirs storing two forks of the same repo</li> <li>Files are touched (e.g., added, modified, deleted) in each forks</li> <li>The most problematic files are the files that are modified in both forks</li> <li>Files that are added or deleted in one fork, should be added / deleted also     in the other fork</li> <li>Often we can integrate \"by directory\", i.e., finding entire directories that   were touched in one branch but not in the other</li> <li>In this case we can simply copy the entire dir from one repo to the other</li> <li> <p>Other times we need to integrate each file</p> </li> <li> <p>There are various interesting Git reference points:</p> </li> <li>The branch point for each fork, at which the integration branch was started</li> <li>The last integration point for each fork, at which the repos are the same,      or at least aligned</li> </ul>"},{"location":"coding/all.integrate_repos.how_to_guide.html#invariants-for-the-integration-workflows","title":"Invariants for the integration workflows","text":"<ul> <li>The user runs commands in an abs dir, e.g., <code>/Users/saggese/src/{amp1,cmamp1}</code></li> <li>The user refers in the command line to <code>dir_basename</code>, which is the basename   of the integration directories (e.g., <code>amp1</code>, <code>cmamp1</code>, <code>kaizenflow1</code>)</li> <li>The <code>src_dir_basename</code> is the one where the command is issued</li> <li>The <code>dst_dir_basename</code> is assumed to be parallel to the <code>src_dir_basename</code></li> <li>The dirs are then transformed in absolute dirs <code>abs_src_dir</code></li> </ul>"},{"location":"coding/all.integrate_repos.how_to_guide.html#integration-process","title":"Integration process","text":""},{"location":"coding/all.integrate_repos.how_to_guide.html#preparation","title":"Preparation","text":"<ul> <li> <p>Pull master</p> </li> <li> <p>Crete the integration branches</p> </li> </ul> <p>```bash</p> <p>cd cmamp1 git checkout master i integrate_create_branch --dir-basename cmamp1 cd kaizenflow1 git checkout master i integrate_create_branch --dir-basename kaizenflow1   ```</p> <ul> <li>In one line</li> </ul> <p><code>bash   cd $HOME/cmamp1 &amp;&amp; \\     git checkout master &amp;&amp; \\     i integrate_create_branch --dir-basename cmamp1 &amp;&amp; \\     cd $HOME/kaizenflow1 &amp;&amp; \\     git checkout master &amp;&amp; \\     i integrate_create_branch --dir-basename kaizenflow1</code></p> <ul> <li>Remove white spaces from both source and destination repos:</li> </ul> <p>```bash</p> <p>dev_scripts/clean_up_text_files.sh git commit -am \"Remove white spaces\"; git push   <code>- One should still run the regressions out of paranoia since some golden     outcomes can be changed</code>     &gt; i gh_create_pr --no-draft     &gt; i gh_workflow_list     ```</p> <ul> <li>Remove empty files:</li> </ul> <p>```bash</p> <p>find . -type f -empty -print | grep -v .git | grep -v init | grep -v \".log$\" | grep -v \".txt$\" | xargs git rm   <code>``   - TODO(gp): Add this step to</code>dev_scripts/clean_up_text_files.sh`</p> <ul> <li>Align <code>lib_tasks.py</code>:</li> </ul> <p>```bash</p> <p>vimdiff ~/src/{cmamp1, kaizenflow1}/tasks.py; diff_to_vimdiff.py --dir1 ~/src/cmamp1 --dir2 ~/src/kaizenflow1 --subdir helpers   ```</p> <ul> <li>Lint both dirs:</li> </ul> <p>```bash</p> <p>cd amp1 i lint --dir-name . --only-format cd cmamp1 i lint --dir-name . --only-format   ```</p> <p>or at least the files touched by both repos:</p> <p>```bash</p> <p>i integrate_files --file-direction only_files_in_src cat tmp.integrate_find_files_touched_since_last_integration.cmamp1.txt tmp.integrate_find_files_touched_since_last_integration.amp1.txt | sort | uniq &gt;files.txt FILES=$(cat files.txt) i lint --only-format -f \"$FILES\"   ```   - This should be done as a single separated PR to be reviewed separately</p> <ul> <li>Align <code>lib_tasks.py</code>:   ```bash <p>vimdiff ~/src/{amp1,cmamp1}/tasks.py; diff_to_vimdiff.py --dir1 ~/src/amp1 --dir2 ~/src/cmamp1 --subdir helpers   ```</p> </li> </ul>"},{"location":"coding/all.integrate_repos.how_to_guide.html#integration","title":"Integration","text":"<ul> <li>Create the integration branches:</li> </ul> <p>```bash</p> <p>cd amp1 i integrate_create_branch --dir-basename amp1 i integrate_create_branch --dir-basename kaizenflow1 cd cmamp1 i integrate_create_branch --dir-basename cmamp1   ```</p> <ul> <li>Check what files were modified in each fork since the last integration:</li> </ul> <p>```bash</p> <p>i integrate_files --file-direction common_files i integrate_files --file-direction common_files --src-dir-basename cmamp1 --dst-dir-basename kaizenflow1</p> <p>i integrate_files --file-direction only_files_in_src i integrate_files --file-direction only_files_in_dst   ```</p> <ul> <li>Look for directory touched on only one branch:   ```bash <p>i integrate_files --file-direction common_files --mode \"print_dirs\" i integrate_files --file-direction only_files_in_src --mode \"print_dirs\" i integrate_files --file-direction only_files_in_dst --mode \"print_dirs\"   ```</p> </li> <li>If we find dirs that are touched in one branch but not in the other we can   copy / merge without running risks</li> </ul> <p>```bash</p> <p>i integrate_diff_dirs --subdir $SUBDIR -c   ```</p> <ul> <li>Check which change was made in each side since the last integration</li> </ul> <p>```bash   # Find the integration point:</p> <p>i integrate_files --file-direction common_files   ...   last_integration_hash='813c7e763'</p> <p># Diff the changes in each side from the integration point:</p> <p>i git_branch_diff_with -t hash -h 813c7e763 -f ... git difftool 813c7e763 ...   ```</p> <ul> <li>Check which files are different between the dirs:</li> </ul> <p>```bash</p> <p>i integrate_diff_dirs   ```</p> <ul> <li>Diff dir by dir</li> </ul> <p>```bash</p> <p>i integrate_diff_dirs --subdir dataflow/system   ```</p> <ul> <li>Copy by dir</li> </ul> <p>```bash</p> <p>i integrate_diff_dirs --subdir market_data -c   ```</p> <ul> <li>Sync a dir to handle moved files</li> <li>Assume that there is a dir where files were moved   ```bash <p>invoke integrate_diff_dirs   ...   ... Only in .../cmamp1/.../alpha_numeric_data_snapshots: alpha   ... Only in .../amp1/.../alpha_numeric_data_snapshots: latest   ```</p> </li> <li>You can accept one side with:   ```bash <p>invoke integrate_rsync $(pwd)/marketing   ```</p> </li> <li>This corresponds to:   ```bash <p>rsync --delete -a -r {src_dir}/ {dst_dir}/   ```</p> </li> </ul>"},{"location":"coding/all.integrate_repos.how_to_guide.html#double-check-the-integration","title":"Double-check the integration","text":"<ul> <li>Check that the regressions are passing on GH</li> </ul> <p>```bash</p> <p>i gh_create_pr --no-draft   ```</p> <ul> <li>Check the files that were changed in both branches (i.e., the \"problematic   ones\") since the last integration and compare them to the base in each branch</li> </ul> <p>```bash</p> <p>cd amp1 i integrate_diff_overlapping_files --src-dir-basename \"amp1\" --dst-dir-basename \"cmamp1\" cd cmamp1 i integrate_diff_overlapping_files --src-dir-basename \"cmamp1\" --dst-dir-basename \"amp1\"   ```</p> <ul> <li>Read the changes to Python files:</li> </ul> <p>```bash</p> <p>cd amp1 i git_branch_diff_with -t base --keep-extensions py cd cmamp1 i git_branch_diff_with -t base --keep-extensions py   ```</p> <ul> <li>Quickly scan all the changes in the branch compared to the base:   ``` <p>cd amp1 i git_branch_diff_with -t base cd cmamp1 i git_branch_diff_with -t base   ```</p> </li> </ul>"},{"location":"coding/all.integrate_repos.how_to_guide.html#run-tests","title":"Run tests","text":"<ul> <li>Check <code>amp</code> / <code>cmamp</code> using GH actions:</li> </ul> <p>```bash</p> <p>i gh_create_pr --no-draft i pytest_collect_only i gh_workflow_list   ```</p> <ul> <li>Check <code>lem</code> on dev1</li> </ul> <p>```bash   # Clean everything.</p> <p>git reset --hard; git clean -fd; git pull; (cd amp; git reset --hard; git clean -fd; git pull)</p> <p>i git_pull</p> <p>AM_BRANCH=AmpTask1786_Integrate_20220916 (cd amp; gco $AM_BRANCH)</p> <p>i pytest_collect_only i pytest_buildmeister</p> <p>i git_branch_create -b $AM_BRANCH   ```</p> <ul> <li> <p>Check <code>lime</code> on dev4</p> </li> <li> <p>Check <code>orange</code> on dev1</p> </li> <li> <p>Check <code>dev_tools</code> on dev1</p> </li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html","title":"Jupyter Notebook","text":""},{"location":"coding/all.jupyter_notebook.how_to_guide.html#jupyter-notebook-best-practices","title":"Jupyter notebook best practices","text":""},{"location":"coding/all.jupyter_notebook.how_to_guide.html#when-to-use-a-jupyter-notebook","title":"When to use a Jupyter notebook","text":"<ul> <li>A notebook can be used for various goals:</li> <li>Tutorial / gallery<ul> <li>Show how some code works (e.g., functions in <code>signal_processing.py</code> or   <code>data_encyclopedia.ipynb</code>)</li> <li>The code should always work</li> <li>We might want to add unit tests for it</li> </ul> </li> <li>Prototyping / one-off<ul> <li>E.g.,</li> <li>We prototype some code, before it becomes library code</li> <li>We did some one-off analysis</li> </ul> </li> <li>Analysis<ul> <li>Aka \"master\" notebooks</li> <li>The notebook should always work so we need to treat it as part of the code   base</li> <li>We might want to add unit tests for it</li> </ul> </li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#general-structure-of-a-notebook","title":"General structure of a notebook","text":""},{"location":"coding/all.jupyter_notebook.how_to_guide.html#description","title":"Description","text":"<ul> <li>At the top of the notebook add a description section explaining a notebook's   goal and what it does, e.g.,   ```   # Description</li> </ul> <p>This notebook was used for prototyping / debugging code that was moved in the file <code>abc.py</code>   ```</p> <ul> <li>Convert section headers and cells with description text to a Markdown format   by selecting a cell and then at Jupyter interface do   <code>Cell -&gt; Cell Type -&gt; Markdown</code></li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#imports","title":"Imports","text":"<ul> <li>Add a code section importing the needed libraries</li> <li>Autoreload modules to keep Jupyter and local code updated in real-time</li> <li>Standard imports, e.g. <code>os</code></li> <li>Third-party imports, e.g. <code>pandas</code></li> <li>Local imports from our lib</li> <li>It's better to put all the imports in one cell and separate different import   types by 1 empty line, e.g.:   ```   # Imports</li> </ul> <p>%load_ext autoreload   %autoreload 2</p> <p>import logging   import os</p> <p>import matplotlib.pyplot as plt   import pandas as pd</p> <p>import helpers.dbg as dbg   import helpers.env as env   import helpers.printing as prnt   import core.explore as exp   import core.signal_processing as sigp   ...   ```</p> <ul> <li>In this way executing one cell is enough to configure the notebook</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#configuration","title":"Configuration","text":"<ul> <li>You can configure the notebooks with some utils, logging, and report info on   how the notebook was executed (e.g., Git commit, libs, etc.) by using the   following cell:   ```   # Configure logger.   hdbg.init_logger(verbosity=logging.INFO)   _LOG = logging.getLogger(name)</li> </ul> <p># Print system signature.   _LOG.info(\"%s\", henv.get_system_signature()[0])</p> <p># Configure the notebook style.   hprint.config_notebook()   ```</p> <ul> <li>The output of the cell looks like:   <code>INFO: &gt; cmd='/venv/lib/python3.8/site-packages/ipykernel_launcher.py -f /home/.local/share/jupyter/runtime/kernel-a48f60fd-0f96-48b4-82d8-879385f2be91.json'   WARNING: Running in Jupyter   DEBUG Effective logging level=10   DEBUG Shut up 1 modules: asyncio   DEBUG &gt; (cd . &amp;&amp; cd \"$(git rev-parse --show-toplevel)/..\" &amp;&amp; (git rev-parse --is-inside-work-tree | grep -q true)) 2&gt;&amp;1   DEBUG &gt; (git rev-parse --show-toplevel) 2&gt;&amp;1   ...   # Packages   python: 3.8.10   cvxopt: 1.3.0   cvxpy: 1.2.2   gluonnlp: ?   gluonts: 0.6.7   joblib: 1.2.0   mxnet: 1.9.1   numpy: 1.23.4   pandas: 1.5.1   pyarrow: 10.0.0   scipy: 1.9.3   seaborn: 0.12.1   sklearn: 1.1.3   statsmodels: 0.13.5</code></li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#make-the-notebook-flow-clear","title":"Make the notebook flow clear","text":"<ul> <li>Each notebook needs to follow a clear and logical flow, e.g:</li> <li>Load data</li> <li>Compute stats</li> <li>Clean data</li> <li>Compute stats</li> <li>Do analysis</li> <li>Show results</li> <li>The flow should be highlighted using headings in markdown:   <code># Level 1   ## Level 2   ### Level 3</code></li> <li>Use the extension for navigating the notebook (see our suggestions for Jupyter   plug-ins)</li> <li>Keep related code and analysis close together so:</li> <li>Readers can understand the logical flow</li> <li>One could \"easily\" split the notebook in parts (e.g., when it becomes too     big)</li> <li>You can collapse the cells and don't scroll back and forth too much</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#general-best-practices","title":"General best practices","text":""},{"location":"coding/all.jupyter_notebook.how_to_guide.html#update-calls-only-for-mastergallery-notebooks","title":"Update calls only for Master/Gallery notebooks","text":""},{"location":"coding/all.jupyter_notebook.how_to_guide.html#convention","title":"Convention:","text":"<ul> <li>We do our best to update the calls in the Master/Gallery notebooks but we   don't guarantee that the fix is correct</li> <li>For other notebooks we either do the fix (e.g., changing a name of a function)   and tweak the call to enforce the old behavior, or even not do anything if   there are too many changes</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#rationale","title":"Rationale:","text":"<ul> <li>We have dozens of ad-hoc research notebooks</li> <li>When a piece of code is updated (e.g., <code>ImClient</code>) the change should be   propagated everywhere in the code base, including the notebooks</li> <li>This results in excessive amount of maintenance work which we want to avoid</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#keep-code-that-belongs-together-in-one-cell","title":"Keep code that belongs together in one cell","text":"<ul> <li>It's often useful to keep in a cell computation that needs to be always   executed together</li> <li>E.g., compute something and then print results</li> <li>In this way a single cell execution computes all data together</li> <li>Often computation starts in multiple cells, e.g., to inline debugging, and   once we are more confident that it works correctly we can merge it in a cell   (or even better in a function)</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#write-beautiful-code-even-in-notebooks","title":"Write beautiful code, even in notebooks","text":"<ul> <li>Follow the conventions and suggestions for   Python code style</li> <li>When prototyping with a notebook, the code can be of lower quality than code,   but still needs to be readable and robust</li> <li>In our opinion it's just better to always do write robust and readable code:   it doesn't buy much time to cut corners</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#show-how-data-is-transformed-as-you-go","title":"Show how data is transformed as you go","text":"<ul> <li>Print a few lines of data structures (e.g., <code>df.head(3)</code>) so one can see how   data is transformed through the cells</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#use-keyboard-shortcuts","title":"Use keyboard shortcuts","text":"<ul> <li>Learn the default keyboard shortcuts to edit efficiently</li> <li>You can use the vim plug-in (see below) and become 3x more ninja</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#strive-for-simplicity","title":"Strive for simplicity","text":"<ul> <li>Always make the notebook easy to be understood and run by somebody else</li> <li>Explain what happens</li> <li>Organize the code in a logical way</li> <li>Use decent variable names</li> <li>Comment the results, when possible / needed</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#dependencies-among-cells","title":"Dependencies among cells","text":"<ul> <li>Try to avoid dependencies between cells</li> <li>Even better avoid any dependency between cells, e.g.:</li> <li>Put all the imports in one cell at the beginning, so with one cell execution     you can make sure that all the imports are done</li> <li>Compare this approach with the case where the imports are randomly sprinkled     in the notebook, then you need to go execute them one by one if you     re-initialize the notebook</li> <li>For the same reason group functions in one cell that you can easily re-execute</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#re-execute-from-scratch","title":"Re-execute from scratch","text":"<ul> <li>Once in a while (e.g., once a day)</li> <li>Commit your changes</li> <li>Make sure you can re-execute everything from the top with   <code>Kernel -&gt; Restart &amp; Clean output</code> and then <code>Kernel -&gt; Run all</code></li> <li>Visually verify that the results didn't change, so that there is no weird   state or dependency in the code</li> <li>Before a commit (and definitively before a PR) do a clean run</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#add-comments-for-complex-cells","title":"Add comments for complex cells","text":"<ul> <li>When a cell is too long, explain in a comment what a cell does, e.g.,   <code>## Count stocks with all nans.   num_nans = np.isnan(rets).sum(axis=0)   num_nans /= rets.shape[0]   num_nans = num_nans.sort_values(ascending=False)   num_stocks_with_no_nans = (num_nans == 0.0).sum()   print(\"num_stocks_with_no_nans=%s\" % hprint.perc(num_stocks_with_no_nans, rets.shape[1]))</code></li> <li>Another approach is to factor out the code in functions with clear names and   simplify the flow</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#do-not-cut-paste-code","title":"Do not cut &amp; paste code","text":"<ul> <li>Cutting + paste + modify is NEVER a good idea</li> <li>It takes more time to clean up cut &amp; paste code than doing right in the first   place</li> <li>Just make a function out of the code and call it!</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#avoid-wall-of-code-cell","title":"Avoid \"wall-of-code\" cell","text":"<ul> <li>Obvious</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#avoid-data-biases","title":"Avoid data biases","text":"<ul> <li>Try to compute statistics on the entire data set so that results are   representative and not dependent on a particular slice of the data</li> <li>You can sample the data and check stability of the results</li> <li>If it takes too long to compute the statistics on the entire data set, report   the problem and we can think of how to speed it up</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#avoid-hardwired-constants","title":"Avoid hardwired constants","text":"<ul> <li>Don't use hardwired constants</li> <li>Try to parametrize the code</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#explain-where-data-is-coming-from","title":"Explain where data is coming from","text":"<ul> <li>If you are using data from a file (e.g., <code>/data/wd/RP_1yr_13_companies.pkl</code>),   explain in a comment how the file was generated</li> <li>Ideally report a command line to regenerate the data</li> <li>The goal is for other people to be able to re-run the notebook from scratch</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#fix-warnings","title":"Fix warnings","text":"<ul> <li>A notebook should run without warnings</li> <li>Warnings can't be ignored since they indicate that the code is relying on a   feature that will change in the future, e.g.,   <code>FutureWarning: Sorting because non-concatenation axis is   not aligned. A future version of pandas will change to not sort by   default.   To accept the future behavior, pass 'sort=False'.   To retain the current behavior and silence the warning, pass 'sort=True'.</code></li> <li>Another example: after a cell execution the following warning appears:   <code>A value is trying to be set on a copy of a slice from a DataFrame   See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy</code></li> <li>This is a typical pandas warning telling us that we created a view on a     dataframe (e.g., by slicing) and we are modifying the underlying data     through the view</li> <li>This is dangerous since it can create unexpected side effects and coupling     between pieces of code that can be painful to debug and fix</li> <li>If we don't fix the issue now, the next time we create a conda environment the   code might either break or (even worse) have a different behavior, i.e.,   silent failure</li> <li>It's better to fix the warning now that we can verify that the code does what   we want to do, instead of fixing it later when we don't remember anymore what   exactly we were doing</li> <li>If you have warnings in your code or notebook you can't be sure that the code   is doing exactly what you think it is doing</li> <li>For what we know your code might be deleting your hard-disk, moving money     from your account to mine, starting World War 3, ...</li> <li>You don't ever want to program by coincidence</li> <li>Typically the warnings are informative and tell us what's the issue and how to   fix it, so please fix your code</li> <li>If it's not obvious how to interpret or fix a warning file a bug, file a bug     reporting clearly a repro case and the error message</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#make-cells-idempotent","title":"Make cells idempotent","text":"<ul> <li>Try to make a notebook cell able of being executed multiple times without   changing its output value, e.g.,</li> <li>Bad <code>df[\"id\"] = df[\"id\"] + 1</code><ul> <li>This computation is not idempotent, since if you execute it multiple times   is going to increment the column <code>id</code> at every iteration</li> </ul> </li> <li>Good <code>df[\"incremented_df\"] = df[\"id\"] + 1</code><ul> <li>A better approach is to always create a new \"copy\"</li> </ul> </li> <li>Another example:</li> <li>Bad <code>tmp = normalize(tmp)</code></li> <li>Good <code>tmp_after_normalize = normalize(tmp)</code><ul> <li>In this way it's easy to add another stage in the pipeline without   changing everything</li> <li>Of course the names <code>tmp_1</code>, <code>tmp_2</code> are a horrible idea since they are   not self-explanatory and adding a new stage screws up the numbering</li> </ul> </li> <li>For data frames and variables is a good idea to create copies of the data   along the way:   <code>df_without_1s = df[df[\"id\"] != 1].copy()</code></li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#always-look-at-the-discarded-data","title":"Always look at the discarded data","text":"<ul> <li>Filtering the data is a risky operation since once the data is dropped, nobody   is going to go back and double check what exactly happened</li> <li>Everything downstream (e.g., all the results, all the conclusions, all the   decisions based on those conclusions) rely on the filtering being correct</li> <li>Any time there is a <code>dropna</code> or a filtering / masking operation, e.g.:   <code>compu_data = compu_data.dropna(subset=[\"CIK\"])</code>   or   <code>selected_metrics = [...]   compu_data = compu_data[compu_data[\"item\"].apply(lambda x : x in selected_metrics)]   compu_data = compu_data[compu_data[\"datadate\"].apply(date_is_quarter_end)]</code></li> <li>Always count what percentage of the rows you dropped (e.g., do a back of the   envelope check that you are dropping what you would expect)   <code>import helpers.printing as hprint   ...   n_rows = compu_form_df.shape[0]   compu_form_df = compu_form_df.drop_duplicates()   n_rows_after = compu_form_df.shape[0]   _LOG.debug(\"After dropping duplicates kept: %s\", hprint.perc(n_rows_after, n_rows))</code></li> <li>Make absolutely sure you are not dropping important data</li> <li>E.g., has the distribution of the data changed in the way you would expect?</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#use-a-progress-bar","title":"Use a progress bar","text":"<ul> <li>Always use progress bars (even in notebooks) so that user can see how long it   will take for a certain computation.</li> <li>It is also possible to let <code>tqdm</code> automatically choose between console or   notebook versions by using   <code>from tqdm.autonotebook import tqdm</code></li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#notebooks-and-libraries","title":"Notebooks and libraries","text":"<ul> <li>It's ok to use functions in notebooks when building the analysis to leverage   notebook interactivity</li> <li>Once the notebook is \"stable\", often it's better to move the code in a   library, i.e., a python file.</li> <li>Make sure you add autoreload modules in <code>Imports</code> section   <code>%load_ext autoreload   %autoreload 2</code></li> <li>Otherwise, if you change a function in the lib, the notebook will not pull     this change and use the old version of the function</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#pros","title":"Pros","text":"<ul> <li>The same notebook code can be used for different notebooks</li> <li>E.g., the function to read the data from disk is an obvious example</li> <li>More people can reuse the same code for different analyses</li> <li>If one changes the code in a library, Git can help tracking changes and   merging, while notebooks are difficult to diff / merge</li> <li>Cleaning up / commenting / untangling the code can help reason carefully about   the assumptions to find issues</li> <li>The notebook becomes more streamlined and easy to understand since now it's a   sequence of functions <code>do_this_and_that</code> and presenting the results</li> <li>One can speed up / parallelize analyses with multiprocessing</li> <li>Notebooks are not great for this</li> <li>E.g., when one does the analyses on a small subset of the data and then     wants to run on the entire large dataset</li> <li>The exploratory analysis can be moved towards modeling and then production</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#cons","title":"Cons","text":"<ul> <li>One have to scroll back and forth between notebook and the libraries to   execute the cell with the functions and fix all the possible mistakes</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#recommendations-for-plots","title":"Recommendations for plots","text":""},{"location":"coding/all.jupyter_notebook.how_to_guide.html#use-the-proper-y-scale","title":"Use the proper y-scale","text":"<ul> <li>If one value can vary from -1.0 to 1.0, force the y-scale between those limits   so that the values are absolutes, unless this would squash the plot</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#make-each-plot-self-explanatory","title":"Make each plot self-explanatory","text":"<ul> <li>Make sure that each plot has a descriptive title, x and y label</li> <li>Explain the set-up of a plot / analysis</li> <li>E.g., what is the universe of stocks used? What is the period of time?</li> <li>Add this information also to the plots</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#avoid-wall-of-text-tables","title":"Avoid wall-of-text tables","text":"<ul> <li>Try to use plots summarizing the results besides the raw results in a table</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#use-common-axes-to-allow-visual-comparisons","title":"Use common axes to allow visual comparisons","text":"<ul> <li>Try to use same axes for multiple graphs when possible to allow visual   comparison between graphs</li> <li>If that's not possible or convenient make individual plots with different   scales and add a plot with multiple graphs inside on the same axis (e.g., with   y-log)</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#use-the-right-plot","title":"Use the right plot","text":"<ul> <li>Pick the right type of graph to make your point</li> <li><code>pandas</code>, <code>seaborn</code>, <code>matplotlib</code> are your friends</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#useful-plugins","title":"Useful plugins","text":"<ul> <li>You can access the extensions menu:</li> <li><code>Edit -&gt; nbextensions config</code></li> <li><code>http://localhost:XYZ/nbextensions/</code></li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#vim-bindings","title":"Vim bindings","text":"<ul> <li>VIM binding   will change your life</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#table-of-content","title":"Table of content","text":"<ul> <li>To see the entire logical flow of the notebook, when you use the headers   properly</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#executetime","title":"ExecuteTime","text":"<ul> <li>To see how long each cell takes to execute</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#spellchecker","title":"Spellchecker","text":"<ul> <li>To improve your English!</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#autosavetime","title":"AutoSaveTime","text":"<ul> <li>To save the code automatically every minute</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#notify","title":"Notify","text":"<ul> <li>Show a browser notification when kernel becomes idle</li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#jupytext","title":"Jupytext","text":"<ul> <li>We use Jupytext as standard part of our development flow</li> <li>See <code>docs/work_tools/all.jupytext.how_to_guide.md</code></li> </ul>"},{"location":"coding/all.jupyter_notebook.how_to_guide.html#gspread","title":"Gspread","text":"<ul> <li>Allow to read g-sheets in Jupyter Notebook</li> <li>First, one needs to configure Google API, just follow the instructions from   here</li> <li>Useful links:</li> <li>Examples of gspread Usage</li> <li>Enabling Gsheet API</li> <li>Adding service email if it\u2019s not working</li> </ul>"},{"location":"coding/all.plotting.how_to_guide.html","title":"Best practices for writing plotting functions","text":""},{"location":"coding/all.plotting.how_to_guide.html#cosmetic-requirements","title":"Cosmetic requirements","text":"<p>A plot should be easy to interpret. This means:</p> <ul> <li>It should have a title</li> <li>Axes should have labels</li> <li>Ticks should be placed in an interpretable way; if there are few of them,   place their labels with zero rotation</li> <li>If there are several lines on the plot, it should have a legend</li> </ul>"},{"location":"coding/all.plotting.how_to_guide.html#technical-requirements","title":"Technical requirements","text":"<ul> <li>Do not add <code>plt.show()</code> at the end of the function. It prevents the user from   using this plot as a subplot and from tweaking it.</li> <li>Expose the <code>ax</code> parameter</li> </ul>"},{"location":"coding/all.plotting.how_to_guide.html#multiple-plots","title":"Multiple plots","text":"<ul> <li>If the function plots multiple plots, it is usually better to create a single   figure for them. This way, the output is more concise and can be copied as one   image.</li> <li>In <code>amp/core/plotting.py</code>, there is a helper called <code>get_multiple_plots()</code>   that is used for generating a figure and axes</li> <li>Add <code>plt.tight_layout()</code> in the end only if you are sure this figure will not   be wrapped inside another figure</li> <li>If there is a possibility the figure will be wrapped, try passing in a list of   axes</li> <li>To combine multiple figures with subplots into one, use   GridSpec</li> </ul>"},{"location":"coding/all.profiling.how_to_guide.html","title":"Profiling","text":""},{"location":"coding/all.profiling.how_to_guide.html#profiling_1","title":"Profiling","text":""},{"location":"coding/all.profiling.how_to_guide.html#profiling-end-to-end-a-command-line","title":"Profiling end-to-end a command line","text":"<ul> <li>You can use the time-tested Linux <code>time</code> command to profile both time and   memory</li> </ul> <p>```bash</p> <p>/usr/bin/time -v COMMAND 2&gt;&amp;1 | tee time.log</p> <p>Command being timed: \"...COMMAND...\"   User time (seconds): 187.70   System time (seconds): 16.27   Percent of CPU this job got: 96%   Elapsed (wall clock) time (h:mm:ss or m:ss): 3:31.38   Average shared text size (kbytes): 0   Average unshared data size (kbytes): 0   Average stack size (kbytes): 0   Average total size (kbytes): 0   Maximum resident set size (kbytes): 13083892   Average resident set size (kbytes): 0   Major (requiring I/O) page faults: 0   Minor (reclaiming a frame) page faults: 9911066   Voluntary context switches: 235772   Involuntary context switches: 724   Swaps: 0   File system inputs: 424   File system outputs: 274320   Socket messages sent: 0   Socket messages received: 0   Signals delivered: 0   Page size (bytes): 4096   Exit status: 0   ```</p> <ul> <li> <p>Information about the spent time are:   <code>User time (seconds): 187.70   System time (seconds): 16.27   Percent of CPU this job got: 96%   Elapsed (wall clock) time (h:mm:ss or m:ss): 3:31.38</code></p> </li> <li> <p>The relevant part is the following line representing the amount of resident   memory (which is ~13GB) <code>Maximum resident set size (kbytes): 13083892</code></p> </li> </ul>"},{"location":"coding/all.profiling.how_to_guide.html#profiling-python-code-from-command-line","title":"Profiling Python code from command line","text":""},{"location":"coding/all.profiling.how_to_guide.html#cprofile","title":"cProfile","text":""},{"location":"coding/all.profiling.how_to_guide.html#install-in-a-docker-container","title":"Install in a Docker container","text":"<ul> <li>From <code>devops/docker_build/install_cprofile.sh</code>   ```bash <p>sudo apt-get install -y python3-dev sudo apt install -y libgraphviz-dev sudo apt-get install -y graphviz pip install gprof2dot   ```</p> </li> </ul>"},{"location":"coding/all.profiling.how_to_guide.html#how-to-use-with-workflow","title":"How to use with workflow","text":"<ul> <li>There is a script that runs the flow <code>amp/dev_scripts/run_profiling.sh</code></li> </ul>"},{"location":"coding/all.profiling.how_to_guide.html#how-to-use-manually","title":"How to use manually","text":"<ul> <li>You need to run the code first with profiling enabled to collect the profiling   data in a binary file (often called <code>prof.bin</code>).   ```bash   # Profile a Python script. <p>python -m cProfile -o prof.bin ${CMD}   ```</p> </li> <li>To profile a unit test you can run:</li> </ul> <p>```bash   # Profile a unit test.</p> <p>python -m cProfile -o profile edgar/forms8/test/test_edgar_utils.py python -m cProfile -o profile -m pytest edgar/forms8/test/test_edgar_utils.py::TestExtractTablesFromForms::test_table_extraction_example_2   ```</p> <ul> <li>Plotting the results</li> </ul> <p>```bash</p> <p>gprof2dot -f pstats profile | dot -Tpng -o output.png gprof2dot -n 10 -f pstats profile | dot -Tpng -o output.png gprof2dot -n 10 -f pstats profile -l   \"extract_tables_from_forms\" | dot -Tpng -o output.png   ```</p> <p>How to read a graph:   https://nesi.github.io/perf-training/python-scatter/profiling-cprofile</p> <ul> <li>Gprof2dot has lots of interesting options to tweak the output, e.g.,   ```bash <p>gprof2dot -h   ...   -n PERCENTAGE, --node-thres=PERCENTAGE   eliminate nodes below this threshold [default: 0.5]   -e PERCENTAGE, --edge-thres=PERCENTAGE   eliminate edges below this threshold [default: 0.1]   --node-label=MEASURE measurements to on show the node (can be specified   multiple times): self-time, self-time-percentage,   total-time or total-time-percentage [default: total-   time-percentage, self-time-percentage]   -z ROOT, --root=ROOT prune call graph to show only descendants of   specified   root function   -l LEAF, --leaf=LEAF prune call graph to show only ancestors of   specified   leaf function   --depth=DEPTH prune call graph to show only descendants or ancestors   until specified depth   --skew=THEME_SKEW skew the colorization curve. Values &lt; 1.0 give   more   variety to lower percentages. Values &gt; 1.0 give less   variety to lower percentages   -p FILTER_PATHS, --path=FILTER_PATHS   Filter all modules not in a specified path   ...   ```</p> </li> </ul>"},{"location":"coding/all.profiling.how_to_guide.html#process_profpy","title":"process_prof.py","text":"<ul> <li>You can use the script <code>dev_scripts/process_prof.py</code> to automate some tasks:</li> <li>Top-level statistics</li> <li>Plotting the call-graph</li> <li>Custom statics</li> </ul>"},{"location":"coding/all.profiling.how_to_guide.html#line_profiler","title":"line_profiler","text":"<ul> <li> <p>CProfile allows to break down the execution time into function calls, while   kernprof allows to profile a function line by line.</p> </li> <li> <p>GitHub: https://github.com/pyutils/line_profiler</p> </li> <li> <p>Install with:   ```bash</p> <p>pip install line_profiler   ```</p> </li> </ul>"},{"location":"coding/all.profiling.how_to_guide.html#how-to-use","title":"How to use","text":"<ul> <li>Instrument the code to profile:</li> </ul> <p>```python   import line_profiler   profiler = line_profiler.LineProfiler()</p> <p># Print the results at the end of the run.   import atexit</p> <p>def exit_handler():       profiler.print_stats()</p> <p>atexit.register(exit_handler)</p> <p>@profiler   def function():</p> <p>...   ```</p> <ul> <li>Through command line:   ```bash <p>kernprof -o prof.lprof -l $cmd   ...   Wrote profile results to run_process_forecasts.py.lprof   ```</p> </li> </ul>"},{"location":"coding/all.profiling.how_to_guide.html#pytest-profiling","title":"pytest-profiling","text":"<ul> <li> <p>Webpage: https://pypi.org/project/pytest-profiling</p> </li> <li> <p>Install it with   ```bash</p> <p>pip install pytest-profiling   ```</p> </li> </ul>"},{"location":"coding/all.profiling.how_to_guide.html#how-to-use_1","title":"How to use","text":"<pre><code> &gt; pytest --profile ./amp/core/dataflow_model/test/test_pnl_simulator.py::TestPnlSimulator2::test_perf1 -s\n</code></pre>"},{"location":"coding/all.profiling.how_to_guide.html#profiling-in-a-jupyter-notebook","title":"Profiling in a Jupyter notebook","text":"<ul> <li>You can find all of the examples below in action in the   <code>amp/core/notebooks/time_memory_profiling_example.ipynb</code> link.</li> </ul>"},{"location":"coding/all.profiling.how_to_guide.html#time-profilers","title":"Time profilers","text":"<ul> <li>In a notebook, execute cell with <code>%time</code> cell-magic:   <code>python   %%time   func()</code></li> </ul>"},{"location":"coding/all.profiling.how_to_guide.html#by-function","title":"By function","text":"<ul> <li> <p>We prefer cProfile for profiling and gprof2dot for visualization.</p> </li> <li> <p>The documentation does not state this, but <code>%prun</code> magic uses cProfile under   the hood, so we can use it in the notebook instead</p> </li> </ul> <p><code>python   # We can suppress output to the notebook by specifying \"-q\".   %%prun -D tmp.pstats func() !gprof2dot -f pstats tmp.pstats | dot -Tpng -o output.png   dspl.Image(filename=\"output.png\")</code></p> <ul> <li> <p>This will output something like this:   </p> </li> <li> <p>If you open the output image in the new tab, you can zoom in and look at the   graph in detail.</p> </li> <li> <p>Gprof2dot supports thresholds that make output more readable:</p> </li> </ul> <p><code>python   !gprof2dot -n 5 -e 5 -f pstats tmp.pstats | dot -Tpng -o output.png   dspl.Image(filename=\"output.png\")</code></p> <ul> <li>This will filter the output into something like this:   </li> </ul>"},{"location":"coding/all.profiling.how_to_guide.html#memory-profilers","title":"Memory profilers","text":"<ul> <li> <p>We prefer using memory-profiler.</p> </li> <li> <p>Peak memory</p> </li> </ul> <p><code>bash   %%memit   func()</code></p> <ul> <li>Memory by line   <code>bash   %mprun -f func func()</code></li> </ul>"},{"location":"coding/all.publish_notebook.how_to_guide.html","title":"Publishing a notebook","text":""},{"location":"coding/all.publish_notebook.how_to_guide.html#what-it-is-about","title":"What it is about","text":"<ul> <li><code>publish_notebook.py</code> is a little tool that allows to:</li> <li>Opening a notebook in your browser (useful for read-only mode)<ul> <li>E.g., without having to use Jupyter notebook (which modifies the file in    your client) or github preview (which is slow or fails when the notebook    is too large)</li> </ul> </li> <li>Sharing a notebook with others in a simple way</li> <li>Pointing to detailed documentation in your analysis Google docs</li> <li>Reviewing someone's notebook</li> <li>Comparing multiple notebooks against each other in different browser      windows</li> <li>Taking a snapshot / checkpoint of a notebook as a backup or before making      changes<ul> <li>This is a lightweight alternative to \"unit testing\" to capture the    desired behavior of a notebook</li> <li>One can take a snapshot and visually compare multiple notebooks    side-by-side for changes</li> </ul> </li> </ul> <p>You can get details by running: <code>dev_scripts/notebooks/publish_notebook.py -h</code></p>"},{"location":"coding/all.publish_notebook.how_to_guide.html#opening-a-notebook","title":"Opening a notebook","text":"<ul> <li>Inside the dev container   <code>docker&gt; FILE=im_v2/common/universe/notebooks/Master_universe_analysis.ipynb; publish_notebook.py --file $FILE --action convert</code></li> <li>This converts the <code>ipynb</code> file into a <code>HTML</code></li> <li>Then you can open it from outside your container   ``` <p>open Master_universe_analysis.20231214-081257.html   ```   or pointing the browser to it</p> </li> </ul>"},{"location":"coding/all.publish_notebook.how_to_guide.html#what-has-changed","title":"What has changed","text":"<p>We've deployed the new service for storing notebooks in HTML format</p> <ul> <li>From now on <code>publish_notebook.py</code> will work from the Docker container. The new   version of <code>publish_notebook.py</code> works using HTTP protocol and does not   require ssh key authorization as it was before</li> <li>We've synchronized all documents. So all old docs already available on the new   service</li> </ul>"},{"location":"coding/all.qgrid.how_to_guide.html","title":"Qgrid","text":""},{"location":"coding/all.qgrid.how_to_guide.html#some-documentation","title":"Some documentation","text":"<ul> <li>GitHub: https://github.com/quantopian/qgrid</li> <li>Official documentation: https://qgrid.readthedocs.io/en/latest</li> <li>Interesting demo:   https://mybinder.org/v2/gh/quantopian/qgrid-notebooks/master?filepath=index.ipynb</li> </ul>"},{"location":"coding/all.qgrid.how_to_guide.html#testing","title":"Testing","text":"<ul> <li>Pandas &gt;1.0 needs qgrid at least 1.3.0 which is installed in our</li> </ul> <p><code>python   print(qgrid.__version__)   1.3.0</code></p> <ul> <li> <p>Go to the gallery notebook as in:   http://localhost:10002/notebooks/amp/core/notebooks/gallery_examples.ipynb#Qgrid</p> </li> <li> <p>Make sure you can see the output of the cells</p> </li> <li> <p>If not you might need to apply the fix as per   https://github.com/quantopian/qgrid/issues/253</p> </li> <li>Stop the notebook server</li> <li>Execute</li> </ul> <p>```bash</p> <p>jupyter nbextension enable --py --sys-prefix qgrid   /Users/saggese/.conda/envs/develop/bin/jupytext   jupyter nbextension enable --py --sys-prefix widgetsnbextensionEnabling notebook extension qgrid/extension...         - Validating: OK</p> <p>jupyter nbextension enable --py --sys-prefix widgetsnbextension   /Users/saggese/.conda/envs/develop/bin/jupytext   Enabling notebook extension jupyter-js-widgets/extension...         - Validating: OK   ```   - Restart the notebook   - Test again with the gallery notebook</p>"},{"location":"coding/all.reading_other_people_code.how_to_guide.html","title":"Reading Other People Code","text":""},{"location":"coding/all.reading_other_people_code.how_to_guide.html#reading-other-people-code_1","title":"Reading other people code","text":"<ul> <li>People don't like reading other people's code</li> <li>Still reading existing code needs to be done</li> <li> <p>Nobody can code in a vacuum</p> </li> <li> <p>When done with the right attitude, reading code can be enjoyable, and you can   actually learn and improve as a coder</p> </li> <li>E.g., writers read and study other writers' book to improve</li> </ul>"},{"location":"coding/all.reading_other_people_code.how_to_guide.html#what-not-to-do","title":"What not to do","text":""},{"location":"coding/all.reading_other_people_code.how_to_guide.html#rewrite-coding","title":"Rewrite coding","text":"<ul> <li>You think \"This code is a complete ugly mess. It needs to be rewritten\"</li> <li> <p>The answer is: ABSOLUTELY NO!</p> </li> <li> <p>The best case of a code rewrite is to have:</p> </li> <li>The same code (in reality is likely that new bugs entered the system)</li> <li>With a different complexity that you only now understand</li> <li> <p>After a long time and effort</p> </li> <li> <p>In other terms, there is no reason to believe that you are going to do a   better job than others did</p> </li> </ul>"},{"location":"coding/all.reading_other_people_code.how_to_guide.html#incremental-renovation","title":"Incremental renovation","text":"<ul> <li> <p>The first thing that programmers want to do is to bulldoze the place flat and   build something great</p> </li> <li> <p>Nobody is excited about incremental renovation:</p> </li> <li>Improving</li> <li>Refactoring</li> <li>Cleaning out</li> <li>Adding unit tests</li> <li>In reality, 99.9% of work is incremental</li> </ul>"},{"location":"coding/all.reading_other_people_code.how_to_guide.html#its-harder-to-read-code-than-to-write-it","title":"It's harder to read code than to write it","text":"<ul> <li>For this reason code reuse is hard</li> <li>For this reason, everybody on the team has the same function to do the same   thing</li> <li>It's easier and more fun to write new code than figuring out how the old code   works</li> </ul>"},{"location":"coding/all.reading_other_people_code.how_to_guide.html#respect-old-code","title":"Respect old code!","text":"<ul> <li> <p>When you think \"the old code is a mess\", you are probably wrong</p> </li> <li> <p>The idea that new code is better than old code is absurd</p> </li> <li>Old code has been used</li> <li>Old code has been tested</li> <li> <p>Lots of bugs in old code have been found and fixed</p> </li> <li> <p>When the code looks a mess is because it handles many corner cases you didn't   even think about, you didn't even know were possible</p> </li> <li> <p>Each of that bug took a long time to be discovered and fixed it</p> </li> <li> <p>When you throw away code and start from scratch, you are throwing away all the   knowledge, all the bug fixes, all the hard thinking</p> </li> </ul>"},{"location":"coding/all.reading_other_people_code.how_to_guide.html#what-makes-code-a-mess","title":"What makes code a mess?","text":"<ul> <li> <p>What makes the code a \"mess\" (at least according to your expert opinion as   world-class coder):</p> </li> <li> <p>Architectural problems</p> </li> <li> <p>E.g.,</p> <ul> <li>Code is not split into pieces in a way that makes sense</li> <li>Interfaces that are too broad and brittle</li> </ul> </li> <li> <p>These problems can be easily fixed!</p> </li> <li> <p>Inefficiency</p> </li> <li> <p>Profile and find what is the problem and fix that</p> </li> <li> <p>Ugly</p> </li> <li>E.g., horrible variable and function names</li> <li> <p>It takes 5 minutes of search-and-replace</p> </li> <li> <p>All these problems can be easily fixed in 100x less time than rewriting</p> </li> </ul>"},{"location":"coding/all.reading_other_people_code.how_to_guide.html#what-to-do","title":"What to do","text":""},{"location":"coding/all.reading_other_people_code.how_to_guide.html#get-into-the-right-attitude","title":"Get into the right attitude","text":"<ol> <li>Assume that whoever wrote the code knew what he/she was doing</li> <li>If that's not the case, he/she would have already been fired from the team</li> <li> <p>Therefore he/she is as competent than you</p> </li> <li> <p>Sorry for the bad news, but no, you are not the best coder on planet Earth</p> </li> <li> <p>So be humble</p> </li> <li> <p>\"Why is the code so complicated? I would have done XYZ and make it much    simpler?\"</p> </li> <li>There is no reason to believe that you can write the code in a simpler way</li> <li>The complexity is almost always needed to solve the complex problem we have</li> </ol>"},{"location":"coding/all.reading_other_people_code.how_to_guide.html#reading-other-people-code-is-painful","title":"Reading other people code is painful","text":"<ul> <li>The problem is that code reflects the thought process of the person who wrote   the code</li> <li> <p>The goal of the style guide, code writing guide, linter is precisely to push     us to write code consistently so that it's less painful to read</p> </li> <li> <p>When you write code first hand, you think about it, and you build your mental   model</p> </li> <li> <p>You see the problems, understand them, and then appreciate why a complex     solution is needed</p> </li> <li> <p>Maybe some constraints and specs were not adequately documented</p> </li> <li>Maybe there were multiple iterations and compromise between different   solutions</li> <li>Maybe several people were involved</li> <li>Maybe a hack solution needed to be added to ship and get the \\$1m from the   customers</li> </ul>"},{"location":"coding/all.reading_other_people_code.how_to_guide.html#suggestions-on-how-to-read-code","title":"Suggestions on how to read code","text":"<ul> <li>Use <code>git blame</code> to understand who wrote the code and over what period of time</li> <li> <p>Knowing the author can help you ask him/her questions directly</p> </li> <li> <p>Use the same approach as for a code review</p> </li> <li> <p>Budget some time, like a few hours and stick to the process for that amount of   time</p> </li> <li> <p>Otherwise, after 5 mins you are like \"Argh! I can't do this!\" and you give     up</p> </li> <li> <p>Read the specs</p> </li> <li>What is the code supposed to do?</li> <li>What are the edge cases to handle?</li> <li> <p>How is it integrated into the rest of the code base?</p> </li> <li> <p>Skim through the entire code, top to bottom without reading line-by-line</p> </li> <li>What's the goal?</li> <li>What are the specs?</li> <li>What are the functions?</li> <li>What are the interfaces?</li> <li> <p>What is the structure?</p> </li> <li> <p>Read and execute the unit tests, if present</p> </li> <li> <p>Run the code with debug output <code>-v DEBUG</code> on simple examples to see what it   does</p> </li> <li> <p>Use PyCharm to navigate the code and jump around</p> </li> <li> <p>Add comments, when missing, to functions, to chunks of code, to each file</p> </li> <li>Watch out for confusing comments</li> <li> <p>Sometimes a comment can be out of date</p> </li> <li> <p>Add TODOs in the code for yourself</p> </li> <li> <p>Remember the coding conventions:</p> </li> <li>Global variables are capital letters</li> <li>Functions starting with <code>_</code> are just for internals</li> <li> <p>We have all the conventions to convey information about the thought process     of who wrote the code</p> </li> <li> <p>Take down notes about the code</p> </li> <li> <p>Write down the questions about what you don't understand</p> <ul> <li>Maybe you will find the answer later (feel free to congratulate yourself!)</li> <li>Send an email to the author with the questions</li> </ul> </li> <li> <p>Approach reading code as an active form</p> </li> <li>Start writing unit tests for each piece that is not unit tested</li> <li> <p>Step through the code with PyCharm</p> </li> <li> <p>Factor out the code</p> </li> <li> <p>Make sure you have plenty of unit tests before touching the code</p> </li> <li> <p>Expect to find garbage</p> </li> <li> <p>Don't feel bad when you get lost</p> </li> <li>Reading code is not linear, like reading a book</li> <li> <p>Instead, it's about building a mental model of how the code is structured,     how it works and why it's done in a certain way</p> </li> <li> <p>The more code you read, the more comfortable you will become</p> </li> </ul>"},{"location":"coding/all.reading_other_people_code.how_to_guide.html#refs","title":"Refs","text":"<ul> <li>How to Read Code (Eight Things to Remember)</li> <li>Things you should never do</li> </ul>"},{"location":"coding/all.run_jupyter_notebook.how_to_guide.html","title":"Run Jupyter notebook","text":""},{"location":"coding/all.run_jupyter_notebook.how_to_guide.html#general","title":"General","text":"<p>Every notebook should have a config that controls its parameters.</p> <p>A notebook can be run in two ways:</p> <ol> <li>Automatically: via a Python script</li> <li>Manually: via Jupyter from a web-browser</li> </ol>"},{"location":"coding/all.run_jupyter_notebook.how_to_guide.html#running-a-notebook-using-a-python-script","title":"Running a notebook using a Python script","text":"<p>Use <code>run_notebook.py</code> to run the notebook via script.</p> <p>Here is an example of the run command inside the dev container, <code>cmamp</code> repository:</p> <pre><code>/app/dev_scripts/notebooks/run_notebook.py \\\n    --notebook /app/oms/notebooks/Master_broker_portfolio_reconciliation.ipynb \\\n    --config_builder 'amp.oms.execution_analysis_configs.get_broker_portfolio_reconciliation_configs_Cmtask5690(\"/shared_data/ecs/test/system_reconciliation/C11a/prod/20240320_134000.20240320_143500/system_log_dir.manual/process_forecasts\")' \\\n    --dst_dir . \\\n    --tee --no_suppress_output --num_threads 'serial' \\\n    --publish_notebook -v DEBUG\n</code></pre> <ul> <li>The script propagates a pointer to a config builder function via environment   variables <code>__CONFIG_BUILDER__</code>, <code>__CONFIG_IDX__</code>, <code>__CONFIG_DST_DIR__</code></li> <li>Given the environment variables above, a notebooks builds a config from a   config builder function</li> </ul> <p>The script also saves a config to a Pickle file. Path to a config file is stored in the <code>__NOTEBOOK_CONFIG_PATH__</code> environment variable which is displayed in a notebook so that a user knows where a config file is stored.</p>"},{"location":"coding/all.run_jupyter_notebook.how_to_guide.html#run-a-notebook-manually","title":"Run a notebook manually","text":"<ol> <li> <p>Load <code>Config</code> from a file</p> </li> <li> <p>Use case: when re-running a notebook that was already run using the Python   script</p> </li> <li>How to get a path to a config file: in a published notebook, search for the   <code>__NOTEBOOK_CONFIG_PATH__</code> environment variable and copy its value</li> <li>Where to use a path: pass the value as <code>config_file_name</code> in   <code>get_notebook_config()</code></li> <li> <p>Note: Make sure to set <code>replace_ecs_tokyo = True</code> to replace the <code>ecs_tokyo</code>     with <code>ecs</code> in the actual pathh</p> </li> <li> <p>Specify a config builder function manually</p> </li> <li> <p>Use case: there is no config file saved</p> </li> <li>This way one needs to specify all of the parameters of a config builder   function manually</li> </ol>"},{"location":"coding/all.run_jupyter_notebook.how_to_guide.html#example-of-the-run-flow","title":"Example of the run flow","text":"<pre><code>flowchart TD\nsubgraph A1[Run a notebook via the `run_notebook.py`]\n    A[Pass the string pointer to a config builder function\n    to the notebook as a parameter] --&gt; B[Save the Config object to a file]\n    B --&gt; C[Display path to a config file inside a notebook]\nend\nsubgraph B1[Run a notebook manually]\n    D{Config source}\n    E[Load the Config object from a file]\n    F[Manually define the Config object]\n    D --&gt; E &amp; F\nend\nA1 ==&gt; B1\nC ..-&gt;|copy-paste\\nfile path| E\n</code></pre>"},{"location":"coding/all.run_unit_tests.how_to_guide.html","title":"Run Unit Tests","text":""},{"location":"coding/all.run_unit_tests.how_to_guide.html#run-unit-tests_1","title":"Run unit tests","text":"<ul> <li>We use <code>pytest</code> and <code>unittest</code> as testing framework</li> <li>Before any PR (and ideally after a few commits), we want to run all the unit   tests to make sure we didn't introduce any new bugs</li> </ul>"},{"location":"coding/all.run_unit_tests.how_to_guide.html#test-lists","title":"Test lists","text":"<ul> <li>We have different test set lists:</li> <li><code>fast</code><ul> <li>Tests that are quick to execute (typically less than 5 secs per test   class)</li> <li>We want to run these tests before/after every commit/PR to make sure   things are not broken</li> </ul> </li> <li><code>slow</code><ul> <li>Tests that we don't want to run all the times because they are:</li> <li>Slow (typically less than 20 seconds per test)</li> <li>They are related to pieces of code that don't change often<ul> <li>E.g., external APIs we don't want to hit continuously</li> </ul> </li> </ul> </li> <li><code>superslow</code><ul> <li>Tests that run long workload, e.g., running a production model, a long   simulation</li> <li>No time limit but we need to be judicious with length</li> <li>Anything above 5-15 mins is problematic</li> </ul> </li> </ul>"},{"location":"coding/all.run_unit_tests.how_to_guide.html#using-invoke","title":"Using <code>invoke</code>","text":"<ul> <li><code>invoke</code> is a task execution framework which   allows to execute some typical workflows in a simple way</li> <li>In our config script <code>invoke</code> is aliased to <code>i</code> to reduce typing</li> <li>E.g., we use it to run the test suites:</li> </ul> <p>```bash   # Run only fast tests.</p> <p>i run_fast_tests   # Run only slow tests. i run_slow_test   # Run only superslow tests. i run_superslow_tests   ```</p> <ul> <li>To see the options use <code>--help</code> option, e.g. <code>i --help run_fast_tests</code>:</li> </ul> <p>```bash   Usage: inv[oke] [--core-opts] run_fast_tests [--options] [other tasks here ...]</p> <p>Docstring:     Run fast tests.     :param stage: select a specific stage for the Docker image     :param pytest_opts: option for pytest     :param skip_submodules: ignore all the dir inside a submodule     :param coverage: enable coverage computation     :param collect_only: do not run tests but show what will be executed     :param tee_to_file: save output of pytest in <code>tmp.pytest.log</code>     :param kwargs: kwargs for <code>ctx.run</code></p> <p>Options:     -c, --coverage     -k, --skip-submodules     -o, --collect-only     -p STRING, --pytest-opts=STRING     -s STRING, --stage=STRING     -t, --tee-to-file     -v STRING, --version=STRING   ```</p>"},{"location":"coding/all.run_unit_tests.how_to_guide.html#docker-image-stage-and-version","title":"Docker image stage and version","text":"<ul> <li>To select a specific stage for Docker image use the <code>--stage</code> option. E.g.,   this might be useful when a user wants to run regressions on the local Docker   image to verify that nothing is broken before promoting it to <code>dev</code> image.</li> </ul> <p>```bash</p> <p>i run_fast_tests --stage local   ```</p> <ul> <li>To run the tests on the specific version of a Docker image, use the   <code>--version</code> option.</li> <li>E.g., this might be useful when releasing a new version of an image.     <code>bash     &gt; i run_fast_tests --stage local --version 1.0.4</code></li> </ul>"},{"location":"coding/all.run_unit_tests.how_to_guide.html#specifying-pytest-options","title":"Specifying <code>pytest</code> options","text":"<ul> <li>With the option <code>--pytest-opts</code> it is possible to pass any <code>pytest</code> option to   <code>invoke</code>.</li> </ul>"},{"location":"coding/all.run_unit_tests.how_to_guide.html#running-in-debug-mode","title":"Running in debug mode","text":"<ul> <li>If a user wants to run the tests in debug mode to show the output   ```bash <p>i run_fast_tests -s --dbg   ```</p> </li> <li>This is equivalent to specifying <code>-v DEBUG</code> through the command line of one of   the executables</li> </ul>"},{"location":"coding/all.run_unit_tests.how_to_guide.html#save-test-output-to-a-file","title":"Save test output to a file","text":"<ul> <li>To save the output of <code>pytest</code> to <code>tmp.pytest.log</code> use the <code>--tee-to-file</code>   option.   ```bash <p>i run_fast_tests --tee-to-file   ```</p> </li> </ul>"},{"location":"coding/all.run_unit_tests.how_to_guide.html#show-the-tests-but-do-not-run","title":"Show the tests but do not run","text":"<ul> <li>To list, but not run, the tests that will be executed, use <code>--collect-only</code>.   ```bash <p>i run_fast_test --collect-only   ```</p> </li> </ul>"},{"location":"coding/all.run_unit_tests.how_to_guide.html#skip-submodules","title":"Skip submodules","text":"<ul> <li>To skip running tests in submodules, use the <code>--skip-submodules</code> option.</li> <li>This option is useful in repos with Git submodules so that you can run only   the tests specific to the repo, skipping the tests in the submodule</li> <li>E.g., to run only the tests in <code>dev_tools</code> but not in <code>cmamp</code> (which is a     submodule)     <code>bash     &gt; cd dev_tools1     &gt; i run_fast_tests --skip-submodules</code></li> </ul>"},{"location":"coding/all.run_unit_tests.how_to_guide.html#compute-test-coverage","title":"Compute test coverage","text":"<ul> <li>To compute test coverage use the <code>--coverage</code> option</li> </ul> <p>```bash</p> <p>i run_fast_tests --coverage   ```</p>"},{"location":"coding/all.run_unit_tests.how_to_guide.html#timeout","title":"Timeout","text":"<ul> <li>We use the <code>pytest-timeout</code>   package to limit durations of fast, slow, and superslow tests</li> <li>The timeout durations for each test type are listed   here</li> <li>The timeout restricts the running time of the test methods, including   <code>set_up_test()</code> and <code>tear_down_test()</code> time, if they are run at the   beginning/end of the methods</li> </ul>"},{"location":"coding/all.run_unit_tests.how_to_guide.html#rerunning-timeout-ed-tests","title":"Rerunning timeout-ed tests","text":"<ul> <li>Running tests can take different amounts of time depending on workload and   machine</li> <li>Because of this, we rerun failing tests using   <code>pytest-rerunfailures</code></li> <li><code>pytest-rerunfailures</code> is not completely compatible with <code>pytest-timeout</code>.   This is why we have to add the <code>-o timeout_func_only=true</code> flag to   <code>pytest-timeout</code>. See   https://github.com/pytest-dev/pytest-rerunfailures/issues/99<code></code>for   more information</li> <li>We rerun time outed fast tests twice and time outed slow and superslow tests   once</li> <li>There is a   way   to provide a rerun delay for individual tests. However, we can\u2019t use it for   now due to   #693 (comment)</li> </ul>"},{"location":"coding/all.run_unit_tests.how_to_guide.html#compute-test-coverage_1","title":"Compute test coverage","text":"<ul> <li> <p>The documentation for <code>coverage</code> is   here.</p> </li> <li> <p>Run a set of unit tests enabling coverage:</p> </li> </ul> <p>```bash   # Run the coverage for a single test:</p> <p>i run_fast_tests --coverage -p oms/test/test_broker.py::TestSimulatedBroker1</p> <p># Run coverage for an entire module like <code>oms</code>:</p> <p>i run_fast_tests --coverage -p oms   ```</p> <ul> <li>This generates and runs a pytest command inside Docker like:</li> </ul> <p><code>bash   docker&gt; /venv/bin/pytest -m \"not slow and not superslow\" oms/test/test_broker.py::TestSimulatedBroker1 --cov=. --cov-branch --cov-report term-missing --cov-report html</code></p> <ul> <li>Which generates:</li> <li>A default coverage report</li> <li>A binary <code>.coverage</code> file that contains the coverage information</li> <li> <p>An <code>htmlcov</code> dir with a browsable code output to inspect the coverage for     the files</p> </li> <li> <p>One can post-process the coverage report in different ways using the command   <code>coverage</code> inside a docker container, since the code was run (as always)   inside the Docker container that contains all the dependencies.</p> </li> </ul> <p>```bash</p> <p>coverage -h</p> <p>Coverage.py, version 5.5 with C extension   Measure, collect, and report on code coverage in Python programs.</p> <p>usage: coverage  [options] [args] <p>Commands:       annotate    Annotate source files with execution information.       combine     Combine a number of data files.       debug       Display information about the internals of coverage.py       erase       Erase previously collected coverage data.       help        Get help on using coverage.py.       html        Create an HTML report.       json        Create a JSON report of coverage results.       report      Report coverage stats on modules.       run         Run a Python program and measure code execution.       xml         Create an XML report of coverage results.</p> <p>Use \"coverage help \" for detailed help on any command.   Full documentation is at https://coverage.readthedocs.io   ``` <p>```bash</p> <p>coverage report -h</p> <p>Usage: coverage report [options] [modules]</p> <p>Report coverage statistics on modules.</p> <p>Options:     --contexts=REGEX1,REGEX2,...                           Only display data from lines covered in the given                           contexts. Accepts Python regexes, which must be                           quoted.     --fail-under=MIN      Exit with a status of 2 if the total coverage is less                           than MIN.     -i, --ignore-errors   Ignore errors while reading source files.     --include=PAT1,PAT2,...                           Include only files whose paths match one of these                           patterns. Accepts shell-style wildcards, which must be                           quoted.     --omit=PAT1,PAT2,...  Omit files whose paths match one of these patterns.                           Accepts shell-style wildcards, which must be quoted.     --precision=N         Number of digits after the decimal point to display                           for reported coverage percentages.     --sort=COLUMN         Sort the report by the named column: name, stmts,                           miss, branch, brpart, or cover. Default is name.     -m, --show-missing    Show line numbers of statements in each module that                           weren't executed.     --skip-covered        Skip files with 100% coverage.     --no-skip-covered     Disable --skip-covered.     --skip-empty          Skip files with no code.     --debug=OPTS          Debug options, separated by commas. [env:                           COVERAGE_DEBUG]     -h, --help            Get help on this command.     --rcfile=RCFILE       Specify configuration file. By default '.coveragerc',                           'setup.cfg', 'tox.ini', and 'pyproject.toml' are                           tried. [env: COVERAGE_RCFILE]   ```</p> <p>```bash</p> <p>i docker_bash</p> <p># Report the coverage for all the files under <code>oms</code> using the workload above (i.e., the fast tests under <code>oms/test/test_broker.py::TestSimulatedBroker1</code>)   docker&gt; coverage report --include=\"oms/*\"</p> <p>Name                                    Stmts   Miss Branch BrPart  Cover</p> <p>oms/init.py                             0      0      0      0   100%   oms/api.py                                154     47     36      2    70%   oms/broker.py                             200     31     50      9    81%   oms/broker_example.py                      23      0      4      1    96%   oms/call_optimizer.py                      31      0      0      0   100%   oms/devops/init.py                      0      0      0      0   100%   oms/devops/docker_scripts/init.py       0      0      0      0   100%   oms/locates.py                              7      7      2      0     0%   oms/mr_market.py                           55      1     10      1    97%   oms/oms_db.py                              47      0     10      3    95%   oms/oms_lib_tasks.py                       64     39      2      0    38%   oms/oms_utils.py                           34     34      6      0     0%   oms/order.py                              101     30     22      0    64%   oms/order_example.py                       26      0      0      0   100%   oms/place_orders.py                       121      8     18      6    90%   oms/pnl_simulator.py                      326     42     68      8    83%   oms/portfolio.py                          309     21     22      0    92%   oms/portfolio_example.py                   32      0      0      0   100%   oms/tasks.py                                3      3      0      0     0%   oms/test/oms_db_helper.py                  29     11      2      0    65%   oms/test/test_api.py                      132     25     12      0    83%   oms/test/test_broker.py                    33      5      4      0    86%   oms/test/test_mocked_portfolio.py           0      0      0      0   100%   oms/test/test_mr_market.py                 46      0      2      0   100%   oms/test/test_oms_db.py                   114     75     14      0    38%   oms/test/test_order.py                     24      0      4      0   100%   oms/test/test_place_orders.py              77      0      4      0   100%   oms/test/test_pnl_simulator.py            235      6     16      0    98%   oms/test/test_portfolio.py                135      0      6      0   100%</p> <p>TOTAL                                    2358    385    314     30    82%   ```</p> <ul> <li>To exclude the test files, which could inflate the coverage</li> </ul> <p>```bash</p> <p>coverage report --include=\"oms/\" --omit=\"/test_*.py\"</p> <p>Name                                    Stmts   Miss Branch BrPart  Cover</p> <p>oms/init.py                             0      0      0      0   100%   oms/api.py                                154     47     36      2    70%   oms/broker.py                             200     31     50      9    81%   oms/broker_example.py                      23      0      4      1    96%   oms/call_optimizer.py                      31      0      0      0   100%   oms/devops/init.py                      0      0      0      0   100%   oms/devops/docker_scripts/init.py       0      0      0      0   100%   oms/locates.py                              7      7      2      0     0%   oms/mr_market.py                           55      1     10      1    97%   oms/oms_db.py                              47      0     10      3    95%   oms/oms_lib_tasks.py                       64     39      2      0    38%   oms/oms_utils.py                           34     34      6      0     0%   oms/order.py                              101     30     22      0    64%   oms/order_example.py                       26      0      0      0   100%   oms/place_orders.py                       121      8     18      6    90%   oms/pnl_simulator.py                      326     42     68      8    83%   oms/portfolio.py                          309     21     22      0    92%   oms/portfolio_example.py                   32      0      0      0   100%   oms/tasks.py                                3      3      0      0     0%   oms/test/oms_db_helper.py                  29     11      2      0    65%</p> <p>TOTAL                                    1562    274    252     30    80%   ```</p> <ul> <li>To open the line coverage from outside Docker, go with your browser to   <code>htmlcov/index.html</code>. The <code>htmlcov</code> is re-written with every coverage run with   the <code>--cov-report html</code> option. If you move out <code>index.html</code> from <code>htmlcov</code>   dir some html features (e.g., filtering) will not work.</li> </ul> <p>```bash   # On macOS:</p> <p>open htmlcov/index.html   ```</p> <p></p> <ul> <li>By clicking on a file you can see which lines are not covered</li> </ul> <p></p>"},{"location":"coding/all.run_unit_tests.how_to_guide.html#an-example-coverage-session","title":"An example coverage session","text":"<ul> <li> <p>We want to measure the unit test coverage of <code>oms</code> component from both fast   and slow tests</p> </li> <li> <p>We start by running the fast tests:</p> </li> </ul> <p>```bash   # Run fast unit tests</p> <p>i run_fast_tests --coverage -p oms   collected 66 items / 7 deselected / 59 selected   ...</p> <p># Compute the coverage for the module sorting by coverage   docker&gt; coverage report --include=\"oms/\" --omit=\"/test_*.py\" --sort=Cover</p> <p>Name                                    Stmts   Miss Branch BrPart  Cover</p> <p>oms/locates.py                              7      7      2      0     0%   oms/oms_utils.py                           34     34      6      0     0%   oms/tasks.py                                3      3      0      0     0%   oms/oms_lib_tasks.py                       64     39      2      0    38%   oms/order.py                              101     30     22      0    64%   oms/test/oms_db_helper.py                  29     11      2      0    65%   oms/api.py                                154     47     36      2    70%   oms/broker.py                             200     31     50      9    81%   oms/pnl_simulator.py                      326     42     68      8    83%   oms/place_orders.py                       121      8     18      6    90%   oms/portfolio.py                          309     21     22      0    92%   oms/oms_db.py                              47      0     10      3    95%   oms/broker_example.py                      23      0      4      1    96%   oms/mr_market.py                           55      1     10      1    97%   oms/init.py                             0      0      0      0   100%   oms/call_optimizer.py                      31      0      0      0   100%   oms/devops/init.py                      0      0      0      0   100%   oms/devops/docker_scripts/init.py       0      0      0      0   100%   oms/order_example.py                       26      0      0      0   100%   oms/portfolio_example.py                   32      0      0      0   100%</p> <p>TOTAL                                    1562    274    252     30    80%   ```</p> <ul> <li> <p>We see that specific files have low coverage, so we want to see what is not   covered.</p> </li> <li> <p>Generate the same report in a browsable format</p> </li> </ul> <p><code>``bash   docker&gt; rm -rf htmlcov; coverage html --include=\"oms/*\" --omit=\"*/test_*.py\"   # Wrote HTML report to</code>htmlcov/index.html`</p> <p>open htmlcov/index.html   ```</p> <ul> <li> <p>The low coverage for <code>tasks.py</code> and <code>oms_lib_tasks.py</code> is due to the fact that   we are running code through invoke that doesn't allow <code>coverage</code> to track it.</p> </li> <li> <p>Now, we run the coverage for the slow tests</p> </li> </ul> <p>```bash   # Save the coverage from the fast tests run</p> <p>cp .coverage .coverage_fast_tests</p> <p>i run_slow_tests --coverage -p oms   collected 66 items / 59 deselected / 7 selected</p> <p>cp .coverage .coverage_slow_tests</p> <p>coverage report --include=\"oms/\" --omit=\"/test_*.py\" --sort=Cover</p> <p>Name                                    Stmts   Miss Branch BrPart  Cover</p> <p>oms/locates.py                              7      7      2      0     0%   oms/oms_utils.py                           34     34      6      0     0%   oms/tasks.py                                3      3      0      0     0%   oms/pnl_simulator.py                      326    280     68      1    13%   oms/place_orders.py                       121    100     18      0    15%   oms/mr_market.py                           55     44     10      0    17%   oms/portfolio.py                          309    256     22      0    18%   oms/call_optimizer.py                      31     25      0      0    19%   oms/broker.py                             200    159     50      0    20%   oms/order.py                              101     78     22      0    20%   oms/order_example.py                       26     19      0      0    27%   oms/broker_example.py                      23     14      4      0    33%   oms/portfolio_example.py                   32     21      0      0    34%   oms/api.py                                154    107     36      0    36%   oms/oms_lib_tasks.py                       64     39      2      0    38%   oms/oms_db.py                              47      5     10      2    84%   oms/init.py                             0      0      0      0   100%   oms/devops/init.py                      0      0      0      0   100%   oms/devops/docker_scripts/init.py       0      0      0      0   100%   oms/test/oms_db_helper.py                  29      0      2      0   100%</p> <p>TOTAL                                    1562   1191    252      3    23%   ```</p> <ul> <li>We see that the coverage from the slow tests is only 23% for 7 tests</li> </ul> <p><code>bash   docker&gt; coverage combine .coverage_fast_tests .coverage_slow_tests   Combined data file .coverage_fast_tests   Combined data file .coverage_slow_tests</code></p>"},{"location":"coding/all.run_unit_tests.how_to_guide.html#an-example-with-customized-pytest-cov-html-run","title":"An example with customized <code>pytest-cov</code> html run","text":"<ul> <li> <p>We want to measure unit test coverage specifically for one test in   <code>im_v2/common/data/transform/</code> and to save generated <code>htmlcov</code> in the same   directory.</p> </li> <li> <p>Run the command below after <code>i docker_bash</code>:</p> </li> </ul> <p><code>bash   docker&gt; pytest --cov-report term-missing     --cov=im_v2/common/data/transform/ im_v2/common/data/transform/test/test_transform_utils.py     --cov-report html:im_v2/common/data/transform/htmlcov \\</code></p> <ul> <li>Output sample:</li> </ul> <p>```bash   ---------- coverage: platform linux, python 3.8.10-final-0 -----------   Name                                                              Stmts Miss Cover Missing</p> <p>im_v2/common/data/transform/convert_csv_to_pq.py                  55    55   0%    2-159   im_v2/common/data/transform/extract_data_from_db.py               55    55   0%    2-125   im_v2/common/data/transform/pq_convert.py                         126   126  0%    3-248   im_v2/common/data/transform/transform_pq_by_date_to_by_asset.py   131   131  0%    2-437   im_v2/common/data/transform/transform_utils.py                    22    0    100%</p> <p>TOTAL                                                             389   367  6%   Coverage HTML written to dir im_v2/common/data/transform/htmlcov   ```</p>"},{"location":"coding/all.run_unit_tests.how_to_guide.html#generate-coverage-report-with-invoke","title":"Generate coverage report with <code>invoke</code>","text":"<ul> <li>One can compute test coverage for a specified directory and generate text and   HTML reports automatically using <code>invoke task run_coverage_report</code></li> </ul> <p>```bash</p> <p>i --help run_coverage_report   INFO: &gt; cmd='/data/grisha/src/venv/amp.client_venv/bin/invoke --help run_coverage_report'</p> <p>Usage: inv[oke] [--core-opts] run_coverage_report [--options] [other tasks here ...]</p> <p>Docstring:</p> <pre><code>Compute test coverage stats.\n</code></pre> <p>:param target_dir: directory to compute coverage stats for running-unit-tests   :param generate_html_report: whether to generate HTML coverage report or not   :param publish_html_on_s3: whether to publish HTML coverage report or not   :param aws_profile: the AWS profile to use for publishing HTML report</p> <p>Options:     -a STRING, --aws-profile=STRING     -g, --[no-]generate-html-report     -p, --[no-]publish-html-on-s3     -t STRING, --target-dir=STRING   ```</p>"},{"location":"coding/all.run_unit_tests.how_to_guide.html#common-usage","title":"Common usage","text":"<ul> <li>Compute coverage for <code>market_data</code> dir, generate text and HTML reports and   publish HTML report on S3</li> </ul> <p>```bash</p> <p>i run_coverage_report --target-dir market_data   ...   Name                                   Stmts   Miss Branch BrPart  Cover</p> <p>market_data/real_time_market_data.py     100     81     32      0    16%   market_data/replayed_market_data.py      111     88     24      0    19%   market_data/abstract_market_data.py      177    141     24      0    19%   market_data/market_data_example.py       124     97     10      0    20%   market_data/market_data_im_client.py      66     50     18      0    21%   market_data/init.py                    5      0      0      0   100%</p> <p>TOTAL                                    583    457    108      0    19%   Wrote HTML report to htmlcov/index.html</p> <p>20:08:53 - INFO  lib_tasks.py _publish_html_coverage_report_on_s3:3679  HTML coverage report is published on S3: path=<code>s3://cryptokaizen-html/html_coverage/grisha_CmTask1038_Tool_to_extract_the_dependency_from_a_project</code>   ```</p>"},{"location":"coding/all.run_unit_tests.how_to_guide.html#publishing-html-report-on-s3","title":"Publishing HTML report on S3","text":"<ul> <li>To make a dir with the report unique, we decorate the dir with a linux user   and a Git branch name, e.g.,   <code>html_coverage/grisha_CmTask1038_Tool_to_extract_the_dependency_from_a_project</code></li> <li><code>html_coverage</code> is the common dir on S3 for coverage reports</li> <li>After publishing the report, one can easily open it via a local web browser</li> <li>See the details in     htmlcov server</li> <li>E.g.     http://172.30.2.44/html_coverage/grisha_CmTask1038_Tool_to_extract_the_dependency_from_a_project/</li> </ul>"},{"location":"coding/all.run_unit_tests.how_to_guide.html#running-pytest-directly","title":"Running <code>pytest</code> directly","text":""},{"location":"coding/all.run_unit_tests.how_to_guide.html#usage-and-invocations-reference","title":"Usage and Invocations reference","text":"<ul> <li>See <code>pytest</code> documentation</li> <li>Some examples of useful command lines:</li> </ul> <p>```bash   # Stop at first failure</p> <p>pytest -x</p> <p># Run a single class</p> <p>pytest -k TestPcaFactorComputer1</p> <p># Run a single test method</p> <p>pytest core/test/test_core.py::TestPcaFactorComputer1::test_linearize_eigval_eigvec</p> <p># Remove cache artifacts</p> <p>find . -name \"pycache\" -o -name \".pytest_cache\"   ./.pytest_cache   ./dev_scripts/test/Test_linter_py1.test_linter1/tmp.scratch/pycache   ./dev_scripts/test/pycache   ./dev_scripts/pycache</p> <p>find . -name \"pycache\" -o -name \".pytest_cache\" | xargs rm -rf</p> <p># Run with a clear cache</p> <p>pytest --cache-clear</p> <p># Run the tests that last failed (this data is stored in .pytest_cache/v/cache/lastfailed)</p> <p>pytest --last-failed   ```</p>"},{"location":"coding/all.run_unit_tests.how_to_guide.html#custom-pytest-options-behaviors","title":"Custom <code>pytest</code> options behaviors","text":""},{"location":"coding/all.run_unit_tests.how_to_guide.html#enable-logging","title":"Enable logging","text":"<ul> <li>To enable logging of <code>_LOG.debug</code> for a single test run:</li> </ul> <p>```bash   # Enable debug info</p> <p>pytest oms/test/test_broker.py::TestSimulatedBroker1 -s --dbg   ```</p>"},{"location":"coding/all.run_unit_tests.how_to_guide.html#update-golden-outcomes","title":"Update golden outcomes","text":"<ul> <li>This switch allows to overwrite the golden outcomes that are used as reference   in the unit tests to detect failures</li> </ul> <p>```bash</p> <p>pytest --update_outcomes   ```</p>"},{"location":"coding/all.run_unit_tests.how_to_guide.html#incremental-test-mode","title":"Incremental test mode","text":"<ul> <li> <p>This switch allows to reuse artifacts in the test directory and to skip the   clean up phase</p> </li> <li> <p>It is used to rerun tests from the middle when they are very long and one   wants to debug them</p> </li> </ul> <p>```bash</p> <p>pytest --incremental   ```</p>"},{"location":"coding/all.run_unit_tests.how_to_guide.html#debugging-notebooks","title":"Debugging Notebooks","text":"<ol> <li>Run a failing test with <code>-s --dbg</code> to get detailed logs</li> <li>E.g., <code>&gt; pytest core/plotting/test/test_gallery_notebook.py -s --dbg</code></li> <li>From the logs take a <code>run_notebook.py</code> script command that was run by the    test</li> <li>E.g., starting like      <code>/app/dev_scripts/notebooks/run_notebook.py --notebook ...</code></li> <li>Append <code>--no_suppress_output</code> to this command and run it again directly from    the bash</li> <li>E.g., like      <code>&gt; /app/dev_scripts/notebooks/run_notebook.py --notebook ... --no_suppress_output</code></li> <li>Scroll up the logs and see a report about the problem, notebooks failures    will be displayed as well</li> <li>E.g.,      </li> </ol>"},{"location":"coding/all.run_unit_tests.how_to_guide.html#running-tests-on-gh-actions","title":"Running tests on GH Actions","text":"<ul> <li>The official documentation is   https://docs.github.com/en/actions</li> </ul>"},{"location":"coding/all.run_unit_tests.how_to_guide.html#how-to-run-a-single-test-on-gh-action","title":"How to run a single test on GH Action","text":"<ul> <li> <p>Unfortunately, there is no way to log in and run interactively on GH machines.   This is a feature requested but not implemented by GH yet.</p> </li> <li> <p>All the code to run GH Actions is in the <code>.github</code> directory in <code>lemonade</code> and   <code>amp</code>.</p> </li> <li> <p>E.g., to run a single test in the fast test target, instead of the entire   regression suite</p> </li> <li> <p>You can modify <code>.github/workflows/fast_tests.yml</code>, by replacing</p> </li> </ul> <p><code>bash    # run: invoke run_fast_tests    run: invoke run_fast_tests --pytest-opts=\"helpers/test/test_git.py::Test_git_modified_files1::test_get_modified_files_in_branch1 -s --dbg\"</code>    - Note that the indentation matters since it's a YAML file</p> <pre><code> ![alt_text](figs/unit_tests/image_3.png)\n</code></pre> <ul> <li> <p>The <code>-s --dbg</code> is to show <code>_LOG.debug</code> in case you care about that to get      more information</p> </li> <li> <p>Commit the code to your branch (not in master, please) since GH runs each    branch independently</p> </li> <li>Kick-off manually the fast test through the GH interface</li> <li>After debugging, you can revert the change from your branch to <code>master</code> and    move along with the usual PR flow</li> </ul>"},{"location":"coding/all.str_to_df.how_to_guide.html","title":"Converting a string into a Pandas df","text":""},{"location":"coding/all.str_to_df.how_to_guide.html#rationale","title":"Rationale","text":"<p>Standard ways to construct a Pandas dataframe from data (e.g., from a dict or a list of lists) are extremely cumbersome, especially for longer dataframes with many rows. Our goal is to introduce a tool that allows to create a Pandas dataframe from its string representation, which is much more readable.</p> <p>Compare the data needed to create a df in a standard way:</p> <pre><code>data = {\n    1030828978: [0.20136, 0.20158, 0.20162, 0.20128, 0.20134, 0.20110],\n    1464553467: [\n        1838.812,\n        1839.690,\n        1839.936,\n        1839.430,\n        1840.016,\n        1839.615,\n    ]\n}\ntimestamps = pd.date_range(\n    start=\"2023-08-15 11:45:00\",\n    end=\"2023-08-15 12:10:00\",\n    tz=\"America/New_York\",\n    freq=\"5T\",\n)\n</code></pre> <p>To the data needed to create a df with our tool:</p> <pre><code>df_as_str = \"\"\"\n                               1030828978  1464553467\nend_timestamp\n\"2023-08-15 11:45:00-04:00\"     0.20136    1838.812\n\"2023-08-15 11:50:00-04:00\"     0.20158    1839.690\n\"2023-08-15 11:55:00-04:00\"     0.20162    1839.936\n\"2023-08-15 12:00:00-04:00\"     0.20128    1839.430\n\"2023-08-15 12:05:00-04:00\"     0.20134    1840.016\n\"2023-08-15 12:10:00-04:00\"     0.20110    1839.615\"\"\"\n</code></pre>"},{"location":"coding/all.str_to_df.how_to_guide.html#how-to-use","title":"How to use","text":"<ul> <li>The function is <code>str_to_df()</code>, located in <code>helpers/hpandas.py</code></li> <li>To use the function, one needs to provide a string representation of the df   and the mappings between the columns and the desired types of column names and   values</li> <li>For example, given the string representation of a df in the previous section   and the following mappings:</li> </ul> <pre><code>col_to_type = {\n    \"__index__\": pd.Timestamp,\n    \"1030828978\": float,\n    \"1464553467\": float,\n}\ncol_to_name_type = {\n    \"1030828978\": int,\n    \"1464553467\": int,\n}\n</code></pre> <p><code>str_to_df()</code> will output the following dataframe:</p> <p></p> <ul> <li>More usage examples can be found, e.g.:</li> <li>In the dedicated unit tests in     <code>helpers/test/test_hpandas.py::Test_str_to_df</code></li> <li>In practical use cases in <code>oms/order/test/test_order_converter.py</code></li> </ul>"},{"location":"coding/all.str_to_df.how_to_guide.html#notes","title":"Notes","text":"<ul> <li>The string representation used as an input to the function should be largely   the same as the output of <code>hpandas.df_to_str()</code>. Note that it is not the same   as the output of <code>str()</code>, which is much less controlled and therefore messier   than of <code>df_to_str()</code> (e.g., the column names are often split into different   rows far away from each other, etc)</li> <li>There are several differences between the raw output of <code>df_to_str()</code> and the   required format of the input to <code>str_to_df()</code>:</li> <li>If the number of rows is over a certain number, then <code>df_to_str()</code> cuts     them, leaving only the head and the tail of the df, and everything in     between is replaced by the \"[...]\" placeholder. In <code>str_to_df()</code>, the     placeholder \"[...]\" is simply removed and the data it stands for cannot be     restored</li> <li>In the input to <code>str_to_df()</code>, values that contain spaces (e.g., a timestamp     like \"2000-01-01 09:00:00\") need to be enclosed in double quotation marks.     If there are values with spaces in the input to <code>df_to_str()</code>, then it will     convert them into strings but will not automatically put quotation marks     around them, which means that if we attempt to convert the output string     back into a df with <code>str_to_df()</code> without adding the quotation marks     manually, these values will be split into two different columns, possibly     also breaking the whole process due to the column number mismatch</li> </ul>"},{"location":"coding/all.submit_code_for_review.how_to_guide.html","title":"First Review Process","text":"<p>We understand that receiving feedback on your code can be a difficult process, but it is an important part of our development workflow. Here we have gathered some helpful tips and resources to guide you through your first review.</p>"},{"location":"coding/all.submit_code_for_review.how_to_guide.html#read-python-style-guide","title":"Read Python Style Guide","text":"<ul> <li>Before submitting your code for review, we highly recommend that you read the   Python Style Guide, which outlines the   major conventions and best practices for writing Python code.</li> <li>Adhering to these standards will help ensure that your code is easy to read,   maintain, and understand for other members of the team.</li> </ul>"},{"location":"coding/all.submit_code_for_review.how_to_guide.html#run-linter","title":"Run linter","text":"<ul> <li>Linter is a tool that checks (and tries to fix automatically) your code for   syntax errors, style violations, and other issues.</li> <li>Run it on all the changed files to automatically catch any code issues before   filing any PR or before requesting a review!</li> <li>To be able to run the linter, you need to you need to set up your client first   since you're outside Docker:</li> <li>The instructions are available at     KaizenFlow_development_setup.md</li> <li>In practice you need to have run     <code>&gt; source dev_scripts/setenv_amp.sh</code></li> <li>Run the linter with <code>invoke</code> command (which is abbreviated as <code>i</code>) and pass   all the files you need to lint in brackets after the <code>--files</code> option,   separated by a space:   ``` <p>i lint --files \"defi/tulip/implementation/order.py defi/tulip/implementation/order_matching.py\"   ```</p> </li> <li>Output example:     <code>defi/tulip/implementation/order_matching.py:14: error: Cannot find implementation or library stub for module named 'defi.dao_cross'  [import]     defi/tulip/implementation/order_matching.py:69: error: Need type annotation for 'buy_heap' (hint: \"buy_heap: List[&lt;type&gt;] = ...\")  [var-annotated]     defi/tulip/implementation/order_matching.py:70: error: Need type annotation for 'sell_heap' (hint: \"sell_heap: List[&lt;type&gt;] = ...\")  [var-annotated]     ...</code></li> <li><code>i lint</code> has options for many workflows. E.g., you can automatically lint     all the files that you touched in your PR with <code>--branch</code>, the files in the     last commit with <code>--last-commit</code>. You can look at all the options with:     <code>&gt; i lint --help</code></li> <li>Fix the lints</li> <li>No need to obsessively fix all of them - just crucial and obvious ones</li> <li>Post unresolved lints in your PR so Reviewer could see them and know which     should be fixed and which are not</li> <li>If we see that people didn't run the linter, we should do a quick PR just   running the linter. This type of PR can be merged even without review.</li> <li>If the linter introduces extensive changes in a PR, causing difficulty in   reading the diff, a new pull request should be created exclusively for the   linter changes, based on the branch of the original PR.</li> </ul>"},{"location":"coding/all.submit_code_for_review.how_to_guide.html#compare-your-code-to-example-code","title":"Compare your code to example code","text":"<ul> <li>To get an idea of what well-formatted and well-organized code looks like, we   suggest taking a look at some examples of code that adhere to our standards.</li> <li>We try to maintain universal approaches to all the parts of the code, so when   looking at a code example, check for:</li> <li>Code style</li> <li>Docstrings and comments</li> <li>Type hints</li> <li>Containing directory structure</li> <li>Here are some links to example code:</li> <li>Classes and functions:<ul> <li><code>defi/tulip/implementation/order.py</code></li> <li><code>defi/tulip/implementation/order_matching.py</code></li> </ul> </li> <li>Unit tests:<ul> <li><code>defi/tulip/test/test_order_matching.py</code></li> <li><code>defi/tulip/test/test_optimize.py</code></li> </ul> </li> <li>Scripts:<ul> <li><code>dev_scripts/replace_text.py</code></li> <li><code>dev_scripts/lint_md.sh</code></li> </ul> </li> </ul>"},{"location":"coding/all.submit_code_for_review.how_to_guide.html#save-reviewer-time","title":"Save Reviewer time","text":""},{"location":"coding/all.submit_code_for_review.how_to_guide.html#assign-reviewers","title":"Assign Reviewers","text":"<ul> <li>Make sure to select a Reviewer in a corresponding GitHub field so he/she gets   notified</li> <li></li> <li>Junior contributors should assign Team Leaders (e.g., Grisha, DanY, Samarth,     ...) to review their PR<ul> <li>Team Leaders will assign integrators (GP &amp; Paul) themselves after all   their comments are implemented</li> </ul> </li> <li>Ping the assigned Reviewer in the issue if nothing happens in 24 hours</li> <li>If you want to keep someone notified about changes in the PR but do not want     to make him/her a Reviewer, type <code>FYI @github_name</code> in a comment section</li> </ul>"},{"location":"coding/all.submit_code_for_review.how_to_guide.html#mention-the-issue","title":"Mention the issue","text":"<ul> <li>Mention the corresponding issue in the PR description to ease the navigation</li> <li>E.g., see an     example<ul> <li></li> </ul> </li> </ul>"},{"location":"coding/all.submit_code_for_review.how_to_guide.html#resolve-conversations","title":"Resolve conversations","text":"<ul> <li>When you've implemented a comment from a Reviewer, press   <code>Resolve conversation</code> button so the Reviewer knows that you actually took   care of it</li> <li></li> </ul>"},{"location":"coding/all.submit_code_for_review.how_to_guide.html#merge-master-to-your-branch","title":"Merge master to your branch","text":"<ul> <li>Before any PR review request do <code>i git_merge_master</code> in order to keep the code   updated</li> <li>Resolve conflicts if there are any</li> <li>Do not forget to push it since this action is a commit itself</li> <li>Actually, a useful practice is to merge master to your branch every time you   to get back to work on it</li> <li>This way you make sure that your branch is always using a relevant code and     avoid huge merge conflicts</li> <li>NEVER press <code>Squash and merge</code> button yourself</li> <li>You need to merge master branch to your branch - not vice verca!</li> <li>This is a strictly Team Leaders and Integrators responsibility</li> </ul>"},{"location":"coding/all.submit_code_for_review.how_to_guide.html#ask-for-reviews","title":"Ask for reviews","text":"<ul> <li>When you've implemented all the comments and need another round of review:</li> <li>Press the circling arrows sign next to the Reviewer for the ping<ul> <li></li> </ul> </li> <li>Remove <code>PR_for_authors</code> and add <code>PR_for_reviewers</code> label (labels     desc)<ul> <li></li> </ul> </li> </ul>"},{"location":"coding/all.submit_code_for_review.how_to_guide.html#do-not-use-screenshots","title":"Do not use screenshots","text":"<ul> <li>Stack trace and logs are much more convenient to use for debugging</li> <li>Screenshots are often too small to capture both input and return logs while   consuming a lot of basically useless memory</li> <li>The exceptions are plots and non-code information</li> <li>Examples:</li> <li> <p>Bad</p> <p>   - Good</p> <p>Input: <code>type_ = \"supply\" supply_curve1 = ddcrsede.get_supply_demand_discrete_curve(     type_, supply_orders_df1 ) supply_curve1</code></p> <p>Error: ```</p> <p>NameError                                 Traceback (most recent call last) Cell In [5], line 2       1 type_ = \"supply\" ----&gt; 2 supply_curve1 = ddcrsede.get_supply_demand_discrete_curve(       3     type_, supply_orders_df1       4 )       5 supply_curve1</p> <p>NameError: name 'ddcrsede' is not defined ```</p> </li> </ul>"},{"location":"coding/all.submit_code_for_review.how_to_guide.html#report-bugs-correctly","title":"Report bugs correctly","text":"<ul> <li>Whenever you face any errors put as much information about the issue as   possible, e.g.,:</li> <li>What you are trying to achieve</li> <li>Command line you ran, e.g.,     <code>&gt; i lint -f defi/tulip/test/test_dao_cross_sol.py</code></li> <li>Copy-paste the error and the stack trace from the cmd line, no     screenshots, e.g.,     <code>Traceback (most recent call last):       File \"/venv/bin/invoke\", line 8, in &lt;module&gt;         sys.exit(program.run())       File \"/venv/lib/python3.8/site-packages/invoke/program.py\", line 373, in run         self.parse_collection()     ValueError: One and only one set-up config should be true:</code></li> <li>The log of the run<ul> <li>Maybe the same run using <code>-v DEBUG</code> to get more info on the problem</li> </ul> </li> <li>What the problem is</li> <li>Why the outcome is different from what you expected</li> <li>E.g. on how to report any issues<ul> <li>Https://github.com/kaizen-ai/kaizenflow/issues/370#issue-1782574355</li> </ul> </li> </ul>"},{"location":"coding/all.submit_code_for_review.how_to_guide.html#talk-through-code-and-not-github","title":"Talk through code and not GitHub","text":"<ul> <li>Authors of the PR should not initiate talking to reviewers through GitHub but   only through code</li> <li>E.g., if there is something you want to explain to the reviewers, you should     not comment your own PR, but should add comments or improve the code</li> <li>Everything in GitHub is lost once the PR is closed, so all knowledge needs     to go inside the code or the documentation</li> <li>Of course it's ok to respond to questions in GitHub</li> </ul>"},{"location":"coding/all.submit_code_for_review.how_to_guide.html#look-at-examples-of-the-first-reviews","title":"Look at examples of the first reviews","text":"<ul> <li>It can be helpful to review some examples of previous first reviews to get an   idea of what common issues are and how to address them.</li> <li>Here are some links to a few \"painful\" first reviews:</li> <li>Adding unit tests:<ul> <li>Https://github.com/kaizen-ai/kaizenflow/pull/166</li> <li>Https://github.com/kaizen-ai/kaizenflow/pull/186</li> </ul> </li> <li>Writing scripts:<ul> <li>Https://github.com/kaizen-ai/kaizenflow/pull/267</li> <li>Https://github.com/kaizen-ai/kaizenflow/pull/276</li> </ul> </li> </ul>"},{"location":"coding/all.type_hints.how_to_guide.html","title":"Type Hints","text":""},{"location":"coding/all.type_hints.how_to_guide.html#type-hints_1","title":"Type hints","text":""},{"location":"coding/all.type_hints.how_to_guide.html#why-we-use-type-hints","title":"Why we use type hints","text":"<ul> <li>We use Python 3 type hints to:</li> <li>Improve documentation</li> <li>Allow mypy to perform static checking of the code, looking for bugs</li> <li>Enforce the type checks at run-time, through automatic assertions (not     implemented yet)</li> </ul>"},{"location":"coding/all.type_hints.how_to_guide.html#what-to-annotate-with-type-hints","title":"What to annotate with type hints","text":"<ul> <li>We expect all new library code (i.e., that is not in a notebook) to have type   annotations</li> <li>We annotate the function signature</li> <li>We don't annotate the variables inside a function unless mypy reports that it   can't infer the type</li> <li>We strive to get no errors / warnings from the linter, including mypy</li> </ul>"},{"location":"coding/all.type_hints.how_to_guide.html#conventions","title":"Conventions","text":""},{"location":"coding/all.type_hints.how_to_guide.html#empty-return","title":"Empty return","text":"<ul> <li>Return <code>-&gt; None</code> if your function doesn't return</li> <li>Pros:<ul> <li><code>mypy</code> checks functions only when there is at least an annotation: so   using <code>-&gt; None</code> enables mypy to do type checking</li> <li>It reminds us that we need to use type hints</li> </ul> </li> <li>Cons:<ul> <li><code>None</code> is the default value and so it might seem redundant</li> </ul> </li> </ul>"},{"location":"coding/all.type_hints.how_to_guide.html#invoke-tasks","title":"Invoke tasks","text":"<ul> <li>For some reason <code>invoke</code> does not like type hints, so we</li> <li>Omit type hints for <code>invoke</code> tasks, i.e. functions with the <code>@task</code>     decorator</li> <li> <p>Put <code># type: ignore</code> so that <code>mypy</code> does not complain</p> </li> <li> <p>Example:   <code>python   @task   def run_qa_tests( # type: ignore     ctx,     stage=\"dev\",     version=\"\",   ):</code></p> </li> </ul>"},{"location":"coding/all.type_hints.how_to_guide.html#annotation-for-kwargs","title":"Annotation for <code>kwargs</code>","text":"<ul> <li>We use <code>kwargs: Any</code> and not <code>kwargs: Dict[str, Any]</code></li> <li><code>*</code> always binds to a <code>Tuple</code>, and <code>**</code> always binds to a <code>Dict[str, Any]</code>.   Because of this restriction, type hints only need you to define the types of   the contained arguments. The type checker automatically adds the   <code>Tuple[_, ...]</code> and <code>Dict[str, _]</code> container types.</li> <li>Reference article</li> </ul>"},{"location":"coding/all.type_hints.how_to_guide.html#any","title":"<code>Any</code>","text":"<ul> <li><code>Any</code> type hint = no type hint</li> <li>We try to avoid it everywhere when possible</li> </ul>"},{"location":"coding/all.type_hints.how_to_guide.html#nparray-and-npndarray","title":"<code>np.array</code> and <code>np.ndarray</code>","text":"<ul> <li>If you get something like the following lint:   <code>bash   dataflow/core/nodes/sklearn_models.py:537:[amp_mypy] error: Function \"numpy.core.multiarray.array\" is not valid as a type [valid-type]</code></li> <li>Then the problem is probably that a parameter that the lint is related to has   been typed as <code>np.array</code> while it should be typed as <code>np.ndarray</code>:   <code>python   `x_vals: np.array` -&gt; `x_vals: np.ndarray`</code></li> </ul>"},{"location":"coding/all.type_hints.how_to_guide.html#handling-the-annoying-incompatible-types-in-assignment","title":"Handling the annoying <code>Incompatible types in assignment</code>","text":"<ul> <li><code>mypy</code> assigns a single type to each variable for its entire scope</li> <li>The problem is in common idioms where we use the same variable to store   different representations of the same data   <code>python   output : str = ...   output = output.split(\"\\n\")   ...   # Process output.   ...   output = \"\\n\".join(output)</code></li> <li>Unfortunately the proper solution is to use different variables   <code>python   output : str = ...   output_as_array = output.split(\"\\n\")   ...   # Process output.   ...   output = \"\\n\".join(output_as_array)</code></li> <li>Another case could be:   <code>python   from typing import Optional   def test_func(arg: bool):   ...   var: Optional[bool] = ...   dbg.dassert_is_not(var, None)   test_func(arg=var)</code></li> <li>Sometimes <code>mypy</code> doesn't pick up the <code>None</code> check, and warns that the function   expects a <code>bool</code> rather than an <code>Optional[bool]</code>. In that case, the solution   is to explicitly use <code>typing.cast</code> on the argument when passing it in, note   that <code>typing.cast</code> has no runtime effects and is purely for type checking.</li> <li>Here're the relevant docs</li> <li>So the solution would be:   <code>python   from typing import cast ...   ...   test_func(arg=cast(bool, var))</code></li> </ul>"},{"location":"coding/all.type_hints.how_to_guide.html#handling-the-annoying-none-has-no-attribute","title":"Handling the annoying <code>\"None\" has no attribute</code>","text":"<ul> <li>In some model classes <code>self._model</code> parameter is being assigned to <code>None</code> in   ctor and being set after calling <code>set_fit_state</code> method</li> <li>The problem is that statically it's not possible to understand that someone   will call <code>set_fit_state</code> before using <code>self._model</code>, so when a model's method   is applied:   <code>python   self._model = self._model.fit(...)</code>   the following lint appears:   <code>bash   dataflow/core/nodes/sklearn_models.py:155:[amp_mypy] error: \"None\" has no attribute \"fit\"</code></li> <li>A solution is to</li> <li>Type hint when assigning the model parameter in ctor:     <code>python     self._model: Optional[sklearn.base.BaseEstimator] = None</code></li> <li>Cast a type to the model parameter after asserting that it is not <code>None</code>:     <code>python     hdbg.dassert_is_not(self._model, None)     self._model = cast(sklearn.base.BaseEstimator, self._model)</code></li> </ul>"},{"location":"coding/all.type_hints.how_to_guide.html#disabling-mypy-errors","title":"Disabling <code>mypy</code> errors","text":"<ul> <li>If <code>mypy</code> reports an error and you don't understand why, please ping one of   the python experts asking for help</li> <li>If you are sure that you understand why <code>mypy</code> reports and error and that you   can override it, you disable this <code>error</code></li> <li>When you want to disable an error reported by <code>mypy</code>:</li> <li>Add a comment reporting the <code>mypy</code> error</li> <li>Explain why this is not a problem</li> <li>Add <code># type: ignore</code> with two spaces as usual for the inline comment</li> <li>Example     <code>python     # mypy: Cannot find module named 'pyannotate_runtime'     # pyannotate is not always installed     from pyannotate_runtime import collect_types # type: ignore</code></li> </ul>"},{"location":"coding/all.type_hints.how_to_guide.html#what-to-do-when-you-dont-know-what-to-do","title":"What to do when you don't know what to do","text":"<ul> <li>Go to the   <code>mypy</code> official cheat sheet</li> <li>Use <code>reveal_type</code></li> <li>To find out what type <code>mypy</code> infers for an expression anywhere in your     program, wrap it in <code>reveal_type()</code></li> <li><code>mypy</code> will print an error message with the type; remove it again before     running the code</li> <li>See     the official <code>mypy</code> documentation</li> </ul>"},{"location":"coding/all.type_hints.how_to_guide.html#library-without-types","title":"Library without types","text":"<ul> <li><code>mypy</code> is unhappy when a library doesn't have types</li> <li>Lots of libraries are starting to add type hints now that python 2 has been   deprecated   <code>bash   *.py:14: error: No library stub file for module 'sklearn.model_selection' [mypy]</code></li> <li>You can go in <code>mypy.ini</code> and add the library (following the alphabetical   order) to the list</li> <li>Note that you need to ensure that different copies of <code>mypy.ini</code> in different   sub projects are equal   ```bash <p>vimdiff mypy.ini amp/mypy.ini   or cp mypy.ini amp/mypy.ini   ```</p> </li> </ul>"},{"location":"coding/all.type_hints.how_to_guide.html#inferring-types-using-unit-tests","title":"Inferring types using unit tests","text":"<ul> <li>Sometimes it is possible to infer types directly from unit tests. We have used   this flow to annotate the code when we switched to Python3 and it worked fine   although there were various mistakes. We still prefer to annotate by hand   based on what the code is intended to do, rather than automatically infer it   from how the code behaves.</li> <li>Install <code>pyannotate</code> <code>bash     &gt; pip install pyannotate</code></li> <li>To enable collecting type hints run     <code>bash     &gt; export PYANNOTATE=True</code></li> <li>Run <code>pytest</code>, e.g., on a subset of unit tests:</li> <li>Run <code>pytest</code>, e.g., on a subset of unit tests like <code>helpers</code>:     <code>bash     &gt; pytest helpers</code></li> <li>A file <code>type_info.json</code> is generated</li> <li>Annotate the code with the inferred types:     <code>bash     &gt; pyannotate -w --type-info type_info.json . --py3</code></li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html","title":"Write Unit Tests","text":""},{"location":"coding/all.write_unit_tests.how_to_guide.html#guidelines-about-writing-unit-tests","title":"Guidelines about writing unit tests","text":""},{"location":"coding/all.write_unit_tests.how_to_guide.html#what-is-a-unit-test","title":"What is a unit test?","text":"<ul> <li>A unit test is a small, self-contained test of a (public) function or method   of a library</li> <li>The test specifies the given inputs, any necessary state, and the expected   output</li> <li>Running the test ensures that the actual output agrees with the expected   output</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#why-is-unit-testing-important","title":"Why is unit testing important?","text":"<ul> <li>Good unit testing improves software quality by:</li> <li>Eliminating bugs (obvious)</li> <li>Clarifying code design and interfaces (\"Design to Test\")</li> <li>Making refactoring safer and easier (\"Refactor Early, Refactor Often\")</li> <li>Documenting expected behavior and usage</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#the-pragmatic-programming-and-unit-testing","title":"The Pragmatic Programming and unit testing","text":"<ul> <li> <p>Unit testing is an integral part of   The Pragmatic Programming   approach</p> </li> <li> <p>Some of the tips that relate to unit testing are:</p> </li> <li>Design with Contracts</li> <li>Refactor Early, Refactor Often</li> <li>Test Your Software, or Your Users Will</li> <li>Coding Ain't Done Till All the Tests Run</li> <li>Test State Coverage, Not Code Coverage</li> <li>You Can't Write Perfect Software</li> <li>Crash Early</li> <li>Design to Test</li> <li>Test Early. Test Often. Test Automatically.</li> <li>Use Saboteurs to Test Your Testing</li> <li>Find Bugs Once</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#unit-testing-tips","title":"Unit testing tips","text":""},{"location":"coding/all.write_unit_tests.how_to_guide.html#test-one-thing","title":"Test one thing","text":"<ul> <li>A good unit test tests only one thing</li> <li>Testing one thing keeps the unit test simple, relatively easy to understand,   and helps isolate the root cause when the test fails</li> <li>How do you test more than one thing? By having more than one unit test!</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#keep-tests-self-contained","title":"Keep tests self-contained","text":"<ul> <li>A unit test should be independent of all other unit tests</li> <li>Each test should be self-sufficient</li> <li>One should never assume that unit tests will be executed in a particular order</li> <li>A corollary of keeping tests self-contained is to keep all information needed   to understand the test within the test itself</li> <li>Specify the data explicitly in the test where it is used</li> <li>This makes the test easier to understand and easier to debug when it fails</li> <li>If multiple unit tests use or can use the same initialization data, do not     hesitate repeating it in each test (or consider using parameterized testing)</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#only-specify-data-related-to-what-is-being-tested","title":"Only specify data related to what is being tested","text":"<ul> <li>Specify the minimum of what is required to test what is being tested</li> <li>E.g., if a function that is being tested supports optional arguments, but   those optional arguments are not needed for a particular unit test, then do   not specify them in the test</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#test-realistic-corner-cases","title":"Test realistic corner cases","text":"<ul> <li>Can the function receive an empty list?</li> <li>Can it return an empty Series?</li> <li>What happens if it receives a numerical value outside of an expected range?</li> <li>How should the function behave in those cases? Should it crash? Should it     return a reasonable default value?</li> <li>Expect these questions to come up in practice and think through what the   appropriate behavior should be. Then, test for it.</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#test-a-typical-scenario","title":"Test a typical scenario","text":"<ul> <li>In ensuring that corner cases are covered, do not overlook testing basic   functionality for typical cases</li> <li>This is useful for verifying current behavior and to support refactoring.</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#test-executable-scripts-end-to-end","title":"Test executable scripts end-to-end","text":"<ul> <li>In some cases, like scripts, it is easy to get lost chasing the coverage %</li> <li>E.g., covering every line of the original, including the parser</li> <li>This is not always necessary</li> <li>If you can run a script with all arguments present, it means that the parser     works correctly</li> <li>So an end-to-end smoke test will also cover the parser</li> <li>This saves a little time and reduces the bloat</li> <li>If you need to test the functionality, consider factoring out as much code as   possible from <code>_main()</code></li> <li>A good practice is to have a <code>_run()</code> function that does all the job and     <code>_main()</code> only brings together the parser and the executable part</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#conventions","title":"Conventions","text":""},{"location":"coding/all.write_unit_tests.how_to_guide.html#naming-and-placement-conventions","title":"Naming and placement conventions","text":"<ul> <li> <p>We follow conventions that happen to be mostly the default to <code>pytest</code></p> </li> <li> <p>A directory <code>test</code> contains all the test code and artifacts</p> </li> <li>The <code>test</code> directory contains all the <code>test_*.py</code> files and all inputs and     outputs for the tests.</li> <li> <p>A unit test file should be close to the library / code it tests</p> </li> <li> <p>The test class should make clear reference to the code that is tested</p> </li> <li>To test a class <code>FooBar</code>, the corresponding test class is named     <code>TestFooBar</code>, i.e. we use the CamelCase for the test classes</li> <li>To test a function <code>generate_html_tables()</code>, the corresponding test class is     named <code>Test_generate_html_tables</code></li> <li>To test a method <code>method_a()</code> of the class <code>FooBar</code>, the corresponding test     class is named <code>TestFooBar</code> and the test method in this class is named     <code>test_method_a</code></li> <li> <p>To test a protected method <code>_gozilla()</code> of <code>FooBar</code>, the corresponding test     method is named <code>test__gozilla</code> (note the double underscore). This is needed     to distinguish testing the public method <code>FooBar.gozilla()</code> from     <code>FooBar._gozilla()</code></p> </li> <li> <p>Numbers can be used to differentiate between separate test cases clearly</p> </li> <li>A number can be added to the test class name, e.g., <code>TestFooBar1()</code>, if     there are multiple test classes that are testing the code in different ways     (e.g., with different set up and tear down actions)</li> <li>We prefer to name classes <code>TestFooBar1</code> and methods <code>TestFooBar1.test1()</code>,     even if there is a single class / method, to make it easier to add another     test class, without having to rename class and <code>check_string</code> files</li> <li> <p>We are OK with using suffixes like <code>01</code>, <code>02</code>, ... , when we believe it's     important that methods are tested in a certain order (e.g., from the     simplest to the most complex)</p> </li> <li> <p>A single test class can have multiple test methods, e.g., for   <code>FooBar.method_a()</code> and <code>FooBar.method_b()</code>, the test class contains the   following methods:</p> </li> </ul> <p>```python   class TestFooBar1(unittest2.TestCase):       def test_method_a(self):           ...</p> <pre><code>  def test_method_b(self):\n      ...\n</code></pre> <p>```</p> <ul> <li>Split test classes and methods in a reasonable way so each one tests one   single thing in the simplest possible way</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#keep-testing-code-in-sync-with-tested-code","title":"Keep testing code in sync with tested code","text":"<ul> <li>If you change the name of a tested class, also the test should be changed</li> <li>If you change the name of a file also the name of the file with the testing   code should be changed</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#test-code-is-not-second-class-citizen","title":"Test code is not second-class citizen","text":"<ul> <li> <p>Test code is not second-class citizen, even though it's auxiliary to the code</p> </li> <li> <p>Add comments and docstring explaining what the code is doing</p> </li> <li> <p>Avoid repetition in test code, but use helper to factor out common code</p> </li> <li>Abhor copy-paste and keep the code DRY</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#testing-code-layout","title":"Testing code layout","text":"<ul> <li>The layout of a test dir should look like:   ```bash <p>ls -1 helpers/test/   Test_dassert1.test2   Test_dassert1.test3   Test_dassert1.test4   ...   Test_dassert_misc1.test6   Test_dassert_misc1.test8   Test_system1.test7   test_dbg.py   test_helpers.py   test_system_interaction.py   ```</p> </li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#our-framework-to-test-using-input-output-data","title":"Our framework to test using input / output data","text":"<ul> <li><code>helpers/unit_test.py</code> has some utilities to create input and output easily   dirs storing data for unit tests</li> <li><code>hut.TestCase</code> has various methods to help you create</li> <li><code>get_input_dir()</code>: return the name of the dir used to store the inputs</li> <li><code>get_scratch_space()</code>: return the name of a scratch dir to keep artifacts of     the test</li> <li> <p><code>get_output_dir()</code>: probably not interesting for the user</p> </li> <li> <p>The directory structure enforced by the out <code>TestCase</code> is like:</p> </li> </ul> <p>```bash</p> <p>tree -d edg/form_8/test/   edg/form_8/test/   \u2514\u2500\u2500 TestExtractTables1.test1       \u251c\u2500\u2500 input       \u2514\u2500\u2500 output   ```</p>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#use-text-and-not-pickle-files-as-inputoutputs","title":"Use text and not pickle files as input/outputs","text":"<ul> <li>The problems with pickle files are the usual ones</li> <li>Pickle files are not stable across different versions of libraries</li> <li> <p>Pickle files are not human-readable</p> </li> <li> <p>Prefer to use text file</p> </li> <li> <p>E.g., use a CSV file</p> </li> <li> <p>If the data used for testing is generated in a non-complicated way</p> </li> <li>Document how it was generated</li> <li>Even better, add a test that generates the data</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#small-testing-data-is-best","title":"Small testing data is best","text":"<ul> <li> <p>Use a subset of the input data</p> </li> <li> <p>The smaller, the better for everybody</p> </li> <li>Fast tests</li> <li>Easier to debug</li> <li> <p>More targeted unit test</p> </li> <li> <p>Do not check in 1 megabyte of test data!</p> </li> </ul> <p>Last review: GP on 2024-05-13</p>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#check_string-vs-selfassertequal","title":"<code>check_string</code> vs <code>self.assertEqual</code>","text":"<ul> <li>TODO(gp): Add</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#use-selfassert_equal","title":"Use <code>self.assert_equal()</code>","text":"<ul> <li>This is a function that helps you understand what the mismatches are</li> <li>It works on <code>str</code></li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#how-to-split-unit-test-code-in-files","title":"How to split unit test code in files","text":"<ul> <li>The two extreme approaches are:</li> <li>All the test code for a directory goes in one file     <code>foo/bar/test/test_$DIRNAME.py</code> (or <code>foo/bar/test/test_all.py</code>)</li> <li>Each file <code>foo/bar/$FILENAME</code> with code gets its corresponding     <code>foo/bar/test/test_$FILENAME.py</code><ul> <li>It should also be named according to the library it tests</li> <li>For example, if the library to test is called <code>pnl.py</code>, then a   corresponding unit test should be called <code>test_pnl.py</code></li> </ul> </li> <li>Pros of 1) vs 2)</li> <li>Less maintenance churn<ul> <li>It takes work to keep the code and the test files in sync, e.g.,</li> <li>If you change the name of the code file, you don't have to change other     file names</li> <li>If you move one class from one file to another, you might not need to     move test code</li> </ul> </li> <li>Fewer files opened in your editor</li> <li>Avoid many files with a lot of boilerplate code</li> <li>Cons of 1) vs 2)</li> <li>The single file can become huge!</li> <li>Compromise solution: Start with a single file   <code>test_$DIRNAME.py</code>(or<code>test*dir_name.py</code>) * In the large file add a framed   comment like:   <code>python   # ##################   # Unit tests for \u2026   # ##################</code></li> <li>So it's easy to find which file is tested were using grep</li> <li>Then split when it becomes too big using <code>test_$FILENAME.py</code></li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#skeleton-for-unit-test","title":"Skeleton for unit test","text":"<ul> <li>Interesting unit tests are in <code>helpers/test</code></li> <li>A unit test looks like:   <code>python   import helpers.unit_test as hut   class Test...(hut.TestCase):       def test...(self):           ...</code></li> <li><code>pytest</code> will take care of running the code so you don't need:   <code>python   if __name__ == '__main__':   unittest.main()</code></li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#hierarchical-testcase-approach","title":"Hierarchical <code>TestCase</code> approach","text":"<ul> <li>Whenever there is a hierarchy in classes, we also create a hierarchy of test   classes</li> <li>A parent test class looks like:   <code>python   import helpers.unit_test as hut   class SomeClientTestCase(hut.TestCase):       def _test...1(self):           ...       def _test...2(self):           ...</code></li> <li>While a child test class looks like this, where test methods use the   corresponding methods from the parent test class:   <code>python   class TestSomeClient(SomeClientTestCase):       def test...1(self):           ...       def test...2(self):           ...</code></li> <li>Each <code>TestCase</code> tests a \"behavior\" like a set of related methods</li> <li>Each <code>TestCase</code> is under the test dir</li> <li>Each derived class should use the proper <code>TestCase</code> classes to reach a decent   coverage</li> <li>It is OK to use non-private methods in test classes to ensure that the code is   in order of dependency so that the reader doesn't have to jump back / forth</li> <li>We want to separate chunks of unit test code using:</li> </ul> <p><code>python   # ########################################################################</code></p> <p>putting all the methods used by that chunk at the beginning and so on</p> <ul> <li>It is OK to skip a <code>TestCase</code> method if it is not meaningful, when coverage is   enough</li> <li>As an example, see <code>im_v2/common/data/client/test/im_client_test_case.py</code> and   <code>im_v2/ccxt/data/client/test/test_ccxt_clients.py</code></li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#use-the-appropriate-selfassert","title":"Use the appropriate <code>self.assert*</code>","text":"<ul> <li>When you get a failure, you don't want to get something like \"True is not   False\", rather an informative message like \"5 is not &lt; 4\"</li> <li>Bad <code>self.assertTrue(a &lt; b)</code></li> <li>Good <code>self.assertLess(a, b)</code></li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#do-not-use-hdbgdassert-in-testing","title":"Do not use <code>hdbg.dassert</code> in testing","text":"<ul> <li><code>dassert</code>s are for checking the self-consistency of the code</li> <li>The invariant is that you can remove <code>dbg.dassert</code> without changing the code's   behavior. Of course, you can't remove the assertion and get unit tests to work</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#always-explain-selfassertraises","title":"Always explain <code>self.assertRaises</code>","text":"<ul> <li>Testing for an assertion needs to always be done with the following idiom to   explain exactly what we are catching and why   <code>python   with self.assertRaises(AssertionError) as cm:       hlitagit.git_patch_create(           ctx, mode, modified, branch, last_commit, files       )   act = str(cm.exception)   exp = r\"\"\"</code></li> <li>Failed assertion * '0' == '1' Specify only one among --modified,   --branch, --last-commit \"\"\" self.assert_equal(act, exp, fuzzy_match=True)   <code></code></li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#interesting-testing-functions","title":"Interesting testing functions","text":"<ul> <li>List of useful testing functions are:</li> <li>General python</li> <li>Numpy</li> <li>Pandas</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#use-set_up_test-tear_down_test","title":"Use set_up_test / tear_down_test","text":"<ul> <li>If you have a lot of repeated code in your tests, you can make them shorter by   moving this code to <code>set_up_test/tear_down_test</code> methods:</li> <li>These methods are our preferred alternative to <code>setUp()</code> and <code>tearDown()</code>     methods that are standardly used in the <code>unittest</code> framework. As a general     rule, we should avoid defining custom <code>setUp()</code> and <code>tearDown()</code> methods in     our test classes. The reason is that the <code>unittest</code>/<code>pytest</code> framework     relies on its built-in <code>setUp()</code> and <code>tearDown()</code> methods, and if we     introduce additional operations in these methods in the individual test     classes (e.g., mocking, opening DBs), it can interfere with the inner     workings of the framework.</li> <li>Instead of <code>setUp()</code>, we define a <code>set_up_test()</code> method within the test     class and then run it at the beginning of each test method.</li> <li>Similarly, instead of <code>tearDown()</code>, we define <code>tear_down_test()</code> and run it     at the end of each test method.</li> <li> <p>To make sure both <code>set_up_test()</code> and <code>tear_down_test()</code> run even if the     test itself fails, we wrap them in a     pytest fixture.</p> <p>Bad:</p> <p>```python def setUp(self) -&gt; None:     super().setUp()     ...     ... # custom code     ...</p> <p>def tearDown(self) -&gt; None:     ...     ... # custom code     ...     super().tearDown()</p> <p>def test1(self) -&gt; None:     ... ```</p> <p>Good:</p> <p>```python</p> </li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#this-will-be-run-before-and-after-each-test","title":"This will be run before and after each test.","text":"<p>@pytest.fixture(autouse=True) def setup_teardown_test(self):     # Run before each test.     self.set_up_test()     yield     # Run after each test.     self.tear_down_test()</p> <p>def set_up_test(self) -&gt; None:     ...     ... # custom code     ...</p> <p>def tear_down_test(self) -&gt; None:     ...     ... # custom code     ...</p> <p>def test1(self) -&gt; None:     ... <code>``   - If there is nothing left in</code>setUp()<code>/</code>tearDown()<code>after removing</code>super().setUp()<code>/</code>super.tearDown()<code>, then</code>setUp()<code>/</code>tearDown()` can be discarded completely.</p>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#nested-set_up_test-tear_down_test","title":"Nested set_up_test / tear_down_test","text":"<ul> <li>When a test class (e.g., TestChild) inherits from another test class (e.g.,   TestParent), <code>setUp()</code>/<code>tearDown()</code> methods in the child class normally   incorporate the corresponding methods from the parent class.</li> <li>We prioritize clarity and explicitness in the naming and usage of our methods.   Therefore, the general rule is that we define   <code>set_up_test()</code>/<code>tear_down_test()</code> in the parent class, and in the child class   we define <code>set_up_test2()</code>/<code>tear_down_test2()</code>, which call the corresponding   methods from the parent class.</li> <li>If the child class itself is used as a parent for another class, then the     deeper nested class's methods are named     <code>set_up_test3()</code>/<code>tear_down_test3()</code>, and so on, with the numerical suffix     increasing at each level.</li> <li>The following cases are possible with regard to the <code>setUp()</code>/<code>tearDown()</code>   configuration:</li> <li>Both of TestParent and TestChild have separate <code>setUp()</code>/<code>tearDown()</code>     methods. Then, in TestParent, <code>setUp()</code>/<code>tearDown()</code> should be replaced by     <code>set_up_test()</code>/<code>tear_down_test()</code> as described above; in TestChild,     <code>setUp()</code>/<code>tearDown()</code> should be replaced by     <code>set_up_test2()</code>/<code>tear_down_test2()</code>, which call     <code>set_up_test()</code>/<code>tear_down_test()</code> from the parent class. For example:</li> </ul> <p>```python   class TestParent(hunitest.TestCase):       def setUp(self) -&gt; None:           super().setUp()           ...</p> <pre><code>  def tearDown(self) -&gt; None:\n      ...\n      super().tearDown()\n\n  def test1(self) -&gt; None:\n      ...\n</code></pre> <p>class TestChild(TestParent):       def setUp(self) -&gt; None:           super().setUp()           ...</p> <pre><code>  def tearDown(self) -&gt; None:\n      ...\n      super().tearDown()\n\n  def test1(self) -&gt; None:\n      ...\n</code></pre> <p>```</p> <p>should be replaced by:</p> <p>```python   class TestParent(hunitest.TestCase):       @pytest.fixture(autouse=True)       def setup_teardown_test(self):           # Run before each test.           self.set_up_test()           yield           # Run after each test.           self.tear_down_test()</p> <pre><code>  def set_up_test(self) -&gt; None:\n      ...\n\n  def tear_down_test(self) -&gt; None:\n      ...\n\n  def test1(self) -&gt; None:\n      ...\n</code></pre> <p>class TestChild(TestParent):       @pytest.fixture(autouse=True)       def setup_teardown_test(self):           # Run before each test.           self.set_up_test2()           yield           # Run after each test.           self.tear_down_test2()</p> <pre><code>  def set_up_test2(self) -&gt; None:\n      self.set_up_test()\n      ...\n\n  def tear_down_test2(self) -&gt; None:\n      ...\n      self.tear_down_test()\n\n  def test1(self) -&gt; None:\n      ...\n</code></pre> <p><code>``   - TestParent has</code>setUp()<code>and</code>tearDown()<code>, while TestChild does not. Then,     in TestParent,</code>setUp()<code>/</code>tearDown()<code>should be replaced by</code>set_up_test()<code>/</code>tear_down_test()<code>as described above; in TestChild,</code>set_up_test()<code>/</code>tear_down_test()` will run in the test methods     automatically via the inherited fixture. For example:</p> <p>```python   class TestParent(hunitest.TestCase):       def setUp(self) -&gt; None:           super().setUp()           ...</p> <pre><code>  def tearDown(self) -&gt; None:\n      ...\n      super().tearDown()\n\n  def test1(self) -&gt; None:\n      ...\n</code></pre> <p>class TestChild(TestParent):       def test1(self) -&gt; None:           ...   ```</p> <p>should be replaced by:</p> <p>```python   class TestParent(hunitest.TestCase):       @pytest.fixture(autouse=True)       def setup_teardown_test(self):           # Run before each test.           self.set_up_test()           yield           # Run after each test.           self.tear_down_test()</p> <pre><code>  def set_up_test(self) -&gt; None:\n      ...\n\n  def tear_down_test(self) -&gt; None:\n      ...\n\n  def test1(self) -&gt; None:\n      ...\n</code></pre> <p>class TestChild(TestParent):       def test1(self) -&gt; None:           ...   <code>``   - If TestParent only has one of the two, either</code>setUp()<code>or</code>tearDown()<code>,     then in both TestParent and TestChild, the test methods should run (from the     fixture), respectively, either</code>set_up_test()<code>or</code>tear_down_test()`, e.g.:</p> <pre><code>    ```python\n    class TestParent(hunitest.TestCase):\n        @pytest.fixture(autouse=True)\n        def setup_teardown_test(self):\n            # Run before each test.\n            self.set_up_test()\n            yield\n\n        def set_up_test(self) -&gt; None:\n            ...\n\n        def test1(self) -&gt; None:\n            ...\n\n    class TestChild(TestParent):\n        def test1(self) -&gt; None:\n            ...\n    ```\n</code></pre> <ul> <li>A combination of the previous two options: TestParent has <code>setUp()</code> and     <code>tearDown()</code>, and TestChild has one of them but not the other. Then, in     TestParent, <code>setUp()</code>/<code>tearDown()</code> should be replaced by     <code>set_up_test()</code>/<code>tear_down_test()</code> as described above. In TestChild, the     method that was present, <code>setUp()</code> or <code>tearDown()</code>, should be replaced by     <code>set_up_test2()</code> or <code>tear_down_test2()</code>, which should call <code>set_up_test()</code>     or <code>tear_down_test()</code> from TestParent; the other method from TestParent,     which was absent in TestChild, should be added directly to TestChild's     fixture to run in its test methods. For example, for the case when TestChild     has <code>setUp()</code> but not <code>tearDown()</code>:</li> </ul> <p>```python   class TestParent(hunitest.TestCase):       def setUp(self) -&gt; None:           super().setUp()           ...</p> <pre><code>  def tearDown(self) -&gt; None:\n      ...\n      super().tearDown()\n\n  def test1(self) -&gt; None:\n      ...\n</code></pre> <p>class TestChild(TestParent):       def setUp(self) -&gt; None:           super().setUp()           ...</p> <pre><code>  def test1(self) -&gt; None:\n      ...\n</code></pre> <p>```</p> <p>should be replaced by:</p> <p>```python   class TestParent(hunitest.TestCase):       @pytest.fixture(autouse=True)       def setup_teardown_test(self):           # Run before each test.           self.set_up_test()           yield           # Run after each test.           self.tear_down_test()</p> <pre><code>  def set_up_test(self) -&gt; None:\n      ...\n\n  def tear_down_test(self) -&gt; None:\n      ...\n\n  def test1(self) -&gt; None:\n      ...\n</code></pre> <p>class TestChild(TestParent):       @pytest.fixture(autouse=True)       def setup_teardown_test(self):           # Run before each test.           self.set_up_test2()           yield           # Run after each test.           self.tear_down_test()</p> <pre><code>  def set_up_test2(self) -&gt; None:\n      self.set_up_test()\n      ...\n\n  def test1(self) -&gt; None:\n      ...\n</code></pre> <p><code>``   - TestChild has</code>setUp()<code>and</code>tearDown()<code>(or just one of the two), while     TestParent does not. Then, in TestChild,</code>setUp()<code>/</code>tearDown()<code>should be     replaced by</code>set_up_test()<code>/</code>tear_down_test()` as described above;     TestParent undergoes no changes. For example:</p> <p>```python   class TestParent(hunitest.TestCase):       def test1(self) -&gt; None:           ...</p> <p>class TestChild(TestParent):       def setUp(self) -&gt; None:           super().setUp()           ...</p> <pre><code>  def tearDown(self) -&gt; None:\n      ...\n      super().tearDown()\n\n  def test1(self) -&gt; None:\n      ...\n</code></pre> <p>```</p> <p>should be replaced by:</p> <p>```python   class TestParent(hunitest.TestCase):       def test1(self) -&gt; None:           ...</p> <p>class TestChild(TestParent):       @pytest.fixture(autouse=True)       def setup_teardown_test(self):           # Run before each test.           self.set_up_test()           yield           # Run after each test.           self.tear_down_test()</p> <pre><code>  def set_up_test(self) -&gt; None:\n      ...\n\n  def tear_down_test(self) -&gt; None:\n      ...\n\n  def test1(self) -&gt; None:\n      ...\n</code></pre> <p>```</p>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#use-setupclass-teardownclass","title":"Use setUpClass / tearDownClass","text":"<ul> <li>If you need some expensive code parts to be done once for the whole test   class, such as opening a database connection, opening a temporary file on the   filesystem, loading a shared library for testing, etc., you can use   <code>setUpClass/tearDownClass</code> methods:</li> <li> <p><code>setUpClass()</code></p> <p>A class method called before tests in an individual class are run. <code>setUpClass</code> is called with the class as the only argument and must be decorated as a classmethod:</p> <p><code>python @classmethod def setUpClass(cls):     ...</code>   - <code>tearDownClass()</code></p> <p>A class method called after tests in an individual class have run. <code>tearDownClass</code> is called with the class as the only argument and must be decorated as a classmethod:</p> <p><code>python @classmethod def tearDownClass(cls):     ...</code></p> </li> <li> <p>For more information, see   official unittest docs</p> </li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#update-test-tags","title":"Update test tags","text":"<ul> <li>There are 2 files with the list of tests' tags:</li> <li><code>amp/pytest.ini</code></li> <li><code>.../pytest.ini (if</code>amp<code>is a submodule)</code></li> <li>In order to update the tags (do it in both files):</li> <li>In the <code>markers</code> section, add a name of a new tag</li> <li>After a <code>:</code> add a short description</li> <li>Keep tags in the alphabetical order</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#mocking","title":"Mocking","text":""},{"location":"coding/all.write_unit_tests.how_to_guide.html#refs","title":"Refs","text":"<ul> <li>Introductory article is   https://realpython.com/python-mock-library/ </li> <li>Official Python documentation for the mock package can be seen here   unit test mock</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#common-usage-samples","title":"Common usage samples","text":"<p>It is best to apply on any part that is deemed unnecessary for specific test</p> <ul> <li>Complex functions</li> <li>Mocked functions can be tested separately</li> <li>3rd party provider calls</li> <li>CCXT</li> <li>AWS<ul> <li>S3</li> <li>See <code>helpers/hmoto.py</code> in <code>cmamp</code> repo</li> <li>Secrets</li> <li>Etc...</li> </ul> </li> <li> <p>DB calls</p> </li> <li> <p>Many more possible combinations can be seen in the official documentation.</p> </li> <li>Below are the most common ones for basic understanding.</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#philosophy-about-mocking","title":"Philosophy about mocking","text":"<ol> <li>We want to mock the minimal surface of a class</li> <li>E.g., assume there is a class that is interfacing with an external provider      and our code places requests and gets values back</li> <li>We want to replace the provider with an object that responds to the      requests with the actual response of the provider</li> <li>In this way, we can leave all the code of our class untouched and tested</li> <li>We want to test public methods of our class (and a few private methods)</li> <li>In other words, we want to test the end-to-end behavior and not how things      are achieved</li> <li>Rationale: if we start testing \"how\" things are done and not \"what\" is      done, we can't change how we do things (even if it doesn't affect the      interface and its behavior), without updating tons of methods</li> <li>We want to test the minimal amount of behavior that enforces what we care      about</li> </ol>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#some-general-suggestions-about-testing","title":"Some general suggestions about testing","text":""},{"location":"coding/all.write_unit_tests.how_to_guide.html#test-from-the-outside-in","title":"Test from the outside-in","text":"<ul> <li>We want to start testing from the end-to-end methods towards the constructor   of an object</li> <li>Rationale: often, we start testing the constructor very carefully and then we   get tired / run out of time when we finally get to test the actual behavior</li> <li>Also, testing the important behavior automatically tests building the objects</li> <li>Use the code coverage to see what's left to test once you have tested the   \"most external\" code</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#we-dont-need-to-test-all-the-assertions","title":"We don't need to test all the assertions","text":"<ul> <li>E.g., testing carefully that we can't pass a value to a constructor doesn't   really test much besides the fact that <code>dassert</code> works (which, surprisingly   works!)</li> <li>We don't care about line coverage or checking boxes for the sake of checking   boxes</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#use-strings-to-compare-output-instead-of-data-structures","title":"Use strings to compare output instead of data structures","text":"<ul> <li>Often, it's easier to do a check like:</li> </ul> <p>```python   # Better:   expected = str(...)   expected = pprint.pformat(...)</p> <p># Worse:   expected = [\"a\", \"b\", { ... }]   ```</p> <p>rather than building the data structure</p> <ul> <li>Some purists might not like this, but</li> <li>It's much faster to use a string (which is or should be one-to-one to the     data structure), rather than the data structure itself<ul> <li>By extension, many of the more complex data structure have a built-in   string representation</li> </ul> </li> <li>It is often more readable and easier to diff (e.g., <code>self.assertEqual</code> vs     <code>self.assert_equal</code>)</li> <li>In case of mismatch, it's easier to update the string with copy-paste rather     than creating a data structure that matches what was created</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#use-selfcheck_string-for-things-that-we-care-about-not-changing-or-are-too-big-to-have-as-strings-in-the-code","title":"Use <code>self.check_string()</code> for things that we care about not changing (or are too big to have as strings in the code)","text":"<ul> <li>Use <code>self.assert_equal()</code> for things that should not change (e.g., 1 + 1 = 2)</li> <li>When using <code>check_string</code> still try to add invariants that force the code to   be correct</li> <li>E.g., if we want to check the PnL of a model, we can freeze the output with   <code>check_string()</code>, but we want to add a constraint like there are more   timestamps than 0 to avoid the situation where we update the string to   something malformed</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#each-test-method-should-test-a-single-test-case","title":"Each test method should test a single test case","text":"<ul> <li>Rationale: we want each test to be clear, simple, fast</li> <li>If there is repeated code we should factor it out (e.g., builders for objects)</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#each-test-should-be-crystal-clear-on-how-it-is-different-from-the-others","title":"Each test should be crystal clear on how it is different from the others","text":"<ul> <li>Often, you can factor out all the common logic into a helper method</li> <li>Copy-paste is not allowed in unit tests in the same way it's not allowed in   production code</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#in-general-you-want-to-budget-the-time-to-write-unit-tests","title":"In general, you want to budget the time to write unit tests","text":"<ul> <li>E.g., \"I'm going to spend 3 hours writing unit tests\". This is going to help   you focus on what's important to test and force you to use an iterative   approach rather than incremental (remember the Monalisa)</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#write-a-skeleton-of-unit-tests-and-ask-for-a-review-if-you-are-not-sure-how-what-to-test","title":"Write a skeleton of unit tests and ask for a review if you are not sure how what to test","text":"<ul> <li>Aka \"testing plan\"</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#object-patch-with-return-value","title":"Object patch with return value","text":"<pre><code>import unittest.mock as umock\nimport im_v2.ccxt.data.extract.extractor as ivcdexex\n\n@umock.patch.object(ivcdexex.hsecret, \"get_secret\")\ndef test_function_call1(self, mock_get_secret: umock.MagicMock):\n    mock_get_secret.return_value = \"dummy\"\n</code></pre> <ul> <li>Function <code>get_secret</code> in <code>helpers/hsecret.py</code> is mocked</li> <li>Pay attention on where is <code>get_secret</code> mocked:<ul> <li>It is mocked in im_v2.ccxt.data.extract.extractor as \u201cget_secret\u201d is   called there in function that is being tested</li> </ul> </li> <li><code>@umock.patch.object(hsecret, \"get_secret\")</code> will not work as mocks are     applied after all modules are loaded, hence the reason for using exact     location<ul> <li>If we import s module in test itself it will work as mock is applied</li> <li>For modules outside of test function it is too late as they are loaded   before mocks for test are applied</li> </ul> </li> <li>On every call, it returns string \"dummy\"</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#path-patch-with-multiple-return-values","title":"Path patch with multiple return values","text":"<pre><code>import unittest.mock as umock\n\n@umock.patch(\"helpers.hsecret.get_secret\")\ndef test_function_call1(self, mock_get_secret: umock.MagicMock):\n\nmock_get_secret.side_effect = [\"dummy\", Exception]\n</code></pre> <ul> <li>On first call, string <code>dummy</code> is returned</li> <li>On second, <code>Exception</code> is raised</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#ways-of-calling-patch-and-patchobject","title":"Ways of calling <code>patch</code> and <code>patch.object</code>","text":"<ul> <li>Via decorator   <code>python   @umock.patch(\"helpers.hsecret.get_secret\")   def test_function_call1(self, mock_get_secret: umock.MagicMock):       pass</code></li> <li>In actual function   <code>python   get_secret_patch = umock.patch(\"helpers.hsecret.get_secret\")   get_secret_mock = get_secret_patch.start()</code></li> <li>This is the only approach in which you need to start/stop patch!<ul> <li>The actual mock is returned as the return value of <code>start()</code> method!</li> </ul> </li> <li>In other two approaches, start/stop is handled under the hood and we are     always interacting with <code>MagicMock</code> object</li> <li>Via <code>with</code> statement (also in function)   <code>python   with umock.patch(\"\"helpers.hsecret.get_secret\"\") as get_secret_mock:       pass</code></li> <li>One of the use cases for this is if we are calling a different function     inside a function that is being mocked<ul> <li>Mostly because it is easy for an eye if there are to much patches via   decorator and we do not need to worry about reverting the patch changes as   that is automatically done at the end of with statement</li> </ul> </li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#mock-object-state-after-test-run","title":"Mock object state after test run","text":"<pre><code>@umock.patch.object(exchange_class._exchange, \"fetch_ohlcv\")\ndef test_function_call1(self, fetch_ohlcv_mock: umock.MagicMock):\n    self.assertEqual(fetch_ohlcv_mock.call_count, 1)\n    actual_args = tuple(fetch_ohlcv_mock.call_args)\n    expected_args = (\n            (\"BTC/USDT\",),\n            {\"limit\": 2, \"since\": 1, \"timeframe\": \"1m\"},\n    )\n    self.assertEqual(actual_args, expected_args)\n</code></pre> <ul> <li>After <code>fetch_ohlcv</code> is patched, <code>Mock</code> object is passed to test</li> <li>In this case, it is <code>fetch_ohlcv_mock</code></li> <li>From sample we can see that function is called once</li> <li>First value in a tuple are positional args passed to <code>fetch_ohlcv</code> function</li> <li>Second value in a tuple are keyword args passed to <code>fetch_ohlcv</code> function</li> <li> <p>As an alternative, <code>fetch_ohlcv_mock.call_args.args</code> and     <code>fetch_ohlcv_mock.call_args.kwargs</code> can be called for separate results of     args/kwargs</p> <p><code>python self.assertEqual(fetch_ohlcv_mock.call_count, 3) actual_args = str(fetch_ohlcv_mock.call_args_list) expected_args = r\"\"\" [call('BTC/USDT', since=1645660800000, bar_per_iteration=500), call('BTC/USDT', since=1645690800000, bar_per_iteration=500), call('BTC/USDT', since=1645720800000, bar_per_iteration=500)] \"\"\" self.assert_equal(actual_args, expected_args, fuzzy_match=True)</code></p> </li> <li> <p>In sample above, that is continuation of previous sample,   <code>fetch_ohlcv_mock.call_args_list</code> is called that returns all calls to mocked   function regardless of how many times it is called</p> </li> <li>Useful for verifying that args passed are changing as expected</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#mock-common-external-calls-in-hunitesttestcase-class","title":"Mock common external calls in <code>hunitest.TestCase</code> class","text":"<pre><code>class TestCcxtExtractor1(hunitest.TestCase):\n    # Mock calls to external providers.\n    get_secret_patch = umock.patch.object(ivcdexex.hsecret, \"get_secret\")\n    ccxt_patch = umock.patch.object(ivcdexex, \"ccxt\", spec=ivcdexex.ccxt)\n\n    def set_up_test(self) -&gt; None:\n        self.get_secret_mock: umock.MagicMock = self.get_secret_patch.start()\n        self.ccxt_mock: umock.MagicMock = self.ccxt_patch.start()\n        # Set dummy credentials for all tests.\n        self.get_secret_mock.return_value = {\"apiKey\": \"test\", \"secret\": \"test\"}\n\n    def tear_down_test(self) -&gt; None:\n        self.get_secret_patch.stop()\n        self.ccxt_patch.stop()\n</code></pre> <ul> <li>For every unit test we want to isolate external calls and replace them with   mocks</li> <li>This way tests are much faster and not influenced by external factors we can     not control</li> <li>Mocking them in <code>set_up_test()</code> (which is run at the beginning of the test     methods) will make the tests using this class simpler and ready out of the     box</li> <li>In current sample we are mocking AWS secrets and <code>ccxt</code> library</li> <li><code>umock.patch.object</code> is creating <code>patch</code> object that is not yet activated<ul> <li><code>patch.start()/stop()</code> is activating/deactivating patch for each test   where <code>set_up_test()</code> and <code>tear_down_test()</code> are run</li> <li><code>patch.start()</code> is returning a standard <code>MagicMock</code> object we can use to   check various states as mentioned in previous examples and control return   values</li> <li>Call_args, call_count, return_value, side_effect, etc.</li> </ul> </li> <li>Note: Although patch initialization in static variables belongs to   <code>set_up_test()</code>, when this code is moved there patch is created for each test   separately. We want to avoid that and only start/stop same patch for each   test.</li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#mocks-with-specs","title":"Mocks with specs","text":"<pre><code>## Regular mock and external library `ccxt` is replaced with `MagicMock`\n@umock.patch.object(ivcdexex, \"ccxt\")\n## Only `ccxt` is spec'd, not actual components that are \"deeper\" in the `ccxt` library.\n@umock.patch.object(ivcdexex, \"ccxt\", spec=ivcdexex.ccxt)\n## Everything is spec'd recursively , including returning values/instances of `ccxt`\n## functions and returned values/instances of returned values/instances, etc.\n@umock.patch.object(ivcdexex, \"ccxt\", autospec=True)\n</code></pre> <ul> <li>First mock is not tied to any spec and we can call any attribute/function   against the mock and the call will be memorized for inspection and the return   value is new <code>MagicMock</code>.</li> <li><code>ccxt_mock.test(123)</code> returns new <code>MagicMock</code> and raises no error</li> <li>In second mock <code>ccxt.test(123)</code> would fail as such function does not exists</li> <li>We can only call valid exchange such as <code>ccxt_mock.binance()</code> that will     return <code>MagicMock</code>, as exchange is not part of the spec</li> <li>In third mock everything needs to be properly called</li> <li><code>ccxt_mock.binance()</code> will return <code>MagicMock</code> with <code>ccxt.Exchange</code> spec_id     (in mock instance as meta)<ul> <li>As newly <code>exchange</code> instance is with spec, we can only call real   functions/attributes of <code>ccxt.Exchange</code> class</li> </ul> </li> </ul>"},{"location":"coding/all.write_unit_tests.how_to_guide.html#caveats","title":"Caveats","text":"<pre><code>## `datetime.now` cannot be patched directly, as it is a built-in method.\n## Error: \"can't set attributes of built-in/extension type 'datetime.datetime'\"\ndatetime_patch = umock.patch.object(imvcdeexut, \"datetime\", spec=imvcdeexut.datetime)\n</code></pre> <ul> <li>Python built-in methods can not be patched</li> </ul> <p><code>python   class TestExtractor1(hunitest.TestCase):       # Mock `Extractor`'s abstract functions.       abstract_methods_patch = umock.patch.object(           imvcdexex.Extractor, \"__abstractmethods__\", new=set()       )       ohlcv_patch = umock.patch.object(           imvcdexex.Extractor,           \"_download_ohlcv\",           spec=imvcdexex.Extractor._download_ohlcv,       )</code></p> <ul> <li>Patching <code>__abstractmethods__</code> function of an abstract class enables us to   instantiate and test abstract class as any regular class</li> </ul>"},{"location":"dataflow/all.batch_and_streaming_mode_using_tiling.explanation.html","title":"Batch And Streaming Mode Using Tiling","text":""},{"location":"dataflow/all.batch_and_streaming_mode_using_tiling.explanation.html#the-property-of-tilability","title":"The property of tilability","text":"<p>The working principle of a DataFlow computation is that nodes should be able to compute their outputs from their inputs without a dependency on how the inputs are partitioned along the dataframe axes of the inputs (e.g., the time and the feature axes). When this property is valid we call a computation \"tilable\".</p> <p>A slightly more formal definition is that a computation $f()$ is tilable if:</p> <p>$$ f(dfX \\cup dfY) = f(dfX) \\cup f(dfY) $$</p> <p>where:</p> <ul> <li>$dfX$ and $dfY$ represent input dataframes (which can optionally) overlap</li> <li>$\\cup$ is an operation of concat along consecutive</li> <li>$f()$ is a node operation</li> </ul> <p>TODO(gp): In reality the property requires that feeding data, computing, and then filtering is invariant, like</p> <p>f(A, B)</p> <p>$$ \\forall t1 \\le t2, t3 \\le t4: \\exists T: f(A[t1 - T:t2] \\cup A[t3 - T:t4])[t1:t4] = f(A[t1 - T:t2])[t1:t4] \\cup f(A[t3 - T:t4])[t1:t4] $$</p> <p>This property resembles linearity, in the sense that a transformation $f()$ is invariant over partitioning of the data.</p> <p>A sufficient condition for a DAG computation to be tileable, is for all the DAG nodes to be tileable. The opposite is not necessarily trye, and not interest in general, since we are interested in finding ways to describe the computation so that it is tileable.</p> <p>A node that has no memory, e.g., whose computation</p> <p>Nodes can have \"memory\", where the output for a given tile depends on previous tile. E.g., a rolling average has memory since samples with different timestamps are combined together to obtain the results. A node with finite memory is always tileable, while nodes with infinite memory are not necessarily tileable. If the computation can be expressed in a recursive form across axes (e.g., an exponentially weighted moving average for adjacent intervals of times), then it can be made tileable by adding auxiliary state to store the partial amount of computatin.</p>"},{"location":"dataflow/all.batch_and_streaming_mode_using_tiling.explanation.html#temporal-tiling","title":"Temporal tiling","text":"<p>In most computations there is a special axis that represents time and moves only from past to future. The data along other axes represent (potentially independent) features.</p> <p>This is an easy requirement to enforce if the computation has no memory, e.g., in the following example of Python code using Pandas</p> <pre><code>df1 =\n                      a         b\n2023-01-01 08:00:00   10        20\n2023-01-01 08:30:00   10        20\n2023-01-01 09:00:00   10        20\n2023-01-01 09:30:00   10        20\n\ndf2 =\n                      a         b\n2023-01-01 08:00:00   10        20\n2023-01-01 08:30:00   10        20\n2023-01-01 09:00:00   10        20\n2023-01-01 09:30:00   10        20\n\ndfo = df1 + df2\n</code></pre> <p>DataFlow can partition df1 and df2 in different slices obtaining the same result, e.g.,</p> <pre><code>df1_0 =\n                      a         b\n2023-01-01 08:00:00   10        20\n2023-01-01 08:30:00   10        20\n\ndf2_0 =\n                      a         b\n2023-01-01 08:00:00   10        20\n2023-01-01 08:30:00   10        20\n\ndfo_0 = df1_0 + df2_0\n\ndf1_1 =\n                      a         b\n2023-01-01 09:00:00   10        20\n2023-01-01 09:30:00   10        20\n\ndf2_1 =\n                      a         b\n2023-01-01 09:00:00   10        20\n2023-01-01 09:30:00   10        20\n\ndfo_1 = df1_1 + df2_1\n\ndfo = pd.concat([dfo_1, dfo_2])\n</code></pre> <p>Consider the case of a computation that relies on past values</p> <pre><code>dfo = df1.diff()\n</code></pre> <p>This computation to be invariant to slicing needs to be fed with a certain amount of previous data</p> <pre><code>df1_0 =\n                      a         b\n2023-01-01 08:00:00   10        20\n2023-01-01 08:30:00   10        20\n\ndfo_0 = df1_0.diff()\ndfo_0 = dfo_0[\"2023-01-01 08:00:00\":\"2023-01-01 08:30:00\"]\n\ndf1_1 =\n                      a         b\n2023-01-01 08:30:00   10        20\n2023-01-01 09:00:00   10        20\n2023-01-01 09:30:00   10        20\n\ndfo_1 = df1_1.diff()\ndfo_1 = dfo_1[\"2023-01-01 09:00:00\":\"2023-01-01 09:30:00\"]\n\ndfo = pd.concat([dfo_1, dfo_2])\n</code></pre> <p>In general as long as the computation doesn't have infinite memory (e.g., an exponentially weighted moving average)</p> <p>This is possible by giving each nodes data that has enough history</p> <p>Many interesting computations with infinite memory (e.g., EMA) can also be decomposed in tiles with using some algebraic manipulations</p> <ul> <li>TODO(gp): Add an example of EMA expressed in terms of previous</li> </ul> <p>Given the finite nature of real-world computing (e.g., in terms of finite approximation of real numbers, and bounded computation) any infinite memory computation is approximated to a finite memory one. Thus the tiling approach described above is general, within any desired level of approximation.</p> <p>The amount of history is function of a node</p>"},{"location":"dataflow/all.batch_and_streaming_mode_using_tiling.explanation.html#cross-sectional-tiling","title":"Cross-sectional tiling","text":"<ul> <li> <p>The same principle can be applied to tiling computation cross-sectionally</p> </li> <li> <p>Computation that needs part of a cross-section need to be tiled properly to be   correct</p> </li> <li>TODO(gp): Make an example</li> </ul>"},{"location":"dataflow/all.batch_and_streaming_mode_using_tiling.explanation.html#temporal-and-cross-sectional-tiling","title":"Temporal and cross-sectional tiling","text":"<ul> <li>These two styles of tiling can be composed</li> <li>The tiling doesn't even have to be regular, as long as the constraints for a   correct computation are correct</li> </ul>"},{"location":"dataflow/all.batch_and_streaming_mode_using_tiling.explanation.html#detecting-incorrect-tiled-computations","title":"Detecting incorrect tiled computations","text":"<ul> <li>One can use the tiling invariance of a computation to verify that it is   correct</li> <li>E.g., if computing a DAG gives different results for different tiled, then the   amount of history to each node is not correct</li> </ul>"},{"location":"dataflow/all.batch_and_streaming_mode_using_tiling.explanation.html#benefits-of-tiled-computation","title":"Benefits of tiled computation","text":"<ul> <li>Another benefit of tiled computation is that future peeking (i.e., a fault in   a computation that requires data not yet available at the computation time)   can be detected by streaming the data with the same timing as the real-time   data would do</li> </ul> <p>A benefit of the tiling is that the compute frame can apply any tile safe transformation without altering the computation, e.g.,</p> <ul> <li>Vectorization across data frames</li> <li>Coalescing of compute nodes</li> <li>Coalescing or splitting of tiles</li> <li>Parallelization of tiles and nodes across different CPUs</li> <li>Select the size of a tile so that the computation fits in memory</li> </ul>"},{"location":"dataflow/all.batch_and_streaming_mode_using_tiling.explanation.html#batch-vs-streaming","title":"Batch vs streaming","text":"<p>Once a computation can be tiled, the same computation can be performed in batch mode (e.g., the entire data set is processed at once) or in streaming mode (e.g., the data is presented to the DAG as it becomes available) yielding the same result</p> <p>This allows a system to be designed only once and be run in batch (fast) and real-time (accurate timing but slow) mode without any change</p> <p>In general the more data is fed to the system at once, the more likely is to being able to increase performance through parallelization and vectorization, and reducing the overhead of the simulation kernel (e.g., assembling/splitting data tiles, bookkeeping), at the cost of a larger footprint for the working memory</p> <p>In general the smaller the chunks of data are fed to the system (with the extreme condition of feeding data with the same timing as in a real-time set-up), the more unlikely is a fault in the design it is (e.g., future peeking, incorrect history amount for a node)</p> <p>Between these two extremes is also possible to chunk the data at different resolutions, e.g., feeding one day worth of data at the time, striking different balances between speed, memory consumption, and guarantee of correctness</p>"},{"location":"dataflow/all.best_practice_for_building_dags.explanation.html","title":"Best Practice For Building Dags","text":""},{"location":"dataflow/all.best_practice_for_building_dags.explanation.html#config","title":"Config","text":""},{"location":"dataflow/all.best_practice_for_building_dags.explanation.html#config-builders","title":"Config builders","text":"<p><code>Config</code>s can be built through functions that can complete a \"template\" config with some parameters passed from the user</p> <p>E.g.,</p> <pre><code>def get_kibot_returns_config(symbol: str) -&gt; cfg.Config:\n    \"\"\"\n    A template configuration for `get_kibot_returns_dag()`.\n    \"\"\"\n    ...\n</code></pre> <p>Config builders can be nested.</p> <p>You can use put <code>nid_prefix</code> in the <code>DagBuilder</code> constructor, since <code>nid_prefix</code> acts as a namespace to avoid <code>nid</code> collisions</p>"},{"location":"dataflow/all.best_practice_for_building_dags.explanation.html#dag-builders","title":"DAG builders","text":""},{"location":"dataflow/all.best_practice_for_building_dags.explanation.html#dag-builder-methods","title":"DAG builder methods","text":"<ul> <li>DAG builders accept a Config and return a DAG</li> <li>E.g.,</li> </ul> <pre><code>def get_kibot_returns_dag(config: cfg.Config, dag: dtf.DAG) -&gt; dtf.DAG:\n\"\"\"\n\nBuild a DAG (which in this case is a linear pipeline) for loading Kibot\ndata and generating processed returns.\n\nThe stages are:\n- read Kibot price data\n- compute returns\n- resample returns (optional)\n- zscore returns (optional)\n- filter returns by ATH (optional)\n- `config` must reference required stages and conform to specific node\n  interfaces\n\"\"\"\n</code></pre> <p>Some DAG builders can also add nodes to user-specified or inferred nodes (e.g., a unique sink node) of an existing DAG. Thus builders allow one to build a complex DAG by adding in multiple steps subgraphs of nodes.</p> <p>DAG builders give meaningful <code>nid</code> names to their nodes. Collisions in graphs built from multiple builders are avoided by the user through the judicious use of namespace-like nid prefixes.</p>"},{"location":"dataflow/all.best_practice_for_building_dags.explanation.html#dag-and-nodes","title":"DAG and Nodes","text":"<p>The DAG structure does not know about what data is exchanged between nodes.</p> <ul> <li>Structural assumptions, e.g., column names, can and should be expressed   through the config</li> <li><code>dataflow/core.py</code> does not impose any constraints on the type of data that   can be exchanged</li> <li>In practice (e.g., all concrete classes for nodes), we assume that   <code>pd.DataFrame</code>s are propagated</li> </ul> <p>The DAG <code>node</code>s are wrappers around Pandas dataframes</p> <ul> <li>E.g., if a node receives a column multi-index dataframe (e.g., with multiple   instruments and multiple features per instruments), the node should, assuming   a well-defined DAG and config, know how to melt and pivot columns</li> </ul>"},{"location":"dataflow/all.best_practice_for_building_dags.explanation.html#keeping-config-and-dagbuilder-in-sync","title":"Keeping <code>config</code> and <code>DagBuilder</code> in sync","text":"<ul> <li><code>Config</code> asserts if a <code>DagBuilder</code> tries to access a hierarchical parameter   that doesn't exist and reports a meaningful error of what the problem is</li> <li><code>Config</code> tracks what parameters are accessed by <code>DagBuilder</code> function</li> <li>A method <code>sanity_check</code> is called after the DAG is completely built and     reports a warning for all the parameters that were not used</li> <li>This is mostly for a sanity check and debugging, so we don't assert</li> </ul>"},{"location":"dataflow/all.best_practice_for_building_dags.explanation.html#dagbuilder-idiom","title":"<code>DagBuilder</code> idiom","text":"<p>When we build DAGs we use <code>DagBuilder</code> that call a constructor from <code>get_dag()</code> with params from the <code>get_config()</code></p> <pre><code>dag_builder = DagBuilder()\ntemplate_config = dag_builder.get_template_config()\n## Complete the config.\nconfig = template_config[...]\ndag = dag_builder.get_dag(config)\n</code></pre>"},{"location":"dataflow/all.best_practice_for_building_dags.explanation.html#invariants","title":"Invariants","text":"<p>Nodes of the DAG propagate dataframes</p> <p>Dataframes can be column multi-index to represent higher dimensionality datasets (e.g., multiple instruments, multiple features for each instrument)</p> <p>The index of each dataframe is always composed of <code>datatime.datetime</code></p> <ul> <li>For performance reasons, we prefer to use a single time zone (e.g., ET) in the   name of the columns rather than using <code>datetimes</code> with <code>tzinfo</code></li> </ul> <p>We assume that dataframes are aligned in terms of timescale</p> <ul> <li>I.e., the DAG has nodes that explicitly merge / align dataframes</li> <li>When data sources have different time resolutions, typically we perform outer   merges either leaving nans or filling with forward fills</li> </ul>"},{"location":"dataflow/all.best_practice_for_building_dags.explanation.html#make-code-easy-to-wrap-code-into-nodes","title":"Make code easy to wrap code into <code>Nodes</code>","text":"<p>We strive to write functions (e.g., from <code>signal_processing.py</code>) that:</p> <ul> <li> <p>Can be wrapped in <code>Node</code>s</p> </li> <li> <p>Operate on <code>pd.Series</code> and can be easily applied to <code>pd.DataFrame</code> columns   when needed using <code>apply_to_df</code> decorator, or operate on <code>pd.DataFrame</code>   directly</p> </li> <li> <p>Return information about the performed operation, so that we can store this   information in the <code>Node</code> info</p> </li> <li> <p>E.g., refer to <code>process_outliers()</code> as an example</p> </li> </ul>"},{"location":"dataflow/all.best_practice_for_building_dags.explanation.html#columntransformer","title":"<code>ColumnTransformer</code>","text":"<p><code>ColumnTransformer</code> is a very flexible <code>Node</code> class that can wrap a wide variety of functions</p> <ul> <li> <p>The function to use is passed to the <code>ColumnTransformer</code> constructor in the   DAG builder</p> </li> <li> <p>Arguments to forward to the function are passed through <code>transformer_kwargs</code></p> </li> <li> <p>Currently <code>ColumnTransformer</code> does not allow index-modifying changes (we may   relax this constraint but continue to enforce it by default)</p> </li> <li> <p><code>DataframeMethodRunner</code> can run any <code>pd.DataFrame</code> method supported and   forwards kwargs</p> </li> </ul>"},{"location":"dataflow/all.best_practice_for_building_dags.explanation.html#one-vs-multiple-graphs","title":"One vs multiple graphs","text":"<ul> <li>We still don't have a final answer about this design issue</li> <li>Pros of one graph:</li> <li>Everything is in one place</li> <li>One config for the whole graph</li> <li>Pros of multiple graphs:</li> <li>Easier to parallelize</li> <li>Easier to manage memory</li> <li>Simpler to configure (maybe), e.g., templatize config</li> <li>One connected component (instead of a number depending upon the number of     tickers)</li> </ul>"},{"location":"dataflow/all.best_practice_for_building_dags.explanation.html#how-to-handle-multiple-features-for-a-single-instrument","title":"How to handle multiple features for a single instrument","text":"<ul> <li>E.g., <code>close</code> and <code>volume</code> for a single futures instrument</li> <li>In this case we can use a dataframe with two columns <code>close_price</code> and   <code>volume</code></li> <li>The solution is to keep columns in the same dataframe either if they are   processed in the same way (i.e., vectorized) or if the computing node needs to   have both features available (like sklearn model)</li> <li>If close_price and volume are \"independent\", they should go in different   branches of the graph using a \"Y\" split</li> </ul>"},{"location":"dataflow/all.best_practice_for_building_dags.explanation.html#how-to-handle-multiple-instruments","title":"How to handle multiple instruments?","text":"<ul> <li>E.g., <code>close</code> price for multiple futures</li> <li>We pass a dataframe with one column per instrument</li> <li>All the transformations are then performed on a column-basis</li> <li>We assume that the timeseries are aligned explicitly</li> </ul>"},{"location":"dataflow/all.best_practice_for_building_dags.explanation.html#how-to-handle-multiple-features-with-multiple-instruments","title":"How to handle multiple features with multiple instruments","text":"<ul> <li>E.g., close price, high price, volume for multiple energy futures instrument</li> <li>In this case we can use a dataframe with hierarchical columns, where the first   dimension is the instrument, and the second dimension is the feature</li> </ul>"},{"location":"dataflow/all.best_practice_for_building_dags.explanation.html#irregular-dags","title":"Irregular DAGs","text":"<ul> <li> <p>E.g., if we have 10 instruments that need to use different models, we could   build a DAG, instantiating 10 different pipelines</p> </li> <li> <p>In general, we try to use vectorization any time that is possible</p> </li> <li> <p>E.g., if the computation is the same, instantiate a single DAG working on all   the 10 instruments in a single dataframe (i.e., vectorization)</p> </li> <li> <p>E.g,. if the computation is the same up to until a point, vectorize the common   part, and then split the dataframe and use different pipelines</p> </li> </ul>"},{"location":"dataflow/all.best_practice_for_building_dags.explanation.html#namespace-vs-hierarchical-config","title":"Namespace vs hierarchical config","text":"<ul> <li>We recognize that sometimes we might want to call the same <code>DagBuilder</code>   function multiple times (e.g., a DAG that is built with a loop)</li> <li>In this case it's not clear if it would be better to prefix the names of each   node with a tag to make them unique or use hierarchical DAG</li> <li>It seems simpler to use prefix for the tags, which is supported</li> </ul>"},{"location":"dataflow/all.best_practice_for_building_dags.explanation.html#how-to-know-what-is-configurable","title":"How to know what is configurable","text":"<ul> <li> <p>By design, DataFlow can loosely wrap Python functions</p> </li> <li> <p>Any argument of the Python function could be a configuration parameter</p> </li> <li> <p><code>ColumnTransformer</code> is an example of an abstract node that wraps python   functions that operate on columns independently</p> </li> <li> <p>Introspection to determine what is configurable would be best</p> </li> <li> <p>Manually specifying function parameters in config may be a reasonable approach   for now</p> </li> <li> <p>This could be coupled with moving some responsibility to the <code>Config</code> class,   e.g., specifying \"mandatory\" parameters along with methods to indicate which   parameters are \"dummies\"</p> </li> <li> <p>Introspection on config should be easy (but may be hard in full generality on   DAG building code)</p> </li> <li> <p>Having the builder completely bail out is another possible approach</p> </li> <li> <p>Dataflow provides mechanisms for conceptually organizing config and mapping   config to configurable functions. This ability is more important than making   it easy to expose all possible configuration parameters.</p> </li> </ul>"},{"location":"dataflow/all.best_practice_for_building_dags.explanation.html#dag-extension-vs-copying","title":"DAG extension vs copying","text":"<ul> <li> <p>Currently DAG builders are chained by progressively extending an existing DAG</p> </li> <li> <p>Another approach is to chain builders by constructing a new DAG from smaller   component DAGs</p> </li> <li> <p>On the one hand, this</p> </li> <li>May provide cleaner abstractions</li> <li> <p>Is functional</p> </li> <li> <p>On the other hand, this approach may require some customization of deep copies   (to be determined)</p> </li> <li> <p>If we have clean configs and builders for two DAGs to be merged / unioned,   then we could simply rebuild by chaining</p> </li> <li> <p>If one of the DAGs was built through, e.g., notebook experimentation, then a   graph-introspective deep copy approach is probably needed</p> </li> <li> <p>If we perform deep copies, do we want to create new \"uninitialized\" nodes, or   also copy state?</p> </li> <li> <p>The answer may depend upon the use case, e.g., notebooks vs production</p> </li> <li> <p>Extending DAGs node by node is in fact how they are built under the hood</p> </li> </ul>"},{"location":"dataflow/all.best_practice_for_building_dags.explanation.html#reusing-parameters-across-nodes-configs","title":"Reusing parameters across nodes' configs","text":"<ul> <li> <p>The same parameter might need to be used by different objects / functions and   DAG nodes and kept in sync somehow</p> </li> <li> <p>E.g., the <code>start_datetime</code> for the reading node and for the <code>ReplayedTime</code></p> </li> <li>Solution #1:</li> <li>A \"late binding\" approach: in the config there is a <code>ConfigParam</code> specifying     the path of the corresponding value to use</li> <li>Solution #2:</li> <li>A \"meta_parameter\" Config key with all the parameters used by multiple nodes</li> </ul>"},{"location":"dataflow/all.best_practice_for_building_dags.explanation.html#composing-vs-deriving-objects","title":"Composing vs deriving objects","text":"<p>We have a lot of composition of objects to create specialized versions of objects E.g., there is an <code>HistoricalDataSource</code> node that allows to connect an <code>AbstractMarketDataInterface</code> to a graph</p> <p>Approach 1)</p> <p>We could create a class for each specialization of <code>DataSource</code> object</p> <pre><code>class EgHistoricalDataSource(HistoricalDataSource):\n  \"\"\"\n  A `HistoricalDataSource` with a `EgReplayedTimeMarketDataInterface` inside.\n  \"\"\"\n\n  def __init__(\n     self,\n     nid: dtfcore.NodeId,\n     market_data_interface: vltabaretimbar.EgReplayedTimeMarketDataInterface,\n     ):\n</code></pre> <p>In this case we use inheritance</p> <p>Pros:</p> <ul> <li>This specialized an <code>HistoricalDataSource</code> fixing some parameters that need to   be fixed Cons:</li> <li>It does the cross-product of objects</li> <li>It introduces a new name that we need to keep track of</li> </ul> <p>We can have further builder methods like <code>get_..._example1()</code> to create specific objects for testing purposes</p> <p>Approach 2)</p> <p>We could create a builder method, like <code>get_EgHistoricalDataSource(params)</code>, instead of a class In this case we use composition</p> <p>This is in practice the same approach as 1), even from the way it is called <code>``python # This is an instance of class</code>EgHistoricalDataSource`. obj = EgHistoricalDataSource(nid, ...)</p> <pre><code> # This is an instance of class `HistoricalDataSource`.\n obj = get_EgHistoricalDataSource(nid, ...)\n```\n</code></pre> <p>We can use both approaches 1) and 2) in the <code>DagBuilder</code> approach</p> <p>Personally I prefer approach 2) since it avoids to create more classes An OOP adage says \"prefer composition over inheritance when possible\"</p>"},{"location":"dataflow/all.computation_as_graphs.explanation.html","title":"Computation As Graphs","text":""},{"location":"dataflow/all.computation_as_graphs.explanation.html#kaizenflow-computing","title":"KaizenFlow computing","text":""},{"location":"dataflow/all.computation_as_graphs.explanation.html#introduction","title":"Introduction","text":"<p><code>KaizenFlow</code> is a computing framework to build and test AI/machine learning models that can run:</p> <ul> <li>Without any changes in batch (i.e., historical) vs streaming mode (i.e.,   real-time)</li> <li>Without any changes between replayed simulation (where events from a real or   synthetic execution are played back) vs real-time (where the events are coming   from a real-world time source)</li> <li>At different abstraction levels to trade off timing accuracy and speed, e.g.,</li> <li>Timed (aka event-accurate) simulation</li> <li>Non-timed (aka vectorized) simulation</li> </ul> <p>The working principle underlying <code>DataFlow</code> is to run a model in terms of time slices of data so that both batch/historical and streaming/real-time semantics can be accommodated without any change in the model description.</p> <p>Some of the advantages of the DataFlow approach are:</p> <ul> <li>Adapt a procedural description of a model to a reactive/streaming semantic</li> <li>Tiling to fit in memory</li> <li>Cached computation</li> <li>A clear timing semantic, which includes support for knowledge time and   detection of future peeking</li> <li>Ability to replay and debug model executions</li> </ul>"},{"location":"dataflow/all.computation_as_graphs.explanation.html#dag-node","title":"DAG Node","text":"<p>A DAG Node is a unit of computation in the DataFlow model.</p> <p>A DAG Node has:</p> <ul> <li>Inputs</li> <li>Outputs</li> <li>A unique node id (aka <code>nid</code>)</li> <li>A (optional) state Inputs and outputs to a DAG Node are dataframes   (represented as <code>Pandas</code> dataframes). DAG node uses the inputs to compute the   output (e.g., using <code>Pandas</code> and <code>Sklearn</code> libraries). A DAG node can execute   in multiple \"phases\", referred to through the corresponding methods called on   the DAG (e.g., <code>fit</code>, <code>predict</code>, <code>save_state</code>, <code>load_state</code>). A DAG node   stores an output value for each output and method name.</li> </ul> <p>TODO(gp): circle with inputs and outputs</p>"},{"location":"dataflow/all.computation_as_graphs.explanation.html#dag-node-examples","title":"DAG node examples","text":"<p>Examples of operations that may be performed by nodes include:</p> <ul> <li>Loading data (e.g., market or alternative data)</li> <li>Resampling data bars (e.g., OHLCV data, tick data in finance)</li> <li>Computing rolling average (e.g., TWAP/VWAP, volatility of returns)</li> <li>Adjusting returns by volatility</li> <li>Applying EMAs (or other filters) to signals</li> <li>Performing per-feature operations, each requiring multiple features</li> <li>Performing cross-sectional operations (e.g., factor residualization, Gaussian   ranking)</li> <li>Learning/applying a machine learning model (e.g., using sklearn)</li> <li>Applying custom (user-written) functions to data</li> </ul> <p>Further examples include nodes that maintain relevant trading state, or that interact with an external environment:</p> <ul> <li>Updating and processing current positions</li> <li>Performing portfolio optimization</li> <li>Generating trading orders</li> <li>Submitting orders to an API</li> </ul> <p>DataFlow model. A DataFlow model (aka <code>DAG</code>) is a direct acyclic graph composed of DataFlow nodes</p> <p>It allows to connect, query the structure, ...</p> <p>Running a method on a DAG means running that method on all its nodes in topological order, propagating values through the DAG nodes.</p> <p>TODO(gp): Add picture.</p> <p>DagConfig. A <code>Dag</code> can be built assembling Nodes using a function representing the connectivity of the nodes and parameters contained in a Config (e.g., through a call to a builder <code>DagBuilder.get_dag(config)</code>).</p> <p>A DagConfig is hierarchical and contains one subconfig per DAG node. It should only include <code>Dag</code> node configuration parameters, and not information about <code>Dag</code> connectivity, which is specified in the <code>Dag</code> builder part.</p>"},{"location":"dataflow/all.computation_as_graphs.explanation.html#dataframe-as-unit-of-computation","title":"DataFrame as unit of computation","text":"<p>The basic unit of computation of each node is a \"dataframe\". Each node takes multiple dataframes through its inputs, and emits one or more dataframes as outputs.</p> <p>In mathematical terms, a DataFrame can be described as a two-dimensional labeled data structure, similar to a matrix but with more flexible features.</p> <p>A DataFrame $\\mathbf{df}$ can be represented as:</p> <p>$$ \\mathbf{df} = \\left[ \\begin{array}{cccc} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\ \\end{array} \\right] $$</p> <p>Where:</p> <ul> <li>$m$ is the number of rows (observations).</li> <li>$n$ is the number of columns (variables).</li> <li>$a_{ij}$ represents the element of the DataFrame in the $i$-th row and $j$-th   column.</li> </ul> <p>Some characteristics of dataframes are:</p> <ol> <li>Labeled Axes:</li> <li>Rows and columns are labeled, typically with strings, but labels can be of      any hashable type.</li> <li> <p>Rows are often referred to as indices and columns as column headers.</p> </li> <li> <p>Heterogeneous Data Types:</p> </li> <li>Each column $j$ can have a distinct data type, denoted as $dtype_j$</li> <li> <p>Common data types include integers, floats, strings, and datetime objects.</p> </li> <li> <p>Mutable Size:</p> </li> <li>Rows and columns can be added or removed, meaning the size of $mathbf{df}$      is mutable.</li> <li> <p>This adds to the flexibility compared to a traditional matrix.</p> </li> <li> <p>Alignment and Operations:</p> </li> <li>DataFrames support alignment and arithmetic operations along rows and      columns.</li> <li> <p>Operations are often element-wise but can be customized with aggregation      functions.</p> </li> <li> <p>Missing Data Handling:</p> </li> <li>DataFrames can contain missing data, denoted as <code>NaN</code> or a similar      placeholder.</li> <li>They provide tools to handle, fill, or remove missing data.</li> </ol>"},{"location":"dataflow/all.computation_as_graphs.explanation.html#dag-execution","title":"DAG execution","text":""},{"location":"dataflow/all.computation_as_graphs.explanation.html#simulation-kernel","title":"Simulation kernel","text":"<p>A computation graph is a directed graph where nodes represent operations or variables, and edges represent dependencies between these operations.</p> <p>For example, in a computation graph for a mathematical expression, nodes would be operations like addition or multiplication, and edges would indicate which operations need to be completed before others.</p> <p>KaizenFlow simulation kernel schedules nodes according to their dependencies.</p>"},{"location":"dataflow/all.computation_as_graphs.explanation.html#implementation-of-simulation-kernel","title":"Implementation of simulation kernel","text":"<p>The most general case of simulation consists of multiple loops:</p> <ol> <li> <p>Multiple DAG computations, each one inferred through a config belonging to a    list of configs describing the entire workload</p> </li> <li> <p>Each DAG computation is independent, although pieces of computations can be   common across the workload. These computations will be cached automatically as   part of the framework execution</p> </li> <li> <p>For each simulation, multiple train/predict loops can represent different    learning styles (e.g., in-sample vs out-of-sample, cross-validation, rolling    window)</p> </li> <li> <p>This accommodates nodes with state</p> </li> <li> <p>Each single DAG execution runs over a period of time</p> </li> <li>The time dimension is partitioned in tiles, as explained in other sections      (see XYZ)</li> <li>Note that tiles over time might overlap to accommodate the amount of memory      needed by each node (see XYZ), thus each timestamp will be covered by at      least tile. In the case of DAG nodes with no memory, then time is      partitioned in non-overlapping tiles.</li> <li> <p>The tiling pattern over time doesn't affect the result as long as the      system is properly designed (see XYZ)</p> </li> <li> <p>Each temporal slice can be computed in terms of multiple sections across the    horizontal dimension of the dataframe inputs</p> </li> <li> <p>This is constrained by nodes that compute features cross-sectionally, which      require the entire space slice to be computed at once</p> </li> <li> <p>Finally a topological sorting based on the specific DAG connectivity is    performed in order to execute nodes in the proper order</p> </li> </ol> <p>Note that it is possible to represent all the above constraints in a single \"scheduling graph\" and use this graph to schedule executions in a global fashion.</p> <p>Parallelization across CPUs comes naturally from the previous approach, since computations that are independent in the scheduling graph can be executed in parallel.</p> <p>Incremental and cached computation is built-in in the scheduling algorithm since it's possible to memoize the output by checking for a hash of all the inputs and of the code in each node.</p> <p>Even though the computation DAG is required to have no loops, a System (see XYZ) can have components introducing loops in the computation (e.g., a Portfolio component in a trading system, where a DAG computes forecasts which are acted upon based on the available funds). In this case, the simulation kernel needs to enforce dependencies in the time dimension.</p>"},{"location":"dataflow/all.computation_as_graphs.explanation.html#nodes-ordering-for-execution","title":"Nodes ordering for execution","text":"<p>TODO(gp, Paul): Extend this to the multiple loop.</p> <p>Topological sorting is a linear ordering of the vertices of a directed graph such that for every directed edge from vertex u to vertex v, u comes before v in the ordering. This sorting is only possible if the graph has no directed cycles, i.e., it must be a Directed Acyclic Graph (DAG).</p> <pre><code>def topological_sort(graph):\n    visited = set()\n    post_order = []\n\n    def dfs(node):\n        if node in visited:\n            return\n        visited.add(node)\n        for neighbor in graph.get(node, []):\n            dfs(neighbor)\n        post_order.append(node)\n\n    for node in graph:\n        dfs(node)\n\n    return post_order[::-1]  # Reverse the post-order to get the topological order\n</code></pre>"},{"location":"dataflow/all.computation_as_graphs.explanation.html#heuristics-for-splitting-code-in-nodes","title":"Heuristics for splitting code in nodes","text":"<p>There are degrees of freedom in splitting the work between various nodes of a graph E.g., the same DataFlow computation can be described with several nodes or with a single node containing all the code</p> <p>The trade-off is often between several metrics:</p> <ul> <li>Observability</li> <li>More nodes make it easier to:<ul> <li>Observe and debug intermediate the result of complex computation</li> <li>Profile graph executions to understand performance bottlenecks</li> </ul> </li> <li>Latency/throughput</li> <li>More nodes:<ul> <li>Allow for better caching of computation</li> <li>Allow for smaller incremental computation when only one part of the inputs   change</li> <li>Prevent optimizations performed across nodes</li> <li>Incur in more simulation kernel overhead for scheduling</li> <li>Allow more parallelism between nodes being extracted and exploited</li> </ul> </li> <li>Memory consumption</li> <li>More nodes:<ul> <li>Allow to partition the computation in smaller chunks requiring less   working memory</li> </ul> </li> </ul> <p>A possible heuristics is to start with smaller nodes, where each node has a clear function, and then merge nodes if this is shown to improve performance</p>"},{"location":"dataflow/all.dag.explanation.html","title":"Debug information","text":"<p>There are switches in the <code>DAG</code> class that control how much debugging information it saves.</p> <ul> <li><code>save_node_io</code> decides whether a <code>DAG</code> instance writes output <code>DataFrame</code> to   disk for each <code>Node</code>, refer to the corresponding section   for details; The permissible values are:</li> <li><code>\"\"</code>: save nothing</li> <li><code>df_as_csv</code>: save output as CSV</li> <li><code>df_as_pq</code>: save output as Parquet</li> <li><code>df_as_csv_and_pq</code>: save output as both CSV and Parquet</li> <li><code>save_node_stats</code> decides whether a <code>DAG</code> instance writes statistics (e.g.,   size, columns, types, index) about output <code>DataFrame</code> to disk for each <code>Node</code>   or not</li> <li>Refer to the corresponding section for details</li> <li><code>profile_execution</code> decides whether a <code>DAG</code> instance writes information about   <code>Node</code> memory consumption and its execution time or not</li> <li>Refer to the corresponding section for details</li> </ul> <p>Worth noting that writing files to disk is expensive in terms of time so the corresponding switches should be turned on only when debugging a system run.</p> <p>When running a System all the data is stored at <code>.../dag/node_io</code>.</p>"},{"location":"dataflow/all.dag.explanation.html#node-output-data","title":"Node output data","text":"<p>The <code>DAG</code> class saves an output <code>DataFrame</code> for each <code>bar_timestamp</code> and each <code>Node</code> at <code>.../dag/node_io/node_io.data</code>. E.g.:</p> <pre><code>&gt; ls system_log_dir/dag/node_io/node_io.data/\n\npredict.0.read_data.df_out.20240123_080000.20240123_080011.parquet\npredict.0.read_data.df_out.20240123_080000.20240123_080011.csv\npredict.0.read_data.df_out.20240123_081200.20240123_081211.parquet\npredict.0.read_data.df_out.20240123_081200.20240123_081211.csv\npredict.1.generate_feature_panels.df_out.20240123_080000.20240123_080014.parquet\npredict.1.generate_feature_panels.df_out.20240123_080000.20240123_080014.csv\npredict.1.generate_feature_panels.df_out.20240123_081200.20240123_081211.parquet\npredict.1.generate_feature_panels.df_out.20240123_081200.20240123_081211.csv\n...\n</code></pre> <p>A file name follows the following pattern: <code>{method}.{topological_id}.{nid}.df_out.{bar_timestamp}.{wall_clock_time}.{extension}</code>, e.g., <code>predict.0.read_data.df_out.20240123_080000.20240123_080011.parquet</code></p> <ul> <li><code>method</code> is <code>fit</code> or <code>predict</code></li> <li><code>topological_id</code> is the <code>Node</code> id, e.g., <code>0</code> for the first <code>Node</code></li> <li><code>nid</code> is <code>Node</code> name, e.g., <code>read_data</code>, <code>resample</code></li> <li><code>bar_timestamp</code> is the timestamp of a bar for which a DAG was computed</li> <li><code>wall_clock_time</code> machine time when a <code>Node</code> is computed</li> <li><code>extension</code> file extensions, e.g., <code>csv</code> or <code>pq</code></li> </ul> <p>An output <code>DataFrame</code> is stored in the <code>DataFlow</code> format, i.e. indexed by timestamps and asset_ids, e.g.:</p> <pre><code>|                           |       open |            |            |      close |            |            |     volume |            |            |\n|---------------------------|-----------:|-----------:|-----------:|-----------:|-----------:|-----------:|-----------:|-----------:|-----------:|\n|                           | 1030828978 | 1464553467 | 8968126878 | 1030828978 | 1464553467 | 8968126878 | 1030828978 | 1464553467 | 8968126878 |\n| 2023-11-03 09:10:00-04:00 |   0.435053 |   0.316767 |   0.575763 |   0.435053 |   0.316767 |   0.002544 |   0.435053 |   0.316767 |   0.575763 |\n| 2023-11-03 09:15:00-04:00 |   0.707034 |   0.144804 |   0.123079 |   0.707034 |   0.144804 |   0.123079 |  0.707034  |   0.144804 |  0.144804  |\n</code></pre>"},{"location":"dataflow/all.dag.explanation.html#node-output-statistics","title":"Node output statistics","text":"<p>The <code>DAG</code> class saves a <code>TXT</code> file with statistics for each <code>bar_timestamp</code> and each <code>Node</code> at <code>.../dag/node_io/node_io.data</code>. E.g.:</p> <pre><code>&gt; ls system_log_dir/dag/node_io/node_io.data/ | grep \"txt\"\n\npredict.0.read_data.df_out.20240123_080000.20240123_080011.txt\npredict.0.read_data.df_out.20240123_081200.20240123_081211.txt\npredict.1.generate_feature_panels.df_out.20240123_080000.20240123_080014.txt\npredict.1.generate_feature_panels.df_out.20240123_081200.20240123_081211.txt\n...\n</code></pre> <p>File content:</p> <ul> <li>Index</li> <li>Columns</li> <li>Data types</li> <li>Memory by column</li> <li>Number of nans</li> <li>Munber of zeroes</li> <li>Dataframe</li> </ul> <p>E.g.:</p> <pre><code>&gt; cat system_log_dir/dag/node_io/node_io.data/predict.0.read_data.df_out.20240123_080000.20240123_080011.txt\nindex=[2024-01-23 07:48:00-05:00, 2024-01-23 08:00:00-05:00]\ncolumns=('open', 1030828978),('open', 1464553467),('open', 8968126878),('close', 1030828978),('close', 1464553467),('close', 8968126878),('volume', 1030828978),('volume', 1464553467),('volume', 8968126878)...\nshape=(13, 240)\n* type=\n| col_name | dtype                 | num_unique                       | num_nans          | first_elem     | type(first_elem)              |\n|----------|-----------------------|----------------------------------|-------------------|----------------|-------------------------------|\n| 0        | index                 | datetime64[ns, America/New_York] | 13 / 13 = 100.00% | 0 / 13 = 0.00% | 2024-01-23T12:48:00.000000000 |\n| 1        | ('open', 1030828978)  | float64                          | 12 / 13 = 92.31%  | 0 / 13 = 0.00% | 0.2483                        |\n| 2        | ('open', 1464553467)  | float64                          | 13 / 13 = 100.00% | 0 / 13 = 0.00% | 2212.24                       |\n| 3        | ('open', 8968126878)  | float64                          | 13 / 13 = 100.00% | 0 / 13 = 0.00% | 38854.5                       |\n| 4        | ('close', 1030828978) | float64                          | 12 / 13 = 92.31%  | 0 / 13 = 0.00% | 0.2483                        |\n| 5        | ('close', 1464553467) | float64                          | 13 / 13 = 100.00% | 0 / 13 = 0.00% | 2212.24                       |\n| 6        | ('close', 8968126878) | float64                          | 13 / 13 = 100.00% | 0 / 13 = 0.00% | 38854.5                       |\n| 7        | ('volume', 1030828978)| float64                          | 12 / 13 = 92.31%  | 0 / 13 = 0.00% | 0.2483                        |\n| 8        | ('volume', 1464553467)| float64                          | 13 / 13 = 100.00% | 0 / 13 = 0.00% | 2212.24                       |\n| 9        | ('volume', 8968126878)| float64                          | 13 / 13 = 100.00% | 0 / 13 = 0.00% | 38854.5                       |\n....\n* memory =\n|                     |         |         |\n|---------------------|---------|---------|\n|                     | shallow | deep    |\n| Index               | 660.0 b | 660.0 b |\n| (open, 1030828978)  | 104.0 b | 104.0 b |\n| (open, 1464553467)  | 104.0 b | 104.0 b |\n| (open, 8968126878)  | 104.0 b | 104.0 b |\n| (close, 1030828978) | 104.0 b | 104.0 b |\n| (close, 1464553467) | 104.0 b | 104.0 b |\n| (close, 8968126878) | 104.0 b | 104.0 b |\n| (volume, 1030828978)| 104.0 b | 104.0 b |\n| (volume, 1464553467)| 104.0 b | 104.0 b |\n| (volume, 8968126878)| 104.0 b | 104.0 b |\n|...                  |         |         |\n|total                | 25.0 KB | 45.3 KB |\nnum_nans=0 / 3120 = 0.00%\nnum_zeros=0 / 3120 = 0.00%\nnum_nan_rows=13 / 3120 = 0.42%\nnum_nan_cols=240 / 3120 = 7.69%\n...\n</code></pre>"},{"location":"dataflow/all.dag.explanation.html#profiling-statistics","title":"Profiling statistics","text":"<p>The <code>DAG</code> class saves <code>TXT</code> files with memory and time statistics for each <code>bar_timestamp</code> and before and after each <code>Node</code> is computed. The files are saved at <code>.../dag/node_io/node_io.prof</code>.</p> <p>The file written after <code>Node</code> execution contains the timestamp when the execution ended, the memory status, the run-time of the node and the memory difference.</p> <p>E.g.:</p> <pre><code>&gt; cat system_log_dir/dag/node_io/node_io.prof/predict.9.process_forecasts.after_execution.20240201_052000.txt\n    timestamp=20240201_102040\n    memory=rss=1.163GB vms=1.621GB mem_pct=15%\n    node_execution done (0.026 s)\n    run_node done (1.774 s)\n    run_node done: start=(1.163GB 1.621GB 15%) end=(1.163GB 1.621GB 15%) diff=(-0.000GB 0.000GB -0%)\n</code></pre>"},{"location":"dataflow/all.dataflow.explanation.html","title":"DataFlow","text":"<p>TODO(gp): Add an example of df with forecasts explaining the timing</p>"},{"location":"dataflow/all.dataflow.explanation.html#different-views-of-system-components","title":"Different views of System components","text":"<p>Different implementations of a component. A DataFlow component is described in terms of an interface and can have several implementations at different levels of detail.</p> <p>Reference implementation. A reference implementation is vendor-agnostic implementation of a component (e.g., DataFrameImClient, DataFrameBroker)</p> <p>Vendor implementation. A vendor implementation is a vendor-specific implementation of a component (e.g., CcxtImClient, CcxtBroker).</p> <p>Mocked implementation. A mocked implementation is a simulated version of a vendor-specific component (e.g., a DataFrameCcxtBroker). A mocked component can have the same timing semantics as the real-component (e.g., an asynchronous or reactive implementation) or not.</p>"},{"location":"dataflow/all.dataflow.explanation.html#architecture","title":"Architecture","text":"<p>In this section we summarize the responsibilities and the high level invariants of each component of a <code>System</code>.</p> <p>A <code>System</code> is represented in terms of a <code>Config</code>.</p> <ul> <li>Each piece of a <code>Config</code> refers to and configures a specific part of the   <code>System</code></li> <li>Each component should be completely configured in terms of a <code>Config</code></li> </ul>"},{"location":"dataflow/all.dataflow.explanation.html#component-invariants","title":"Component invariants","text":"<p>All data in components should be indexed by the knowledge time (i.e., when the data became available to that component) in terms of current time.</p> <p>Each component has a way to know:</p> <ul> <li>What is the current time (e.g., the real-time machine time or the simulated   one)</li> <li>The timestamp of the current data bar it's working on</li> </ul> <p>Each component</p> <ul> <li>Should print its state so that one can inspect how exactly it has been   initialized</li> <li>Can be serialized and deserialized from disk</li> <li>Can be mocked for simulating</li> <li>Should save data in a directory as it executes to make the system observable</li> </ul> <p>Models are described in terms of DAGs using the DataFlow framework</p> <p>Misc. Models read data from historical and real-time data sets, typically not mixing these two styles.</p> <p>Raw data is typically stored in S3 bucket in the same format as it comes or in Parquet format.</p>"},{"location":"dataflow/all.dataflow.explanation.html#dataflow-computing","title":"DataFlow computing","text":"<p>Resampling VWAP (besides potential errors). This implies hardcoded formula in a mix with resampling functions.</p> <pre><code>vwap_approach_2 = (\n        converted_data[\"close\"] *\n      converted_data[\"volume\"]).resample(resampling_freq)\n    ).mean() /\n    converted_data[\"volume\"].resample(resampling_freq).sum()\nvwap_approach_2.head(3)\n</code></pre> <ul> <li>TODO(gp): Explain this piece of code</li> </ul>"},{"location":"dataflow/all.dataflow.explanation.html#template-configs","title":"Template configs","text":"<ul> <li>Are incomplete configs, with some \"mandatory\" parameters unspecified but   clearly identified with <code>cconfig.DUMMY</code> value</li> <li>Have reasonable defaults for specified parameters</li> <li>This facilitates config extension (e.g., if we add additional parameters /     flexibility in the future, then we should not have to regenerate old     configs)</li> <li>Leave dummy parameters for frequently-varying fields, such as <code>ticker</code></li> <li>Should be completable and be completed before use</li> <li>Should be associated with a <code>Dag</code> builder</li> </ul> <p>DagBuilder. It is an object that builds a DAG and has a <code>get_config_template()</code> and a <code>get_dag()</code> method to keep the config and the Dag in sync.</p> <p>The client:</p> <ul> <li>Calls <code>get_config_template()</code> to receive the template config</li> <li>Fills / modifies the config</li> <li>Uses the final config to call <code>get_dag(config)</code> and get a fully built DAG</li> </ul> <p>A <code>DagBuilder</code> can be passed to other objects instead of <code>Dag</code> when the template config is fully specified and thus the <code>Dag</code> can be constructed from it.</p> <p>DagRunner. It is an object that allows to run a <code>Dag</code>. Different implementations of a <code>DagRunner</code> allow to run a <code>Dag</code> on data in different ways, e.g.,</p> <ul> <li><code>FitPredictDagRunner</code>: implements two methods <code>fit</code> / <code>predict</code> when we want   to learn on in-sample data and predict on out-of-sample data</li> <li><code>RollingFitPredictDagRunner</code>: allows to fit and predict on some data using a   rolling pattern</li> <li><code>IncrementalDagRunner</code>: allows to run one step at a time like in real-time</li> <li><code>RealTimeDagRunner</code>: allows to run using nodes that have a real-time semantic</li> </ul>"},{"location":"dataflow/all.dataflow_data_format.explanation.html","title":"DataFlow Data Format","text":"<p>As explained in /docs/datapull/all.datapull_client_stack.explanation.md, raw data from <code>DataPull</code> is stored in a \"long format\", where the data is conditioned on the asset (e.g., full_symbol), e.g.,</p> <pre><code>                             full_symbol        open    high    low     close ...\ntimestamp\n2021-09-01 00:00:00+00:00   binance::ADA_USDT   2.768   2.770   2.762   2.762\n2021-09-01 00:00:00+00:00   binance::AVAX_USDT  39.510  39.540  39.300 39.320\n2021-09-01 00:00:00+00:00   binance::ADA_USDT   2.763   2.765   2.761   2.764\n</code></pre> <p><code>DataFlow</code> represents data through multi-index dataframes, where:</p> <ul> <li>The index is a full timestamp</li> <li>The outermost column index is the \"feature\"</li> <li>The innermost column index is the asset, e.g.,</li> </ul> <pre><code>                                                            close           high\n                           binance::ADA_USDT   binance::AVAX_USDT            ...\ntimestamp\n2021-09-01 00:00:00+00:00              2.762                39.32\n2021-09-01 00:00:00+00:00              2.764                39.19\n</code></pre> <p>The reason for this convention is that typically features are computed in a uni-variate fashion (e.g., asset by asset), and DataFlow can vectorize computation over the assets by expressing operations in terms of the features. E.g., we can express a feature as</p> <pre><code>df[\"close\", \"open\"].max() - df[\"high\"]).shift(2)\n</code></pre> <p>A user can work with DataFlow at 4 levels of abstraction:</p> <ol> <li> <p>Pandas long-format (non multi-index) dataframes and for-loops</p> <ul> <li>We can do a group-by or filter by full_symbol</li> <li>Apply the transformation on each resulting dataframe</li> <li>Merge the data back into a single dataframe with the long-format</li> </ul> </li> <li> <p>Pandas multiindex dataframes</p> <ul> <li>The data is in the DataFlow native format</li> <li>We can apply the transformation in a vectorized way</li> <li>This approach is best for performance and with compatibility with DataFlow   point of view</li> <li>An alternative approach is to express multi-index transformations in terms   of approach 1 (i.e., single asset transformations and then concatenation).   This approach is functionally equivalent to a multi-index transformation,   but typically slow and memory inefficient</li> </ul> </li> <li> <p>DataFlow nodes</p> <ul> <li>A DataFlow node implements a certain transformations on DataFrames   according to the DataFlow convention and interfaces</li> <li>Nodes operate on the multi-index representation by typically calling   functions from level 2 above</li> </ul> </li> <li> <p>DAG</p> <ul> <li>A series of transformations in terms of DataFlow nodes</li> </ul> </li> </ol> <p>An example is /dataflow/notebooks/gallery_synthetic_data_example.ipynb</p>"},{"location":"dataflow/all.simulation_output.reference.html","title":"All.simulation output.reference","text":"<p># TODO(Danya): this is more general than simulation, the output format is common for all types of Systems (e.g., prod system).</p>"},{"location":"dataflow/all.simulation_output.reference.html#streaming-simulation-output","title":"Streaming simulation output","text":""},{"location":"dataflow/all.simulation_output.reference.html#example-of-the-output-of-the-simulation","title":"Example of the output of the simulation","text":"<ul> <li>The output is truncated to include only 2 files from each subdirectory</li> </ul> <pre><code>&gt; i docker_bash\n&gt; cd /shared_data/ecs/preprod/system_reconciliation/C5b/paper_trading/20240115_131000.20240116_130500/system_log_dir.scheduled\n&gt; find . | sed -e \"s/[^-][^\\/]*\\// |/g\" -e \"s/|\\([^ ]\\)/|-\\1/\"\n.\n |-process_forecasts\n | |-portfolio\n | | |-executed_trades_notional\n | | | |-20240115_124500.20240115_124531.csv\n | | | |-20240116_073000.20240116_073050.csv\n | | |-executed_trades_shares\n | | | |-20240115_124500.20240115_124531.csv\n | | | |-20240115_182000.20240115_182033.csv\n | | |-holdings_shares\n | | | |-20240115_124500.20240115_124531.csv\n | | | |-20240115_182000.20240115_182033.csv\n | | |-statistics\n | | | |-20240115_124500.20240115_124531.csv\n | | | |-20240115_182000.20240115_182033.csv\n | | |-holdings_notional\n | | | |-20240115_124500.20240115_124531.csv\n | | | |-20240115_182000.20240115_182033.csv\n | |-target_positions\n | | |-20240115_124500.20240115_124531.csv\n | | |-20240115_182000.20240115_182033.csv\n | |-orders\n | | |-20240115_124500.20240115_124531.csv\n | | |-20240115_182000.20240115_182033.csv\n |-system_config.output.values_as_strings.pkl\n |-dag\n | |-node_io\n | | |-node_io.prof\n | | | |-predict.9.process_forecasts.before_execution.20240115_190000.txt\n | | | |-predict.5.compress_rets.after_execution.20240116_062500.txt\n | | |-node_io.data\n | | | |-predict.4.adjust_rets.df_out.20240115_235500.20240115_235526.parquet\n | | | |-predict.2.compute_vol.df_out.20240116_040000.20240116_040027.parquet\n |-system_config.input.txt\n |-system_config.output.txt\n |-system_config.input.values_as_strings.pkl\n</code></pre>"},{"location":"dataflow/all.simulation_output.reference.html#config","title":"Config","text":"<p>For explanation of <code>SystemConfig</code> structure and <code>SystemConfig</code> examples, refer to docs/kaizenflow/ck.system_config.explanation.md</p>"},{"location":"dataflow/all.simulation_output.reference.html#process_forecasts","title":"Process_forecasts","text":""},{"location":"dataflow/all.simulation_output.reference.html#portfolio","title":"Portfolio","text":"<p># TODO(Danya): the section should refer to a Portfolio doc, since the files just represent Portfolio state.</p>"},{"location":"dataflow/all.simulation_output.reference.html#executed_trades_notional","title":"executed_trades_notional","text":"<ul> <li>Total notional value of executed trades (in dollars), indexed by order ID and   timestamp</li> <li>Positive value for buys, negative value for sells</li> </ul> <p>Example:</p> <pre><code>&gt; csvlook portfolio/executed_trades_notional/20240115_081000.20240115_081029.csv\n| a                                | 6051632686 | 8717633868 | 2540896331 | 1528092593 | 8968126878 | 1467591036 | 5115052901 | 3065029174 | 1891737434 | 3401245610 | 1464553467 | 1966583502 | 1030828978 | 2601760471 | 2683705052 | 9872743573 | 2484635488 | 2099673105 | 4516629366 | 2237530510 | 2425308589 | 1776791608 | 2384892553 | 5118394986 |\n| -------------------------------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- |\n| 2024-01-15 08:15:28.918502-05:00 |     63.774 |   -72.493\u2026 |   -55.294\u2026 |    52.737\u2026 |  -231.655\u2026 |  -469.389\u2026 |    61.316\u2026 |    98.954\u2026 |   -66.558\u2026 |   -44.272\u2026 |   289.258\u2026 |   -51.685\u2026 |    76.234\u2026 |   -65.109\u2026 |   -83.608\u2026 |   -46.724\u2026 |  -255.200\u2026 |    78.367\u2026 |   116.388\u2026 |    94.789\u2026 |    55.854\u2026 |     -96.39 |    64.439\u2026 |   414.729\u2026 |\n</code></pre>"},{"location":"dataflow/all.simulation_output.reference.html#executed_trades_shares","title":"executed_trades_shares","text":"<ul> <li>Total number of shares in executed trades, indexed by timestamp and order ID</li> <li>Positive value for buys, negative value for sells</li> </ul> <p>Example:</p> <pre><code>&gt; csvlook portfolio/executed_trades_shares/20240115_081500.20240115_081529.csv\n| a                                | 6051632686 | 8717633868 | 2540896331 | 1528092593 | 8968126878 | 1467591036 | 5115052901 | 3065029174 | 1891737434 | 3401245610 | 1464553467 | 1966583502 | 1030828978 | 2601760471 | 2683705052 | 9872743573 | 2484635488 | 2099673105 | 4516629366 | 2237530510 | 2425308589 | 1776791608 | 2384892553 | 5118394986 |\n| -------------------------------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- |\n| 2024-01-15 08:15:28.918502-05:00 |         45 |         -2 |         -7 |        139 |      -0.73 |     -0.011 |         87 |      1,217 |       -8.8 |      -15.8 |      0.114 |       -130 |        247 |      -4.12 |        -97 |        -14 |     -1,514 |         17 |        233 |          1 |         93 |      -15.3 |       26.8 |      717.4 |\n</code></pre>"},{"location":"dataflow/all.simulation_output.reference.html#holdings_notional","title":"holdings_notional","text":"<ul> <li>Notional of held shares at the beginning of the bar in dollars, based on   number of shares and price</li> <li>Positive for long positions, negative for short positions</li> </ul> <p>Example:</p> <pre><code>&gt; csvlook portfolio/holdings_notional/20240115_081000.20240115_081029.csv\n| a                                | 1030828978 | 1464553467 | 1467591036 | 1528092593 | 1776791608 | 1891737434 | 1966583502 | 2099673105 | 2237530510 | 2384892553 | 2425308589 | 2484635488 | 2540896331 | 2601760471 | 2683705052 | 3065029174 | 3401245610 | 4516629366 | 5115052901 | 5118394986 | 6051632686 | 8717633868 | 8968126878 | 9872743573 |\n| -------------------------------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- |\n| 2024-01-15 08:15:28.918502-05:00 |    76.234\u2026 |   289.258\u2026 |  -469.389\u2026 |    52.737\u2026 |     -96.39 |   -66.558\u2026 |   -51.685\u2026 |    78.367\u2026 |    94.789\u2026 |    64.439\u2026 |    55.854\u2026 |  -255.200\u2026 |   -55.294\u2026 |   -65.109\u2026 |   -83.608\u2026 |    98.954\u2026 |   -44.272\u2026 |   116.388\u2026 |    61.316\u2026 |   414.729\u2026 |     63.774 |   -72.493\u2026 |  -231.655\u2026 |   -46.724\u2026 |\n</code></pre>"},{"location":"dataflow/all.simulation_output.reference.html#holdings_shares","title":"holdings_shares","text":"<ul> <li>Number of held shares at the beginning of the bar</li> <li>Positive for long positions, negative for short positions</li> </ul> <p>Example:</p> <pre><code>&gt; csvlook portfolio/holdings_shares/20240115_081500.20240115_081529.csv\n| a                                | 6051632686 | 8717633868 | 2540896331 | 1528092593 | 8968126878 | 1467591036 | 5115052901 | 3065029174 | 1891737434 | 3401245610 | 1464553467 | 1966583502 | 1030828978 | 2601760471 | 2683705052 | 9872743573 | 2484635488 | 2099673105 | 4516629366 | 2237530510 | 2425308589 | 1776791608 | 2384892553 | 5118394986 |\n| -------------------------------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- |\n| 2024-01-15 08:15:28.918502-05:00 |         45 |         -2 |         -7 |        139 |      -0.73 |     -0.011 |         87 |      1,217 |       -8.8 |      -15.8 |      0.114 |       -130 |        247 |      -4.12 |        -97 |        -14 |     -1,514 |         17 |        233 |          1 |         93 |      -15.3 |       26.8 |      717.4 |\n</code></pre>"},{"location":"dataflow/all.simulation_output.reference.html#statistics","title":"statistics","text":"<ul> <li>Various statistics calculated based on simulation execution inside the bar</li> </ul> <p>Example:</p> <pre><code>&gt; csvlook portfolio/statistics/20240115_081500.20240115_081529.csv\n| a                                | pnl | gross_volume | net_volume |        gmv |      nmv |     cash | net_wealth | leverage |\n| -------------------------------- | --- | ------------ | ---------- | ---------- | -------- | -------- | ---------- | -------- |\n| 2024-01-15 08:15:28.918502-05:00 |   0 |   3,005.216\u2026 |   -71.539\u2026 | 3,005.216\u2026 | -71.539\u2026 | 771.539\u2026 |   700.000\u2026 |   4.293\u2026 |\n</code></pre>"},{"location":"dataflow/all.simulation_output.reference.html#orders","title":"Orders","text":"<ul> <li>Examples of orders submitted in the simulation</li> </ul> <p>Example:</p> <pre><code>&gt; head -3 orders/20240115_124500.20240115_124531.csv\nOrder: order_id=0 creation_timestamp=2024-01-15 08:10:29.273867-05:00 asset_id=6051632686 type_=price@twap start_timestamp=2024-01-15 08:10:29.273867-05:00 end_timestamp=2024-01-15 08:15:00-05:00 curr_num_shares=0.0 diff_num_shares=45.0 tz=America/New_York extra_params={}\nOrder: order_id=1 creation_timestamp=2024-01-15 08:10:29.273867-05:00 asset_id=8717633868 type_=price@twap start_timestamp=2024-01-15 08:10:29.273867-05:00 end_timestamp=2024-01-15 08:15:00-05:00 curr_num_shares=0.0 diff_num_shares=-2.0 tz=America/New_York extra_params={}\nOrder: order_id=2 creation_timestamp=2024-01-15 08:10:29.273867-05:00 asset_id=2540896331 type_=price@twap start_timestamp=2024-01-15 08:10:29.273867-05:00 end_timestamp=2024-01-15 08:15:00-05:00 curr_num_shares=0.0 diff_num_shares=-7.0 tz=America/New_York extra_params={}\n</code></pre>"},{"location":"dataflow/all.simulation_output.reference.html#target_positions","title":"Target_positions","text":"<ul> <li>Orders that are planned to execute at the beginning of the bar, projected   price and other stats</li> <li>Calculated in <code>oms/order_processing/target_positions_and_order_generator.py</code></li> </ul> <pre><code>&gt; csvlook target_positions/20240115_081500.20240115_081529.csv\n|      asset_id | holdings_shares |       price | holdings_notional | wall_clock_timestamp             | prediction | volatility | spread | target_holdings_notional | target_trades_notional | target_trades_shares | target_holdings_shares | target_holdings_shares.before_apply_cc_limits | target_holdings_notional.before_apply_cc_limits | target_trades_shares.before_apply_cc_limits | target_trades_notional.before_apply_cc_limits |\n| ------------- | --------------- | ----------- | ----------------- | -------------------------------- | ---------- | ---------- | ------ | ------------------------ | ---------------------- | -------------------- | ---------------------- | --------------------------------------------- | ----------------------------------------------- | ------------------------------------------- | --------------------------------------------- |\n| 6,051,632,686 |          45.000 |      1.417\u2026 |           63.774\u2026 | 2024-01-15 08:15:28.918502-05:00 |    -0.881\u2026 |     0.001\u2026 |  False |                -358.552\u2026 |              -422.326\u2026 |             -298.000 |              -253.000\u2026 |                                     -253.000\u2026 |                                       -358.552\u2026 |                                    -298.000 |                                     -422.326\u2026 |\n| 8,717,633,868 |          -2.000 |     36.247\u2026 |          -72.493\u2026 | 2024-01-15 08:15:28.918502-05:00 |     0.909\u2026 |     0.002\u2026 |  False |                  72.493\u2026 |               144.986\u2026 |                4.000 |                 2.000\u2026 |                                        2.000\u2026 |                                         72.493\u2026 |                                       4.000 |                                      144.986\u2026 |\n| 2,540,896,331 |          -7.000 |      7.899\u2026 |          -55.294\u2026 | 2024-01-15 08:15:28.918502-05:00 |     0.003\u2026 |     0.002\u2026 |  False |                 118.488\u2026 |               173.782\u2026 |               22.000 |                15.000\u2026 |                                       15.000\u2026 |                                        118.488\u2026 |                                      22.000 |                                      173.782\u2026 |\n| 1,528,092,593 |         139.000 |      0.379\u2026 |           52.737\u2026 | 2024-01-15 08:15:28.918502-05:00 |    -0.278\u2026 |     0.002\u2026 |  False |                 -63.739\u2026 |              -116.476\u2026 |             -307.000 |              -168.000\u2026 |                                     -168.000\u2026 |                                        -63.739\u2026 |                                    -307.000 |                                     -116.476\u2026 |\n| 8,968,126,878 |          -0.730 |    317.336\u2026 |         -231.655\u2026 | 2024-01-15 08:15:28.918502-05:00 |     0.723\u2026 |     0.003\u2026 |  False |                  31.734\u2026 |               263.389\u2026 |                0.830 |                 0.100\u2026 |                                        0.100\u2026 |                                         31.734\u2026 |                                       0.830 |                                      263.389\u2026 |\n| 1,467,591,036 |          -0.011 | 42,671.700\u2026 |         -469.389\u2026 | 2024-01-15 08:15:28.918502-05:00 |    -0.588\u2026 |     0.001\u2026 |  False |                -298.702\u2026 |               170.687\u2026 |                0.004 |                -0.007\u2026 |                                       -0.007\u2026 |                                       -298.702\u2026 |                                       0.004 |                                      170.687\u2026 |\n| 5,115,052,901 |          87.000 |      0.705\u2026 |           61.316\u2026 | 2024-01-15 08:15:28.918502-05:00 |     0.431\u2026 |     0.004\u2026 |  False |                  20.439\u2026 |               -40.877\u2026 |              -58.000 |                29.000\u2026 |                                       29.000\u2026 |                                         20.439\u2026 |                                     -58.000 |                                      -40.877\u2026 |\n| 3,065,029,174 |       1,217.000 |      0.081\u2026 |           98.954\u2026 | 2024-01-15 08:15:28.918502-05:00 |    -0.257\u2026 |     0.001\u2026 |  False |                -164.409\u2026 |              -263.363\u2026 |           -3,239.000 |            -2,022.000\u2026 |                                   -2,022.000\u2026 |                                       -164.409\u2026 |                                  -3,239.000 |                                     -263.363\u2026 |\n| 1,891,737,434 |          -8.800 |      7.563\u2026 |          -66.558\u2026 | 2024-01-15 08:15:28.918502-05:00 |    -0.145\u2026 |     0.002\u2026 |  False |                 -86.979\u2026 |               -20.421\u2026 |               -2.700 |               -11.500\u2026 |                                      -11.500\u2026 |                                        -86.979\u2026 |                                      -2.700 |                                      -20.421\u2026 |\n| 3,401,245,610 |         -15.800 |      2.802\u2026 |          -44.272\u2026 | 2024-01-15 08:15:28.918502-05:00 |    -0.548\u2026 |     0.002\u2026 |  False |                 -63.325\u2026 |               -19.054\u2026 |               -6.800 |               -22.600\u2026 |                                      -22.600\u2026 |                                        -63.325\u2026 |                                      -6.800 |                                      -19.054\u2026 |\n| 1,464,553,467 |           0.114 |  2,537.348\u2026 |          289.258\u2026 | 2024-01-15 08:15:28.918502-05:00 |     0.075\u2026 |     0.001\u2026 |  False |                 332.393\u2026 |                43.135\u2026 |                0.017 |                 0.131\u2026 |                                        0.131\u2026 |                                        332.393\u2026 |                                       0.017 |                                       43.135\u2026 |\n| 1,966,583,502 |        -130.000 |      0.398\u2026 |          -51.685\u2026 | 2024-01-15 08:15:28.918502-05:00 |     0.523\u2026 |     0.002\u2026 |  False |                  77.926\u2026 |               129.611\u2026 |              326.000 |               196.000\u2026 |                                      196.000\u2026 |                                         77.926\u2026 |                                     326.000 |                                      129.611\u2026 |\n| 1,030,828,978 |         247.000 |      0.309\u2026 |           76.234\u2026 | 2024-01-15 08:15:28.918502-05:00 |    -0.305\u2026 |     0.003\u2026 |  False |                 -49.691\u2026 |              -125.925\u2026 |             -408.000 |              -161.000\u2026 |                                     -161.000\u2026 |                                        -49.691\u2026 |                                    -408.000 |                                     -125.925\u2026 |\n| 2,601,760,471 |          -4.120 |     15.803\u2026 |          -65.109\u2026 | 2024-01-15 08:15:28.918502-05:00 |    -0.540\u2026 |     0.004\u2026 |  False |                 -22.757\u2026 |                42.353\u2026 |                2.680 |                -1.440\u2026 |                                       -1.440\u2026 |                                        -22.757\u2026 |                                       2.680 |                                       42.353\u2026 |\n| 2,683,705,052 |         -97.000 |      0.862\u2026 |          -83.608\u2026 | 2024-01-15 08:15:28.918502-05:00 |     0.063\u2026 |     0.002\u2026 |  False |                  72.403\u2026 |               156.011\u2026 |              181.000 |                84.000\u2026 |                                       84.000\u2026 |                                         72.403\u2026 |                                     181.000 |                                      156.011\u2026 |\n| 9,872,743,573 |         -14.000 |      3.337\u2026 |          -46.724\u2026 | 2024-01-15 08:15:28.918502-05:00 |    -0.538\u2026 |     0.002\u2026 |  False |                 -66.748\u2026 |               -20.024\u2026 |               -6.000 |               -20.000\u2026 |                                      -20.000\u2026 |                                        -66.748\u2026 |                                      -6.000 |                                      -20.024\u2026 |\n| 2,484,635,488 |      -1,514.000 |      0.169\u2026 |         -255.200\u2026 | 2024-01-15 08:15:28.918502-05:00 |     0.351\u2026 |     0.004\u2026 |  False |                  23.093\u2026 |               278.293\u2026 |            1,651.000 |               137.000\u2026 |                                      137.000\u2026 |                                         23.093\u2026 |                                   1,651.000 |                                      278.293\u2026 |\n| 2,099,673,105 |          17.000 |      4.610\u2026 |           78.367\u2026 | 2024-01-15 08:15:28.918502-05:00 |     0.268\u2026 |     0.002\u2026 |  False |                 115.245\u2026 |                36.878\u2026 |                8.000 |                25.000\u2026 |                                       25.000\u2026 |                                        115.245\u2026 |                                       8.000 |                                       36.878\u2026 |\n| 4,516,629,366 |         233.000 |      0.500\u2026 |          116.388\u2026 | 2024-01-15 08:15:28.918502-05:00 |    -0.113\u2026 |     0.002\u2026 |  False |                -110.394\u2026 |              -226.782\u2026 |             -454.000 |              -221.000\u2026 |                                     -221.000\u2026 |                                       -110.394\u2026 |                                    -454.000 |                                     -226.782\u2026 |\n| 2,237,530,510 |           1.000 |     94.789\u2026 |           94.789\u2026 | 2024-01-15 08:15:28.918502-05:00 |    -0.054\u2026 |     0.002\u2026 |  False |                 -94.789\u2026 |              -189.578\u2026 |               -2.000 |                -1.000\u2026 |                                       -1.000\u2026 |                                        -94.789\u2026 |                                      -2.000 |                                     -189.578\u2026 |\n| 2,425,308,589 |          93.000 |      0.601\u2026 |           55.854\u2026 | 2024-01-15 08:15:28.918502-05:00 |    -0.000\u2026 |     0.002\u2026 |  False |                  99.096\u2026 |                43.242\u2026 |               72.000 |               165.000\u2026 |                                      165.000\u2026 |                                         99.096\u2026 |                                      72.000 |                                       43.242\u2026 |\n| 1,776,791,608 |         -15.300 |      6.300\u2026 |          -96.390\u2026 | 2024-01-15 08:15:28.918502-05:00 |     0.508\u2026 |     0.001\u2026 |  False |                 152.460\u2026 |               248.850\u2026 |               39.500 |                24.200\u2026 |                                       24.200\u2026 |                                        152.460\u2026 |                                      39.500 |                                      248.850\u2026 |\n| 2,384,892,553 |          26.800 |      2.404\u2026 |           64.439\u2026 | 2024-01-15 08:15:28.918502-05:00 |     1.019\u2026 |     0.003\u2026 |  False |                  51.215\u2026 |               -13.224\u2026 |               -5.500 |                21.300\u2026 |                                       21.300\u2026 |                                         51.215\u2026 |                                      -5.500 |                                      -13.224\u2026 |\n| 5,118,394,986 |         717.400 |      0.578\u2026 |          414.729\u2026 | 2024-01-15 08:15:28.918502-05:00 |    -0.645\u2026 |     0.001\u2026 |  False |                -429.644\u2026 |              -844.373\u2026 |           -1,460.600 |              -743.200\u2026 |                                     -743.200\u2026 |                                       -429.644\u2026 |                                  -1,460.600 |                                     -844.373\u2026 |\n</code></pre>"},{"location":"dataflow/all.simulation_output.reference.html#dag","title":"DAG","text":"<p># TODO(Grisha): Describe the DAG folder contents.</p>"},{"location":"dataflow/all.time_series.explanation.html","title":"Time series","text":""},{"location":"dataflow/all.time_series.explanation.html#overview","title":"Overview","text":"<ul> <li>Each price data source (Kibot, Quandl CHRIS, Quandl SCF, ...) is idiosyncratic   with respect to, e.g.,</li> <li>What price resolution is available (e.g., 1min, 5mins, daily)</li> <li>How prices are labeled (e.g., at the beginning or at the end of the     interval)</li> <li>How intervals are interpreted (e.g., [a, b) vs (a, b])</li> <li> <p>What fields are available (close-only, OHLC, L1, L2, ...)</p> </li> <li> <p>We want to adopt clear and uniform internal conventions to help us reason   about the data and to minimize any mistakes due to misinterpretations</p> </li> </ul>"},{"location":"dataflow/all.time_series.explanation.html#guiding-principles-and-conventions","title":"Guiding principles and conventions","text":"<ol> <li>The primary time associated with a value should be the \"knowledge time\"</li> <li>Some time series naturally have multiple times associated with them, e.g.,      end of collection period, publication time, our knowledge time<ul> <li>E.g., EIA data is natively labeled according to the survey week, which is    not made available until the following week</li> </ul> </li> <li>Adopting knowledge times by default affords protection against      future-peeking</li> <li>Unfortunately, knowledge times for historical data may need to be      estimated. We may also want to impute knowledge times in the event that      real-time collection fails on our end.</li> <li>Knowledge times are always represented as datetimes to, e.g., avoid ambiguity    stemming from a date without an hour</li> <li>E.g., for daily closing price data with a label such as \"2019-01-04\", the      label should be converted to \"2019-01-04 16:00:00 ET\"</li> <li>Additional information may be required for this conversion (e.g., trading      calendars for instruments, data release times for government data, etc.)</li> <li>All datetimes should have a timezone<ul> <li>In pandas we may not explicitly make a series <code>tz</code>-aware for performance    reasons. In this case we encode the timezone in the column name</li> </ul> </li> <li>Time series represented in pandas Series or DataFrames are indexed by their    knowledge datetimes</li> <li>We use left-closed right-open time intervals (e.g., <code>[a, b)</code>) and choose the    right endpoint (e.g., <code>b</code>) as the interval label</li> <li>This labeling convention respects knowledge times and behaves well under      downsampling</li> <li>In the ideal setting (e.g., instantaneous computation and execution),      information from <code>[a, b)</code> could be acted upon at time <code>b</code></li> <li>Whenever we use the pandas <code>resample</code> function, we adopt the conventions      (<code>closed=\"left\", label=\"right\"</code>)</li> </ol>"},{"location":"dataflow/all.time_series.explanation.html#prices-and-returns","title":"Prices and returns","text":"<ul> <li>We call the \"closing\" price the last price quote in an interval <code>[a, b)</code>, and   we label it with time <code>b</code>. In series/dataframes, we label this price series   with <code>close</code>. An \"instantaneous\" price at time <code>b</code> we also label in this way   (assuming in practice that it is equivalent to the end-of-interval price of   <code>[a, b)</code>).</li> <li>Given a price time series following these conventions, say <code>prices</code>, we   calculate returns using</li> <li><code>prices</code> and <code>prices.shift(1)</code><ul> <li>We typically implicitly assume a uniform time grid</li> <li>In using a uniform grid, consecutive times may be represented as <code>t - 1</code>,   <code>t</code>, <code>t + 1</code>, etc., where <code>1</code> is understood to be with respect to the   sampling frequency</li> </ul> </li> <li>Time labels from <code>prices</code></li> <li>We call returns calculated in this way <code>ret_0</code> by convention</li> <li>In general, <code>ret_0</code> at time <code>t</code> is the return realized upon exiting at time     <code>t</code> a position held at time <code>t - 1</code><ol> <li>Note that <code>ret_0</code> at time <code>t</code> is observable at time <code>t</code></li> <li>We use <code>ret_j</code> to denote <code>ret_0.shift(-j)</code></li> </ol> </li> <li>If, e.g., it takes one time interval to enter a position and one time     interval to exit, then to realize <code>ret_0</code> at time <code>t</code>, a decision to enter     must be made by time <code>t - 2</code></li> <li>We aim to predict forward returns, e.g., <code>ret_j</code> for <code>j &gt; 0</code></li> <li>In the ideal setting for \"instantaneous\" prices, we can come close to     achieving <code>ret_1</code></li> <li>Achieving <code>ret_2</code> is subject to fewer constraints (one time step to enter a     position, one time step to exit)</li> <li>If prices represent aggregated prices (e.g., twap or vwap), then in the     ideal setting <code>ret_2</code> is the earliest realizable return</li> </ul>"},{"location":"dataflow/all.time_series.explanation.html#aligning-predictors-and-responses","title":"Aligning predictors and responses","text":"<ul> <li>Typically, a prediction at time <code>t_0</code> of the time <code>t_0</code> response value   <code>resp_0</code> is not actionable. Therefore we actually want to predict forward   response values (using the same timing conventions as <code>rets</code>; so <code>resp_n</code> for   the forward response <code>n</code> steps ahead).</li> <li>For convenience, we want to align (e.g., put in the same dataframe row)   predictors with the corresponding response value that we are using the   predictors to predict. E.g., if we are predicting <code>ret_2</code>, then we want to   align row-wise the predictor and <code>ret_2</code> columns.</li> <li>We have essentially two equivalent ways of performing the alignment:</li> <li>Shift the predictors</li> <li>Shift the response</li> <li>If we shift the response:</li> <li>Predictor knowledge times are preserved</li> <li>In real-time mode, predictor timestamps correspond to \"now\" rather than the     future</li> <li>Multiple forward returns can be used simultaneously (e.g., if we want to     predict a forward curve / optimize how many unit steps ahead to predict)</li> <li>If we shift (lag) the predictor:</li> <li>The return semantics are always clear (especially so if we ever restrict     returns windows to ATH, etc., violating a uniform-grid assumption)</li> <li>Causality is respected in the sense that at any given datetime (row),     everything in that row or preceding it is known (however, we know the     \"future\" values of predictors)</li> <li>A reasonable default would be to</li> <li>Enforce a uniform grid on the response variables (e.g., use <code>freq</code> for the     dataframes)</li> <li>Shift and rename the response column to be explicit about what is being     predicted and when</li> <li>Do not change the predictor timestamps (treat them as knowledge times)</li> </ul>"},{"location":"dataflow/all.timing_semantic_and_clocks.html","title":"Timing Semantic And Clocks","text":""},{"location":"dataflow/all.timing_semantic_and_clocks.html#time-semantics","title":"Time semantics","text":"<p>Time semantics. Any DataFlow component can be executed in real-time or simulated accounting for different ways to represent the passing of time.</p> <p>E.g., it can be simulated depending on how data is delivered to the system</p> <ul> <li>A streaming (aka timed simulation according to knowledge time) or</li> <li>Batch (non-timed simulation, with different tile chunking)</li> </ul> <p>Clock. A function that reports the current timestamp. There are 3 versions of clock:</p> <ol> <li> <p>Static clock. A clock that remains the same during a system run.</p> <p>a. Future peeking is allowed</p> </li> <li> <p>Replayed clock. A moving clock that can be in the past or future with     respect to a real clock</p> <p>a. Use time passing at the same pace of real-time wall-clock or</p> <p>b. Simulate time based on events, e.g., as soon as the workload corresponding to one timestamp is complete we move to the next timestamp, without waiting for the actual time to pass</p> <p>c. Future peeking is technically possible but is prohibited</p> </li> <li> <p>Real clock. The wall-clock time matches what we observe in real-life, data     is provided to processing units as it is produced by systems.</p> <p>a. Future peeking is not possible in principle</p> </li> </ol> <p>Knowledge time. It is the time when data becomes available (e.g., downloaded or computed) to a system. Each row of data is tagged with the corresponding knowledge time. Data with knowledge time after the current clock time must not be observable in order to avoid future peeking.</p> <p>Timed simulation. Sometimes referred to as historical, vectorized, bulk, batch simulation. In a timed simulation the data is provided with a clock that reports the current timestamp. Data with knowledge time after the current timestamp must not be observable in order to avoid future peeking.</p> <p>TODO(gp): Add an example of df with forecasts explaining the timing</p> <p>Non-timed simulation. (Sometimes referred to as event-based, reactive simulation). Clock type is \"static clock\". Typically wall-clock time is a timestamp that corresponds to the latest knowledge time (or greater) in a dataframe. In this way all data in a dataframe is available because every row has a knowledge time that is less than or equal to the wall-clock time. Note that the clock is static, i.e. not moving. In a non-timed simulation, the data is provided in a dataframe for the entire period of interest.</p> <p>E.g., for a system predicting every 5 mins, all the input data are equally spaced on a 5-min grid and indexed with knowledge time.</p> <p>TODO(gp): Add an example of df with forecasts explaining the timing</p> <pre><code>df[\"c\"] = (df[\"a\"] + df[\"b\"]).shift(1)\n</code></pre> <p>Real-time execution. In real-time the clock type is \"real clock\".</p> <p>E.g., for a system predicting every 5 mins, one forecast is delivered every 5 mins of wall-clock.</p> <p>TODO(Grisha): add an example.</p> <p>Replayed simulation. In replayed simulation, the data is provided in the same \"format\" and with the same timing as it would be provided in real-time, but the clock type is \"replayed clock\".</p>"},{"location":"dataflow/all.timing_semantic_and_clocks.html#how-clock-is-handled","title":"How clock is handled","text":""},{"location":"dataflow/all.timing_semantic_and_clocks.html#asynchronous-mode","title":"Asynchronous mode","text":"<ul> <li>In asynchronous mode there are multiple things happening at the same time</li> <li>E.g., DAG computes, orders are sent to the market, some components wait</li> <li>It is implemented using Python <code>asyncio</code></li> <li>In general one should need multiple CPUs to simulate/execute a truly     asynchronous system</li> <li>E.g, one CPU executes/simulates the DAG, another CPU executes/simulates the     <code>Portfolio</code>, etc.</li> </ul>"},{"location":"dataflow/all.timing_semantic_and_clocks.html#synchronous-mode","title":"Synchronous mode","text":"<ul> <li>In synchronous mode only one thing happens at the same time</li> <li>E.g., executing a piece of code using Pandas</li> </ul>"},{"location":"dataflow/all.timing_semantic_and_clocks.html#async-vs-sync-simulation","title":"Async vs sync simulation","text":"<ul> <li> <p>We can simulate the same system in sync or async mode</p> </li> <li> <p>Sync</p> </li> <li>The DAG computes</li> <li>Passes the df to OMS</li> <li> <p>The OMS executes orders, updates the Portfolio, ...</p> </li> <li> <p>Async</p> </li> <li>Create different objects that are always active and need to block on each     other</li> <li>Under certain constraints (e.g., when I/O overlaps with computation) a     single CPU can run/simulate a truly asynchronous system</li> </ul>"},{"location":"dataflow/all.timing_semantic_and_clocks.html#some-cross-products-of-the-3-directions","title":"Some cross-products of the 3 directions","text":"<ul> <li>Not all the combinations are possible of mixing:</li> <li>Historical vs replayed vs real-time</li> <li>Reference vs mocked vs implemented</li> <li> <p>Async vs sync</p> </li> <li> <p>The execution of a DAG can be historical + synchronous</p> </li> <li>We feed the entire history of data as a single DataFrame</li> <li> <p>The computation is vectorized and synchronous (in one shot)</p> </li> <li> <p>The execution of a DAG can be replayed and async</p> </li> <li>The DAG waits for new data to come in</li> <li>5-mins of (historical) data arrives every 5 minutes</li> <li>The computation is carried out only for that period of time</li> <li> <p>The DAG goes back to waiting</p> </li> <li> <p>The execution of a DAG can be real-time and async</p> </li> <li>Same as above but the data comes from a real-time source (e.g., DB)</li> </ul> <p>A system is composed of</p> <ul> <li> <p>Intermediate step: RT / Mocked EG</p> </li> <li> <p>RT / EG</p> </li> <li> <p>We have the part that places orders</p> </li> <li> <p>We don't have the part that reads the state back</p> </li> </ul> <p>) Historical / EG</p> <ul> <li>We can't do this since they don't provide the right interface</li> </ul> <p>6) Historical / Mocked EG</p> <ul> <li> <p>It is possible</p> </li> <li> <p>INV: portfolio persists across invocations of <code>place_orders()</code> in RT mode</p> </li> <li> <p>It doesn't make a difference for batch mode, since there is a single   invocation of <code>place_orders</code></p> </li> <li> <p>INV: portfolio is created and passed inside the config. It doesn't need to be   passed back since the \"pointer\" to it is passed back-and-forth</p> </li> <li>For batch mode, we have all the forecasts (they are computed in one shot) the   Portfolio can be populated with all the trades and then discarded.</li> <li>In fact we have a loop that does exactly that</li> <li>We can run the portfolio in \"debug mode\" where we have a precomputed df</li> </ul>"},{"location":"dataflow/all.timing_semantic_and_clocks.html#research-mode","title":"Research mode","text":"<ul> <li>Run the DAG without <code>process_forecast</code></li> <li>Save ResultBundles</li> <li>Use the notebook to read the ResultBundle and compute pnl</li> <li>\"Research pnl\" is the pnl we compute from the research mode</li> <li>Dot product + volatility normalization + target GMV + other magic</li> <li>TODO(Paul): to formally defined</li> </ul>"},{"location":"dataflow/all.timing_semantic_and_clocks.html#real-time-mode","title":"Real-time mode","text":"<ul> <li>Run DAG one step at the time using RealTimeDataSource and MarketDataInterface</li> <li>Save ResultBundle / intermediate state</li> <li>Compute rolling pnl</li> </ul>"},{"location":"dataflow/all.timing_semantic_and_clocks.html#historical","title":"Historical","text":"<ul> <li>Do all the predictions and then run the SimulatedPortfolio   (DataFramePortfolio) one step at the time</li> <li>Maybe useful for \"looping\" around the Optimizer</li> </ul>"},{"location":"dataflow/all.timing_semantic_and_clocks.html#flows","title":"Flows","text":"<ul> <li>Evaluating a model requires computing forecasts and then the corresponding PnL</li> </ul>"},{"location":"dataflow/all.timing_semantic_and_clocks.html#forecast-flow","title":"Forecast flow","text":"<ul> <li>= compute forecasts from data using the DAG</li> <li>It can be historical, replayed, real-time</li> <li>The outcome is a data frame with all the forecasts</li> </ul>"},{"location":"dataflow/all.timing_semantic_and_clocks.html#pnl-profit-and-loss-flow","title":"Pnl (profit and loss) flow","text":"<ul> <li>= given forecasts and prices compute the corresponding PnL</li> <li>It can be computed using:</li> <li>Dot product approach (pnl = \\sum f^T \\dot rets_{t-2})</li> <li>Prices and positions<ul> <li>Without Portfolio and Broker, but a simplified approach</li> </ul> </li> <li>Order by order<ul> <li>Using Portfolio and Broker</li> </ul> </li> </ul> <p>Some configurations are used more often than others, and so we give them a specific name</p>"},{"location":"dataflow/all.timing_semantic_and_clocks.html#research-flow","title":"Research flow","text":"<ul> <li>= historical flow for computing + dot product</li> <li>We use it to assess the presence of alpha</li> </ul>"},{"location":"dataflow/all.timing_semantic_and_clocks.html#real-time-flow","title":"Real-time flow","text":"<ul> <li>All components are:</li> <li>Vendor-specific implemented (e.g., TalosImClient, TalosBroker)</li> <li>Executed in real-time and asynchronous</li> </ul>"},{"location":"dataflow/all.train_and_predict_phases.explanation.html","title":"Phases of evaluation of Dag","text":"<p>A <code>Dag</code> computation can go through multiple phases (aka \"methods\").</p> <p>In the following we use different terms with the same meaning:</p> <ul> <li>\"fit\", \"learn\", \"train\"</li> <li>\"predict\", \"test\"</li> </ul> <p>Examples of phases are:</p> <ul> <li>Initialization phase</li> <li>It entails any computation that needs to be done to reach a certain initial     state</li> <li>Fit</li> <li>Learn the state of the stateful nodes</li> <li>Validate</li> <li>Learn hyperparameters of a system that has design parameters that need to be     learned</li> <li>E.g., number of epochs or layers in a neural network, time constant of a     smoothing parameter</li> <li>Predict</li> <li>Use the learned state of each node to predict values on unseen data</li> <li>Load state</li> <li>Load previously learned state of stateful DAG nodes (e.g., weights of a     model)</li> <li>Save state</li> <li>Save learned state of DAG after a fit stage</li> <li>E.g., learn model and serialize it for production</li> <li>Save results</li> <li>Save artifacts from model execution (e.g., results of a predict phase)</li> </ul> <p>The simulation kernel is in charge of scheduling these phases appropriately, depending on the type of simulation to be performed, and on the dependency across <code>Dag</code> nodes</p> <p>E.g., the initialization phase can be used to load the state of a <code>Dag</code> resulting from a previous fit phase, so that the <code>Dag</code> can run a subsequent predict phase</p> <p>This allows performing <code>Dag</code> operations in multiple ways</p>"},{"location":"dataflow/all.train_and_predict_phases.explanation.html#1-in-sample-only-experiment","title":"1. In-sample only experiment","text":"<ul> <li>This involves testing the model on the same dataset that was used for training</li> <li>Of course, performance estimated only in-sample are optimistic</li> <li>The steps are:</li> <li>Feed all the data to the <code>Dag</code> in fit mode (independently on batch,     chunking, or streaming mode)</li> <li>Learn parameters for <code>Dag</code></li> <li> <p>Run <code>Dag</code> in predict mode on the same data used for fitting</p> </li> <li> <p>TODO(gp): AddPicture</p> </li> </ul>"},{"location":"dataflow/all.train_and_predict_phases.explanation.html#2-traintest-experiment","title":"2. Train/test experiment","text":"<ul> <li> <p>Aka in-sample/out-of-sample experiment</p> </li> <li> <p>The steps are:</p> </li> <li>Split the data in train and test sets without any overlap</li> <li>Feed the train data to the <code>Dag</code> in fit mode</li> <li>Learn parameters for <code>Dag</code></li> <li>Run the <code>Dag</code> in predict mode ont he test data</li> </ul> <pre><code>sequenceDiagram\n    participant DataSet as Dataset\n    participant TrainSet as Training Set\n    participant TestSet as Test Set\n    participant Model as `Dag` Model\n\n    DataSet-&gt;&gt;TrainSet: Split into Training Data\n    DataSet-&gt;&gt;TestSet: Split into Test Data\n    TrainSet-&gt;&gt;Model: Feed Training Data to `Dag`\n    Model-&gt;&gt;Model: Train and Learn Parameters\n    TestSet-&gt;&gt;Model: Feed Test Data to `Dag`\n    Model-&gt;&gt;TestSet: Run `Dag` in Predict Mode\n    TestSet-&gt;&gt;TestSet: Evaluate Performance\n</code></pre>"},{"location":"dataflow/all.train_and_predict_phases.explanation.html#3-trainvalidatetest-experiment","title":"3. Train/validate/test experiment","text":"<ul> <li> <p>This is an extension of train/test experiment where the validation set is used   to tune some hyper-parameters of the system</p> </li> <li> <p>TODO(gp): AddPicture</p> </li> </ul>"},{"location":"dataflow/all.train_and_predict_phases.explanation.html#4-cross-validation-experiment","title":"4. Cross-Validation Experiment","text":"<ul> <li> <p>This method involves using cross-validation for more robust model evaluation.</p> </li> <li> <p>The steps are:</p> </li> <li>Splitting the entire dataset into multiple smaller subsets.</li> <li>For each subset:<ul> <li>Use the subset as the test set and the remaining data as the training set.</li> <li>Feed the training data to the <code>Dag</code> in fit mode.</li> <li>Learn parameters for the <code>Dag</code>.</li> <li>Run the <code>Dag</code> in predict mode on the subset used as the test set.</li> </ul> </li> <li> <p>Aggregate the performance across all subsets to assess the overall     performance of the model.</p> </li> <li> <p>This approach provides a more comprehensive evaluation by using each part of   the dataset for both training and testing, thereby reducing bias in the   performance estimate.</p> </li> <li> <p>Note that this approach is valid using blocks of contiguous data for   train/test but also with data points as long as there is no future peeking</p> </li> <li> <p>TODO(gp): AddPicture</p> </li> </ul>"},{"location":"dataflow/all.train_and_predict_phases.explanation.html#5-rolling-traintest-experiment","title":"5. Rolling Train/Test Experiment","text":"<ul> <li> <p>This approach is typically used in time-series analysis, where data is   sequential.</p> </li> <li> <p>The steps are:</p> </li> <li>Sequentially partition the dataset into a series of train and test sets,     ensuring that each test set immediately follows its corresponding training     set in time.</li> <li>For each partition:<ul> <li>Use the earlier data as the training set and the immediately following   data as the test set.</li> <li>Feed the training data to the <code>Dag</code> in fit mode.</li> <li>Learn parameters for the <code>Dag</code>.</li> <li>Run the <code>Dag</code> in predict mode on the test data.</li> </ul> </li> <li> <p>This method simulates a real-world scenario where the model is trained on     past data and tested on future, unseen data.</p> </li> <li> <p>The rolling aspect ensures that the model is continually updated and tested on   the most recent data, reflecting more realistic and practical use cases for   time-sensitive models.</p> </li> </ul> <pre><code>sequenceDiagram\n  participant DataSet as Dataset\n  participant TrainSet as Training Set\n  participant TestSet as Test Set\n  participant Model as `Dag` Model\n\n  DataSet-&gt;&gt;TrainSet: Split into Training Data\n  DataSet-&gt;&gt;TestSet: Split into Test Data\n  loop For each partition\n      TrainSet-&gt;&gt;Model: Train `Dag` on Training Data\n      Model-&gt;&gt;Model: Learn Parameters\n      Model-&gt;&gt;TestSet: Test `Dag` on Test Data\n      Note over TestSet: Evaluate Performance\n      TestSet-&gt;&gt;DataSet: Roll forward to next partition\n      DataSet-&gt;&gt;TrainSet: Update Training Data\n      DataSet-&gt;&gt;TestSet: Update Test Data\nend\n</code></pre>"},{"location":"datapull/all.data_schema.explanation.html","title":"Data Schema","text":""},{"location":"datapull/all.data_schema.explanation.html#data-schema_1","title":"Data schema","text":"<ul> <li>The <code>dataset_schema</code> is a structured representation of metadata attributes   used to describe a dataset</li> <li>It comprises several fields separated by a dot <code>.</code></li> <li> <p>Each field provides specific information about the dataset, such as the mode     of download, the entity responsible for downloading, the format of the data,     the type of data, the asset type, the vendor, the exchange ID, and the     version of the dataset.</p> </li> <li> <p>This structured representation facilitates easy understanding and organization   of dataset metadata, enabling efficient data management and analysis.</p> </li> </ul>"},{"location":"datapull/all.data_schema.explanation.html#dataset-schema","title":"Dataset schema","text":"<ul> <li> <p>The data schema signature has the following schema   <code>{download_mode}.{downloading_entity}.{action_tag}.{data_format}.{data_type}.{asset_type}.{universe}.{vendor}.{exchange_id}.{version\\[-snapshot\\]}.{extension}</code></p> </li> <li> <p>Examples of dataset names are in   /im_v2/common/notebooks/Master_raw_data_gallery.ipynb</p> </li> <li><code>realtime.airflow.resampled_1min.postgres.bid_ask.futures.v8.ccxt.binance.v1_0_0</code></li> <li><code>periodic_daily.airflow.archived_200ms.postgres.bid_ask.spot.v7.ccxt.binance.v1_0_0</code></li> <li><code>realtime.airflow.downloaded_200ms.postgres.bid_ask.futures.v7_4.ccxt.cryptocom.v1_0_0</code></li> </ul>"},{"location":"datapull/all.data_schema.explanation.html#description-of-fields","title":"Description of fields","text":"<ul> <li><code>download_mode</code>: Indicates the mode in which the dataset was downloaded.</li> <li> <p>E.g., <code>bulk</code>, <code>realtime</code> and <code>periodic_daily</code></p> </li> <li> <p><code>downloading_entity</code>: Specifies the entity responsible for downloading the   dataset</p> </li> <li> <p>E.g., <code>airflow</code> or <code>manual</code>.</p> </li> <li> <p><code>action_tag</code>: Describes the action performed on the dataset</p> </li> <li> <p>E.g., <code>downloaded_all</code>, <code>resampled_1min</code> and <code>archived_200ms</code>.</p> </li> <li> <p><code>data_format</code> : Indicates the format of the dataset</p> </li> <li> <p>E.g., <code>csv</code>, <code>parquet</code> or <code>postgres</code>.</p> </li> <li> <p><code>data_type</code>: Specifies the type of data contained in the dataset</p> </li> <li> <p>E.g., <code>ohlcv</code> (Open, High, Low, Close, Volume), <code>bid_ask</code>, <code>trades</code></p> </li> <li> <p><code>asset_type</code>: Describes the type of assets included in the dataset</p> </li> <li> <p>E.g., <code>futures</code> or <code>spot</code>.</p> </li> <li> <p><code>universe</code>: version of the universe to be used</p> </li> <li> <p>E.g., <code>v7.4</code></p> </li> <li> <p><code>vendor</code>: Specifies the vendor from which the dataset originates</p> </li> <li> <p>E.g., <code>ccxt</code></p> </li> <li> <p><code>exchange_id</code>: Indicates the ID of the exchange associated with the dataset.</p> </li> <li> <p>E.g., <code>binance</code>, <code>okx</code> or <code>kraken</code></p> </li> <li> <p><code>version</code>: Denotes the version of the dataset</p> </li> <li>E.g., <code>v1_0_0</code></li> </ul>"},{"location":"datapull/all.data_schema.explanation.html#data-signature-validation","title":"Data signature validation","text":"<p>Perform syntactic and semantic validation of a specified dataset signature. Signature is validated by the latest dataset schema version.</p> <ol> <li>Syntax validation: checks if the signature is not malformed.</li> <li> <p>If the schema specifies dataset signature as <code>{data_type}.{asset_type}</code>,      then <code>ohlcv.futures</code> is a valid signature, but <code>ohlcv-futures</code> is not.</p> </li> <li> <p>Semantic validation: checks if the signature tokens are correct.</p> </li> <li>If the schema specifies allowed values for      <code>data_type = [\"ohlcv\", \"bid_ask\"]</code>, then for dataset signature      <code>{data_type}.{asset_type}</code> <code>ohlcv.futures</code> is a valid signature, but      <code>bidask.futures</code> is not.</li> </ol>"},{"location":"datapull/all.data_schema.explanation.html#code","title":"Code","text":"<ul> <li>The code corresponding to parsing and validating is under <code>//data_schema/</code>   ``` <p>tree.sh -p data_schema   data_schema/   |-- dataset_schema_versions/   |   <code>-- dataset_schema_v3.json     Description of the current schema   |-- test/   |   |-- __init__.py   |</code>-- test_dataset_schema_utils.py   |-- init.py   |-- changelog.txt     Changelog for dataset schema updates   |-- dataset_schema_utils.py     Utilities to parse schema   `-- validate_dataset_signature.py*     Script to test a schema   ```</p> </li> </ul> <p>Last review: GP on 2024-05-14</p>"},{"location":"datapull/all.datapull_client_stack.explanation.html","title":"Data client stack","text":"<p>As said in other documents, the data is downloaded and saved by <code>DataPull</code> with minimal or no transformation. Once the data is downloaded, it needs to be retrieved for processing in a common format (e.g., <code>DataPull</code> format).</p> <p>We use a two-layer approach to handle the complexity of reading and serving the data to clients.</p> <pre><code>flowchart\n  Vendor Data --&gt; ImClient --&gt; MarketData --&gt; User\n</code></pre> <ul> <li><code>ImClient</code></li> <li>Is data vendor and dataset specific</li> <li>Adapt data from the vendor data to a standard internal <code>MarketData</code> format</li> <li>Handle all the peculiarities in format and semantic of a specific vendor     data</li> <li>All timestamps are UTC</li> <li> <p>Asset ids are handled as strings</p> </li> <li> <p><code>MarketData</code></p> </li> <li>Is independent of the data vendor</li> <li>Implement behaviors that are orthogonal to vendors, such as:<ul> <li>Streaming/real-time or batch/historical</li> <li>Time-stitching of streaming/batch data, i.e., merge multiple data sources   giving a single and homogeneous view of the data</li> <li>E.g., the data from the last day comes from a real-time source while the     data before that can come from an historical source. The data served by     <code>MarketData</code> is a continuous snapshot of the data</li> <li>Replaying, i.e., serialize the data to disk and read it back, implementing   as-of-time semantic based on knowledge time</li> <li>This behavior is orthogonal to streaming/batch and stitching, i.e., one     can replay any <code>MarketData</code>, including an already replayed one</li> </ul> </li> <li>Data is accessed based on intervals <code>[start_timestamp, end_timestamp]</code> using     different open/close semantics, but always preventing future peeking</li> <li>Support real-time behaviors, such as knowledge time, wall clock time, and     blocking behaviors (e.g., \"is the last data available?\")</li> <li>Handle desired timezone for timestamps</li> <li>Asset ids are handled as ints</li> </ul>"},{"location":"datapull/all.datapull_client_stack.explanation.html#interfaces","title":"Interfaces","text":"<ul> <li> <p>Both <code>ImClient</code> and <code>MarketData</code> have an output format that is enforced by the   base abstract class and the derived classes together</p> </li> <li> <p><code>ImClient</code> and <code>MarketData</code> have 3 interfaces each:</p> </li> <li> <p>An external \"input\" format for a class</p> </li> <li> <p>Format of the data as input to a class derived from <code>MarketData</code>/<code>ImClient</code></p> </li> <li> <p>An internal \"input\" format</p> </li> <li> <p>It's the format that derived classes need to adhere so that the base class     can do its job, i.e., apply common transformations to all classes</p> </li> <li> <p>An external \"output\" format</p> </li> <li>It's the <code>MarketData</code>/<code>ImClient</code> format, which is fixed</li> </ul>"},{"location":"datapull/all.datapull_client_stack.explanation.html#transformations","title":"Transformations","text":"<ul> <li>The chain of transformations of the data from <code>Vendor</code> to <code>User</code> are as   follow:</li> </ul> <p><code>mermaid   flowchart     Vendor --&gt; DerivedImClient --&gt; AbstractImClient --&gt; DerivedMarketData --&gt; AbstractMarketData --&gt; User</code></p> <ul> <li>Classes derived from <code>ImClient</code></li> <li>The transformations are vendor-specific</li> <li>Only derived classes <code>ImClient</code> know what is exact semantic of the     vendor-data</li> <li> <p>Whatever is needed to transform the vendor data into the internal format     accepted by base <code>ImClient</code></p> </li> <li> <p>Abstract class <code>ImClient</code></p> </li> <li>The transformations are fixed</li> <li> <p>Implemented by <code>ImClient._apply_im_normalization()</code></p> </li> <li> <p>Class derived from <code>MarketData</code></p> </li> <li> <p>The transformations are specific to the <code>MarketData</code> derived class</p> </li> <li> <p><code>MarketData</code></p> </li> <li>The transformations are fixed</li> </ul>"},{"location":"datapull/all.datapull_client_stack.explanation.html#output-format-of-imclient","title":"Output format of <code>ImClient</code>","text":"<ul> <li>The data in output of a class derived from <code>ImClient</code> is normalized so that:</li> <li>The index:</li> <li>Represents the knowledge time</li> <li>Is the end of the sampling interval</li> <li>Is called <code>timestamp</code></li> <li> <p>Is a tz-aware timestamp in UTC</p> </li> <li> <p>The data:</p> </li> <li>(optional) Is re-sampled on a 1 minute grid and filled with NaN values</li> <li>Is sorted by index and <code>full_symbol</code></li> <li>Is guaranteed to have no duplicates</li> <li>Belongs to intervals like <code>[a, b]</code></li> <li> <p>Has a <code>full_symbol</code> column with a string representing the canonical name of     the instrument</p> </li> <li> <p>An example of data in output from an <code>ImClient</code> is:   <code>full_symbol     close     volume                   timestamp   2021-07-26 13:42:00+00:00  binance:BTC_USDT  47063.51  29.403690   2021-07-26 13:43:00+00:00  binance:BTC_USDT  46946.30  58.246946   2021-07-26 13:44:00+00:00  binance:BTC_USDT  46895.39  81.264098</code></p> </li> <li> <p>TODO(gp): We are planning to use an <code>ImClient</code> data format closer to   <code>MarketData</code> by using <code>start_time</code>, <code>end_time</code>, and <code>knowledge_time</code> since   these can be inferred only from the vendor data semantic</p> </li> </ul>"},{"location":"datapull/all.datapull_client_stack.explanation.html#transformations-by-classes-derived-from-marketdata","title":"Transformations by classes derived from <code>MarketData</code>","text":"<ul> <li>Classes derived from <code>MarketData</code> do whatever they need to do in <code>_get_data()</code>   to get the data, but always pass back data that:</li> <li>Is indexed with a progressive index</li> <li>Has <code>asset</code>, <code>start_time</code>, <code>end_time</code>, <code>knowledge_time</code></li> <li> <p><code>start_time</code>, <code>end_time</code>, <code>knowledge_time</code> are timezone aware</p> </li> <li> <p>E.g.,   <code>asset_id                 start_time                   end_time     close   volume   idx   0    17085  2021-07-26 13:41:00+00:00  2021-07-26 13:42:00+00:00  148.8600   400176   1    17085  2021-07-26 13:30:00+00:00  2021-07-26 13:31:00+00:00  148.5300  1407725   2    17085  2021-07-26 13:31:00+00:00  2021-07-26 13:32:00+00:00  148.0999   473869</code></p> </li> </ul>"},{"location":"datapull/all.datapull_client_stack.explanation.html#transformations-by-abstract-class-marketdata","title":"Transformations by abstract class <code>MarketData</code>","text":"<ul> <li>The transformations are done inside <code>get_data_for_interval()</code>, during   normalization, and are:</li> <li>Indexing by <code>end_time</code></li> <li>Converting <code>end_time</code>, <code>start_time</code>, <code>knowledge_time</code> to the desired     timezone</li> <li>Sorting by <code>end_time</code> and <code>asset_id</code></li> <li>Applying column remaps</li> </ul>"},{"location":"datapull/all.datapull_client_stack.explanation.html#output-format-of-marketdata","title":"Output format of <code>MarketData</code>","text":"<ul> <li>The abstract base class <code>MarketData</code> normalizes the data by:</li> <li>Sorting by the columns that correspond to <code>end_time</code> and <code>asset_id</code></li> <li> <p>Indexing by the column that corresponds to <code>end_time</code>, so that it is     suitable to DataFlow computation</p> </li> <li> <p>E.g.,   <code>asset_id                start_time    close   volume   end_time   2021-07-20 09:31:00-04:00    17085 2021-07-20 09:30:00-04:00  143.990  1524506   2021-07-20 09:32:00-04:00    17085 2021-07-20 09:31:00-04:00  143.310   586654   2021-07-20 09:33:00-04:00    17085 2021-07-20 09:32:00-04:00  143.535   667639</code></p> </li> </ul>"},{"location":"datapull/all.datapull_client_stack.explanation.html#asset-ids-format","title":"Asset ids format","text":""},{"location":"datapull/all.datapull_client_stack.explanation.html#imclient-asset-ids","title":"<code>ImClient</code> asset ids","text":"<ul> <li><code>ImClient</code> uses assets encoded as <code>full_symbols</code> strings</li> <li>E.g., <code>binance::BTC_UTC</code></li> <li>There is a vendor-specific mapping:</li> <li>From <code>full_symbols</code> to corresponding data</li> <li>From <code>asset_ids</code> (ints) to <code>full_symbols</code> (strings)</li> <li>If the <code>asset_ids</code> -&gt; <code>full_symbols</code> mapping is provided by the vendor, then   we reuse it</li> <li>Otherwise, we build a mapping hashing <code>full_symbols</code> strings into numbers</li> </ul>"},{"location":"datapull/all.datapull_client_stack.explanation.html#marketdata-asset-ids","title":"<code>MarketData</code> asset ids","text":"<ul> <li><code>MarketData</code> and everything downstream uses <code>asset_ids</code> that are encoded as   ints</li> <li>This is because we want to use ints and not strings in dataframe</li> </ul>"},{"location":"datapull/all.datapull_client_stack.explanation.html#handling-of-asset_ids","title":"Handling of <code>asset_ids</code>","text":"<ul> <li> <p>Different implementations of <code>ImClient</code> backing a <code>MarketData</code> are possible,   e.g.:</p> </li> <li> <p>The caller needs to specify the requested <code>asset_ids</code></p> </li> <li>In this case the universe is provided by <code>MarketData</code> when calling the data   access methods</li> <li> <p>The reading backend is initialized with the desired universe of assets and   then <code>MarketData</code> just uses or subsets that universe</p> </li> <li> <p>For these reasons, assets are selected at 3 different points:</p> </li> <li> <p><code>MarketData</code> allows to specify or subset the assets through <code>asset_ids</code>      through the constructor</p> </li> <li><code>ImClient</code> backends specify the assets returned</li> <li> <p>E.g., a concrete implementation backed by a DB can stream the data for its     entire available universe</p> </li> <li> <p>Certain class methods allow querying data for a specific asset or subset of      assets</p> </li> <li> <p>For each stage, a value of <code>None</code> means no filtering</p> </li> </ul>"},{"location":"datapull/all.datapull_client_stack.explanation.html#data","title":"Data","text":""},{"location":"datapull/all.datapull_client_stack.explanation.html#handling-of-filtering-by-time","title":"Handling of filtering by time","text":"<ul> <li>Clients of <code>MarketData</code> might want to query data by:</li> <li>Using different interval types, namely <code>[a, b), [a, b], (a, b], (a, b)</code></li> <li>Filtering on either the <code>start_ts</code> or <code>end_ts</code></li> <li>For this reason, this class supports all these different ways of providing   data</li> <li><code>ImClient</code> has a fixed semantic of the interval <code>\\[a, b\\]</code></li> <li><code>MarketData</code> adapts the fixed semantic to multiple ones</li> </ul>"},{"location":"datapull/all.datapull_client_stack.explanation.html#handling-timezone","title":"Handling timezone","text":"<ul> <li><code>ImClient</code> always uses UTC as output</li> <li><code>MarketData</code> adapts UTC to the desired timezone, as requested by the client</li> </ul>"},{"location":"datapull/all.datapull_derived_data.explanation.html","title":"Derived data workflows","text":"<p>Derived data workflows. Data workflows can transform datasets into other datasets</p> <ul> <li>E.g., resample 1 second data into 1 minute data</li> </ul> <p>The data is then written back to the same data source as the originating data (e.g., DB for period / real-time data, Parquet / csv / S3 for historical data).</p> <p>TODO(gp): Add a plot (we are the source of the provider)</p> <p>Derived data naming scheme. We use the same naming scheme as in downloaded data set {dataset_signature}</p> <p>but we encode the information about the content of the newly generated data in the <code>action_tag</code> attribute of the data, e.g., <code>resample_1min</code> to distinguish it from <code>downloaded_1sec</code>.</p> <p>We use this approach so that the scheme of the derived data is the same as a downloaded data set.</p> <p>Derived data research flow. The goal is to decide how to transform the raw data into derived data and come up with QA metrics to assess the quality of the transformation</p> <ul> <li> <p>It can be cross-vendor or not</p> </li> <li> <p>E.g., sample 1sec data to 1min and compare to a reference. The sampling is   done on the fly since the researcher is trying to understand how to resample   (e.g., removing outliers) to get a match</p> </li> </ul> <p>Derived data production flow. The research flow is frozen and put in production</p> <ul> <li>E.g., run the resample script to sample and write back to DB and historical   data. This flow can be run in historical mode (to populate the backend with   the production data) and in real-time mode (to compute the streaming data)</li> </ul> <p>Derived data QA flow. the goal is to monitor that the production flow is still performing properly with respect to the QA metrics</p> <ul> <li> <p>E.g., the 1-sec to 1-min resampling is not performed on-the-fly, but it uses   the data already computed by the script in the production flow.</p> </li> <li> <p>This flow is mainly run in real-time, but we might want to look at QA   performance also historically</p> </li> </ul> <p>This same distinction can also be applied to feature computation and to the machine learning flow.</p> <p>Provider -&gt; data -&gt; Us -&gt; derived flow -&gt; Us -&gt; features -&gt; Us -&gt; ML -&gt; Exchange</p>"},{"location":"datapull/all.datapull_qa_flow.explanation.html","title":"Data QA workflows","text":""},{"location":"datapull/all.datapull_qa_flow.explanation.html#quality-assurance-metrics","title":"Quality-assurance metrics","text":"<p>Each data set has QA metrics associated with it to ensure the data has the minimum expected data quality.</p> <ul> <li>E.g., for 1-minute OHLCV data, some possible QA metrics are:</li> <li>Missing bars for a given timestamp</li> <li>Missing/nan OHLCV values within an individual bar</li> <li>Data points with OHLC data and volume = 0</li> <li>Data points where OHLCV data is not in the correct relationship<ul> <li>E.g., H and L are not higher or lower than O and C</li> </ul> </li> <li>Outliers data points<ul> <li>E.g., a data is more than N standard deviations from the running mean</li> </ul> </li> </ul> <p>The code for the QA flow is independent of bulk (i.e., historical) and periodic (i.e., real-time) data.</p>"},{"location":"datapull/all.datapull_qa_flow.explanation.html#bulk-data-single-dataset-qa-metrics","title":"Bulk data single-dataset QA metrics","text":"<p>It is possible to run the QA flow to compute the quality of the historical data. This is done typically as a one-off operation right after the historical data is downloaded in bulk. This touches only one dataset, namely the one that was just downloaded.</p>"},{"location":"datapull/all.datapull_qa_flow.explanation.html#periodic-qa-metrics","title":"Periodic QA metrics","text":"<p>Every N minutes of downloading real-time data, the QA flow is run to generate statistics about the quality of the data. In case of low data quality data the system sends a notification.</p>"},{"location":"datapull/all.datapull_qa_flow.explanation.html#cross-datasets-qa-metrics","title":"Cross-datasets QA metrics","text":"<p>There are QA workflows that compare different data sets that are related to each other, e.g.:</p> <ul> <li> <p>Consider the case of downloading the same data (e.g., 1-minute OHLCV for spot   <code>BTC_USDT</code> from Binance exchange) from different providers (e.g., Binance   directly and a third-party provider).</p> </li> <li> <p>Consider the case where there is a REST API that allows to get data for a   period of data and a websocket that streams the data</p> </li> <li> <p>Consider the case where one gets an historical dump of the data from a third   party provider vs the data from the exchange real-time stream</p> </li> <li> <p>Consider the case of NASDAQ streaming data vs TAQ data disseminated once the   market is close</p> </li> </ul>"},{"location":"datapull/all.datapull_qa_flow.explanation.html#historical-vs-real-time-qa-flow","title":"Historical vs real-time QA flow","text":"<p>Every period $T_{dl,hist}$, a QA flow is run where the real-time data is compared to the historical data to ensure that the historical view of the data matches the real-time one.</p> <p>This is necessary but not sufficient to guarantee that the bulk historical data can be reliably used as a proxy for the real-time data as-of, in fact this is simply a self-consistency check. We do not have any guarantee that the data source collected correctly historical data.</p>"},{"location":"datapull/all.datapull_qa_flow.explanation.html#data-qa-workflow-naming-scheme","title":"Data QA workflow naming scheme","text":"<p>A QA workflow has a name that represents its characteristics in the format:</p> <pre><code>{qa_type}.{dataset_signature}\n</code></pre> <p>i.e.,</p> <pre><code>production_qa.{download_mode}.{downloading_entity}.{action_tag}.{data_format}.{data_type}.{asset_type}.{universe}.{vendor}.{exchange}.{version\\[-snapshot\\]}.{asset}.{extension}\n</code></pre> <p>where:</p> <ul> <li><code>qa_type</code>: the type of the QA flow, e.g.,</li> <li><code>production_qa</code>: perform a QA flow on historical and real-time data. The     interface should be an IM client, which makes it possible to run QA on both     historical and real-time data</li> <li><code>research_analysis</code>: perform a free-form analysis of the data. This can then     be the basis for a <code>qa</code> analysis</li> <li><code>compare_historical_real_time</code>: compare historical and real-time data coming     from the same source of data</li> <li><code>compare_historical_cross_comparison</code>: compare historical data from two     different data sources The same rules apply as in downloader and derived     dataset for the naming scheme.</li> </ul> <pre><code>research_cross_comparison.periodic.airflow.downloaded_1sec_1min.all.bid_ask.futures.all.ccxt_cryptochassis.all.v1_0_0\n</code></pre> <p>Since cross-comparison involves two (or more dataset) we use a short notation merging the attributes that differ.</p> <p>E.g., a comparison between the datasets</p> <ul> <li><code>periodic.1minute.postgres.ohlcv.futures.1minute.ccxt.binance</code></li> <li><code>periodic.1day.postgres.ohlcv.futures.1minute.ccxt.binance</code></li> </ul> <p>is called:</p> <pre><code>compare_qa.periodic.1minute-1day.postgres.ohlcv.futures.1minute.ccxt.binance\n</code></pre> <p>since the only difference is in the frequency of the data sampling.</p> <p>It is possible to use a long format <code>{dataset_signature1}-vs-{dataset_signature2}</code>.</p> <p>E.g.,</p> <pre><code>| Name                  | Dataset Signature                | Description                                                          | Frequency                 | Dashboard | Data Location | Active? |\n| --------------------- | -------------------------------- | -------------------------------------------------------------------- | ------------------------- | --------- | ------------- | ------- |\n| hist_dl1              | Historical download              | - All of the past day data&lt;br&gt;- Once a day at 0:00:00 UTC            | -                         | s3://...  | Yes           |\n| rt_dl1                | Real-time download               | - Every minute                                                       | -                         | s3://...  | Yes           |\n| rt_dl1.qa1            | Real-time QA check               | Check QA metrics for dl1                                             | Every 5 minutes           | -         | s3://...      | Yes     |\n| hist_dl1.rt_dl1.check | Check of historical vs real-time | Check consistency between historical and real-time CCXT binance data | Once a day at 0:15:00 UTC | -         | -             | -       |\n| rt_dl2                | Real-time download               | - vendor=CryptoChassis&lt;br&gt;- exchange=Binance&lt;br&gt;- data type=bid/ask  | Every minute              | -         | s3://...      | Yes     |\n| rt_dl2.qa2            | Real-time QA check               | Check QA metrics for dl3                                             | Every 5 minutes           | -         | s3://...      | Yes     |\n| rt_dl1_dl2.check      | Cross-data QA check              | Compare data from rt_dl1 and rt_dl2                                  | Every 5 minutes           | -         | -             | -       |\n</code></pre>"},{"location":"datapull/all.datapull_sandbox.explanation.html","title":"DataPull sandbox","text":"<p>This paragraph describes an example of infrastructure that implements the <code>DataPull</code> protocol.</p> <p>It is implemented as a Docker Container containing the following services:</p> <ul> <li>Airflow</li> <li>Jupyter notebook</li> <li>Postgres</li> <li> <p>MongoDB</p> </li> <li> <p>It is a separated code base from CK and it shares only a few base library   (e.g., <code>helpers</code>)</p> </li> <li> <p>It is a scaled down version of CK production infrastructure (e.g., managed   Airflow is replaced by a local Airflow instance)</p> </li> </ul>"},{"location":"datapull/all.dataset_onboarding_checklist.reference.html","title":"Dataset onboarding checklist","text":"<p>We follow a standard flow when onboarding a new dataset.</p> <p>Create a GH issue and paste the following checklist in the issue specification whenever a request for a new dataset is made. The structure is pre-formatted as a markdown checklist.</p>"},{"location":"datapull/all.dataset_onboarding_checklist.reference.html#preparation-and-exploratory-analysis","title":"Preparation and exploratory analysis","text":"<p>From <code>docs/datapull/all.dataset_onboarding_checklist.reference.md</code></p> <ul> <li>[ ] Decide on the timeline</li> <li>E.g., is this a high-priority dataset or a nice-to-have?</li> <li>[ ] Decide on the course of action</li> <li>E.g., do we download only historical bulk data and/or also prepare a     real-time downloader?</li> <li>[ ] Review existing code</li> <li>Is there any downloader that is similar to the new one, in terms of     interface, frequency, etc.?</li> <li>What code already existing can be generalized to accomplish the task at     hand?</li> <li>What needs to be implemented from scratch?</li> <li>[ ] Create an exploratory notebook that includes:</li> <li>Description of the data type, if this is the first time downloading a     certain data type</li> <li>Example code to obtain a snippet of historical/real-time data</li> <li>If we are interested in historical data, e.g.,<ul> <li>How far in the past we need the data to be?</li> <li>How far in the past the data source goes?</li> </ul> </li> <li>[ ] Create example code to obtain data in realtime</li> <li>Is there any issue with the realtime data?<ul> <li>E.g., throttling, issues with APIs, unreliability</li> </ul> </li> <li>[ ] Perform initial QA on the data sample, e.g.,</li> <li>Compute some statistics in terms of missing data, outliers</li> <li>Does real-time and historical data match at first sight in terms of schema     and content</li> </ul>"},{"location":"datapull/all.dataset_onboarding_checklist.reference.html#implement-historical-downloader","title":"Implement historical downloader","text":"<ul> <li>[ ] Decide what's the name of the data set according to <code>dataset_schema</code>       conventions</li> <li>[ ] Implement the code to perform the historical downloader</li> <li>TODO(Juraj): Add a pointer to examples and docs</li> <li>[ ] Test the flow to download a snippet of data locally in the test stage</li> <li>Apply QA to confirm data is being downloaded correctly</li> <li>[ ] Perform a bulk download for historical datasets</li> <li>Manually, i.e., via executing a script, if the history is short or the     volume of data is low</li> <li>Via an Airflow DAG if the volume of the data is too large for downloading     manually<ul> <li>E.g.,   <code>im_v2/airflow/dags/test.download_bulk_data_fargate_example_guide.py</code></li> </ul> </li> </ul>"},{"location":"datapull/all.dataset_onboarding_checklist.reference.html#automated-aka-scheduled-downloader","title":"Automated AKA Scheduled downloader","text":"<ul> <li>[ ] Setup automatic download of data in pre-production:</li> <li>Since pre-prod runs with code from the master branch (updated twice a day     automatically), make sure to merge any PRs related to the dataset onboarding     first</li> <li>For historical datasets:<ul> <li>To provide a single S3 location to access the entire dataset, move the   bulk history from the test bucket to the pre-prod bucket (source and   destination path should be identical)</li> <li>Add a daily download Airflow task to get data from a previous day and   append it to the existing bulk dataset</li> </ul> </li> <li> <p>For real-time datasets:</p> <ul> <li>Add a real-time download Airflow task to get data continuously 24/7</li> </ul> </li> <li> <p>[ ] For some real-time datasets, an archival flow needs to be added in order       not to overwhelm the storage</p> </li> <li>Consult with the team leader if it's needed for a particular dataset</li> <li> <p>Example Airflow DAG is     preprod.europe.postgres_data_archival_to_s3.py</p> </li> <li> <p>[ ] Add an entry into the</p> </li> <li>Monster dataset matrix</li> <li>[ ] Once the download is enabled in production, update the       Master_raw_data_gallery</li> </ul>"},{"location":"datapull/all.dataset_onboarding_checklist.reference.html#quality-assurance","title":"Quality Assurance","text":""},{"location":"datapull/all.dataset_onboarding_checklist.reference.html#1-check-for-existing-qa-dags","title":"1. Check for Existing QA DAGs","text":"<ul> <li>[ ] Verify if there is already a similar QA DAG running.</li> <li>[ ] Check for existing QA DAGs (e.g., bid_ask/OHLCV, Cross QA for OHLCV         comparing real-time with historical data).</li> <li>[ ] Action: If the new QA is just a change in the universe or vendor, append         a new task to the existing running DAGs. Reference:         Link to Relevant Section].</li> </ul>"},{"location":"datapull/all.dataset_onboarding_checklist.reference.html#2-create-a-new-qa-dag-if-necessary","title":"2. Create a New QA DAG (if necessary)","text":""},{"location":"datapull/all.dataset_onboarding_checklist.reference.html#21-create-and-test-qa-notebook","title":"2.1. Create and Test QA Notebook","text":"<ul> <li>[ ] Develop a notebook to test the QA process.</li> <li>[ ] Test over a small period to ensure it functions as expected.</li> <li>[ ] Tip: Use a small dataset or limited time frame for quick testing.</li> </ul>"},{"location":"datapull/all.dataset_onboarding_checklist.reference.html#22-run-qa-notebook-via-invoke-command","title":"2.2. Run QA Notebook via Invoke Command","text":"<ul> <li>[ ] Execute the QA notebook using the invoke command to validate       functionality.</li> <li>[ ] Example:         Invoke Command Example</li> </ul>"},{"location":"datapull/all.dataset_onboarding_checklist.reference.html#23-create-a-new-dag-file","title":"2.3. Create a New DAG File","text":"<ul> <li>[ ] Create a new DAG file after QA process validation.</li> <li>[ ] Follow the standard procedure for DAG creation. Reference:         DAG Creation Tutorial.</li> </ul> <p>Last review: GP on 2024-04-20</p>"},{"location":"datapull/all.update_CCXT_version.how_to_guide.html","title":"Update Ccxt Version","text":""},{"location":"datapull/all.update_CCXT_version.how_to_guide.html#testing-ccxt-stability-before-docker-container-update","title":"Testing CCXT stability before docker container update","text":"<p>In order to ensure the stability of our code following a CCXT update, a thorough testing process is required. Prior to constructing a new container, we will update the CCXT version locally and execute tests on the actual API to verify the reliability of our codebase.</p>"},{"location":"datapull/all.update_CCXT_version.how_to_guide.html#steps-for-performing-ccxt-api-tests","title":"Steps for performing CCXT API tests:","text":"<ol> <li>Update CCXT version locally in the container using the following command:</li> </ol> <p><code>bash    docker&gt; sudo /venv/bin/pip install ccxt --upgrade</code></p> <ol> <li>Open the file <code>im_v2/test/test_ccxt.py</code> and comment the following code    snippet:</li> </ol> <p><code>python    @pytest.mark.skip(        \"Cannot be run from the US due to 451 error API error. Run manually.\"    )</code></p> <ol> <li>Run the test by executing the following code snippet in the terminal:</li> </ol> <p><code>bash    docker&gt; pytest im_v2/test/test_ccxt.py</code></p> <ol> <li>Run the following commands locally with the new version installed and file a    PR with fixes to any breaks that may appear</li> <li>The PR will be merged directly after the new image release to minimize time      when build is broken.</li> </ol> <p>```bash</p> <p>i run_fast_tests i run_slow_tests i run_superslow_tests    ```</p> <ol> <li>Verify that all test results are marked as \"green\" before proceeding with the    update of the Docker container.</li> </ol>"},{"location":"datapull/all.update_CCXT_version.how_to_guide.html#failure-handling","title":"Failure handling","text":"<p>In the event that any test fails to pass successfully, an issue should be filed. The issue report must include details regarding the failure, allowing for an accurate diagnosis of the problem.</p>"},{"location":"datapull/all.update_CCXT_version.how_to_guide.html#read-ccxt-exchange-timestamp-interpretation","title":"Read CCXT exchange timestamp interpretation","text":"<p>Read CCXT Exchange Timestamp Interpretation</p>"},{"location":"datapull/all.update_CCXT_version.how_to_guide.html#steps-to-confirm-timestamp-representation","title":"Steps to confirm timestamp representation","text":"<p>In order to ensure accurate and up-to-date information regarding the interpretation of timestamps in the CCXT exchange library, follow these detailed steps:</p> <ol> <li>Examine the Library Code:</li> <li> <p>Thoroughly review the library code to confirm that the timestamp      representation remains unchanged. Note that code refactoring might have      occurred, but the semantics should remain consistent.</p> <ul> <li>Update links to code references for precise navigation of all exchanges.</li> </ul> </li> <li> <p>Report Significant Changes:</p> </li> <li> <p>Identify and report any significant changes in the timestamp      interpretation. For example, if the timestamp initially represented the      start of the interval and has now been updated to represent the end of the      interval, document this alteration.</p> </li> <li> <p>Update \"As of Version\" Information:</p> </li> <li>Ensure that the \"as of version\" information is updated to reflect the      version of the CCXT exchange library that is about to be used.</li> </ol> <p>Last review: GP on 2024-04-20</p>"},{"location":"dev_tools/thin_env/all.gh_and_thin_env_requirements.reference.html","title":"Required Packages for the thin environment and GH Actions","text":""},{"location":"dev_tools/thin_env/all.gh_and_thin_env_requirements.reference.html#thin-environment","title":"Thin environment","text":"<p>File location:</p> <ul> <li>requirements.txt</li> </ul>"},{"location":"dev_tools/thin_env/all.gh_and_thin_env_requirements.reference.html#packages","title":"Packages","text":"<ul> <li><code>boto3</code></li> <li>Interacts with the AWS services:</li> <li><code>boto3</code> import in the <code>haws</code></li> <li> <p><code>haws</code> usage in the <code>lib_tasks_docker_release.py</code></p> </li> <li> <p><code>invoke</code></p> </li> <li>Need for running the invoke targets:</li> <li> <p>_run_tests</p> </li> <li> <p><code>poetry</code></p> </li> <li>Manage dependencies in the dev image:</li> <li> <p>docker_build_local_image</p> </li> <li> <p><code>pytest</code></p> </li> <li>To run <code>Docker image QA tests</code>:</li> <li> <p>_run_qa_tests</p> </li> <li> <p><code>tqdm</code></p> </li> <li>Widely used for showing the progress of the process for example:</li> <li> <p>_fix_invalid_owner</p> </li> <li> <p><code>s3fs</code></p> </li> <li>Needed for some invoke targets, for example:</li> <li> <p>docker_update_prod_task_definition</p> </li> <li> <p><code>requests</code></p> </li> <li>Dependency for the <code>docker</code>, for now pinned to the version <code>2.31.0</code> since     the versions &gt;=<code>2.32.1</code> is causing the issue with the <code>docker-compose</code>:     https://github.com/psf/requests/issues/6707</li> <li>See the https://github.com/cryptokaizen/cmamp/issues/8340 for details</li> </ul>"},{"location":"dev_tools/thin_env/all.gh_and_thin_env_requirements.reference.html#candidate-packages-to-remove","title":"Candidate Packages to remove","text":"<ul> <li><code>docker</code> and <code>docker-compose</code> should be moved to OS installation   https://github.com/cryptokaizen/cmamp/issues/6498</li> </ul>"},{"location":"dev_tools/thin_env/all.gh_and_thin_env_requirements.reference.html#gh-actions","title":"GH Actions","text":"<p>File location:</p> <ul> <li>gh_requirements.txt</li> </ul>"},{"location":"dev_tools/thin_env/all.gh_and_thin_env_requirements.reference.html#packages_1","title":"Packages","text":"<ul> <li><code>invoke</code></li> <li>Need for running the invoke targets:</li> <li> <p>_run_tests</p> </li> <li> <p><code>poetry</code></p> </li> <li>Manages dependencies in the dev image:</li> <li> <p>docker_build_local_image</p> </li> <li> <p><code>pytest</code></p> </li> <li>To run <code>Docker image QA tests</code>:</li> <li> <p>_run_qa_tests</p> </li> <li> <p><code>tqdm</code></p> </li> <li>Widely used for showing the progress of the process for example:</li> <li> <p>_fix_invalid_owner</p> </li> <li> <p><code>s3fs</code></p> </li> <li>Needed for some invoke targets, for example:</li> <li> <p>docker_update_prod_task_definition</p> </li> <li> <p><code>requests</code></p> </li> <li>Dependency for the <code>docker</code>, for now pinned to the version <code>2.31.0</code> since     the versions &gt;=<code>2.32.1</code> is causing the issue with the <code>docker-compose</code>:     https://github.com/psf/requests/issues/6707</li> <li>See the https://github.com/cryptokaizen/cmamp/issues/8340 for details</li> </ul>"},{"location":"dev_tools/thin_env/all.gh_and_thin_env_requirements.reference.html#candidate-packages-to-remove_1","title":"Candidate Packages to remove","text":"<ul> <li><code>docker</code> and <code>docker-compose</code> see in the   Thin environment section</li> </ul>"},{"location":"dev_tools/thin_env/all.gh_thin_env_dependencies.how_to_guide.html","title":"All.gh thin env dependencies.how to guide","text":""},{"location":"dev_tools/thin_env/all.gh_thin_env_dependencies.how_to_guide.html#description","title":"Description","text":"<ul> <li> <p>We have 3 sources of package requirements in the project:   1) The thin environment to run <code>invoke</code> targets outside the container</p> <ul> <li>/dev_scripts/client_setup/requirements.txt</li> <li>This is managed with <code>pip</code>   2) GitHub requirements used for GitHub Actions specifically</li> <li>/.github/gh_requirements.txt</li> <li>This is managed with <code>pip</code>   3) Requirements necessary for the container:</li> <li>/devops/docker_build/pyproject.toml</li> <li>This is managed with <code>poetry</code></li> </ul> </li> <li> <p>We want to keep the thin environment as \"thin\" as possible (i.e., with fewer   dependencies)</p> </li> <li>The thin environment and GitHub requirements have to be in sync</li> <li>The only difference is that the GitHub requirements have some limitations     due to the GitHub Actions environment</li> <li> <p>TODO(Vlad): Still not clear what exact difference between the two     requirements files</p> </li> <li> <p>This document provides a step-by-step guide for adding or make any changes in   the requirements file of both the thin env and GitHub</p> </li> </ul>"},{"location":"dev_tools/thin_env/all.gh_thin_env_dependencies.how_to_guide.html#change-in-requirements-file","title":"Change in requirements file","text":"<ul> <li>Some reasons for updating/changing the <code>requirements.txt</code> file are:</li> <li>A new feature requires a new package outside the container, e.g., a new or     updated <code>invoke</code> target</li> <li>Upgrading the package version since the current one is outdated</li> <li>Removing a package since it is not used anymore</li> </ul>"},{"location":"dev_tools/thin_env/all.gh_thin_env_dependencies.how_to_guide.html#confirm-with-build-team","title":"Confirm with Build team","text":"<ul> <li>Changes in any of the requirement files should be confirmed with the Build team   before merging the PR</li> <li>Is the new dependencies really needed?</li> <li>If the new dependencies is really needed, can we limit the scope of the     dependency? E.g.,<ul> <li>Move the related imports to where it is strictly needed in the code</li> <li>Do a try-catch <code>ImportError</code></li> </ul> </li> </ul> <p>Example:</p> <ul> <li>The /helpers/lib_tasks_gh.py module has some   <code>invoke</code> targets that are executed only in the container</li> <li>If the new package is needed for the <code>invoke</code> target only in the container, we   should move the import to the function where it is strictly needed</li> <li>See the <code>gh_publish_buildmeister_dashboard_to_s3()</code> in the   /helpers/lib_tasks_gh.py   for reference.</li> </ul>"},{"location":"dev_tools/thin_env/all.gh_thin_env_dependencies.how_to_guide.html#update-requirements-file","title":"Update requirements file","text":"<ul> <li>Update both the requirements file if relevant   /dev_scripts/client_setup/requirements.txt   and /.github/gh_requirements.txt</li> <li>This file should be changed in every repository (e.g., <code>cmamp</code>, <code>kaizenflow</code>,     <code>orange</code>)</li> <li>After adding the new requirements the build team will run all the tests locally   as well as on GitHub</li> </ul>"},{"location":"dev_tools/thin_env/all.gh_thin_env_dependencies.how_to_guide.html#update-documentation","title":"Update Documentation","text":"<ul> <li>Update the   /docs/dev_tools/thin_env/all.gh_and_thin_env_requirements.reference.md</li> </ul>"},{"location":"dev_tools/thin_env/all.gh_thin_env_dependencies.how_to_guide.html#notify-team","title":"Notify Team","text":"<p>In the @all Telegram channel, notify the team about the new package and ask them to rebuild the thin env.</p> <p>Example:</p> <pre><code>Hi! In the PR: https://github.com/cryptokaizen/cmamp/pull/6800 we removed\nunused packages from the thin environment.\n\nYou need to update the thin environment by running:\n\n&gt; cd ~/src/cmamp1\n&gt; dev_scripts/client_setup/build.sh\n</code></pre> <p>Last review: GP on 2024-05-07</p>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html","title":"Architecture Diagrams","text":""},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#architecture-diagrams_1","title":"Architecture Diagrams","text":""},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#summary","title":"Summary","text":"<ul> <li>We use C4 as a way to describe graphically software architecture together with   some conventions</li> <li>Both Mermaid and PlantUML support C4 diagrams</li> <li>Mermaid is preferred since it can be rendered natively by GitHub</li> <li>PlantUML can be rendered through some of our scripts in regular markdown</li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#brief-introduction-to-c4","title":"Brief introduction to C4","text":"<ul> <li> <p>A detailed description of C4 is https://C4model.com</p> </li> <li> <p>C4 stands for \"context, container, component, code\" (the 4 Cs)</p> </li> <li> <p>C4 model helps developers describe software architecture</p> </li> <li>It maps code at various level of detail</li> <li>It is useful for both software architects and developers</li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#different-levels-of-detail","title":"Different levels of detail","text":"<ul> <li>The 4 levels of detail are:</li> <li>(System) Context system<ul> <li>How the system fits in the world</li> </ul> </li> <li>Container<ul> <li>High-level technical blocks</li> </ul> </li> <li>Component<ul> <li>Show the components inside a container (i.e., a high-level block)</li> </ul> </li> <li>Code<ul> <li>Show how components are implemented</li> <li>Represented in terms of UML class diagrams</li> </ul> </li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#system-context-level-1","title":"(System) Context (Level 1)","text":"<ul> <li>A system context describes something that delivers value to its users</li> <li> <p>Typically a system is owned by a single software development team</p> </li> <li> <p>System context diagram shows the big picture of how the software system   interacts with users and other systems in the IT environment</p> </li> <li> <p>The focus is not on:</p> </li> <li>Technologies</li> <li>Protocols</li> <li> <p>Low-level details</p> </li> <li> <p>Audience:</p> </li> <li>Both technical and non-technical people</li> <li> <p>Both inside and outside the software development team</p> </li> <li> <p>A system system is made up of one or more containers</p> </li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#container-level-2","title":"Container (Level 2)","text":"<ul> <li>A container represents an application</li> <li> <p>E.g.,</p> <ul> <li>Server-side web application (e.g., Tomcat running Java EE web application,   Ruby on Rails application)</li> <li>Client-side web application (e.g., JavaScript running in a web browser,   e.g., using Angular)</li> <li>Client-side desktop application (e.g., an macOS application)</li> <li>Mobile app (e.g., an iOS or Android app)</li> <li>Server-side console application</li> <li>Server-less function (e.g., AWS Lambda)</li> <li>Database (e.g., MySQL, MongoDB)</li> <li>Content-store (e.g., AWS S3)</li> <li>File-system (e.g., a local filesystem)</li> <li>Shell script</li> </ul> </li> <li> <p>A container runs some code and stores some data</p> </li> <li>Typically each container runs in its own process space</li> <li> <p>Containers communicate through inter-process communication</p> </li> <li> <p>A container diagram shows the high-level shape of the software architecture   and how responsibilities are distributed across it</p> </li> <li> <p>A container is the sum of components</p> </li> <li>All components inside a container execute together</li> <li> <p>Components can't be deployed as separate units</p> </li> <li> <p>Audience:</p> </li> <li>Technical people</li> <li>Inside and outside of the software development team</li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#component-level-3","title":"Component (level 3)","text":"<ul> <li>Component is a group of related functionality encapsulated behind a   well-defined interface</li> <li> <p>E.g., collection of classes behind an interface</p> </li> <li> <p>A component diagram decomposes each container to identify major structural   building blocks and interactions</p> </li> <li> <p>Audience</p> </li> <li>Software architects and developers</li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#code-level-4","title":"Code (level 4)","text":"<ul> <li>Code is the implementation of the software system</li> <li>Each component can represented in terms of UML class diagrams, entity     relationship diagrams, etc.</li> <li> <p>This diagram should be generated automatically from code</p> </li> <li> <p>Audience</p> </li> <li>Software architects and developers</li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#our-conventions-for-c4-diagrams","title":"Our conventions for C4 diagrams","text":""},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#mapping-c4-and-code-structure","title":"Mapping C4 and code structure","text":"<ul> <li>To simplify, we map the 4 levels of C4 in the code structure</li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#system-context-level-1_1","title":"(System) Context (Level 1)","text":"<ul> <li>= big picture of how the system interacts with users and other systems</li> <li>Mapped onto a code repository</li> <li>E.g.,</li> <li><code>//...</code> is a system providing data and analytics for commodity</li> <li><code>//pre-commit</code> is a system implementing a code linter</li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#container-level-2_1","title":"Container (Level 2)","text":"<ul> <li>= high-level software architecture and how responsibilities are split in the   system</li> <li>Mapped onto the first level of directories in a repo</li> <li>E.g., in <code>//...</code></li> <li><code>automl</code>: application for automatic machine learning for commodity analysis</li> <li><code>edgar</code>: application to handle EDGAR data</li> <li><code>etl3</code>: back-end db for time series with real-time and point-in-time     semantics</li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#component-level-3_1","title":"Component (Level 3)","text":"<ul> <li>= a group of related functionality encapsulated behind a well-defined   interface (e.g., collection of classes behind an interface)</li> <li>Mapped onto the second level of directory</li> <li>E.g., in <code>//.../edgar</code></li> <li><code>api</code>: real-time system storing the data from EDGAR</li> <li><code>company_commodity_mapping</code>: data pipeline to process mapping between     commodities and companies</li> <li><code>form8</code>: data pipeline processing form 8</li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#component-level-4","title":"Component (Level 4)","text":"<ul> <li>= OOP classes</li> <li>Typically we organize multiple related classes in files</li> <li>E.g., in <code>//.../edgar/form8</code></li> <li><code>analyze_results.py</code>: classes and functions to analyze results from the data     pipeline</li> <li><code>extract_tables.py</code>: class <code>TableExtractor</code> extracting tables from Form 8</li> <li><code>filter_tables.py</code>: class <code>TableFilterer</code></li> <li><code>match_targets.py</code></li> <li><code>normalize_table.py</code></li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#use-classes","title":"Use classes!","text":"<ul> <li> <p>In order to be able to describe the system with C4 it is best to use classes   to separate responsibilities and package code</p> </li> <li> <p>Using classes has the following advantages:</p> </li> <li>Organizes the code in cohesive parts</li> <li>Makes clear what is a public interface vs a private interface (e.g.,     helpers)</li> <li>Highlights responsibility (e.g., builder, annotation, processor, analyzer)</li> <li> <p>Simplifies the interface of functions by sharing state in the object</p> </li> <li> <p>Note that classes still allow our favorite functional style of programming</p> </li> <li>E.g., pandas is implemented with classes and it allows functional style</li> <li>The difference is going from:     <code>python     f(ton of arguments)</code>     to     <code>python     o(some argument).f(other arguments)</code></li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#generating-class-diagram","title":"Generating class diagram","text":"<ul> <li>To generate a class diagram (level 4 of C4), you can run   ```bash <p>dev_scripts/create_class_diagram.sh   ```</p> </li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#mermaid","title":"Mermaid","text":""},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#class-diagram","title":"Class diagram","text":"<ul> <li>See https://mermaid.js.org/syntax/classDiagram.html</li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#support-for-c4","title":"Support for C4","text":"<ul> <li>Mermaid supports most features of C4</li> <li>See https://mermaid.js.org/syntax/c4.html</li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#render-on-line","title":"Render on-line","text":"<ul> <li>See https://mermaid.live/edit</li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#plantuml","title":"PlantUML","text":"<ul> <li> <p>Unified Modeling Language (UML) is a modeling language for software   engineering to provide a standard way to visualize design of a system</p> </li> <li> <p>We use mainly Class Diagrams</p> </li> <li> <p>For information on some class diagram convention see     https://en.wikipedia.org/wiki/Class_diagram</p> </li> <li> <p>You can refer to the PDF guide at http://plantuml.com/guide for an extensive   description of what PlantUML can do</p> </li> <li> <p>We are mainly interested in the \"Class diagram\" section</p> </li> <li> <p>The website https://structurizr.com has lots of information on using tools for   C4 and lots of examples</p> </li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#plantuml-is-markdown","title":"PlantUML is Markdown","text":"<ul> <li>We use PlantUML for rendering diagrams in our documentation</li> <li> <p>For interactive use you can rely on online tools like:</p> <ul> <li>Online editors:</li> <li>planttext</li> <li>liveuml</li> <li>PlantUML Web Server</li> <li>PyCharm plugin (create and edit <code>.puml</code> file locally):</li> <li>PlantUML integration</li> </ul> </li> <li> <p>We create <code>README.md</code> and <code>architecture.md</code> markdown files to document   software. <code>README.md</code> is for general content, <code>architecture.md</code> is for code   architecture description. You can embed the diagrams in <code>architecture.md</code> file   in a correspondent folder.</p> </li> <li> <p>To render PlantUML in our markdown files instead of <code>@startuml</code> you need to   use the tag:   <code>txt   ```plantuml   ...   ```</code></p> </li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#render_mdpy-tool","title":"<code>render_md.py</code> tool","text":"<ul> <li>We have a <code>render_md.py</code> tool to embed images after <code>plantuml</code> section.   Typical usage to insert images to the markdown file and to preview it:   ```bash <p>render_md.py -i knowledge_graph/vendors/README.md   ```</p> </li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#how-to-use","title":"How to use","text":"<ol> <li> <p>Make sure <code>plantuml</code> is installed on your machine. The easiest way is to use    the Docker container. All the packages typically needed for development are    installed in the container.</p> </li> <li> <p>How to use:    ```bash</p> <p>render_md.py -h    ```</p> </li> <li> <p>We try to let the rendering engine do its job of deciding where to put stuff   even if sometimes it's not perfect. Otherwise, with any update of the text we   need to iterate on making it look nice: we don't want to do that.</p> </li> <li> <p><code>.md</code> files should be linted by our tools</p> </li> <li> <p>If you want to use <code>open</code> action, make sure that your machine is able to open    <code>.html</code> files in the browser.</p> </li> </ol>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#our-conventions","title":"Our conventions","text":"<ul> <li>Names</li> <li> <p>Each name in mappings should be exactly the same (maybe without some invalid     chars, like <code>.</code>) to not create a cognitive burden to the reader. It's better     to optimize for readability rather than by the number of chars. E.g.,</p> <p><code>plantuml [build_configs.py] as build_configs_py [TableExtractor] as TableExtractor</code>   - We keep components / classes in alphabetical order, so we can find them quickly in the code</p> </li> <li> <p>Notes</p> </li> <li> <p>Put notes describing some components / classes inside the blocks they refer     to. E.g.,</p> <p><code>plantuml node mapping as map { [CIK&lt;-&gt;Ticker] as ctmap note top of ctmap: My useful note. [CIK&lt;-&gt;GVKEY] as cgmap }</code>   - We use conventions for notes as for the code comments: - Start a note with a capital and end with <code>.</code>. In this way, it may be even   easier to visually distinguish notes from arrow labels. - Put notes straight after their related component definition, so a note   will look like a comment in the code</p> </li> <li> <p>Arcs</p> </li> <li> <p>The direction of the arcs represents the direction of the action. E.g.,</p> <p><code>plantuml apple --&gt; ground : falls to</code>   - We use the third person for describing actions</p> </li> <li> <p>We use comments as headers to organize the <code>architecture.md</code>. Note that the   comments in <code>plantuml</code> are introduced with <code>'</code>. Some frequently used headers   are:</p> </li> <li><code>' Components</code></li> <li><code>' Databases</code></li> <li><code>' Containers</code></li> <li><code>' Edge labels</code></li> <li> <p><code>' Notes</code></p> </li> <li> <p>An example of acceptable C4 diagram plantuml snippet:</p> </li> </ul> <p>```plantuml     ' Components     component [Edgar API] as Edgar_API     note top of Edgar_API : System storing the real-time\\nand historical data from EDGAR.     component [Headers dataset] as Headers_dataset</p> <pre><code>' Databases\ndatabase \"Compustat DB\" as Compustat_DB\nnote top of Compustat_DB : Third-party database\\nwith financial data.\n\n' Containers\nnode Form8 as form8 {\n [analyze_results.py] as analyze_results_py\n note left of analyze_results_py: Computes matching statistics.\n [build_configs.py] as build_configs_py\n [edgar_utils.py] as edgar_utils_py\n [run_pipeline.py] as run_pipeline_py\n [TableExtractor]\n note right of TableExtractor: Extracts forms tables.\n [TableNormalizer]\n note right of TableNormalizer: Normalizes extracted tables.\n [TableFilterer]\n note right of TableFilterer: Takes only financial tables\\nfrom normalized tables.\n [TargetMatcher]\n note right of TargetMatcher: Matches financial values in tables.\n}\nnode mapping as mapping {\n [CIK&lt;-&gt;Ticker] as CIK_Ticker\n [CIK&lt;-&gt;GVKEY] as CIK_GVKEY\n}\nnode universe as universe{\n [S&amp;P400]\n [S&amp;P500]\n [S&amp;P600]\n [S&amp;P1500]\n}\nnote left of universe: Universe of companies\\n as Tickers/GVKEYs.\n\n' Edge labels\nEdgar_API --&gt; edgar_utils_py: provides filings payloads to\nCompustat_DB --&gt; run_pipeline_py: provides target\\nvalues to match on to\nbuild_configs_py --&gt; run_pipeline_py: provides pipeline\\nparameters to\nedgar_utils_py --&gt; TableExtractor: provides universe filings to\nanalyze_results_py --&gt; run_pipeline_py: provides functions\\nto run the matching in to\nmapping --&gt; edgar_utils_py: provides mapping to construct\\n universe as CIKs to\nHeaders_dataset --&gt; analyze_results_py: provides filing\\ndates to\nTableExtractor --&gt; TableNormalizer: provides tables to be normalized to\nTableFilterer --&gt; run_pipeline_py: provides forms\\n values to be matched to\nTargetMatcher --&gt; analyze_results_py: matches values in\nTableNormalizer --&gt; TableFilterer: provides tables to be filtered to\nuniverse --&gt; mapping: provides universe of companies to\n</code></pre> <p>```</p> <p>You can find the correspondent <code>architecture.md</code> file   here.</p>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#plotting-online","title":"Plotting online","text":"<ul> <li>Plantuml on-line</li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#uml-unified-modeling-language","title":"UML - Unified Modeling Language","text":"<ul> <li> <p>The Unified Modeling Language (UML) serves as a versatile visual modeling   language designed to offer a standard way to visualize the design of a system</p> </li> <li> <p>UML employs a standardized notation for various diagram types, broadly   categorized into three primary groups:</p> </li> <li>Structure diagrams: These diagrams depict the static elements within the     system, highlighting essential components necessary in the modeled system.     As they focus on system structure, they are extensively utilized in     documenting software architecture</li> <li>Behavior diagrams: These diagrams portray the dynamic aspects of the     system, emphasizing the actions and processes required within the modeled     system. Given their role in illustrating system behavior, they are     extensively used to describe the functionality of software systems</li> <li>Interaction diagrams: A subset of behavior diagrams, these emphasize the     flow of data and control among the components within the modeled system</li> </ul> <p></p>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#uml-class-diagrams","title":"UML Class Diagrams","text":"<ul> <li>The UML Class Diagram is a graphical notation used to construct and visualize   object-oriented systems</li> <li>A class diagram in the Unified Modeling Language (UML) is a type of static   structure diagram that describes the structure of a system by showing the   system's:</li> <li>Classes</li> <li>Attributes</li> <li>Methods</li> <li>Relationships among objects</li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#class-notation","title":"Class Notation","text":"<ul> <li>A class represents a concept which encapsulates state (attributes) and   behavior (methods)</li> <li>The class name is the only mandatory information</li> <li>The class diagram consists of</li> <li>Class Name:<ul> <li>The name of the class appears in the first partition</li> <li>The first letter is capitalized</li> </ul> </li> <li>Class Attributes:<ul> <li>Attributes are shown in the second partition</li> <li>The attribute type is shown after the colon</li> <li>The first letter is lowercase</li> <li>Attributes map onto member variables (data members) in code</li> </ul> </li> <li>Class Methods:<ul> <li>Methods are shown in the third partition. They are services the class   provides</li> <li>The return type of a method is shown after the colon at the end of the   method signature</li> <li>The return type of method parameters are shown after the colon following   the parameter name</li> <li>The first letter is lowercase</li> <li>Methods map onto class methods in code</li> </ul> </li> <li> <p>Class Visibility:</p> <ul> <li>The <code>+, -, #</code> symbols before an attribute and operation name in a class   denote the visibility of the attribute and operation</li> <li><code>+</code> denotes public attributes or methods</li> <li><code>-</code> denotes private attributes or methods</li> <li><code>#</code> denotes protected attributes or methods</li> </ul> </li> <li> <p>An example of a Class Diagram is below</p> </li> </ul> <p>```mermaid   classDiagram</p> <p>class BankAccount{     #owner: str     +balance: int     +deposit(amount: float) bool     +withdrawal(amount: float) int     -checkBalance(account_number: int) float   }   ```</p>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#class-relationships","title":"Class Relationships","text":"<ul> <li>Classes can engage in multiple relationships with other classes</li> <li>Relationships in UML class diagrams can be defined in several distinct types</li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#inheritancegeneralization","title":"Inheritance/Generalization","text":"<ul> <li>It indicates that:</li> <li>One of the two related classes (the subclass) is considered to be a     specialized form of the other (the superclass) and</li> <li>The superclass is considered a generalization of the subclass</li> <li>Each instance of the subclass is also an indirect instance of the superclass</li> <li>Represents an \"is-a\" relationship</li> <li> <p>An abstract class name is shown in italics</p> </li> <li> <p>Example: in the given diagram, class <code>Animal</code> is the superclass and class   <code>Duck</code> and <code>Fish</code> are its subclass</p> </li> </ul> <p>```mermaid   classDiagram</p> <p>Animal &lt;|-- Duck   Animal &lt;|-- Fish   Animal : +int age   Animal: +isMammal()   class Duck{     +String beakColor     +swim()   }   class Fish{     -int sizeInFeet     -canEat()   }   ```</p>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#association","title":"Association","text":"<ul> <li>Associations are relationships between classes in a UML Class Diagram</li> <li>They are represented by a solid line between classes</li> <li>Simple Association</li> <li>A structural link between two peer classes</li> <li>There is an association between Class1 and Class2</li> </ul> <p>```mermaid   classDiagram</p> <p>Class1 --&gt; Class2   ```</p> <ul> <li>Aggregation</li> <li>A special type of association which represents a \"part of\" relationship</li> <li>Variant of the \"has-a\" association relationship</li> <li>Many instances (denoted by the diamond) of <code>Duck</code> can be associated with     <code>Pond</code></li> <li><code>Duck</code> is part of <code>Pond</code></li> <li>Objects of <code>Pond</code> and <code>Duck</code> have separate lifetimes</li> <li>In the given e.g., a Pond has zero or more Ducks, and a Duck has at most one     Pond (at a time). Duck can exist separately from a Pond</li> </ul> <p>```mermaid   classDiagram</p> <p>Pond o-- Duck   ```</p> <ul> <li>Composition</li> <li>A special type of aggregation where parts are destroyed when the whole is     destroyed</li> <li>Objects of <code>Class2</code> live and die with <code>Class1</code></li> <li><code>Class2</code> cannot stand by itself</li> <li>In the given e.g., a <code>Car</code> has exactly one <code>Carburetor</code>, and a <code>Carburetor</code></li> <li>Is a part of one <code>Car</code>. <code>Carburetor</code>s cannot exist as separate parts,     detached from a specific <code>Car</code></li> </ul> <p>```mermaid   classDiagram</p> <p>Car *-- Carburetor   ```</p> <ul> <li>Dependency</li> <li>An object of one class might use an object of another class in the code of a     method. If the object is not stored in any field, then this is modeled as a     dependency relationship</li> <li>Exists between two classes if changes to the definition of one may cause     changes to the other (but not the other way around)</li> <li><code>Class1</code> depends on <code>Class2</code></li> </ul> <p>```mermaid   classDiagram</p> <p>Class1 ..&gt; Class2   ```</p> <ul> <li>Realization</li> <li>It is a relationship between the blueprint class and the object containing     its respective implementation level details</li> <li>This object is said to realize the blueprint class</li> <li>For e.g., the <code>Owner</code> interface might specify methods for acquiring property     and disposing of property. The <code>Person</code> and <code>Corporation</code> classes need to     implement these methods, possibly in very different ways</li> </ul> <p>```mermaid   classDiagram</p> <p>class Owner{     &lt;&gt;     +accquire(property: str) bool     +dispose(property: str) bool   } <p>class Person{     -real     -tangible     +accquire(property: str) bool     +dispose(property: str) bool   }</p> <p>class Corporation{     -current     -fixed     -longterm     +accquire(property: str) bool     +dispose(property: str) bool   }</p> <p>Person ..|&gt; Owner   Corporation ..|&gt; Owner   ```</p>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#complete-example","title":"Complete Example","text":"<pre><code>\nclassDiagram\n    class Animal {\n        - String name\n        - int age\n        + Animal(name: String, age: int)\n        + setName(name: String): void\n        + setAge(age: int): void\n        + getName(): String\n        + getAge(): int\n        + makeSound(): void\n    }\n\n    class Dog {\n        - String breed\n        + Dog(name: String, age: int, breed: String)\n        + setBreed(breed: String): void\n        + getBreed(): String\n        + makeSound(): void\n    }\n\n    class Cat {\n        - boolean isLazy\n        + Cat(name: String, age: int, isLazy: boolean)\n        + setIsLazy(isLazy: boolean): void\n        + getIsLazy(): boolean\n        + makeSound(): void\n    }\n\n    class Car {\n        - String manufacturer\n        - String model\n        + Car(manufacturer: String, model: String)\n        + setManufacturer(manufacturer: String): void\n        + setModel(model: String): void\n        + getManufacturer(): String\n        + getModel(): String\n        + startEngine(): void\n    }\n\n    class Person {\n        - String name\n        - int age\n        - List&lt;Car&gt; ownedCars\n        + Person(name: String, age: int)\n        + setName(name: String): void\n        + setAge(age: int): void\n        + getName(): String\n        + getAge(): int\n        + addCar(car: Car): void\n        + removeCar(car: Car): void\n        + getOwnedCars(): List&lt;Car&gt;\n    }\n\n    Animal &lt;|-- Dog\n    Animal &lt;|-- Cat\n    Person \"1\" *-- \"*\" Car : owns\n\n    class SoundMaker {\n        &lt;&lt;interface&gt;&gt;\n        + makeSound(): void\n    }\n\n    SoundMaker &lt;|-- Animal\n    SoundMaker &lt;|-- Dog\n    SoundMaker &lt;|-- Cat\n\n    class Zoo {\n        - List&lt;Animal&gt; animals\n        + Zoo()\n        + addAnimal(animal: Animal): void\n        + removeAnimal(animal: Animal): void\n        + getAnimals(): List&lt;Animal&gt;\n    }\n\n    Zoo \"1\" *-- \"*\" Animal : contains\n</code></pre>"},{"location":"documentation_meta/all.diataxis.explanation.html","title":"Diataxis","text":""},{"location":"documentation_meta/all.diataxis.explanation.html#diataxis-a-framework-to-write-documentation","title":"Diataxis: a framework to write documentation","text":"<p>For more information look https://diataxis.fr/</p> <p>There are 4 modes of documentation</p> <ul> <li>Tutorial</li> <li>How-to guide</li> <li>Reference</li> <li>Explanation</li> </ul> <p></p>"},{"location":"documentation_meta/all.diataxis.explanation.html#tutorial","title":"Tutorial","text":"<ul> <li>Learning oriented</li> <li>Is a playground for users to learn something about the product by completing a   set of steps and achieving a meaningful result</li> <li>Helps the user/client achieve more understanding of the product</li> <li>Assumes the user does not have prior knowledge of the features used in the   tutorial</li> <li>Food analogy: teaching a child how to cook starting from cutting carrots,   celery, and onions for lasagna \"battuto\"</li> <li>File suffix: <code>.tutorial.md</code></li> <li>Example:   datapull/ck.create_airflow_dag.tutorial.md</li> </ul>"},{"location":"documentation_meta/all.diataxis.explanation.html#how-to-guide","title":"How-to guide","text":"<ul> <li>Goal oriented</li> <li>Is a guide to complete a real-world task</li> <li>Assumes the user has some knowledge about the background of used technology</li> <li>Food analogy: a recipe for cooking lasagna</li> <li>File suffix: <code>.how_to_guide.md</code></li> <li>Example:   work_tools/all.pycharm.how_to_guide.md</li> </ul>"},{"location":"documentation_meta/all.diataxis.explanation.html#reference","title":"Reference","text":"<ul> <li>Information oriented</li> <li>Provide a technical description of a component/piece of infra. The emphasis is   on completeness and accuracy</li> <li>Is difficult to keep everything up to date, so mostly rely on code and   docstring</li> <li>Food analogy: a reference encyclopaedia article about ingredients used in   lasagna (e.g., tomatoes, basil)</li> <li>File suffix: <code>.reference.md</code></li> <li>Example:   datapull/ck.ccxt_exchange_timestamp_interpretation.reference.md</li> </ul>"},{"location":"documentation_meta/all.diataxis.explanation.html#explanation","title":"Explanation","text":"<ul> <li>Understanding oriented</li> <li>Is used in our documentation to explain design decisions and choices,   architecture of components, how components interacted</li> <li>E.g., high level broker explanation (what kind of behavior the broker class     encapsulates)</li> <li>Does not provide specifications or instruction</li> <li>Food analogy: an article on culinary social history</li> <li>File suffix: <code>.explanation.md</code></li> <li>Example:   documentation_meta/all.architecture_diagrams.explanation.md</li> </ul>"},{"location":"documentation_meta/all.gdocs.how_to_guide.html","title":"All.gdocs.how to guide","text":"<p><code>- Use bullet lists to organize the whole Markdown for consistency with         other docs. See         [all.coding_style.how_to_guide.md](https://github.com/cryptokaizen/cmamp/blob/master/docs/coding/all.coding_style.how_to_guide.md)         or any other published Markdown format as reference       - Add missing</code> around code blocks. These could be missing in the         original Google doc. Also adjust code block indentations if needed       - The generated markdown may convert http links as <code>html</code> <code>&lt;span&gt;</code>         objects. This hinders the readability of the <code>md</code> file. In this case,         manually convert to a standard <code>http://</code> link:         - <code>[&lt;span class=\"underline\"&gt;https://www.sorrentum.org/&lt;/span&gt;](https://www.sorrentum.org/)</code>           -&gt; <code>https://www.sorrentum.org/</code>       - Replace the <code>html</code> <code>&lt;img&gt;</code> tag with a markdown link:         - <code>&lt;img src=\"docs/work_tools/figs/visual_studio_code/image1.png\"/&gt;</code> -&gt;           <code>![alt_text](docs/work_tools/figs/visual_studio_code/image1.png\")</code></p> <ul> <li>Remove empty lines manually   <code>markdown   :'&lt;,'&gt;! perl -ne 'print if /\\S/'</code></li> <li>Run the <code>lint_md.sh</code></li> <li>Usage:     <code>bash     &gt; dev_scripts/lint_md.sh docs/documentation_meta/all.writing_docs.how_to_guide.md</code></li> <li>What the linter will do:<ul> <li>Build TOC automatically</li> <li>Adjust the indentation to improve the Markdown's format (but the   precondition is that you have properly adjusted the indentation levels).</li> <li>Remove extra empty lines under headings</li> <li>Adjust text layout</li> </ul> </li> <li>Do not mix manual edits and linter runs</li> <li>If the linter messes up the text<ul> <li>File bugs in <code>amp</code> with examples what the linter does incorrectly</li> </ul> </li> <li>Last steps</li> <li>Compare the generated markdown file with the original Gdoc from top to     bottom to ensure accurate rendering.</li> <li>Review the markdown file on GitHub to make sure it looks good, as it may     slightly differ from the preview in your local markdown editor</li> <li>When a gdoc becomes obsolete or it's deleted</li> <li>Add a note at the top of a gdoc explaining what happened</li> <li>Example: \"Moved to /new_markdown_file.md\"</li> <li>Strike out the entire document</li> <li>Move the gdoc to the     _OLD directory</li> </ul>"},{"location":"documentation_meta/all.gdocs.how_to_guide.html#other-approaches","title":"Other approaches","text":"<ul> <li>Best for a large document</li> <li>Approach 1 - Chrome Docs to Markdown extension:</li> <li>Use the Docs to Markdown     extension<ul> <li>Install   the extension   from the G Suite marketplace</li> <li>User guide   for the extension</li> </ul> </li> <li>One needs to accept/reject all suggestions in a gdoc as the extension works     poorly when a document is edited in the suggestion mode</li> <li>Approach 2 - Online converter:</li> <li> <p>Google-docs-to-markdown/</p> </li> <li> <p>Also need to go through   Cleaning up converted markdown</p> </li> <li>You might need to remove artifacts manually</li> </ul>"},{"location":"documentation_meta/all.gdocs.how_to_guide.html#markdown-gdocs","title":"Markdown -&gt; Gdocs","text":"<ul> <li>Approach 1:</li> <li>Run     <code>bash     &gt; pandoc MyFile.md -f markdown -t odt -s -o MyFile.odt</code></li> <li>Download the     template     in odt format</li> <li>Run     <code>bash     &gt; pandoc code_organization.md -f markdown -t odt -s -o code_org.odt --reference-doc /Users/saggese/Downloads/Gdoc\\ -\\ Template.odt</code></li> <li>Open it with TextEdit, copy-paste to Gdoc</li> <li>Approach 2:</li> <li>Instead of copy-paste the markdown into Gdocs, you can copy the rendered     markdown in a Gdoc<ul> <li>Gdocs does a good job of maintaining the formatting, levels of the   headers, the links, and so on</li> </ul> </li> <li>Approach 3:</li> <li>https://markdownlivepreview.com/</li> <li>TODO(gp): Check if the roundtrip works</li> </ul>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html","title":"Google Technical Writing","text":"<p>// From https://developers.google.com/tech-writing/one/</p>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#googles-technical-writing-part-1","title":"Google's technical writing: Part 1","text":""},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#define-new-or-unfamiliar","title":"Define new or unfamiliar","text":"<ul> <li>If your document introduces a term, define the term</li> <li>If the term already exists, link to a good existing explanation</li> </ul>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#use-terms-consistently","title":"Use terms consistently","text":"<ul> <li>Don't change the name of something while talking about it</li> <li>E.g., <code>Protocol Buffers</code> vs <code>protobufs</code></li> <li>You can do something like:   <code>Protocol Buffers (or protobufs for short)</code></li> </ul>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#use-acronyms-properly","title":"Use acronyms properly","text":"<ul> <li>On the initial use of an unfamiliar acronym spell out the full term</li> <li>E.g., <code>Telekinetic Tactile Network (TTN) ...</code></li> <li>Acronyms take attention to be expanded in their full form</li> <li>Sometimes acronyms develop their own identity (e.g., HTML)</li> <li>An acronym should be significantly shorter than the full term</li> <li>Don't define acronyms that will be used only a few times</li> </ul>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#use-strong-verbs","title":"Use strong verbs","text":"<ul> <li>Choose precise, strong, and specific verbs</li> <li>Weak verbs are \"be\", \"occur\", \"happen\"</li> </ul> <p>Good</p> <pre><code>Dividing by zero raises the exception.\n</code></pre> <p>Bad</p> <pre><code>The exception occurs when dividing by zero.\n</code></pre>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#use-short-sentences","title":"Use short sentences","text":"<ul> <li>Each sentence should convey a single idea, thought, concept</li> <li>Break long sentences into single-idea sentences</li> <li>Convert long sentences into bulleted list</li> <li>E.g., \"and\", \"or\" suggest to refactor into a bulleted list</li> </ul>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#remove-fillers","title":"Remove fillers","text":"<p>Good</p> <pre><code>This design document describes Project Frambus.\n</code></pre> <p>Bad</p> <pre><code>This design document provides a detailed description of Project Frambus.\n</code></pre>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#focus-each-paragraph-on-a-single-topic","title":"Focus each paragraph on a single topic","text":"<ul> <li>A paragraph is an independent unit of logic</li> <li>Ruthlessly delete sentence that doesn't relate to the current topic</li> </ul>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#avoid-wall-of-text","title":"Avoid wall-of-text","text":"<ul> <li>Readers often ignore long paragraphs</li> <li>Paragraphs should contain 3 to 5 sentences</li> </ul>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#answer-what-why-and-how","title":"Answer what, why, and how","text":"<ul> <li>Good paragraphs answer the following questions</li> <li>What: what are you trying to tell your reader?</li> <li>Why: why is it important for the reader to know this?</li> <li> <p>How: how should the reader use this knowledge</p> </li> <li> <p>E.g.,</p> </li> <li>[What]: The <code>garp()</code> function returns the delta between a dataset's mean and     median.</li> <li>[Why]: Many people believe unquestioningly that a mean always holds the     truth. However, a mean is easily influenced by a few very large or very     small data points.</li> <li>[How]: Call <code>garp()</code> to help determine whether a few very large or very     small data points are influencing the mean too much. A relatively small     <code>garp()</code> value suggests that the mean is more meaningful than when the     <code>garp()</code> value is relatively high.</li> </ul>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#know-your-audience","title":"Know your audience","text":"<ul> <li>Your document needs to provide information that your audience needs but   doesn't already have</li> <li>Define your audience<ul> <li>E.g., software engineers vs program managers</li> <li>E.g., graduate students vs first-year undergraduate students</li> </ul> </li> <li>Determine what your audience needs to learn<ul> <li>E.g.,   ```verbatim   After reading the documentation, the audience will know how to do the   following tasks</li> <li>Use ...</li> <li>Do ...   ...   ```</li> </ul> </li> <li>Fit documentation to your audience<ul> <li>Avoid the \"curse of knowledge\": experts forget that novices don't know   what you already know</li> </ul> </li> </ul>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#state-documents-scope","title":"State document's scope","text":"<ul> <li>A good document begins by defining its scope and its non-scope, e.g.,   <code>This document describes the design of Project Frambus, but not the related   technology Froobus.</code></li> </ul>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#summarize-the-key-points-at-the-start","title":"Summarize the key points at the start","text":"<ul> <li>Ensure that the start of your document answers your readers' essential   questions</li> <li>The first page of a document determines if the readers makes it to page two</li> </ul> <p>// From https://developers.google.com/tech-writing/two</p>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#googles-technical-writing-part-2","title":"Google's technical writing: Part 2","text":""},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#adopt-a-style-guide","title":"Adopt a style guide","text":"<ul> <li>Many companies and large open source projects adopt a style guide for   documentation</li> <li>E.g., https://developers.google.com/style</li> </ul>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#think-like-your-audience","title":"Think like your audience","text":"<ul> <li>Step back and try to read your draft from the point of view of your audience</li> </ul>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#come-back-to-it-later","title":"Come back to it later","text":"<ul> <li>After you write your first (or second or third) draft, set it aside</li> <li>Come back later and read it with fresh eyes to find things you can improve</li> </ul>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#organizing-large-docs","title":"Organizing large docs","text":"<ul> <li>You can organize a collection of information into</li> <li>A longer standalone document; or</li> <li>Set of shorter interconnected documents (e.g., website, wiki)<ul> <li>Pros: easy to find information searching in the single back</li> </ul> </li> </ul> <p>// TODO</p>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#resources","title":"Resources","text":"<ul> <li>[https://developers.google.com/tech-writing/overview]</li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html","title":"Writing Docs","text":""},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#conventions","title":"Conventions","text":""},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#make-no-assumptions-on-the-users-knowledge","title":"Make no assumptions on the user's knowledge","text":"<ul> <li>Nothing is obvious to somebody who doesn't know</li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#verify-that-things-worked","title":"Verify that things worked","text":"<ul> <li>Add ways to verify if a described process worked</li> <li>E.g., \"do this and that, if this and that is correct should see this\"</li> <li>Have a trouble-shooting procedure</li> <li>One approach is to always start from scratch</li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#always-use-the-linter","title":"Always use the linter","text":"<ul> <li>Most cosmetic lints described further can be taken care automatically by our   markdown linter, so make sure to run it after implementing the changes</li> <li>Use <code>i lint</code>, since <code>dev_scripts/lint_md.sh</code> is discontinued</li> <li>Do not mix manual edits and linter runs. Best practice is to run the linter   and commit the changes it made as separate commit</li> <li>If the linter messes up the text, file an issue with examples of what the   linter does incorrectly</li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#add-a-table-of-content","title":"Add a table of content","text":"<ul> <li>Unfortunately both markdown and GitHub don't support automatically generating   a TOC for a document</li> <li>To generate a table of content add the following tag at the top of the   markdown file:   ```markdown   </li> </ul> <p><code>``   - Run</code>i lint` to build the TOC automatically</p>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#_1","title":"Writing Docs","text":"<ul> <li>Make sure the headings structure contains exactly one level 1 heading   (<code># This one</code>)</li> <li>This is important for displaying MkDocs documentation correctly via browser</li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#use-80-columns-formatting-for-md-files","title":"Use 80 columns formatting for md files","text":"<ul> <li>Our markdown linter takes care of reflowing the text</li> <li>Vim has a <code>:gq</code> command to reflow the comments</li> <li>There are plugins for PyCharm and VisualStudio</li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#use-good-vs-bad","title":"Use good vs bad","text":"<ul> <li>Make examples of \"good\" ways of doing something and contrast them with \"bad\"   ways</li> </ul> <p>Good</p> <p><code>markdown   ...</code></p> <p>Bad</p> <p><code>markdown   ...</code></p>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#use-an-empty-line-after-heading","title":"Use an empty line after heading","text":"<ul> <li>Leave an empty line after a heading to make it more visible, e.g.,</li> </ul> <p>Good</p> <p><code>markdown   # Very important title   - Less important text</code></p> <p>Bad</p> <p><code>markdown   # Coming through! I've big important things to do!   - ... and his big important wheels got STUCK!</code></p> <ul> <li>Our linter automatically takes care of this</li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#bullet-lists","title":"Bullet lists","text":"<ul> <li>We like using bullet list since they represent the thought process, force   people to focus on short sentences (instead of rambling wall-of-text), and   relation between sentences</li> <li>E.g.,   ```markdown</li> <li>This is thought #1<ul> <li>This is related to thought #1</li> </ul> </li> <li>This is thought #2<ul> <li>Well, that was cool!</li> <li>But this is even better   ```</li> </ul> </li> <li>We use <code>-</code> instead of <code>*</code> or circles</li> <li>The linter automatically enforces this</li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#use-the-right-syntax-highlighting","title":"Use the right syntax highlighting","text":"<ul> <li>When using a block of code use the write syntax highlighting</li> <li>Code (```python)</li> <li>Dirs (e.g.,<code>/home/users</code>)</li> <li>Command lines (e.g., <code>&gt; git push</code> or <code>docker&gt; pytest</code>)</li> <li>Bash     <code>bash     &gt; git push</code></li> <li>Python     <code>python     if __name__ == \"__main__\":         predict_the_future()         print(\"done!\")</code></li> <li>Markdown     <code>markdown     ...</code></li> <li>Nothing     <code>verbatim     ....</code></li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#indent-code-style","title":"Indent <code>code</code> style","text":"<ul> <li>GitHub / Pandoc seems to render incorrectly a code block unless it's indented   over the previous line</li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#embed-screenshots-only-when-strictly-necessary","title":"Embed screenshots only when strictly necessary","text":"<ul> <li>Avoid to use screenshots whenever possible and use copy-paste of text with the   right highlighting</li> <li>However, sometimes we need to use screenshots (e.g., plots, website interface)</li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#improve-your-written-english","title":"Improve your written English","text":"<ul> <li>Use English spell-checker, but unfortunately this is not enough</li> <li>Type somewhere where you can use several choices:</li> <li>Grammarly</li> <li>ChatGPT</li> <li>LanguageTool</li> <li>This is super-useful to improve your English since you see the error and the   correction</li> <li>Otherwise you will keep making the same mistakes forever</li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#make-sure-your-markdown-looks-good","title":"Make sure your markdown looks good","text":"<ul> <li> <p>Compare your markdown with other already published</p> </li> <li> <p>You can:</p> </li> <li>Check in the code a branch and use GitHub to render it</li> <li>Use Pycharm to edit, which also renders it side-by-side</li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#do-not-overcapitalize-headings","title":"Do not overcapitalize headings","text":"<ul> <li>Paragraph titles should be like <code>Data schema</code> not <code>Data Schema</code></li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#update-the-last-review-tag","title":"Update the <code>Last review</code> tag","text":"<ul> <li>When you read/refresh a file update the last line of the text   <code>verbatim   Last review: GP on 2024-04-20, Paul on 2024-03-10</code></li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#comment-the-code-structure","title":"Comment the code structure","text":"<ul> <li>When you want to describe and comment the code structure do something like   this   ``` <p>tree.sh -p data_schema   data_schema/   |-- dataset_schema_versions/   |   <code>-- dataset_schema_v3.json     Description of the current schema   |-- test/   |   |-- __init__.py   |</code>-- test_dataset_schema_utils.py   |-- init.py   |-- changelog.txt     Changelog for dataset schema updates   |-- dataset_schema_utils.py     Utilities to parse schema   `-- validate_dataset_signature.py*     Script to test a schema   ```</p> </li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#convention-for-file-names","title":"Convention for file names","text":"<ul> <li>Each file name should have a format like   <code>docs/{component}/{audience}.{topic}.{diataxis_tag}.md</code></li> <li>E.g., <code>docs/documentation_meta/all.diataxis.explanation.md</code></li> <li>Where</li> <li><code>component</code> is one of the software components (e.g., <code>datapull</code>, <code>dataflow</code>)</li> <li><code>audience</code> is the target audience (e.g., <code>all</code>, <code>ck</code>)</li> <li><code>topic</code> is the topic of the file</li> <li>An how to guide should have a verb-object format<ul> <li>E.g., <code>docs/oms/broker/ck.generate_broker_test_data.how_to_guide.md</code></li> </ul> </li> <li>A reference often has just a name<ul> <li>E.g., <code>docs/oms/broker/ck.binance_terms.reference.md</code></li> </ul> </li> </ul> <p>// From https://opensource.com/article/20/3/documentation</p>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#use-active-voice","title":"Use active voice","text":"<ul> <li>Use the active voice most of th time and use the passive voice sparingly</li> <li>Active voice is shorter than passive voice</li> <li>Readers convert passive voice to active voice</li> </ul> <p>Good</p> <ul> <li>You can change these configuration by ...</li> </ul> <p>Bad</p> <ul> <li>There configurations can be changed by ...</li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#use-simple-short-sentences","title":"Use simple short sentences","text":"<ul> <li>Use Grammarly/ChatGPT</li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#format-for-easy-reading","title":"Format for easy reading","text":"<ul> <li>Use headings, bullet points, and links to break up information into chunks   instead of long explanatory paragraphs</li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#keep-it-visual","title":"Keep it visual","text":"<ul> <li>Use tables and diagrams, together with text, whenever possible</li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#mind-your-spelling","title":"Mind your spelling","text":"<ul> <li>Always, always, always spell check for typos and grammar check</li> <li>Use Grammarly/ChatGPT</li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#be-efficient","title":"Be efficient","text":"<ul> <li>Nobody wants to read meandering paragraphs in documentation</li> <li>Engineers want to get technical information as efficiently as possible</li> <li>Do not add \"fluff\"</li> <li>Do not explain things in a repetitive way</li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#do-not-add-fluff","title":"Do not add fluff","text":"<ul> <li>Always point to documentation on the web instead of summarizing it</li> <li>If you want to summarize some doc (e.g., so that people don't have to read too   much) add it to a different document instead of mixing with our documentation</li> <li>Focus on how we do, why we do, rather than writing AI-generated essays</li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#resources","title":"Resources","text":"<ul> <li>[https://opensource.com/article/20/3/documentation]</li> <li>Markdown cheatsheet</li> <li>Google guide to Markdown</li> <li>TODO(gp): Make sure it's compatible with our linter</li> </ul>"},{"location":"documentation_meta/plotting_in_latex.how_to_guide.html","title":"Plotting in latex.how to guide","text":"<p>For plotting a certain classes of drawings (e.g., diagrams, graph) one should use frameworks like dot, mermaid, plantuml</p> <p>For technical drawing there are several solutions, as described below</p>"},{"location":"documentation_meta/plotting_in_latex.how_to_guide.html#tikz","title":"Tikz","text":"<ul> <li>Language for producing vector graphics from a textual description</li> <li> <p>Several drawing programs can export figures as Tikz format (e.g., Inkspace,   matplotlib, gnuplot)</p> </li> <li> <p>Refs</p> </li> <li>https://en.wikipedia.org/wiki/PGF/TikZ</li> <li>https://tikz.net/</li> <li>https://tikz.org/</li> <li>Examples<ul> <li>https://tikz.dev/</li> <li>https://texample.net/tikz/examples/</li> <li>https://tex.stackexchange.com/questions/175969/block-diagrams-using-tikz</li> </ul> </li> <li>Web application<ul> <li>https://tikzmaker.com/editor</li> </ul> </li> <li>Local editor<ul> <li>https://tikzit.github.io/</li> </ul> </li> <li>Misc<ul> <li>https://tex.stackexchange.com/?tags=tikz-pgf</li> </ul> </li> </ul>"},{"location":"documentation_meta/plotting_in_latex.how_to_guide.html#pgfplots","title":"Pgfplots","text":"<ul> <li>Plots functions directly in Tex/Latex</li> <li> <p>Based on TikZ</p> </li> <li> <p>Refs</p> </li> <li>https://ctan.math.washington.edu/tex-archive/graphics/pgf/contrib/pgfplots/doc/pgfplots.pdf</li> <li>https://www.overleaf.com/learn/latex/Pgfplots_package</li> </ul>"},{"location":"documentation_meta/plotting_in_latex.how_to_guide.html#asymptote","title":"Asymptote","text":"<ul> <li>A descriptive vector graphics language</li> <li>Provide a coordinate-based framework for technical drawing</li> <li> <p>It has a Python frontend</p> </li> <li> <p>Refs</p> </li> <li>https://en.wikipedia.org/wiki/Asymptote_(vector_graphics_language)</li> <li>https://asymptote.sourceforge.io/</li> <li>Gallery<ul> <li>https://asymptote.sourceforge.io/gallery/</li> </ul> </li> <li>Asymptote web application<ul> <li>http://asymptote.ualberta.ca/</li> </ul> </li> </ul>"},{"location":"documentation_meta/plotting_in_latex.how_to_guide.html#plotting-in-markdown","title":"Plotting in markdown","text":""},{"location":"documentation_meta/plotting_in_latex.how_to_guide.html#how-to-draw-in-markdown","title":"How to draw in markdown","text":"<p>We would like to use the same plots for both Latex and Markdown documents</p> <p>We can use pandoc</p> <p>TODO(gp): Consider extending ./dev_scripts/documentation/render_md.py to render also complex Latex, tikz</p> <p>https://tex.stackexchange.com/questions/586285/pandoc-markdown-drawing-circuit-diagrams-using-circuitikz</p>"},{"location":"general_background/all.common_abbreviations.reference.html","title":"Common abbreviations","text":"<ul> <li>-&gt; room, -&gt; = \"let\u2019s go to the conference room\" (typically the one pinned in   the chat)</li> <li>AFAIK = as far as I know</li> <li>AFK, AFTK = away from (the) keyboard</li> <li>BM = Build-meister</li> <li>BRB = be right back</li> <li>CWR = call when ready</li> <li>GH = GitHub</li> <li>IMO = in my opinion</li> <li>KG = knowledge graph</li> <li>KG-OG = KG Original Gangsta</li> <li>KG-fication = The process of turning something in a KG</li> <li>KOTH = King Of The Hill (the best model so far)</li> <li>OOO, OOTO = Out Of The Office</li> <li>PR = Pull Request (we prefer this to MR = merge request)</li> <li>PTAL = please take a look</li> <li>SO = stack overflow</li> <li>Skateboard = getting something working end-to-end even with hacks (as long as   reversible)</li> <li>TBH = to be honest</li> <li>TG = Telegram</li> <li>TTYL = talk to you later</li> <li>WFH = working from home</li> <li>WIP = work in progress</li> <li>Windows = the worst OS ever made</li> <li>ZH = ZenHub</li> <li>Np = no problem</li> <li>Sg = sounds good</li> <li>Vim = the best editor ever made</li> </ul>"},{"location":"general_background/all.glossary.reference.html","title":"Glossary","text":""},{"location":"general_background/all.glossary.reference.html#meta","title":"Meta","text":"<ul> <li>Keep the terms in alphabetical order</li> <li>People can add terms that they are not clear and others can add definitions</li> <li>Always use \"suggestion\" mode for adding new terms or new definitions</li> </ul>"},{"location":"general_background/all.glossary.reference.html#definitions","title":"Definitions","text":"<ul> <li>Asset</li> <li>A financial instrument with an associated price that changes over time</li> <li>Aka: symbol, name, ticker</li> <li> <p>E.g., bitcoin, ethereum, Apple stock (US equity), orange futures</p> </li> <li> <p>FM (Financial Instrument)</p> </li> <li> <p>Financial instruments are monetary contracts between parties</p> </li> <li> <p>GH (GitHub)</p> </li> <li> <p>HLD (High Level Design)</p> </li> <li> <p>Is a general system design and includes the description of the System     architecture and design</p> </li> <li> <p>IM (Instrument Master)</p> </li> <li> <p>A software component that associates symbolic names to assets and their     prices</p> </li> <li> <p>Integrator</p> </li> <li>Someone on the team that is in charge of merging code to the main line of     development</li> <li> <p>Aka: master</p> </li> <li> <p>OHLCV bar</p> </li> <li> <p>An open-high-low-close chart (also OHLC) is a type of chart     typically used to illustrate movements in the price of a financial     instrument over time</p> </li> <li> <p>OMS (Order Management System)</p> </li> <li> <p>A software component in charge of placing and monitoring trading orders to     market or broker</p> </li> <li> <p>PR (Pull Request)</p> </li> <li> <p>Request to merge code in GitHub</p> </li> <li> <p>RP (Responsible Party)</p> </li> <li>Someone on the team that helps following our process</li> <li> <p>Aka: tech lead</p> </li> <li> <p>ZH (ZenHub)</p> </li> <li>Our tool for project management</li> </ul>"},{"location":"general_background/all.literature_review.reference.html","title":"Literature Review","text":""},{"location":"general_background/all.literature_review.reference.html#meta","title":"Meta","text":"<ul> <li>Year - Title</li> <li>Paper authors:</li> <li>Link to the paper (ideally on gdrive)</li> <li>Review author / date</li> <li>Score in [0, 5], where:</li> <li>5/5: Must-read</li> <li>4/5: Some interesting ideas we can reuse</li> <li>3/5: Pretty much what one would have done as first experiment</li> <li>2/5: ...</li> <li>1/5: Horrible: same bullet-proof logic as in a politician speech</li> <li>Summary:</li> <li>At most 5-10 bullet points explaining what the paper tries to accomplish</li> <li>Describe the data used, setup, model formulation, ...</li> <li>Good references</li> <li>Praises:</li> <li>At most 5 bullet points</li> <li>Focus on what is different, interesting, and not on the obvious</li> <li>Critiques:</li> <li>At most 5 bullet points</li> <li>Explain what is not solid in the analysis, suggestions on how to improve</li> <li>Next steps:</li> <li>What next steps should we take, if any, e.g.,<ul> <li>Read the bibliography</li> <li>Try experiments</li> </ul> </li> </ul>"},{"location":"general_background/all.literature_review.reference.html#to-cut-and-paste","title":"To cut and paste","text":"<pre><code>\n### Year - Title\n- Paper authors:\n- [Link]()\n- Review author / date:\n- Score:\n- Summary:\n- Praises:\n- Critiques:\n- Next steps:\n</code></pre>"},{"location":"general_background/all.literature_review.reference.html#news-for-commodity-prediction","title":"News for commodity prediction","text":""},{"location":"general_background/all.literature_review.reference.html#2015-the-role-of-news-in-commodity-markets","title":"2015 - The role of news in commodity markets","text":"<ul> <li>Paper authors: Borovkova</li> <li>Link</li> <li>Review author / date: GP, 2019-11-22</li> <li>Score: 4/5</li> <li>Summary:</li> <li>Dataset: prepackaged Thomson-Reuters sentiment (TRNA)</li> <li>Studies the effect of sentiment on commodities through event studies</li> <li>Forecast prices and volatility</li> <li>Praises:</li> <li>Decent statistics about the data set</li> <li>States that one needs to understand if the sentiment is attached to demand     and supply<ul> <li>Not sure if TR actually does that</li> </ul> </li> <li>Confirms our point about \"momentum-related news\" (i.e., news about the fact     that the price is going up)</li> <li>Confirms periodicity we are aware of</li> <li>Interesting local level model to extract the hidden sentiment<ul> <li>Very similar to what we thought to do (including the idea of using Kalman   smoother)</li> </ul> </li> <li>Critiques:</li> <li>Nothing really</li> <li>Next steps:</li> <li>Understand if TR considers sentiment distinguishing supply or demand<ul> <li>We should do this (not sure how PR does that)</li> </ul> </li> <li>Remove carefully momentum-related news</li> <li>Remove or count carefully repeated news (maybe use a measure of similarity     between articles)</li> <li>How to deliver \"event study\" models to customers? Should we \"unroll the     model\" for them providing a stream of predictions?</li> </ul>"},{"location":"general_background/all.literature_review.reference.html#social-sentiment","title":"Social sentiment","text":""},{"location":"general_background/all.literature_review.reference.html#2015-predicting-global-economic-activity-with-media-analytics","title":"2015, Predicting global economic activity with media analytics","text":"<ul> <li>Paper authors: Peterson et al.</li> <li>Link: In <code>Tech/papers</code></li> <li>Review author / date: GP, 2019/12/08</li> <li>Score: 2/5</li> <li>Summary:</li> <li>Predict PMI indices (which are related to the</li> <li>Praises:</li> <li>Interesting approach for going beyond polarity in sentiment considering</li> <li>Critiques:</li> <li>No seasonal component</li> <li>Usual problems with methodology OOS</li> <li>Next steps:</li> <li>Consider the TRMI \"indices\" (optimism, fear, joy, trust, violence)</li> <li>Consider the difference in professional news vs social news sentiment<ul> <li>What does it mean if there are large statistically significant difference?</li> </ul> </li> </ul>"},{"location":"general_background/all.literature_review.reference.html#2018-twitter-investor-sentiment-and-capital-markets-what-do-we-know","title":"2018 - Twitter, Investor Sentiment and Capital Markets, what do we know?","text":"<ul> <li>Paper authors:</li> <li>Review author: GP, 2019-08-21</li> <li>Link:</li> <li>Score: 3 / 5</li> <li>Summary:</li> <li>Good survey of the literature about social sentiment used for finance</li> <li>Most authors report predictivity of social sentiment for:</li> <li>Different metrics (returns, risk, trading volume)</li> <li>Assets (US stocks, exchange rates, commodities)</li> <li>Events (IPO, earnings)</li> <li>Next steps:</li> <li>Read all the bibliography and reproduce some of the results</li> <li>TODO: Update this to new template</li> </ul>"},{"location":"general_background/all.literature_review.reference.html#time-series","title":"Time series","text":""},{"location":"general_background/all.literature_review.reference.html#on-line-learning-of-linear-dynamical-systems-exponential-forgetting-in-kalman-filters","title":"On-Line Learning of Linear Dynamical Systems: Exponential Forgetting in Kalman Filters","text":"<ul> <li>Paper authors: Mark Kozdoba, Jakub Marecek, Tigran Tchrakian, and Shie Mannor</li> <li>Review author: Paul, 2019-12-02</li> <li>arXiv,   AAAI</li> <li>Score: 4/5</li> <li>Summary:</li> <li>Interesting insight into how to approximate a non-convex optimization     problem with an approximate convex one</li> <li>Shows that for observable Linear Dynamical Systems with non-degenerate     noise, the dependence of the Kalman filter on the past decays exponentially</li> <li>For this class of systems, predictions may be modeled as autoregressions. In     practice, not many terms are needed for a \"good\" approximation.</li> <li>The algorithm is on-line</li> <li>Comparison to the Kalman filter is formalized with regret bounds</li> <li>IBM / Technion research</li> <li>The setting is one where we are learning the best fixed but unknown     autoregression coefficients (rather than one where we are interested in     truly dynamic updates)<ul> <li>The learning rate decays like $1 / \\sqrt{t}$, and so under some mild   constraints on the time series being modeled, the autoregression   coefficients converge</li> <li>The linear dynamical system setup considered is one where the state   transition matrix and the observation direction are time-independent</li> </ul> </li> <li>Praises:</li> <li>References standard big works in the time series literature, like West and     Harrison (1997) and Hamilton (1994)</li> <li>Introduces a relatively simple online technique that competes well with the     more complex Kalman filter</li> <li>Critiques:</li> <li>Bounds / constants aren't quantitative</li> <li>Next steps:</li> <li>Look at the code accompanying the paper:     https://github.com/jmarecek/OnlineLDS</li> <li>Implement and compare to, e.g., z-scoring (a particularly simple case of     Kalman filtering)</li> <li>If we have a long history, it may be better to perform a single     autoregression over the whole history<ul> <li>This suggests</li> </ul> </li> <li>What if we keep the learning rate fixed over time?<ul> <li>This would effectively allow for \"drifting\" dynamics</li> <li>The proofs of the results of the paper would no longer apply</li> <li>It isn't obvious how the learning rate ought to be chosen</li> </ul> </li> </ul>"},{"location":"general_background/all.literature_review.reference.html#predictive-state-smoothing-press-scalable-non-parametric-regression-for-high-dimensional-data-with-variable-selection","title":"Predictive State Smoothing (PRESS): Scalable non-parametric regression for high-dimensional data with variable selection","text":"<ul> <li>Paper author: Georg M. Goerg</li> <li>Review author: Paul, 2019-12-03</li> <li>Link</li> <li>Score: 4/5</li> <li>Summary:</li> <li>A kernel smoother, but unlike traditional ones, it<ul> <li>Allows non-local (with respect to the x-var space) pooling</li> <li>Is scalable (e.g., computationally efficient)</li> </ul> </li> <li>PRESS is a generative, probabilistic model</li> <li>States are interpretable</li> <li>Compatible with deep neural networks (though experiments referenced in the     paper suggest depth doesn't help, e.g., a wide net with one softmax is     enough)</li> <li>Competitive with SVMs, Random Forests, and DNN</li> <li> <p>Predictive state representations are statistically and computationally     &gt; efficient for obtaining optimal forecasts of non-linear dynamical systems     &gt; (Shalizi and Crutchfield, 2001). Examples include time series forecasting     &gt; via epsilon-machines (Shalizi and Shalizi, 2004)...</p> </li> <li>Praises:</li> <li>Combines some clever insights</li> <li>References a TensorFlow implementation and suggests that implementing in     various frameworks is straightforward</li> <li>Critiques:</li> <li>No pointers to actual implementations</li> <li>Time series applications are referenced in Section 2, but many relevant (to     our work) practical ts-specific points are not developed in the paper</li> <li>Next steps:</li> <li>See if someone has already implemented PRESS publicly</li> <li>If no implementation is available, scope out how much work a minimal     pandas-compatible implementation would require</li> </ul>"},{"location":"general_background/all.literature_review.reference.html#2019-high-dimensional-multivariate-forecasting-with-low-rank-gaussian-copula-processes","title":"2019, High-Dimensional Multivariate Forecasting with Low-Rank Gaussian Copula Processes","text":"<ul> <li>Paper authors: David Salinas, Michael Bohlke-Schneider, Laurent Callot,   Roberto Medico, Jan Gasthaus</li> <li>Review author: Paul, 2019-12-28</li> <li>arXiv</li> <li>Score: 4/5</li> <li>Summary:</li> <li>Learns covariance structure and model together</li> <li>Handles series with time-varying, high-dimensional covariance structure</li> <li>Simultaneously handles series at different scales (in terms of the range)</li> <li>Uses a non-linear, deterministic state space model with transition dynamics     parametrized using an LSTM-RNN</li> <li>Praises:</li> <li>Implemented in GluonTS (https://github.com/awslabs/gluon-ts/pull/497) by one     of the coauthors who works on time series forecasting at AWS</li> <li>Code for the paper at     https://github.com/mbohlkeschneider/gluon-ts/tree/mv_release</li> <li>Good choice of baselines comparisons</li> <li>Demonstrates the importance of data transformations</li> <li>Next steps:</li> <li>Use in cases where we have a large number of time series known to have     meaningful correlations</li> </ul>"},{"location":"general_background/all.literature_review.reference.html#2014-the-topology-of-macro-financial-flow-using-stochastic-flow-diagrams","title":"2014, The topology of macro financial flow using stochastic flow diagrams","text":"<ul> <li>Paper authors: Calkin, De Prado</li> <li>Link</li> <li>Review author / date: GP, 2020-01-17</li> <li>Score: 1 / 5</li> <li>Summary:</li> <li>Praises:</li> <li>PCA on futures sectors</li> <li>Interesting graphical representation<ul> <li>Width of the arc represents strength of relationship (in terms of $R^2$)</li> <li>Color (green / red) and intensity represent sign and magnitude</li> <li>Lags are delays</li> <li>Geometric topology represents relationships better than tables</li> <li>Connectivity of a vertex represents importance</li> </ul> </li> <li>Agreed that econometrics as it is, is close to a pseudo-science that more     complex techniques are needed than inverting a matrix</li> <li>Critiques:</li> <li>Various inflammatory remarks and very little content</li> <li>Next steps:</li> <li>None</li> </ul>"},{"location":"general_background/all.literature_review.reference.html#computer-engineering","title":"Computer engineering","text":""},{"location":"general_background/all.literature_review.reference.html#2015-hidden-technical-debt-in-machine-learning-systems","title":"2015, Hidden technical debt in machine learning systems","text":"<ul> <li>Paper authors: Sculler et al.</li> <li>Link</li> <li>Review author / date: GP, 2020-01-07</li> <li>Score: 4/5</li> <li>Summary:</li> <li>Many interesting little observations about ML practices and engineering</li> <li>Praises:</li> <li>Validates how approach of minimizing technical debt and paying it off the     interest, e.g.,<ul> <li>Treat configuration as code, as we do</li> <li>Design abstraction carefully</li> <li>Routinely clean up the code</li> <li>No distinction in quality between research and production</li> <li>Use a single language for everything</li> <li>Need for committing to the healthy engineering practices</li> </ul> </li> <li>Critiques:</li> <li>None</li> <li>Next steps:</li> <li>None</li> </ul>"},{"location":"general_background/all.reading_list.reference.html","title":"Reading List","text":""},{"location":"general_background/all.reading_list.reference.html#reading-list_1","title":"Reading List","text":""},{"location":"general_background/all.reading_list.reference.html#git","title":"Git","text":"<ul> <li>Short tutorial</li> <li>Pro Git book</li> <li>To achieve mastery</li> </ul>"},{"location":"general_background/all.reading_list.reference.html#bash-linux","title":"Bash / Linux","text":"<ul> <li>Short tutorial</li> <li>Missing semester of CS</li> </ul>"},{"location":"general_background/all.reading_list.reference.html#coding","title":"Coding","text":"<ul> <li>The Pragmatic Programmer</li> <li>Aka the Black Book</li> <li>Reading and (really) understanding this is equivalent to accumulate 20 years     of coding</li> <li> <p>It will change your life</p> </li> <li> <p>The Joel Test</p> </li> <li>We should probably listen to the guy that started StackOverflow</li> <li>Today the 12 steps are obvious, but in 2000 these simple ideas were     revolutionary</li> <li>And, yes you are correct noticing that Joel is holding the table tennis     racquet incorrectly in the picture</li> </ul>"},{"location":"general_background/all.reading_list.reference.html#data-analysis","title":"Data analysis","text":"<ul> <li>Python for Data Analysis</li> <li>Reading is not enough: you should have tried all the examples of the book</li> <li>Remember: whatever you want to do, there is a more effective pandas way to     do it in one line</li> </ul>"},{"location":"general_background/all.reading_list.reference.html#sre","title":"SRE","text":"<ul> <li>Site Reliability Engineering</li> <li>\"Members of the SRE team explain how their engagement with the entire     software lifecycle has enabled Google to build, deploy, monitor, and     maintain some of the largest software systems in the world.\"</li> <li>An outstanding reference drawing on a wealth of experience</li> </ul>"},{"location":"general_background/all.reading_list.reference.html#arbitrage","title":"Arbitrage","text":"<ul> <li>Trading and Arbitrage in Cryptocurrency Markets</li> </ul>"},{"location":"infra/all.auto_scaling.explanation.html","title":"Auto Scaling Explanation","text":""},{"location":"infra/all.auto_scaling.explanation.html#introduction","title":"Introduction","text":"<p>Auto Scaling ensures that the applications maintain performance and manage costs effectively by automatically adjusting the number of computing resources in use based on demand. In Kubernetes environments, this capability is crucial for handling workload spikes, improving resource utilization, and maintaining application availability. This document outlines the implementation of Auto Scaling in the Kubernetes setup, focusing on the Cluster Autoscaler (CA), Horizontal Pod Autoscaler (HPA), and Auto Scaling Groups (ASG).</p>"},{"location":"infra/all.auto_scaling.explanation.html#cluster-autoscaler-ca","title":"Cluster Autoscaler (CA)","text":"<p>The Cluster Autoscaler dynamically adjusts the number of nodes in a Kubernetes cluster to meet the current workload demands. It increases the number of nodes during high demand periods and decreases them when the demand drops, ensuring optimal cost efficiency and resource utilization.</p>"},{"location":"infra/all.auto_scaling.explanation.html#ca-configuration-highlights","title":"CA Configuration Highlights","text":"<p>CA monitors the demand on the Kubernetes cluster and communicates with AWS ASGs to scale the number of nodes in the cluster. It ensures that there are enough nodes to run all pods without significant overprovisioning. If pods fail to launch due to lack of resources, CA will request ASG to add more nodes. Conversely, if nodes are underutilized, CA will scale down the ASG to remove excess nodes, optimizing costs. Generally, a node is considered underutilized if it has been running below a certain threshold of resource usage (e.g., CPU, memory) for a specific period. Cluster Autoscaler uses various metrics, including resource requests versus usage, to identify underutilization. These thresholds and periods are configurable. By default, CA considers a node underutilized if it has been running at low capacity (below 50% CPU utilization) for a predefined period (10 minutes).</p> <p>The deployment configuration for the Cluster Autoscaler is defined in <code>cluster-autoscaler-autodiscover.yaml</code>. This configuration ensures that the Cluster Autoscaler monitors node groups tagged for Auto Scaling and adjusts them based on the workload requirements. Key points in this configuration include:</p> <ul> <li>Auto-discovery of node groups: The Cluster Autoscaler is configured to   automatically discover node groups tagged with specific labels indicating they   are enabled for Auto Scaling within the Kubernetes cluster environment.</li> <li>Expander Strategy: The <code>least-waste</code> expander strategy is used, which aims   to minimize resource wastage by choosing the node group that would leave the   smallest amount of CPU and memory unused after a scale-up event.</li> </ul>"},{"location":"infra/all.auto_scaling.explanation.html#auto-scaling-group-asg","title":"Auto Scaling Group (ASG)","text":"<p>Auto Scaling Groups (ASG) in AWS provide a mechanism to automatically adjust the number of EC2 instances within the set to match the load on the application. ASG ensures that the application has the right amount of capacity to handle the current demand level.</p>"},{"location":"infra/all.auto_scaling.explanation.html#asg-integration-with-kubernetes","title":"ASG Integration with Kubernetes","text":"<p>Although ASG operates at the EC2 instance level, its integration with the Kubernetes Cluster Autoscaler allows for a seamless scaling experience. When the Cluster Autoscaler determines that a cluster needs to grow, it requests the ASG to launch additional EC2 instances. Conversely, when nodes are underutilized, the Cluster Autoscaler can signal the ASG to terminate instances, ensuring efficient resource use.</p> <ul> <li>Tag-based Discovery: The Cluster Autoscaler discovers ASGs through   specific tags, enabling it to manage the scaling of nodes within those groups   automatically.</li> <li>Scaling Actions: Based on the workload requirements and current cluster   state, the CA sends commands to the ASG to either increase or decrease the   number of nodes in the cluster, ensuring that the number of nodes always   aligns with the workload needs.</li> </ul>"},{"location":"infra/all.auto_scaling.explanation.html#horizontal-pod-autoscaler-hpa","title":"Horizontal Pod Autoscaler (HPA)","text":"<p>The Horizontal Pod Autoscaler automatically scales the number of pods in a replication controller, deployment, or replica set based on observed CPU and memory usage. This means it can adjust the number of pods in a deployment to meet the workload demands.</p>"},{"location":"infra/all.auto_scaling.explanation.html#hpa-configuration-highlights","title":"HPA Configuration Highlights","text":"<p>The HPA configuration, as defined in <code>scheduler-hpa.yaml</code>, targets the <code>airflow-scheduler</code> deployment within the <code>airflow</code> namespace. It adjusts the number of pods based on CPU and memory utilization, with the aim to maintain these usages near 90%. This ensures that the scheduler component of Airflow has sufficient resources to handle workload variations efficiently. The configuration specifics are:</p> <ul> <li>Scaling Targets: It targets the <code>airflow-scheduler</code> deployment, with a   scaling range between 1 to 6 replicas based on the demand.</li> <li>Resource Utilization: Utilization targets for both CPU and memory are set   to 90%, enabling the HPA to add or remove pods to maintain this level of   utilization. This means if the CPU/Mem usage of the Airflow Schedulers reaches   90% of the requested resources, HPA will automatically increase the number of   scheduler pod replicas to distribute the load more evenly.</li> </ul>"},{"location":"infra/all.auto_scaling.explanation.html#workflow-overview","title":"Workflow Overview","text":"<p>The workflow of Cluster Autoscaler (CA), Auto Scaling Groups (ASG), and Horizontal Pod Autoscaler (HPA) within a Kubernetes cluster forms a comprehensive auto-scaling solution that dynamically adjusts both the number of pods and nodes based on the workload demands.</p> <ol> <li> <p>Monitoring Resource Demands: The HPA monitors the CPU and memory usage of    pods against the defined targets. Simultaneously, the CA monitors for pods    that cannot be scheduled due to insufficient resources.</p> </li> <li> <p>Horizontal Pod Autoscaler (HPA) Reaction:</p> </li> <li> <p>If the HPA detects that the CPU or memory usage of the pods exceeds or      falls below the configured thresholds, it triggers a scale-out or scale-in      action for the pods within the target deployment (e.g., increasing or      decreasing the number of replicas of the Airflow Scheduler).</p> </li> <li> <p>Cluster Autoscaler (CA) Reaction:</p> </li> <li>If new pods cannot be scheduled because there are not enough resources in      the cluster, the CA identifies this and decides to scale up the number of      nodes.</li> <li> <p>Conversely, if the CA detects that some nodes are underutilized and their      pods can be placed on fewer nodes, it triggers a scale-down action, safely      evicting pods and terminating excess nodes.</p> </li> <li> <p>Auto Scaling Groups (ASG) Execution:</p> </li> <li>When the CA decides to scale up, it communicates with the cloud provider's      ASG to add more nodes into the cluster.</li> <li> <p>For scaling down, it selects nodes to remove, ensures pods are safely      evicted to other nodes, and then reduces the node count in the ASG.</p> </li> <li> <p>New Nodes Joining the Cluster:</p> </li> <li> <p>As new nodes are provisioned by the ASG, they join the Kubernetes cluster,      becoming available resources for scheduling pods.</p> </li> <li> <p>Rebalancing and Optimization:</p> </li> <li>With the addition of new nodes, the HPA might adjust the number of pod      replicas again, based on the now-available resources and the target      utilization rates.</li> <li> <p>The CA continually monitors for opportunities to optimize the distribution      of pods across nodes, including the possibility of scaling down if      resources are underutilized.</p> </li> <li> <p>Continuous Monitoring:</p> </li> <li>Both HPA and CA continuously monitor the cluster's state and workload      demands, ready to adjust the number of pods or nodes as needed to meet the      set targets for resource utilization and efficiency.</li> </ol> <p>This orchestrated workflow of HPA, CA, and ASG ensures that Kubernetes clusters are dynamically scalable, self-healing, and efficient, meeting both current and future demands with minimal manual intervention.</p>"},{"location":"infra/all.auto_scaling.explanation.html#conclusion","title":"Conclusion","text":"<p>Auto Scaling within Kubernetes through Cluster Autoscaler, Auto Scaling Groups, and Horizontal Pod Autoscaler ensures that applications remain performant and cost-effective by dynamically adjusting resources based on demand. Proper configuration and monitoring of these components are crucial for maintaining an efficient and reliable Kubernetes environment. By leveraging ASG, CA, and HPA together, the Kubernetes infrastructure achieves both micro (pod-level) and macro (node-level) scaling, ensuring that our applications are always backed by the right amount of resources. This orchestration between different layers of scaling mechanisms enables us to maintain high availability, performance, and cost efficiency.</p>"},{"location":"infra/all.aws_iam_configuration.explanation.html","title":"AWS IAM Configuration Best Practices Guide","text":""},{"location":"infra/all.aws_iam_configuration.explanation.html#introduction","title":"Introduction","text":"<p>This document outlines the best practices for structuring, segmenting, and naming AWS Identity and Access Management (IAM) roles, groups, and policies. Adhering to these practices will ensure that IAM configurations remain scalable, auditable, and aligned with the principle of least privilege (PoLP), supporting both current needs and future expansions.</p>"},{"location":"infra/all.aws_iam_configuration.explanation.html#principle-of-least-privilege-polp","title":"Principle of Least Privilege (PoLP)","text":"<ul> <li>Definition: Ensure that IAM entities (users, roles, groups) have only the   permissions necessary to perform their assigned tasks, no more, no less. Each   IAM policy should grant only the permissions necessary for the user or service   to perform its intended tasks.</li> <li>Application: Review existing permissions regularly and adjust to fit   changing requirements while adhering to this minimal access principle. Employ   conditions in policies to restrict access further, such as limiting actions to   specific IP ranges, or restricting access between environments by leveraging   tags.</li> </ul> <p>Ensure that IAM policies grant only the permissions necessary for users to perform their job functions.</p> <ul> <li>Restrictive Resource Access: Instead of granting broad permissions like   <code>\"Resource\": \"*\"</code>, specify resources more explicitly wherever possible. For   instance, avoid definitions such as <code>secretsmanager:*</code>, and <code>kms:*</code>. Consider   specifying only required actions unless absolutely necessary, and always   restrict the resources to specific ARNs when possible.</li> <li>Action-Specific Policies: Limit actions to those absolutely necessary. For   example, if a user or service only needs to read from an S3 bucket, they   should not have write access.</li> <li>Condition Statements: Use condition statements to enforce policy   application under specific circumstances, adding an additional layer of   security.</li> </ul>"},{"location":"infra/all.aws_iam_configuration.explanation.html#structuring-iam-policies","title":"Structuring IAM Policies","text":"<ul> <li>Atomic Policies: Create policies that are specific to a single purpose or   service.</li> <li>Minimize Wildcards: Use specific resource ARNs instead of broad wildcards   where practical to limit access scope.</li> <li>Use Conditions: Apply conditions to control when and how permissions are   granted.</li> <li>Organize Statements Logically: Group related permissions into the same   statement where it makes sense for clarity and manageability. DRY!</li> <li>Separate Critical and Non-critical Access: Clearly differentiate policies   handling critical resources (like production databases) from non-critical   resources. For instance, avoid using <code>s3:*</code> permissions on non-critical   buckets; specify allowed actions.</li> </ul>"},{"location":"infra/all.aws_iam_configuration.explanation.html#segmentation-of-permissions","title":"Segmentation of Permissions","text":"<p>Segmentation involves dividing IAM policies based on the type of access or function they serve. This makes policies easier to manage and understand.</p> <ul> <li>Functional Segmentation: Group permissions by AWS service (e.g., ECR, S3,   ECS) and by the nature of access (read-only, read-write). This would make it   easier to manage and audit permissions.</li> <li>Resource-Specific Policies: Instead of using wildcards, specify which   resources a group or user can access. This minimizes the risk of unintentional   access.</li> <li>Environment Segmentation: Differentiate between production and   non-production environments within policies to prevent accidental access to   critical resources.</li> <li>Role-Based Access Control (RBAC): Assign users to groups based on their   job function and assign policies to these groups.</li> <li>Temporary Credentials: Use roles and temporary credentials for short-term   access, minimizing long-term security risks.</li> </ul>"},{"location":"infra/all.aws_iam_configuration.explanation.html#naming-conventions","title":"Naming Conventions","text":"<p>Clear naming conventions help in quickly identifying the purpose of a policy, which resources it relates to, and the permissions level it grants.</p> <ol> <li>Environment Prefix: Use prefixes such as <code>Dev</code>, <code>Preprod</code>, or <code>Prod</code> to    indicate the environment.</li> <li>Service or Functional Descriptors: Include the AWS service or the    function (e.g., <code>ECR</code>, <code>S3</code>) in the policy name.</li> <li>Access Level: Specify the access level (e.g., <code>ReadOnly</code>, <code>ReadWrite</code>) in    the policy name.</li> <li>Resource Type or Identifier: Where applicable, include a resource    identifier to specify the scope, to provide additional context about the type    or specific identifiers of resources involved (e.g., <code>DataBuckets</code>).</li> </ol>"},{"location":"infra/all.aws_iam_configuration.explanation.html#iam-roles","title":"IAM Roles","text":"<p>IAM roles should clearly reflect the service and purpose they are designed to support.</p> <ul> <li>Format: <code>[Environment][Service][Purpose]Role</code></li> <li>Example for EC2: <code>ProdEC2InstanceManagementRole</code></li> <li>Example for EKS: <code>PreprodEKSClusterAdminRole</code></li> </ul> <p>This format identifies the environment (Prod, Preprod), the AWS service (EC2, EKS), the role's purpose (InstanceManagement, ClusterAdmin), and it ends with the word \"Role\" to distinguish it as an IAM role.</p>"},{"location":"infra/all.aws_iam_configuration.explanation.html#iam-groups","title":"IAM Groups","text":"<p>Groups often represent a collection of users with similar permissions. Names should reflect the organizational units (OUs) or user role they are intended for:</p> <ul> <li>Format: <code>[userRole]-[permissionLevel]-group</code></li> <li>Example for Developer: <code>developer-limited-group</code></li> <li>Example for DevOps: <code>devops-extended-group</code></li> </ul> <p>This convention highlights the role (e.g., Developer, DevOps) and the level of privilege (e.g., 'Limited' for basic access, 'Extended' for broader permissions, 'Custom' for specially crafted permissions), which are crucial for understanding what the users in the group can do.</p>"},{"location":"infra/all.aws_iam_configuration.explanation.html#iam-group-policies","title":"IAM Group Policies","text":"<p>Group policies should be named similarly to individual IAM policies but should indicate they are associated with a group.</p> <ul> <li>Format:   <code>[Environment][Service][AccessLevel][ResourceIdentifier]GroupPolicy</code></li> <li>Example for S3 access: <code>PreprodS3ReadOnlyDataBackupGroupPolicy</code></li> <li>Example for ECS access: <code>DevECSReadOnlyServicesGroupPolicy</code></li> </ul> <p>This naming convention makes it clear which environment the policy applies to, what service it pertains to, the level of access provided, and that it is a group policy.</p>"},{"location":"infra/all.aws_iam_configuration.explanation.html#iam-policies","title":"IAM Policies","text":"<p>For IAM policies that apply to specific services, the name should indicate the environment, service, and scope of access.</p> <ul> <li>Format: <code>[Environment][Service][AccessLevel][ResourceIdentifier]Policy</code></li> <li>Example for DynamoDB: <code>PreprodDynamoDBReadWriteTablePolicy</code></li> <li>Example for IAM access: <code>ProdIAMFullAccessUserPolicy</code></li> <li>Example for EC2 access: <code>ProdEC2ReadOnlyAirflowPolicy</code></li> </ul>"},{"location":"infra/all.aws_iam_configuration.explanation.html#example-naming-conventions","title":"Example Naming Conventions","text":"<p>Here\u2019s an example naming convention for an IAM policy intended for the development environment, with read-only access to S3 Data Buckets:</p> <ul> <li><code>DevS3ReadOnlyDataBucketsPolicy</code></li> </ul>"},{"location":"infra/all.aws_iam_configuration.explanation.html#explanation","title":"Explanation","text":"<ul> <li><code>Dev</code> indicates that this policy is intended for use in the development   environment.</li> <li><code>S3</code> specifies that the policy pertains to Amazon S3 service.</li> <li><code>ReadOnly</code> clearly states the permission level, which is read-only access.</li> <li><code>DataBuckets</code> tells us that the policy is specifically for actions related to   data storage buckets.</li> </ul>"},{"location":"infra/all.aws_scripts.reference.html","title":"Python Scripts Guide","text":"<p>Documentation of the AWS scripts available at: https://drive.google.com/drive/u/3/folders/1hAB_lhAvL69pnK2cKYG5YRHsInru0mk0</p>"},{"location":"infra/all.infrastructure_glossary.explanation.html","title":"Infrastructure Glossary","text":"<ul> <li>This file contains a short description of the technologies we use to manage   infrastructure</li> <li>We refer to further docs for an in-depth analysis</li> </ul>"},{"location":"infra/all.infrastructure_glossary.explanation.html#general-technologies","title":"General technologies","text":""},{"location":"infra/all.infrastructure_glossary.explanation.html#kubernetes","title":"Kubernetes","text":"<ul> <li>Platform to automate deploying, scaling, and operating containers across a   cluster of machines</li> <li>Supports several container runtimes (e.g., Docker, <code>containerd</code>)</li> <li>Runs on physical machines, VMs, cloud providers in a consistent way</li> <li>Places containers based on resource requirements and constraints to optimize   resource utilization</li> <li>Self-healing</li> <li>Replaces a container if it fails or doesn't respond to health checks</li> <li>Horizontal scaling: up and down based on commands, UI, or on resource usage</li> <li>Automated rollouts and rollbacks</li> </ul>"},{"location":"infra/all.infrastructure_glossary.explanation.html#terraform","title":"Terraform","text":"<ul> <li>Tool for building, changing, and versioning infrastructure</li> <li>IaC: users define and provide infrastructure in a configuration file</li> <li>Platform agnostic: supports many service providers</li> <li>Build a graph of all resources and executes operations in parallel</li> <li>Support modules to create reusable components</li> <li>Manage state (through a file) mapping real world resources to the   configuration</li> <li>Show a preview of what happens when you modify infrastructure before applying   the changes</li> </ul>"},{"location":"infra/all.infrastructure_glossary.explanation.html#ansible","title":"Ansible","text":"<ul> <li>Automation tool for configuration management, application deployment, and   service provisioning</li> <li>Agentless: doesn't require agent software installed on the nodes to manage,   but uses SSH</li> <li>Playbook describe automation jobs in YAML</li> <li>Handles dependencies and roll-back to automate server lifecycle</li> <li>Support multi-node deployments</li> <li>Work against multiple systems using a list (aka \"inventory\")</li> </ul>"},{"location":"infra/all.infrastructure_glossary.explanation.html#airflow","title":"Airflow","text":"<ul> <li>Tool to schedule and monitor workflows automatically</li> <li>Workflows are set up as DAGs to reflect dependencies</li> <li>Dynamic pipeline generation in Python</li> <li>Scheduler allows a scalable architecture, handling failures and retries   according to user parameters</li> <li>Rich web UI to visualize pipelines, monitor progress, troubleshoot issues</li> <li>Many plugins to manage workflows spanning many systems</li> </ul>"},{"location":"infra/all.infrastructure_glossary.explanation.html#zabbix","title":"Zabbix","text":"<ul> <li>Open-source monitoring solution for network and applications</li> <li>Monitor applications metrics, processes, and performance indicators</li> <li>Real-time monitoring</li> <li>Send alerts through emails, SMS, scripts</li> <li>Web-based interface</li> <li>Performance based visualization</li> <li>Agent and agent-less monitoring</li> </ul>"},{"location":"infra/all.infrastructure_glossary.explanation.html#prometheus","title":"Prometheus","text":"<ul> <li>Monitoring and alerting toolkit</li> </ul>"},{"location":"infra/all.infrastructure_glossary.explanation.html#helm","title":"Helm","text":""},{"location":"infra/all.infrastructure_glossary.explanation.html#eksctl","title":"eksctl","text":""},{"location":"infra/all.infrastructure_glossary.explanation.html#amazon-web-services-aws","title":"Amazon Web Services (AWS)","text":""},{"location":"infra/all.infrastructure_glossary.explanation.html#virtual-private-cloud-vpc","title":"Virtual private cloud (VPC)","text":"<ul> <li>Provide logically isolated sections of AWS cloud</li> <li>You can launch AWS resources</li> <li>Resembles a traditional network you operate in your network</li> <li>Select IP address range</li> <li>Create subnets</li> <li>Configure route tables and network gateways</li> <li>Create a public-facing subnet to let access to your web servers</li> <li>Create a private-facing subnet (with no Internet access) for your backend     systems</li> </ul>"},{"location":"infra/all.infrastructure_glossary.explanation.html#elastic-kubernetes-service-eks","title":"Elastic Kubernetes Service (EKS)","text":"<ul> <li>Run K8 on AWS without installing and maintain K8 control plane or nodes</li> <li>Integrates with EC2, IAM, VPC</li> <li>Automatic scaling, load balancing</li> <li>Provide managed node groups to automate provisioning and lifecycle management   of nodes (EC2)</li> </ul>"},{"location":"infra/all.infrastructure_glossary.explanation.html#elastic-file-system-efs","title":"Elastic File System (EFS)","text":"<ul> <li>Scalable, cloud-native file storage service</li> <li>Fully managed service</li> <li>Elasticity</li> <li>Automatically scale storage capacity and performance</li> <li>Pay only for the storage you use</li> <li>Shared file storage</li> <li>Multiple EC2 instances can access an EFS file system</li> <li>High durability and availability</li> <li>Store data across multiple availability zones in AWS region</li> <li>Different performance modes (general purpose or max I/O)</li> </ul>"},{"location":"infra/all.infrastructure_glossary.explanation.html#identity-and-access-management-iam","title":"Identity and Access Management (IAM)","text":"<ul> <li>Create users and groups permissions to access and deny access to AWS resources</li> <li>Permanent or temporary credentials (e.g., expire after a certain duration)</li> <li>Multi-factor authentication (besides username and password)</li> <li>Can test and validate effects of IAM policies changes before applying them</li> </ul>"},{"location":"infra/all.infrastructure_glossary.explanation.html#cloudwatch","title":"CloudWatch","text":"<ul> <li> <p>See   https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html</p> </li> <li> <p>CloudWatch monitors your AWS resources and the applications you run on AWS in   real-time</p> </li> <li>TODO(gp): Finish</li> </ul>"},{"location":"infra/all.infrastructure_update_rollout.how_to_guide.html","title":"Standardized Update Rollout Checklist","text":""},{"location":"infra/all.infrastructure_update_rollout.how_to_guide.html#introduction","title":"Introduction","text":"<p>This document outlines a standardized procedure for testing, deploying, and monitoring software updates across various environments. By following this checklist, we aim to ensure that all updates are rolled out efficiently, consistently, and safely.</p>"},{"location":"infra/all.infrastructure_update_rollout.how_to_guide.html#checklist","title":"Checklist","text":""},{"location":"infra/all.infrastructure_update_rollout.how_to_guide.html#phase-1-planning","title":"Phase 1: Planning","text":"<ul> <li>[ ] Issue Creation:</li> <li> <p>File a GitHub issue detailing the purpose of the update, specific versions     we plan to update, and the specific benefits we aim to achieve.</p> </li> <li> <p>[ ] Identify Update Requirements:</p> </li> <li> <p>Determine which components need updates and the reasons (security updates,     feature enhancements, bug fixes).</p> </li> <li> <p>[ ] Review Update Documentation:</p> </li> <li> <p>Study the release notes and documentation of the updates to understand the     changes, new features, new dependencies, breaking changes, and potential     impacts.</p> </li> <li> <p>[ ] Stakeholder Communication:</p> </li> <li> <p>Inform relevant stakeholders about the planned update and gather any initial     feedback or concerns.</p> </li> <li> <p>[ ] Schedule Rollout:</p> </li> <li>Choose an appropriate time for the update rollout that minimizes impact on     users.</li> </ul>"},{"location":"infra/all.infrastructure_update_rollout.how_to_guide.html#phase-2-testing","title":"Phase 2: Testing","text":"<ul> <li>[ ] Environment Setup:</li> <li> <p>Set up testing environments that closely mirror production systems.</p> </li> <li> <p>[ ] Apply Updates:</p> </li> <li> <p>Deploy the updates in the testing environment.</p> </li> <li> <p>[ ] Automated Testing:</p> </li> <li> <p>Run automated regression and new feature tests to ensure nothing breaks with     the new updates.</p> </li> <li> <p>[ ] Manual Testing:</p> </li> <li> <p>Conduct thorough manual testing to check for issues not covered by automated     tests.</p> </li> <li> <p>[ ] Performance Benchmarking:</p> </li> <li> <p>Compare performance metrics before and after the update to ensure no     degradation.</p> </li> <li> <p>[ ] Security Assessment:</p> </li> <li>Perform security audits on the updated components to ensure no new     vulnerabilities are introduced.</li> </ul>"},{"location":"infra/all.infrastructure_update_rollout.how_to_guide.html#phase-3-pre-deployment","title":"Phase 3: Pre-Deployment","text":"<ul> <li>[ ] Final Review Meeting:</li> <li> <p>Conduct a meeting with relevant stakeholders to review and decide whether to     proceed with the deployment to production.</p> </li> <li> <p>[ ] Backup Production Data:</p> </li> <li> <p>Ensure that all relevant production data is backed up and restore points are     created.</p> </li> <li> <p>[ ] Rollback Plan:</p> </li> <li> <p>Prepare detailed rollback procedures in case the update needs to be     reversed. (Utilize Kubernetes\u2019 capabilities for a seamless rollback, which     can be executed with minimal to no downtime.)</p> </li> <li> <p>[ ] Stakeholder Announcement:</p> </li> <li>Ensure all stakeholders are informed about the upcoming update and its     implications. Issue a notification via Telegram and email detailing the     scope of the update, deployment start time, and expected downtime duration     if applicable, before the actual deployment to ensure sufficient preparation     time.</li> </ul>"},{"location":"infra/all.infrastructure_update_rollout.how_to_guide.html#phase-4-deployment","title":"Phase 4: Deployment","text":"<ul> <li>[ ] Phased Rollout:</li> <li> <p>If applicable, roll out the update incrementally (canary release, blue-green     deployments).</p> </li> <li> <p>[ ] Monitoring:</p> </li> <li> <p>Closely monitor the system for any immediate issues during and after the     deployment.</p> </li> <li> <p>[ ] Stakeholder Update:</p> </li> <li>Keep stakeholders updated on the deployment status and any critical issues.</li> </ul>"},{"location":"infra/all.infrastructure_update_rollout.how_to_guide.html#phase-5-post-deployment","title":"Phase 5: Post-Deployment","text":"<ul> <li>[ ] Post-Deployment Testing and Monitoring:</li> <li> <p>Conduct additional testing to ensure the system operates as expected in the     production environment. Closely monitor the system after deployment for any     unforeseen issues.</p> </li> <li> <p>[ ] Performance Monitoring:</p> </li> <li> <p>Monitor system performance over time to catch any delayed effects of the     update.</p> </li> <li> <p>[ ] Issue Log:</p> </li> <li> <p>Document any issues encountered during deployment and how they were     resolved.</p> </li> <li> <p>[ ] Feedback Loop:</p> </li> <li>Gather feedback from users and stakeholders to assess the impact of the     update.</li> </ul>"},{"location":"infra/all.infrastructure_update_rollout.how_to_guide.html#conclusion","title":"Conclusion","text":"<p>This checklist serves as a framework to guide the update process for software and infrastructure within Kaizen. By adhering to these steps, we ensure that updates are implemented smoothly, securely, and with minimal disruption.</p>"},{"location":"infra/all.rds.comparison.html","title":"Comparison of AWS RDS Instance Types and Storage Performance","text":""},{"location":"infra/all.rds.comparison.html#table-of-contents","title":"Table of Contents","text":""},{"location":"infra/all.rds.comparison.html#introduction","title":"Introduction","text":"<p>This guide aims to conduct a comprehensive comparison of various AWS RDS instance types and storage options. The focus is to understand the real-world performance implications of different configurations, especially in terms of instance types, and storage types like <code>gp2</code>, <code>gp3</code>, and <code>IO</code> options.</p>"},{"location":"infra/all.rds.comparison.html#rds-instance-types-evaluation","title":"RDS Instance Types Evaluation","text":""},{"location":"infra/all.rds.comparison.html#general-purpose-instances-eg-m5-t3","title":"General Purpose Instances (e.g., <code>M5</code>, <code>T3</code>)","text":"<ul> <li>Performance: Balanced CPU and memory, suitable for moderate load   applications.</li> <li>Use Case: Ideal for web applications, development, and test environments.</li> </ul>"},{"location":"infra/all.rds.comparison.html#memory-optimized-instances-eg-r5-r6g","title":"Memory-Optimized Instances (e.g., <code>R5</code>, <code>R6g</code>)","text":"<ul> <li>Performance: Higher memory to CPU ratio. R6g instances leverage AWS   Graviton processors for better price-performance.</li> <li>Use Case: Suitable for memory-intensive applications like high-performance   databases.</li> </ul>"},{"location":"infra/all.rds.comparison.html#compute-optimized-instances-eg-c5-c6g","title":"Compute-Optimized Instances (e.g., <code>C5</code>, <code>C6g</code>)","text":"<ul> <li>Performance: High CPU resources relative to memory. <code>C6g</code> instances are   powered by AWS Graviton2 processors offering better compute performance.</li> <li>Use Case: Ideal for compute-intensive applications, like gaming servers,   high-performance computing.</li> </ul>"},{"location":"infra/all.rds.comparison.html#comparison-summary","title":"Comparison Summary","text":"<ul> <li>Memory-Optimized vs General Purpose: Memory-optimized instances (like   <code>R6g</code>) offer better performance for memory-heavy applications compared to   general-purpose instances.</li> <li>Compute-Optimized vs Memory-Optimized: For CPU-intensive tasks,   compute-optimized instances are preferable, whereas memory-optimized instances   are better for memory-demanding applications.</li> </ul>"},{"location":"infra/all.rds.comparison.html#storage-types","title":"Storage Types","text":""},{"location":"infra/all.rds.comparison.html#gp2-general-purpose-ssd","title":"<code>gp2</code> (General Purpose SSD)","text":"<ul> <li>Performance: Offers a balance of price and performance. Performance scales   with volume size.</li> <li>Use Case: Suitable for a broad range of transactional workloads.</li> </ul>"},{"location":"infra/all.rds.comparison.html#gp3-next-gen-general-purpose-ssd","title":"<code>gp3</code> (Next-Gen General Purpose SSD)","text":"<ul> <li>Performance: Provides better performance at a lower cost than <code>gp2</code>.   Offers customizable IOPS and throughput. Baseline of 3,000 IOPS and throughput   of 125 MiB/s, scalable to 16,000 IOPS and 1,000 MiB/s</li> <li>Use Case: Ideal for applications requiring high-performance at a lower   cost.</li> </ul>"},{"location":"infra/all.rds.comparison.html#io-optimized-storage","title":"IO-optimized Storage","text":"<ul> <li>Performance: Designed for high I/O operations per second (IOPS) and   throughput. Offers provisioned IOPS (PIOPS) up to 64,000 IOPS</li> <li>Use Case: Essential for high-performance OLTP, big data, and applications   with high IO requirements.</li> </ul>"},{"location":"infra/all.rds.comparison.html#comparison-summary_1","title":"Comparison Summary","text":"<ul> <li><code>gp3</code> vs <code>gp2</code>: <code>gp3</code> offers better cost efficiency and performance   customization compared to <code>gp2</code>.</li> <li>IO-optimized vs General Purpose: IO-optimized storage is superior for   applications requiring high I/O throughput.</li> </ul>"},{"location":"infra/all.rds.comparison.html#performance-metrics","title":"Performance Metrics","text":"<ul> <li>IO x thousand: Refers to the number of I/O operations per second (IOPS).   Higher IOPS indicates better performance for read/write intensive operations.   It represents the number of input/output operations per second (IOPS)   multiplied by a factor of thousand.</li> <li>Real-World Implication: High IOPS (like in IO-optimized storage)   translates to faster data retrieval and handling, crucial for databases with   heavy read/write operations.</li> <li>Impact on Database Performance:</li> <li>Database performance heavily relies on IO operations.</li> <li>Higher IO rates typically lead to faster data retrieval and handling, which     is crucial for databases with high transaction rates or large data sets.</li> </ul>"},{"location":"infra/all.rds.comparison.html#conclusion","title":"Conclusion","text":"<p>The choice of instance type and storage option in AWS RDS largely depends on the specific application requirements. Memory-optimized instances like <code>r6g</code> are ideal for high-performance databases, while <code>t4g</code> instances are more suited for less demanding workloads. In terms of storage, <code>gp3</code> offers a good balance of performance and cost, making it a suitable choice for a variety of applications. Understanding AWS-specific metrics like IOPS is crucial in making informed decisions about database configurations.</p>"},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html","title":"Dev Scripts Catalogue","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#bash","title":"<code>bash</code>","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#kga","title":"kga","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#mkbak","title":"mkbak","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#path","title":"path","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#print_pathssh","title":"print_paths.sh","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#timestamp","title":"timestamp","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#treesh","title":"tree.sh","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#development","title":"Development","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#create_class_diagramsh","title":"create_class_diagram.sh","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#ctagssh","title":"ctags.sh","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#call_graphsh","title":"call_graph.sh","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#go_ampsh","title":"go_amp.sh","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#setenv_ampconfigure_envsh","title":"setenv_amp.configure_env.sh","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#setenv_ampsh","title":"setenv_amp.sh","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#tmux_ampsh","title":"tmux_amp.sh","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#tmux_kill_sessionsh","title":"tmux_kill_session.sh","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#repo-integration","title":"Repo integration","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#clean_up_text_filessh","title":"clean_up_text_files.sh","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#searching","title":"Searching","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#ack","title":"ack","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#ffindpy","title":"ffind.py","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#jack","title":"jack","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#jackipynb","title":"jackipynb","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#jackmd","title":"jackmd","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#jackmk","title":"jackmk","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#jackppy","title":"jackppy","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#jackpy","title":"jackpy","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#jackpyc","title":"jackpyc","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#jacktxt","title":"jacktxt","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#markdown","title":"Markdown","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#lint_mdsh","title":"lint_md.sh","text":"<ul> <li>Lint a markdown file</li> </ul>"},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#convert_gdoc_to_markdownsh","title":"convert_gdoc_to_markdown.sh","text":"<ul> <li>Convert a Google Doc in docx format to markdown removing artifacts</li> </ul>"},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#remove_empty_linessh","title":"remove_empty_lines.sh","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#-used-in-vim-to-remove-empty-spaces","title":"- Used in vim to remove empty spaces","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#vim","title":"<code>vim</code>","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#traceback_to_cfilepy","title":"traceback_to_cfile.py","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#viack","title":"viack","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#vic","title":"vic","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#vigit","title":"vigit","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#vigitp","title":"vigitp","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#vil","title":"vil","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#vit","title":"vit","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#viw","title":"viw","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#to-reorg","title":"To reorg","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#aws","title":"aws","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#cie","title":"cie","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#cleanup_scripts","title":"cleanup_scripts","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#client_setup","title":"client_setup","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#code_statssh","title":"code_stats.sh","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#compile_allpy","title":"compile_all.py","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#compress_filessh","title":"compress_files.sh","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#cvxpy_setup","title":"cvxpy_setup","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#diff_to_vimdiffpy","title":"diff_to_vimdiff.py","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#docker_clean_allsh","title":"docker_clean_all.sh","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#docker_clean_postgressh","title":"docker_clean_postgres.sh","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#email_notifypy","title":"email_notify.py","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#export_varssh","title":"export_vars.sh","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#find_unused_golden_filespy","title":"find_unused_golden_files.py","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#fix_permssh","title":"fix_perms.sh","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#git","title":"git","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#github","title":"github","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#grsyncpy","title":"grsync.py","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#helperssh","title":"helpers.sh","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#infra","title":"infra","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#integrate_repos","title":"integrate_repos","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#lib_tasks_data_qapy","title":"lib_tasks_data_qa.py","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#lib_tasks_data_reconcilepy","title":"lib_tasks_data_reconcile.py","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#lint_soliditysh","title":"lint_solidity.sh","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#manage_cachepy","title":"manage_cache.py","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#measure_import_timespy","title":"measure_import_times.py","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#mk_targets","title":"mk_targets","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#notebooks","title":"notebooks","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#old","title":"old","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#parallel_script_skeletonpy","title":"parallel_script_skeleton.py","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#poetry","title":"poetry","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#process_profpy","title":"process_prof.py","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#release_encrypted_modelsh","title":"release_encrypted_model.sh","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#release_sorrentum","title":"release_sorrentum","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#remove_escape_charspy","title":"remove_escape_chars.py","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#remove_jupyter_metadatash","title":"remove_jupyter_metadata.sh","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#remove_redundant_pathssh","title":"remove_redundant_paths.sh","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#replace_textpy","title":"replace_text.py","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#run_profilingsh","title":"run_profiling.sh","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#save_screenshotsh","title":"save_screenshot.sh","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#script_skeletonpy","title":"script_skeleton.py","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#string_to_filepy","title":"string_to_file.py","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#sync_reposh","title":"sync_repo.sh","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#test","title":"test","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#testing","title":"testing","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#tgpy","title":"tg.py","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#to_clean","title":"to_clean/","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#toml_mergepy","title":"toml_merge.py","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#transform_skeletonpy","title":"transform_skeleton.py","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#urlpy","title":"url.py","text":""},{"location":"kaizenflow/all.dev_scripts_catalogue.reference.html#zip_filespy","title":"zip_files.py","text":""},{"location":"kaizenflow/all.devops_code_organization.reference.html","title":"All.devops code organization.reference","text":"<ul> <li>See changelog in <code>version.txt</code></li> </ul>"},{"location":"kaizenflow/all.devops_code_organization.reference.html#code-organization","title":"Code organization","text":"<ul> <li>The organization of a <code>devops</code> dir is like in the following</li> <li><code>compose</code><ul> <li>Docker compose files</li> </ul> </li> <li><code>docker_build</code><ul> <li>Everything related to building a Docker image</li> </ul> </li> <li><code>docker_run</code><ul> <li>Everything related to running a Docker image</li> </ul> </li> <li> <p><code>env</code></p> <ul> <li>Docker env files</li> </ul> </li> <li> <p>An example is below   ```</p> <p>tree devops</p> </li> </ul> <p>devops   \u251c\u2500\u2500 README.md   \u251c\u2500\u2500 compose   \u2502\u00a0\u00a0 \u251c\u2500\u2500 docker-compose.yml   \u2502\u00a0\u00a0 \u2514\u2500\u2500 docker-compose_as_submodule.yml   \u251c\u2500\u2500 debug   \u2502\u00a0\u00a0 \u2514\u2500\u2500 repo_compare.sh   \u251c\u2500\u2500 docker_build   \u2502\u00a0\u00a0 \u251c\u2500\u2500 create_users.sh   \u2502\u00a0\u00a0 \u251c\u2500\u2500 dev.Dockerfile   \u2502\u00a0\u00a0 \u251c\u2500\u2500 etc_sudoers   \u2502\u00a0\u00a0 \u251c\u2500\u2500 fstab   \u2502\u00a0\u00a0 \u251c\u2500\u2500 install_dind.sh   \u2502\u00a0\u00a0 \u251c\u2500\u2500 install_jupyter_extensions.sh   \u2502\u00a0\u00a0 \u251c\u2500\u2500 install_os_packages.sh   \u2502\u00a0\u00a0 \u251c\u2500\u2500 install_python_packages.sh   \u2502\u00a0\u00a0 \u251c\u2500\u2500 old   \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 conda.yml   \u2502\u00a0\u00a0 \u251c\u2500\u2500 poetry.lock   \u2502\u00a0\u00a0 \u251c\u2500\u2500 poetry.toml   \u2502\u00a0\u00a0 \u2514\u2500\u2500 pyproject.toml   \u251c\u2500\u2500 docker_run   \u2502\u00a0\u00a0 \u251c\u2500\u2500 aws_credentials.sh   \u2502\u00a0\u00a0 \u251c\u2500\u2500 bashrc   \u2502\u00a0\u00a0 \u251c\u2500\u2500 entrypoint.sh   \u2502\u00a0\u00a0 \u251c\u2500\u2500 run_jupyter_server.sh   \u2502\u00a0\u00a0 \u251c\u2500\u2500 setenv.sh   \u2502\u00a0\u00a0 \u2514\u2500\u2500 test_setup.sh   \u2514\u2500\u2500 env       \u2514\u2500\u2500 default.env   ```</p> <ul> <li>The layout for versions 1.x.x was   ``` <p>tree devops.OLD   devops.OLD/   \u251c\u2500\u2500 compose   \u2502\u00a0\u00a0 \u251c\u2500\u2500 docker-compose.yml   \u2502\u00a0\u00a0 \u2514\u2500\u2500 docker-compose_as_submodule.yml   \u251c\u2500\u2500 debug   \u2502\u00a0\u00a0 \u2514\u2500\u2500 repo_compare.sh   \u251c\u2500\u2500 docker_build   \u2502\u00a0\u00a0 \u251c\u2500\u2500 README.md   \u2502\u00a0\u00a0 \u251c\u2500\u2500 dev.Dockerfile   \u2502\u00a0\u00a0 \u251c\u2500\u2500 entrypoint   \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 aws_credentials.sh   \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 patch_environment_variables.sh   \u2502\u00a0\u00a0 \u251c\u2500\u2500 entrypoint.sh   \u2502\u00a0\u00a0 \u251c\u2500\u2500 fstab   \u2502\u00a0\u00a0 \u251c\u2500\u2500 install_jupyter_extensions.sh   \u2502\u00a0\u00a0 \u251c\u2500\u2500 install_packages.sh   \u2502\u00a0\u00a0 \u251c\u2500\u2500 install_requirements.sh   \u2502\u00a0\u00a0 \u251c\u2500\u2500 old   \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 conda.yml   \u2502\u00a0\u00a0 \u251c\u2500\u2500 poetry.lock   \u2502\u00a0\u00a0 \u251c\u2500\u2500 poetry.toml   \u2502\u00a0\u00a0 \u251c\u2500\u2500 pyproject.toml   \u2502\u00a0\u00a0 \u2514\u2500\u2500 test   \u2502\u00a0\u00a0     \u251c\u2500\u2500 test_mount_fsx.sh   \u2502\u00a0\u00a0     \u251c\u2500\u2500 test_mount_s3.sh   \u2502\u00a0\u00a0     \u2514\u2500\u2500 test_volumes.sh   \u251c\u2500\u2500 docker_scripts   \u2502\u00a0\u00a0 \u2514\u2500\u2500 run_jupyter_server.sh   \u2514\u2500\u2500 env       \u2514\u2500\u2500 default.env   ```</p> </li> </ul>"},{"location":"kaizenflow/all.install_helpers.how_to_guide.html","title":"Install Helpers","text":""},{"location":"kaizenflow/all.install_helpers.how_to_guide.html#helpers-distribution-package","title":"Helpers Distribution Package","text":"<ul> <li> <p>This document describes how to build, distribute and install the <code>helpers</code>   package</p> </li> <li> <p>Note for dev/data science members: if you are looking for how to install   packages for your daily work, go to Client Configuration.</p> </li> </ul>"},{"location":"kaizenflow/all.install_helpers.how_to_guide.html#creating-and-installing-the-package","title":"Creating and installing the package","text":""},{"location":"kaizenflow/all.install_helpers.how_to_guide.html#pypi-local-file","title":"PyPI local file","text":"<ul> <li>You can create the <code>helpers</code> package with:</li> </ul> <p>```bash</p> <p>cd helpers python setup.py bdist_wheel   ```</p> <ul> <li> <p>This creates a file <code>dist/helpers-1.0.0-py3-none-any.whl</code> that contains the   package</p> </li> <li> <p>You can install the package with:   ```</p> <p>python -m pip install dist/helpers-1.0.0-py3-none-any.whl   ```</p> </li> </ul>"},{"location":"kaizenflow/all.install_helpers.how_to_guide.html#pypi-workflow","title":"PyPI workflow","text":"<ul> <li> <p>This section describes a temporary solution while we build the CI pipeline.</p> </li> <li> <p>The maintainer of <code>helpers</code> package after merging changes of <code>helpers/</code> into   <code>master</code>, should run:</p> </li> <li> <p>Edit or create a <code>~/.pypirc</code> file with:</p> </li> </ul> <p><code>ini    [distutils]    index-servers =    part    [part]    username:&lt;upload_pypi_username&gt;    password:&lt;upload_pypi_passwd&gt;</code></p> <ol> <li> <p>Update the <code>helpers/CHANGELOG</code> and add version</p> </li> <li> <p>Edit <code>setup.py</code>, changing <code>version</code> in accordance in <code>CHANGELOG</code>, update    <code>install_requires</code> parameters.</p> </li> <li> <p>Run <code>python setup.py sdist upload -r part</code></p> </li> </ol>"},{"location":"kaizenflow/all.install_helpers.how_to_guide.html#pypi-server-installation","title":"PyPI server installation","text":""},{"location":"kaizenflow/all.install_helpers.how_to_guide.html#general-information","title":"General Information","text":"<ul> <li>We use pypiserver as a corporate   PyPI Index server for installing <code>pip</code></li> <li>It implements the same interfaces as PyPI, allowing   standard Python packaging tooling such as <code>pip</code> to interact with it as a   package index just as they would with PyPI</li> <li>The server is based on bottle and serves packages from regular directories</li> <li>Wheels, bdists, eggs can be uploaded either with <code>pip</code>, <code>setuptools</code> or simply   copied with <code>scp</code> to the server directory</li> </ul>"},{"location":"kaizenflow/all.install_helpers.how_to_guide.html#client-configuration-installation","title":"Client Configuration / Installation","text":"<p>You have two options:</p> <ol> <li> <p>Since <code>pypiserver</code> redirects <code>pip install</code> to the pypi.org index if it    doesn't have a requested package, it is a good idea to configure them to    always use your local pypi index.</p> </li> <li> <p>For pip command this can be done by setting the environment variable   <code>PIP_EXTRA_INDEX_URL</code></p> </li> </ol> <p>```bash</p> <p>export PIP_EXTRA_INDEX_URL=http://172.31.36.23:8855/simple/   ```</p> <p>or by adding the following lines to <code>~/.pip/pip.conf</code>:</p> <p><code>ini   [global]   extra-index-url = http://172.31.36.23:8855/simple/   trusted-host = 172.31.36.23</code></p> <ol> <li>Manual installation:</li> </ol> <p>```bash</p> <p>pip install --extra-index-url http://172.31.36.23:8855/simple --trusted-host 172.31.36.23 helpers    ```</p> <p>or</p> <p>```bash</p> <p>pip install --extra-index-url http://172.31.36.23:8855 helpers    ```</p> <ul> <li>Search hosted packages:</li> </ul> <p>```bash</p> <p>pip search --index http://172.31.36.23:8855 ...   ```</p> <ul> <li>Note that pip search does not currently work with the /simple/ endpoint.</li> </ul>"},{"location":"kaizenflow/all.install_helpers.how_to_guide.html#server-details","title":"Server Details","text":"<p>Simple Index WebUI: http://172.31.36.23:8855/simple</p> <p>Host: ubuntu@172.31.36.23</p> <p>Runtime: by docker (standalone container)</p>"},{"location":"kaizenflow/all.install_helpers.how_to_guide.html#server-configuration","title":"Server Configuration","text":"<ul> <li> <p>The corporate PyPI Index server runs with Docker as a standalone container   with mapped volumes on the host.</p> </li> <li> <p>All corporate packages serve from host directory: <code>/home/ubuntu/pypi/packages</code></p> </li> <li> <p>Uploading a new packages or updates existing packages password-protected. Only   authorized user can upload packages. Authorized user credential serves from   host Apache-Like authentication file: <code>/home/ubuntu/pypi/.htpasswd</code>.</p> </li> <li> <p>Credential details you can find on the server in a file <code>~/.pypi_credentials</code></p> </li> <li> <p>To serve packages and authenticate against local <code>.htpasswd</code>:</p> </li> </ul> <p>```bash</p> <p>docker run -d -p 8855:8080 -v ~/pypi/packages:/data/packages -v ~/pypi/.htpasswd:/data/.htpasswd --restart=always pypiserver/pypiserver:latest -v  -P .htpasswd packages   ```</p>"},{"location":"kaizenflow/all.install_helpers.how_to_guide.html#limitation","title":"Limitation","text":"<ul> <li>The <code>pypiserver</code> does not implement the full API as seen on   PyPI. It implements just enough to make <code>pip install</code>,   and <code>search</code> work.</li> </ul>"},{"location":"kaizenflow/all.install_helpers.how_to_guide.html#links","title":"Links","text":"<ul> <li>pip user guide</li> <li>pypiserver</li> <li>setuptool</li> <li>packaging python projects</li> </ul>"},{"location":"kaizenflow/all.install_helpers.how_to_guide.html#code-organization-in-helpers","title":"Code organization in <code>helpers</code>","text":"<ul> <li>In <code>helpers</code> the following hierarchy should be respected:</li> <li><code>repo_config.py</code></li> <li><code>hwarnings</code>, <code>hserver</code>, <code>hlogging</code></li> <li><code>hdbg</code></li> <li><code>hintrospection</code>, <code>hprint</code></li> <li><code>henv</code>, <code>hsystem</code>, <code>hio</code>, <code>hversio</code></li> <li> <p><code>hgit</code></p> </li> <li> <p>A library should only import libs that precede it or are on the same level in   the hierarchy above.</p> </li> <li>E.g. <code>henv</code> can import <code>hdbg</code>, <code>hprint</code> and <code>hio</code>, but it cannot import     <code>hgit</code>.</li> <li>While importing a lib on the same level, make sure you are not creating an     import cycle.</li> <li>For more general infomation, see   all.imports_and_packages.how_to_guide.md.</li> </ul>"},{"location":"kaizenflow/all.run_Mock2_in_batch_mode.how_to_guide.html","title":"Run Mock2 In Batch Mode","text":"<p>The goal is to run a simple system (Mock2) end-to-end in batch mode and compute PnL. This is the typical flow that Quants run to estimate performance of a model.</p>"},{"location":"kaizenflow/all.run_Mock2_in_batch_mode.how_to_guide.html#description-of-the-forecast-system","title":"Description of the forecast system","text":"<ul> <li> <p>A notebook running the forecast system interactively is   /docs/kaizenflow/all.run_Mock2_pipeline_in_notebook.how_to_guide.ipynb</p> </li> <li> <p>The <code>Mock2</code> pipeline is described in   /dataflow_amp/pipelines/mock2/mock2_pipeline.py</p> </li> <li> <p><code>Mock2_DagBuilder</code> is a <code>DagBuilder</code> with</p> <ul> <li>A <code>get_config_template()</code> to expose the parameters that can be configured   for each node; and</li> <li>A <code>_get_dag()</code> to build the entire DAG from a fully specified <code>Config</code></li> </ul> </li> <li> <p>The notebook assembles a complete <code>DAG</code> including:</p> </li> <li>An <code>ImClient</code> (which reads the raw data adapting to the <code>MarketData</code>     semantics)</li> <li>A <code>MarketData</code> (which provides an abstractions to get data on intervals)</li> <li> <p>A <code>HistoricalDataSource</code> (which adapts <code>MarkatData</code> to the <code>DataFlow</code>     semantics)</p> </li> <li> <p>Finally the entire <code>DAG</code> is run</p> </li> </ul>"},{"location":"kaizenflow/all.run_Mock2_in_batch_mode.how_to_guide.html#description-of-the-system","title":"Description of the System","text":"<ul> <li> <p>The same <code>System</code> can be built using various utilities from   dataflow/system/system.py</p> </li> <li> <p>A <code>Mock2_NonTime_ForecastSystem</code></p> </li> <li>Is built in     /dataflow_amp/system/mock2/mock2_forecast_system.py</li> <li>Contains an <code>HistoricalMarketData</code></li> <li> <p>A non-timed DAG since it runs in batch mode</p> </li> <li> <p>Concrete fully-configured <code>System</code>s are built in   /dataflow_amp/system/mock2/mock2_forecast_system_example.py</p> </li> </ul>"},{"location":"kaizenflow/all.run_Mock2_in_batch_mode.how_to_guide.html#run-a-backtest","title":"Run a backtest","text":"<p>Pull the latest <code>master</code></p> <pre><code>&gt; git checkout master\n&gt; git pull\n</code></pre> <ul> <li>Run backtest</li> </ul> <pre><code>&gt; i docker_bash\n&gt; dataflow_amp/system/mock2/run_backtest.sh\n</code></pre> <ul> <li>The script runs a backtest for a simple dummy \"strategy\" using equities data   for 1 month (2023-08) and 1 asset (MSFT). Trading frequency is 5 minutes.</li> </ul>"},{"location":"kaizenflow/all.run_Mock2_in_batch_mode.how_to_guide.html#explanation-of-the-backtesting-script","title":"Explanation of the backtesting script","text":"<ul> <li>Inside <code>docker_bash</code></li> </ul> <p>```bash   tag=\"mock2\"   backtest_config=\"bloomberg_v1-top1.5T.2023-08-01_2023-08-31\"   config_builder=\"dataflow_amp.system.mock2.mock2_tile_config_builders.build_Mock2_tile_config_list(\\\"${backtest_config}\\\")\"</p> <p>dst_dir=\"build_Mock2_tile_config_list.${tag}.${backtest_config}.run1\"</p> <p>./dataflow/backtest/run_config_list.py \\       --experiment_builder \"dataflow.backtest.master_backtest.run_in_sample_tiled_backtest\" \\       --config_builder $config_builder \\       --dst_dir $dst_dir \\       $OPTS 2&gt;&amp;1 | tee run_config_list.txt   ```</p> <ul> <li> <p><code>backtest_config</code> is used to configure <code>_get_Mock2_NonTime_ForecastSystem()</code></p> </li> <li> <p>The code in <code>dataflow_amp/system/mock2/mock2_tile_config_builders.py</code> is used   to create the config list to run sweeps</p> </li> <li> <p>The basic type of experiment is configured through   <code>dataflow.backtest.master_backtest.run_in_sample_tiled_backtest</code></p> </li> <li> <p>See   /docs/dataflow/ck.run_backtest.how_to_guide.md   for more details</p> </li> </ul>"},{"location":"kaizenflow/all.run_Mock2_in_batch_mode.how_to_guide.html#analyze-the-results","title":"Analyze the results","text":"<ul> <li> <p>```</p> <p>i docker_jupyter   ```</p> </li> <li> <p>Run the notebook   /docs/kaizenflow/all.analyze_Mock2_pipeline_simulation.how_to_guide.ipynb   top-to-bottom</p> </li> <li> <p>The notebook computes a portfolio given the predictions and computes model   performance metrics</p> </li> <li> <p>Note: make sure that dir_name in <code>Build the config section</code> points to the   right output   <code>/app/build_Mock2_tile_config_list.mock2.bloomberg_v1-top1.5T.2023-08-01_2023-08-31.run1/tiled_results</code></p> </li> <li> <p>See more on how to run a Jupyter here under Start a Jupyter server. The flow   is:</p> </li> <li>Open a web-browser</li> <li>Use the IP of your machine</li> <li>Get the port from the output of <code>i docker_jupyter</code></li> </ul>"},{"location":"kaizenflow/all.run_end_to_end_Mock2_system.tutorial.html","title":"Run End To End Mock2 System","text":""},{"location":"kaizenflow/all.run_end_to_end_Mock2_system.tutorial.html#overview","title":"Overview","text":"<p>The goal is to run a <code>System</code> with <code>Portfolio</code> in the replayed time mode for a few bars.</p> <p>The <code>System</code> is /dataflow_amp/system/mock2/mock2_forecast_system.py</p> <p>The system is composed by the following components</p> <pre><code>flowchart LR\n    MarketData --&gt; PredictionDag\n    subgraph Dag\n      PredictionDag --&gt; ProcessForecasts\n    end\n    subgraph Portfolio\n        DataFramePortfolio --&gt; DataFrameBroker\n    end\n    ProcessForecasts --&gt; DataFramePortfolio\n    DataFrameBroker --&gt; Trades\n</code></pre>"},{"location":"kaizenflow/all.run_end_to_end_Mock2_system.tutorial.html#high-level-architecture-and-code-organization","title":"High-level architecture and code organization","text":"<ul> <li><code>dataflow_amp/pipelines/mock2/mock2_pipeline.py</code></li> <li>Builder that creates the prediction model</li> <li> <p><code>Mock2_DagBuilder</code></p> </li> <li> <p><code>dataflow_amp/system/mock2/mock2_forecast_system.py</code></p> </li> <li>Build the full-system</li> <li><code>Mock2_NonTime_ForecastSystem</code></li> <li> <p><code>Mock2_Time_ForecastSystem_with_DataFramePortfolio</code></p> </li> <li> <p><code>dataflow_amp/system/mock2/mock2_forecast_system_example.py</code></p> </li> <li>Builder for the System used in backtesting and unit testing</li> <li><code>get_Mock2_NonTime_ForecastSystem_example1()</code></li> <li> <p><code>get_Mock2_NonTime_ForecastSystem_example2()</code></p> </li> <li> <p><code>dataflow_amp/system/mock2/mock2_tile_config_builders.py</code></p> </li> <li>Build tile configs for simulation</li> <li><code>build_Mock2_tile_config_list()</code></li> <li> <p><code>build_Mock2_tile_config_list_for_unit_test()</code></p> </li> <li> <p><code>dataflow_amp/system/mock2/test/test_mock2_forecast_system.py</code></p> </li> <li>Unit tests for <code>Mock2_NonTime_ForecastSystem</code></li> <li><code>Test_Mock2_System_CheckConfig</code></li> <li><code>Test_Mock2_NonTime_ForecastSystem_FitPredict</code></li> <li> <p>...</p> </li> <li> <p><code>dataflow_amp/system/mock2/test/test_mock2_tiledbacktest.py</code></p> </li> <li>Run an end-to-end backtest for a Mock2 pipeline and analysis flow</li> <li> <p><code>Test_Mock2_NonTime_ForecastSystem_TiledBacktest</code></p> </li> <li> <p><code>dataflow_amp/system/mock2/scripts/run_end_to_end_Mock2_system.py</code></p> </li> <li>Run an end-to-end streaming simulation</li> </ul>"},{"location":"kaizenflow/all.run_end_to_end_Mock2_system.tutorial.html#system-configuration","title":"System Configuration","text":"<p><code>System</code> parameters are controlled via <code>SystemConfig</code>, which is built <code>dataflow_amp/system/mock2/scripts/run_end_to_end_Mock2_system.py</code>.</p> <p>The snippet of code below configures the input data, e.g., bar duration, history amount, number of assets</p> <pre><code>## Bar duration in seconds, e.g., 60 * 60 is 1 hour bar.\nsystem.config[\"bar_duration_in_seconds\"] = 60 * 60\nsystem.config[\"market_data_config\", \"number_of_assets\"] = 10\n## History amount, e.g., 10 days worth of data.\nsystem.config[\"market_data_config\", \"history_lookback\"] = pd.Timedelta(\n    days=10\n)\n</code></pre> <p>The snippet of code below configures the maximum portfolio notional:</p> <pre><code>system.config[\n    \"process_forecasts_node_dict\",\n    \"process_forecasts_dict\",\n    \"optimizer_config\",\n    \"params\",\n    \"kwargs\",\n] = cconfig.Config.from_dict({\"target_gmv\": 1e7})\n</code></pre> <p>The snippet of code below configures for how long to run the <code>System</code>:</p> <pre><code>system.config[\"dag_runner_config\", \"rt_timeout_in_secs_or_time\"] = (\n    system.config[\"bar_duration_in_seconds\"] * 2\n)\n</code></pre> <p>The script saves the results to <code>/app/system_log_dir</code> together with the configuration file <code>/app/system_log_dir/system_config.output.txt</code>.</p> <p>For the demo purposes only a few most important parameters are exposed while there are more parameters to control. The full <code>SystemConfig</code> looks like:</p> <pre><code>dag_config (marked_as_used=False, writer=None, val_type=core.config.config_.Config):\n  filter_weekends (marked_as_used=False, writer=None, val_type=core.config.config_.Config):\n    in_col_groups (marked_as_used=False, writer=None, val_type=list): [('close',), ('high',), ('low',), ('open',), ('volume',)]\n    out_col_group (marked_as_used=False, writer=None, val_type=tuple): ()\n    join_output_with_input (marked_as_used=False, writer=None, val_type=bool): False\n  filter_ath (marked_as_used=False, writer=None, val_type=core.config.config_.Config):\n    in_col_groups (marked_as_used=False, writer=None, val_type=list): [('close',), ('high',), ('low',), ('open',), ('volume',)]\n    out_col_group (marked_as_used=False, writer=None, val_type=tuple): ()\n    transformer_kwargs (marked_as_used=False, writer=None, val_type=core.config.config_.Config):\n      start_time (marked_as_used=False, writer=None, val_type=datetime.time): 09:30:00\n      end_time (marked_as_used=False, writer=None, val_type=datetime.time): 16:00:00\n    join_output_with_input (marked_as_used=False, writer=None, val_type=bool): False\n  resample (marked_as_used=False, writer=None, val_type=core.config.config_.Config):\n    in_col_groups (marked_as_used=False, writer=None, val_type=list): [('open',), ('high',), ('low',), ('close',), ('volume',)]\n    out_col_group (marked_as_used=False, writer=None, val_type=tuple): ()\n    transformer_kwargs (marked_as_used=False, writer=None, val_type=core.config.config_.Config):\n      rule (marked_as_used=False, writer=None, val_type=str): 60T\n      resampling_groups (marked_as_used=False, writer=None, val_type=list): [({'close': 'close'}, 'last', {}), ({'high': 'high'}, 'max', {}), ({'low': 'low'}, 'min', {}), ({'open': 'open'}, 'first', {}), ({'volume': 'volume'}, 'sum', {'min_count': 1}), ({'close': 'twap'}, 'mean', {})]\n      vwap_groups (marked_as_used=False, writer=None, val_type=list): [('close', 'volume', 'vwap')]\n    reindex_like_input (marked_as_used=False, writer=None, val_type=bool): False\n    join_output_with_input (marked_as_used=False, writer=None, val_type=bool): False\n  compute_ret_0 (marked_as_used=False, writer=None, val_type=core.config.config_.Config):\n    in_col_groups (marked_as_used=False, writer=None, val_type=list): [('close',), ('vwap',), ('twap',)]\n    out_col_group (marked_as_used=False, writer=None, val_type=tuple): ()\n    transformer_kwargs (marked_as_used=False, writer=None, val_type=core.config.config_.Config):\n      mode (marked_as_used=False, writer=None, val_type=str): pct_change\n    col_mapping (marked_as_used=False, writer=None, val_type=core.config.config_.Config):\n      close (marked_as_used=False, writer=None, val_type=str): close.ret_0\n      vwap (marked_as_used=False, writer=None, val_type=str): vwap.ret_0\n      twap (marked_as_used=False, writer=None, val_type=str): twap.ret_0\n  compute_vol (marked_as_used=False, writer=None, val_type=core.config.config_.Config):\n    in_col_groups (marked_as_used=False, writer=None, val_type=list): [('vwap.ret_0',)]\n    out_col_group (marked_as_used=False, writer=None, val_type=tuple): ()\n    transformer_kwargs (marked_as_used=False, writer=None, val_type=core.config.config_.Config):\n      tau (marked_as_used=False, writer=None, val_type=int): 32\n    col_mapping (marked_as_used=False, writer=None, val_type=core.config.config_.Config):\n      vwap.ret_0 (marked_as_used=False, writer=None, val_type=str): vwap.ret_0.vol\n  adjust_rets (marked_as_used=False, writer=None, val_type=core.config.config_.Config):\n    in_col_groups (marked_as_used=False, writer=None, val_type=list): [('vwap.ret_0',), ('vwap.ret_0.vol',)]\n    out_col_group (marked_as_used=False, writer=None, val_type=tuple): ()\n    transformer_kwargs (marked_as_used=False, writer=None, val_type=core.config.config_.Config):\n      term1_col (marked_as_used=False, writer=None, val_type=str): vwap.ret_0\n      term2_col (marked_as_used=False, writer=None, val_type=str): vwap.ret_0.vol\n      out_col (marked_as_used=False, writer=None, val_type=str): vwap.ret_0.vol_adj\n      term2_delay (marked_as_used=False, writer=None, val_type=int): 2\n      operation (marked_as_used=False, writer=None, val_type=str): div\n    drop_nans (marked_as_used=False, writer=None, val_type=bool): True\n  clip (marked_as_used=False, writer=None, val_type=core.config.config_.Config):\n    in_col_groups (marked_as_used=False, writer=None, val_type=list): [('vwap.ret_0.vol_adj',)]\n    out_col_group (marked_as_used=False, writer=None, val_type=tuple): ()\n    col_mapping (marked_as_used=False, writer=None, val_type=core.config.config_.Config):\n        vwap.ret_0.vol_adj (marked_as_used=False, writer=None, val_type=str): vwap.ret_0.vol_adj.c\ndag_builder_object (marked_as_used=True, writer=/app/amp/dataflow/system/system_builder_utils.py::491::apply_dag_property, val_type=dataflow_amp.pipelines.mock2.mock2_pipeline.Mock2_DagBuilder): nid_prefix=\ndag_builder_class (marked_as_used=False, writer=None, val_type=str): Mock2_DagBuilder\nsystem_class (marked_as_used=False, writer=None, val_type=str): Mock2_Time_ForecastSystem_with_DataFramePortfolio\nsystem_log_dir (marked_as_used=False, writer=None, val_type=str): ./system_log_dir\nmarket_data_config (marked_as_used=False, writer=None, val_type=core.config.config_.Config):\n  system_log_dir (marked_as_used=False, writer=None, val_type=str): /app/system_log_dir\n  bar_duration_in_seconds (marked_as_used=False, writer=None, val_type=int): 3600\n  number_of_assets (marked_as_used=False, writer=None, val_type=int): 10\n  history_lookback (marked_as_used=False, writer=None, val_type=pandas._libs.tslibs.timedeltas.Timedelta): 10 days 00:00:00\n  replayed_delay_in_mins_or_timestamp (marked_as_used=False, writer=None, val_type=pandas._libs.tslibs.timestamps.Timestamp): 2023-08-15 10:30:00-04:00\n  asset_ids (marked_as_used=True, writer=/app/amp/dataflow/system/system_builder_utils.py::707::get_DataFramePortfolio_from_System, val_type=list): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n  data (marked_as_used=True, writer=/app/amp/dataflow/system/system_builder_utils.py::329::get_ReplayedMarketData_from_df, val_type=pandas.core.frame.DataFrame):\n    index=[0, 7209]\n    columns=start_datetime,end_datetime,timestamp_db,open,high,low,close,volume,asset_id\n    shape=(7210, 9)\n                 start_datetime              end_datetime              timestamp_db     open     high      low    close  volume  asset_id\n    0 2023-08-01 09:00:00-04:00 2023-08-01 10:00:00-04:00 2023-08-01 10:00:10-04:00   999.89   999.89   999.89   999.89    13.0         0\n    1 2023-08-01 09:00:00-04:00 2023-08-01 10:00:00-04:00 2023-08-01 10:00:10-04:00  1000.00  1000.00  1000.00  1000.00    14.0         1\n    2 2023-08-01 09:00:00-04:00 2023-08-01 10:00:00-04:00 2023-08-01 10:00:10-04:00  1000.00  1000.00  1000.00  1000.00    12.0         2\n    ...\n    7207 2023-08-31 09:00:00-04:00 2023-08-31 10:00:00-04:00 2023-08-31 10:00:10-04:00  1002.39  1003.28  1002.39  1003.28   477.0         7\n    7208 2023-08-31 09:00:00-04:00 2023-08-31 10:00:00-04:00 2023-08-31 10:00:10-04:00   986.35   986.35   985.84   986.15   466.0         8\n    7209 2023-08-31 09:00:00-04:00 2023-08-31 10:00:00-04:00 2023-08-31 10:00:10-04:00   997.93   998.48   997.69   997.84   493.0         9\n  delay_in_secs (marked_as_used=False, writer=None, val_type=int): 10\nportfolio_config (marked_as_used=False, writer=None, val_type=core.config.config_.Config):\n  mark_to_market_col (marked_as_used=True, writer=/app/amp/dataflow/system/system_builder_utils.py::713::get_DataFramePortfolio_from_System, val_type=str): close\n  pricing_method (marked_as_used=True, writer=/app/amp/dataflow/system/system_builder_utils.py::716::get_DataFramePortfolio_from_System, val_type=str): twap.60T\n  column_remap (marked_as_used=False, writer=None, val_type=core.config.config_.Config):\n    bid (marked_as_used=True, writer=/app/amp/dataflow/system/system_builder_utils.py::704::get_DataFramePortfolio_from_System, val_type=str): bid\n    ask (marked_as_used=True, writer=/app/amp/dataflow/system/system_builder_utils.py::704::get_DataFramePortfolio_from_System, val_type=str): ask\n    midpoint (marked_as_used=True, writer=/app/amp/dataflow/system/system_builder_utils.py::704::get_DataFramePortfolio_from_System, val_type=str): midpoint\n    price (marked_as_used=True, writer=/app/amp/dataflow/system/system_builder_utils.py::704::get_DataFramePortfolio_from_System, val_type=str): close\nprocess_forecasts_node_dict (marked_as_used=False, writer=None, val_type=core.config.config_.Config):\n  prediction_col (marked_as_used=False, writer=None, val_type=str): vwap.ret_0.vol_adj.c\n  volatility_col (marked_as_used=False, writer=None, val_type=str): vwap.ret_0.vol\n  spread_col (marked_as_used=False, writer=None, val_type=NoneType): None\n  portfolio (marked_as_used=False, writer=None, val_type=oms.portfolio.dataframe_portfolio.DataFramePortfolio):\n    &lt;oms.portfolio.dataframe_portfolio.DataFramePortfolio at 0x7fbc7502f340&gt;\n      # holdings_shares=\n      Empty DataFrame\n      Columns: []\n      Index: []\n      # holdings_notional=\n      Empty DataFrame\n      Columns: []\n      Index: []\n      # executed_trades_shares=\n      Empty DataFrame\n      Columns: []\n      Index: []\n      # executed_trades_notional=\n      Empty DataFrame\n      Columns: []\n      Index: []\n      # pnl=\n      Empty DataFrame\n      Columns: []\n      Index: []\n      # statistics=\n      Empty DataFrame\n      Columns: [pnl]\n      Index: []\n  process_forecasts_dict (marked_as_used=False, writer=None, val_type=core.config.config_.Config):\n    execution_mode (marked_as_used=False, writer=None, val_type=str): real_time\n    log_dir (marked_as_used=False, writer=None, val_type=str): ./system_log_dir/process_forecasts\n    order_config (marked_as_used=False, writer=None, val_type=core.config.config_.Config):\n      order_type (marked_as_used=False, writer=None, val_type=str): price@twap\n      passivity_factor (marked_as_used=False, writer=None, val_type=NoneType): None\n      order_duration_in_mins (marked_as_used=False, writer=None, val_type=int): 60\n    optimizer_config (marked_as_used=False, writer=None, val_type=core.config.config_.Config):\n      backend (marked_as_used=False, writer=None, val_type=str): pomo\n      params (marked_as_used=False, writer=None, val_type=core.config.config_.Config):\n        style (marked_as_used=False, writer=None, val_type=str): cross_sectional\n        kwargs (marked_as_used=False, writer=None, val_type=core.config.config_.Config):\n          bulk_frac_to_remove (marked_as_used=False, writer=None, val_type=float): 0.0\n          bulk_fill_method (marked_as_used=False, writer=None, val_type=str): zero\n          target_gmv (marked_as_used=False, writer=None, val_type=float): 10000000.0\n    ath_start_time (marked_as_used=False, writer=None, val_type=datetime.time): 09:30:00\n    trading_start_time (marked_as_used=False, writer=None, val_type=datetime.time): 09:30:00\n    ath_end_time (marked_as_used=False, writer=None, val_type=datetime.time): 16:00:00\n    trading_end_time (marked_as_used=False, writer=None, val_type=datetime.time): 15:00:00\n    liquidate_at_trading_end_time (marked_as_used=False, writer=None, val_type=bool): False\ndag_runner_config (marked_as_used=False, writer=None, val_type=core.config.config_.Config):\n  bar_duration_in_secs (marked_as_used=True, writer=/app/amp/dataflow/system/system_builder_utils.py::1108::get_RealTimeDagRunner_from_System, val_type=int): 3600\n  rt_timeout_in_secs_or_time (marked_as_used=True, writer=/app/amp/dataflow/system/system_builder_utils.py::1111::get_RealTimeDagRunner_from_System, val_type=int): 7200\nevent_loop_object (marked_as_used=True, writer=/app/amp/dataflow/system/system_builder_utils.py::711::get_DataFramePortfolio_from_System, val_type=helpers.hasyncio._EventLoop): &lt;_EventLoop running=False closed=False debug=False&gt;\n</code></pre>"},{"location":"kaizenflow/all.run_end_to_end_Mock2_system.tutorial.html#system-components","title":"System components","text":""},{"location":"kaizenflow/all.run_end_to_end_Mock2_system.tutorial.html#marketdata","title":"MarketData","text":"<p>The source of data is <code>ReplayedMarketData</code>, an object that can replay a synthetic or previously capture dataframe. The data is represented by random OHLCV bars for <code>N</code> assets, <code>K</code> days of history and <code>D</code> bar duration in seconds, where <code>N</code>, <code>K</code>, <code>D</code> are configurable parameters that we control using <code>SystemConfig</code>.</p> <p>Data snippet:</p> <pre><code>                                     start_datetime              timestamp_db     open     high      low    close  volume  asset_id\n\nend_datetime 2023-08-08 15:00:00-04:00 2023-08-08 14:00:00-04:00 2023-08-08\n15:00:10-04:00 985.34 986.36 984.87 986.25 935.0 0 2023-08-08 15:00:00-04:00\n2023-08-08 14:00:00-04:00 2023-08-08 15:00:10-04:00 1005.38 1006.65 1005.07\n1006.34 948.0 1 2023-08-08 15:00:00-04:00 2023-08-08 14:00:00-04:00 2023-08-08\n15:00:10-04:00 1002.44 1002.49 1001.60 1001.88 1013.0 2\n</code></pre> <p>We also control via <code>SystemConfig</code>:</p> <ul> <li>When to start replaying the data, e.g., the dataframe starts at   <code>2023-08-01 10:00:00</code> but we start computing at <code>2023-08-15 10:30:00</code> so that   there is enough history to warm up the system</li> <li>Delay in seconds, i.e. the <code>System</code> simulates delay and waits for data to   become available for <code>X</code> seconds</li> </ul>"},{"location":"kaizenflow/all.run_end_to_end_Mock2_system.tutorial.html#dagbuilder","title":"DagBuilder","text":"<p>A <code>DagBuilder</code> configures <code>Nodes</code> and connects them into a <code>Dag</code> in order to generate forecasts. The <code>DagBuilder</code> used in this example is the toy model <code>Mock2_DagBuilder</code> in /dataflow_amp/pipelines/mock2/mock2_pipeline.py</p> <p><code>Mock2_DagBuilder</code> is a pipeline that:</p> <ul> <li>Filters out weekends</li> <li>Removes rows outside active trading hours</li> <li>Resamples the data to the desired frequency</li> <li>Computes returns</li> <li>Computes volatility</li> <li>Adjusts returns using volatility</li> <li>Clips returns</li> </ul>"},{"location":"kaizenflow/all.run_end_to_end_Mock2_system.tutorial.html#dag","title":"DAG","text":"<p><code>Dag</code> is represented by:</p> <ul> <li><code>RealTimeDataSource</code> node that contains <code>ReplayedMarketData</code> and introduces   real-time data behavior, e.g., the System waits for <code>X</code> seconds for the data</li> <li><code>Nodes</code> that are described by the <code>Mock2_DagBuilder</code></li> <li><code>ProcessForecastsNode</code> that controls the <code>Portfolio</code> configuration</li> </ul> <p>One can configure portfolio construction (e.g, maximum portfolio notional, i.e. <code>target_gmv</code>) via <code>SystemConfig</code>.</p>"},{"location":"kaizenflow/all.run_end_to_end_Mock2_system.tutorial.html#dagrunner","title":"DagRunner","text":"<p><code>DagRunner</code> is represented by <code>RealTimeDagRunner</code> which is an executor that controls how to run the <code>System</code> in streaming mode (both real-time and simulated).</p> <p>One can configure any parameter (e.g., for how long to run the <code>System</code>, e.g., for 2 bars) via <code>SystemConfig</code>.</p>"},{"location":"kaizenflow/all.run_end_to_end_Mock2_system.tutorial.html#portfolio","title":"Portfolio","text":"<p><code>Portfolio</code> is implemented by <code>DataFramePortfolio</code> with a <code>DataFrameBroker</code> which:</p> <ul> <li>Keeps track of holdings using a <code>DataFrame</code> (instead of a database)</li> <li>Has no advanced mechanism to control trade execution, i.e. all orders always   are fully filled</li> </ul>"},{"location":"kaizenflow/all.run_end_to_end_Mock2_system.tutorial.html#system-run","title":"System run","text":"<p>To run the <code>System</code> and save logs execute the following cmd:</p> <pre><code>docker&gt; ./dataflow_amp/system/mock2/scripts/run_end_to_end_Mock2_system.py 2&gt;&amp;1 | tee tmp.log_system.txt\n</code></pre> <p>The System starts at <code>2023-08-15 11:00:00-04:00</code> and computes the DAG for 2 bars.</p> <pre><code>## Real-time loop: num_it=1: rt_time_out_in_secs=7200 wall_clock_time='2023-08-15 11:00:00-04:00' real_wall_clock_time='2024-01-05 08:39:32.981505-05:00'\n</code></pre> <p>It waits for the data to become available for 10 seconds (configurable):</p> <pre><code>#### waiting on last bar: num_iter=10/120: current_bar_timestamp=2023-08-15 11:00:00-04:00 wall_clock_time=2023-08-15 11:00:10-04:00 last_db_end_time=2023-08-15 11:00:00-04:00\n\n08:39:33 rss=0.292GB vms=1.261GB mem_pct=1% Task-3 hprint.py log_frame:604\n\n## Waiting on last bar: done\n</code></pre> <p>And once the data is ready the <code>System</code> computes the <code>Dag</code>:</p> <pre><code>#################################################################################\nExecuting method 'predict' for node topological_id=0 nid='read_data' ...\n#################################################################################\n...\n#################################################################################\nExecuting method 'predict' for node topological_id=8 nid='process_forecasts' ...\n#################################################################################\n</code></pre> <p>When executing the <code>ProcessForecastsNode</code> the System:</p> <ul> <li>Computes target positions</li> <li>Generates orders</li> <li>Submits orders (in this case all orders are fully filled)</li> </ul> <pre><code>## last target positions=\n\n          holdings_shares    price  holdings_notional      wall_clock_timestamp  prediction  volatility  spread  target_holdings_notional  target_trades_notional  target_trades_shares  target_holdings_shares\n\nasset_id 0 0 983.66 0 2023-08-15 11:00:11-04:00 0.9674 0.000967 0 586609.98842\n586609.98842 596.35442 596.35442 1 0 1010.59 0 2023-08-15 11:00:11-04:00\n0.772048 0.001099 0 454185.047454 454185.047454 449.42563 449.42563 2 0 1005.54\n0 2023-08-15 11:00:11-04:00 -0.607692 0.000675 0 -1202955.27038 -1202955.27038\n-1196.327615 -1196.327615 ... 7 0 1014.55 0 2023-08-15 11:00:11-04:00 -0.632565\n0.000568 0 -1699546.60519 -1699546.60519 -1675.17284 -1675.17284 8 0 996.88 0\n2023-08-15 11:00:11-04:00 -1.287815 0.000904 0 -670108.728394 -670108.728394\n-672.206011 -672.206011 9 0 993.89 0 2023-08-15 11:00:11-04:00 -0.02503 0.00066\n0 1259035.977478 1259035.977478 1266.775979 1266.775979 ...\n\n## last orders=\n\nOrder: order*id=0 creation_timestamp=2023-08-15 11:00:11-04:00 asset_id=0\ntype*=price@twap start*timestamp=2023-08-15 11:00:11-04:00\nend_timestamp=2023-08-15 12:00:00-04:00 curr_num_shares=0.0\ndiff_num_shares=596.354419637 tz=America/New_York extra_params={} Order:\norder_id=1 creation_timestamp=2023-08-15 11:00:11-04:00 asset_id=1\ntype*=price@twap start*timestamp=2023-08-15 11:00:11-04:00\nend_timestamp=2023-08-15 12:00:00-04:00 curr_num_shares=0.0\ndiff_num_shares=449.425630032 tz=America/New_York extra_params={} Order:\norder_id=2 creation_timestamp=2023-08-15 11:00:11-04:00 asset_id=2\ntype*=price@twap start*timestamp=2023-08-15 11:00:11-04:00\nend_timestamp=2023-08-15 12:00:00-04:00 curr_num_shares=0.0\ndiff_num_shares=-1196.327615391 tz=America/New_York extra_params={} Order:\norder_id=3 creation_timestamp=2023-08-15 11:00:11-04:00 asset_id=3\ntype*=price@twap start*timestamp=2023-08-15 11:00:11-04:00\nend_timestamp=2023-08-15 12:00:00-04:00 curr_num_shares=0.0\ndiff_num_shares=-686.685193608 tz=America/New_York extra_params={} Order:\norder_id=4 creation_timestamp=2023-08-15 11:00:11-04:00 asset_id=4\ntype*=price@twap start*timestamp=2023-08-15 11:00:11-04:00\nend_timestamp=2023-08-15 12:00:00-04:00 curr_num_shares=0.0\ndiff_num_shares=386.275392292 tz=America/New_York extra_params={} Order:\norder_id=5 creation_timestamp=2023-08-15 11:00:11-04:00 asset_id=5\ntype*=price@twap start*timestamp=2023-08-15 11:00:11-04:00\nend_timestamp=2023-08-15 12:00:00-04:00 curr_num_shares=0.0\ndiff_num_shares=1960.903428683 tz=America/New_York extra_params={} Order:\norder_id=6 creation_timestamp=2023-08-15 11:00:11-04:00 asset_id=6\ntype*=price@twap start*timestamp=2023-08-15 11:00:11-04:00\nend_timestamp=2023-08-15 12:00:00-04:00 curr_num_shares=0.0\ndiff_num_shares=-1086.072750644 tz=America/New_York extra_params={} Order:\norder_id=7 creation_timestamp=2023-08-15 11:00:11-04:00 asset_id=7\ntype*=price@twap start*timestamp=2023-08-15 11:00:11-04:00\nend_timestamp=2023-08-15 12:00:00-04:00 curr_num_shares=0.0\ndiff_num_shares=-1675.172840363 tz=America/New_York extra_params={} Order:\norder_id=8 creation_timestamp=2023-08-15 11:00:11-04:00 asset_id=8\ntype*=price@twap start*timestamp=2023-08-15 11:00:11-04:00\nend_timestamp=2023-08-15 12:00:00-04:00 curr_num_shares=0.0\ndiff_num_shares=-672.206011149 tz=America/New_York extra_params={} Order:\norder_id=9 creation_timestamp=2023-08-15 11:00:11-04:00 asset_id=9\ntype*=price@twap start_timestamp=2023-08-15 11:00:11-04:00\nend_timestamp=2023-08-15 12:00:00-04:00 curr_num_shares=0.0\ndiff_num_shares=1266.775978708 tz=America/New_York extra_params={}\n</code></pre> <p>Then the <code>System</code> goes to sleep waiting for the next bar to start:</p> <pre><code>08:39:37 rss=0.296GB vms=1.325GB mem*pct=1%% Task-3 process_forecasts*.py\nprocess_forecasts:353 Event: exiting process_forecasts() for loop. 08:39:37\nrss=0.296GB vms=1.325GB mem_pct=1% Task-3 real_time_dag_runner.py \\_run_dag:264\nWaiting on node 'process_forecasts': done 08:39:37 rss=0.296GB vms=1.325GB\nmem_pct=1% Task-1 real_time.py execute_with_real_time_loop:422 await done\n(wall_clock_time=2023-08-15 12:00:00-04:00)\n</code></pre> <p>Since the clock is simulated, instead of waiting 1 hour, the <code>System</code> moves the clock forward within a few seconds and starts to compute the 2nd bar:</p> <pre><code>## Real-time loop: num_it=2: rt_time_out_in_secs=7200 wall_clock_time='2023-08-15 12:00:00-04:00' real_wall_clock_time='2024-01-05 08:39:37.416849-05:00'\n</code></pre> <p>Then the <code>System</code> repeats the <code>Dag</code> computation and order submission but for the next bar and exits once the termination condition becomes True (run for 2 bars in this case):</p> <pre><code>08:39:42 rss=0.297GB vms=1.326GB mem_pct=1% Task-1 hwall_clock_time.py\nset_current_bar_timestamp:105 timestamp=2023-08-15 13:00:00-04:00 08:39:42\nrss=0.297GB vms=1.326GB mem_pct=1%% Task-1 real_time.py\nexecute_with_real_time_loop:433 rt_timeout_in_secs_or_time=7200,\nbar_duration_in_secs=3600, num_it=2, num_iterations=2, is_done=True 08:39:42\nrss=0.297GB vms=1.326GB mem_pct=1% - ^[[36mINFO ^[[0m Task-1 real_time.py\nexecute_with_real_time_loop:443 Exiting loop: num_it=2, num_iterations=2\n</code></pre> <p>The output is in the dir:</p> <pre><code>&gt; tree.sh -p system_log_dir\nsystem_log_dir/\n|-- process_forecasts/\n|   |-- orders/\n|   |   |-- 20230815_110000.20230815_110011.csv\n|   |   `-- 20230815_120000.20230815_120011.csv\n|   |-- portfolio/\n|   |   |-- executed_trades_notional/\n|   |   |   |-- 20230815_110000.20230815_110011.csv\n|   |   |   `-- 20230815_120000.20230815_120011.csv\n|   |   |-- executed_trades_shares/\n|   |   |   |-- 20230815_110000.20230815_110011.csv\n|   |   |   `-- 20230815_120000.20230815_120011.csv\n|   |   |-- holdings_notional/\n|   |   |   |-- 20230815_110000.20230815_110011.csv\n|   |   |   `-- 20230815_120000.20230815_120011.csv\n|   |   |-- holdings_shares/\n|   |   |   |-- 20230815_110000.20230815_110011.csv\n|   |   |   `-- 20230815_120000.20230815_120011.csv\n|   |   `-- statistics/\n|   |       |-- 20230815_110000.20230815_110011.csv\n|   |       `-- 20230815_120000.20230815_120011.csv\n|   `-- target_positions/\n|       |-- 20230815_110000.20230815_110011.csv\n|       `-- 20230815_120000.20230815_120011.csv\n|-- system_config.input.txt\n|-- system_config.input.values_as_strings.pkl\n|-- system_config.output.txt\n`-- system_config.output.values_as_strings.pkl\n\n10 directories, 18 files\n</code></pre>"},{"location":"kaizenflow/all.run_end_to_end_Mock2_system.tutorial.html#todogp-all-describe-the-output","title":"TODO(gp): @all Describe the output","text":""},{"location":"onboarding/admin.onboarding_process.html","title":"On-boarding Process","text":"<ol> <li> <p>A candidate (intern or full-time candidate) submits a request</p> </li> <li> <p>The <code>ResearchMeister</code> gets the notification from the submission Gsheet and   creates an Asana task under <code>Not started</code> with:</p> </li> <li>Name (nickname) email</li> <li>E.g, <code>Yuanxuan ****** &lt;yuanxuan@*******.edu&gt;</code></li> <li>Put the form links, the GitHub account, the LinkedIn / CV link, etc. in the     Asana description (all the info that we typically need to access quickly)</li> <li>Google form: ?</li> <li>CV / LinkedIn: ?</li> <li>GitHub handle: ?</li> <li>Email: ?</li> <li>Telegram handle: ?</li> <li>Devops candidate: Yes / no</li> <li>Intern vs Full-time candidate: ?</li> <li>Quick review if the person has the skills we are looking for</li> <li> <p>Assign to a shepherd</p> <ul> <li>If interns -&gt; Samarth/Sonaal</li> <li>If full-time candidate -&gt; shepherd</li> <li>If not clear ask GP</li> </ul> </li> <li> <p>The shepherd reviews the CV to get a sense if they are decent or not</p> </li> <li>It posts a quick summary of pros and cons</li> <li>We want to increase the quality of the collaborators, so if there is a red      flag we can decide to not on-board</li> <li>If you are uncertain, ask more people to take a look</li> <li> <p>The goal is to avoid on-boarding collaborators that will likely disappoint      us</p> </li> <li> <p>If the candidate is a no go, GP sends an email of rejection</p> </li> <li>The on-boarding shepherd is in charge of updating the Asana task with every    interesting event</li> <li> <p>We start the on-boarding process as per</p> </li> <li> <p>Update the      Contributor List</p> </li> <li>Copy the information from      Contributor Info      response sheet to the      Contributor List      as it is a master sheet for all the collaborators</li> <li>Ping GP on the Asana task for that collaborator for invitation to the repo</li> <li> <p>Add Contributor email as Commenter to the      KaizenFlow - Contributors gdrive</p> </li> <li> <p>When the collaborator is ready to be on-boarded, file an issue like    \"On-board \" <li>The content of the issue will be the following checklist (note that GitHub      needs full paths to point to the documentation from an issue)      ```<ul> <li>[ ] Acknowledge the pledge to put effort and time in the project here</li> <li>[ ] Fork, star, watch the KaizenFlow repo so that GitHub promotes our repo (we gotta spread the work)</li> <li>[ ] Read and execute set up development environment</li> <li>[ ] Read and start internalizing KaizenFlow Python coding style guide</li> <li>[ ] Read about your first code review</li> <li>[ ] Peruse the map of all the documentation</li> <li>[ ] Learn about our org process</li> <li>[ ] Get assigned a warm-up issue</li> <li>If you are graduating soon and you would like to get a full-time job in    one of the companies in the KaizenFlow ecosystem reach out to GP at    gp@kaizen-tech.io  ```</li> </ul> </li> <li> <p>Admins need to keep track on the progress being made by the collaborator.</p> </li> <li> <p>We score candidates every two weeks</p> </li> <li>Public gsheet</li> <li>Private gsheet</li>"},{"location":"onboarding/admin.onboarding_process.html#off-boarding-process","title":"Off-boarding process","text":"<p>A list of things to do to off-board an intern / collaborator</p> <ol> <li>[ ] Remove from GitHub</li> <li>[ ] Remove from contributors@</li> <li>[ ] Remove from the TG channel</li> <li>[ ] Remove Gdocs access:</li> <li>KaizenFlow</li> <li>Process</li> <li>Crypto-tech</li> </ol>"},{"location":"onboarding/all.communicate_in_telegram.how_to_guide.html","title":"Communicate In Telegram","text":""},{"location":"onboarding/all.communicate_in_telegram.how_to_guide.html#telegram","title":"Telegram","text":""},{"location":"onboarding/all.communicate_in_telegram.how_to_guide.html#general","title":"General","text":"<ul> <li>We use Telegram for</li> <li>Discussions that need<ul> <li>Tight interaction (like a debug session)</li> <li>Immediacy (e.g., \"are you ready for the sync up?\")</li> </ul> </li> <li>Github Actions notifications from Telegram bots<ul> <li>E.g., regressions fail in one of our repos</li> </ul> </li> </ul>"},{"location":"onboarding/all.communicate_in_telegram.how_to_guide.html#secret-vs-regular-chats","title":"Secret vs Regular chats","text":""},{"location":"onboarding/all.communicate_in_telegram.how_to_guide.html#secret","title":"Secret","text":"<ul> <li>We use secret chats for private one-on-one communication</li> <li>We prefer to send all the sensitive information using encrypted chats</li> <li>A Secret chat uses end-to-end encryption which means that even Telegram does     not have access to a conversion</li> <li>How to start secret chat - see   here</li> <li>Limitations:</li> <li>Telegram desktop version does not support secret chats so one uses secret     chats only using his/her phone</li> <li>You cannot add multiple people to a secret chat, only one-on-one     communication is allowed</li> <li>All secret chats in Telegram are device-specific and are not part of the     Telegram cloud. This means you can only access messages in a secret chat     from their device of origin.</li> </ul>"},{"location":"onboarding/all.communicate_in_telegram.how_to_guide.html#regular","title":"Regular","text":"<ul> <li>We use regular chats for</li> <li>General discussions within a team</li> <li>Group chats</li> <li>We do not share sensitive information via regular chats</li> </ul>"},{"location":"onboarding/all.communicate_in_telegram.how_to_guide.html#username","title":"Username","text":"<ul> <li>We ask everyone to set a username so that is easier to find a person</li> <li>See the instructions   here</li> </ul>"},{"location":"onboarding/all.communicate_in_telegram.how_to_guide.html#google-meet-room","title":"Google meet room","text":"<ul> <li>It is always nice to pin a google meeting room in a chat</li> <li>We usually use <code>-&gt;</code> as an invitation to join a google meet room pinned in a   chat</li> </ul>"},{"location":"onboarding/all.development_documents.reference.html","title":"Development Documents","text":""},{"location":"onboarding/all.development_documents.reference.html#on-boarding","title":"On-boarding","text":"<ul> <li>Signing up for KaizenFlow</li> </ul>"},{"location":"onboarding/all.development_documents.reference.html#how-to-start-developing","title":"How to start developing","text":"<p>This contains the absolute minimal amount of info to start developing</p> <ul> <li> <p>General Rules of Collaboration</p> </li> <li> <p>Quick start for developing</p> </li> <li> <p>How to organize your work</p> </li> <li> <p>First Review Process</p> </li> <li> <p>KaizenFlow Python coding style guide</p> </li> <li> <p>Jupyter notebook best practices</p> </li> <li> <p>Unit tests</p> </li> </ul>"},{"location":"onboarding/all.development_documents.reference.html#project-management","title":"Project management","text":"<ul> <li>Contributor Scoring</li> <li> <p>How we give feedback to contributors</p> </li> <li> <p>Code review</p> </li> <li> <p>PR review workflow from the code author and code reviewer points of view</p> </li> <li> <p>Development workflow</p> </li> <li> <p>Extended instructions on how to deploy your working environment and interact     with it</p> </li> <li> <p>GitHub/ZenHub workflows</p> </li> <li> <p>How to use GitHub and ZenHub, file a PR, review GitHub/ZenHub workflows</p> </li> <li> <p>Git workflow and best practices</p> </li> <li> <p><code>git</code> workflows, branch management, troubleshooting, common procedures</p> </li> <li> <p>Documentation about guidelines</p> </li> <li> <p>How to write guidelines, format documentation, convert Gdocs to markdown     files and vice versa</p> </li> <li> <p>Imports and packages</p> </li> <li> <p>How to import code, use packages, and resolve circular dependencies</p> </li> <li> <p>How to integrate repos</p> </li> <li>Detailed instruction on how to integrate repos</li> </ul>"},{"location":"onboarding/all.development_documents.reference.html#learn-how-to-become-efficient-at-developing","title":"Learn how to become efficient at developing","text":"<p>This contains a set of resources that over time will make 10x more productive</p> <ul> <li> <p>Docker</p> </li> <li> <p>Reading List</p> </li> <li> <p>Design Philosophy</p> </li> <li> <p>Codebase Clean-up</p> </li> <li> <p>Scrum Methodology</p> </li> </ul>"},{"location":"onboarding/all.development_documents.reference.html#in-depth-docs","title":"In-depth docs","text":"<ul> <li> <p>Code organization</p> </li> <li> <p>Buildmeister process</p> </li> <li> <p>Profiling</p> </li> <li> <p>Visual Studio Code</p> </li> <li> <p>Telegram</p> </li> <li> <p>Gsheet into pandas</p> </li> <li> <p>Email</p> </li> <li> <p>Glossary</p> </li> <li> <p>Type Hints</p> </li> <li> <p>PyCharm</p> </li> </ul>"},{"location":"onboarding/all.development_documents.reference.html#defi","title":"DeFi","text":"<ul> <li> <p>DeFi README</p> </li> <li> <p>From 0 to Web3</p> </li> </ul>"},{"location":"onboarding/all.onboarding_checklist.html","title":"Onboarding Checklist","text":""},{"location":"onboarding/all.onboarding_checklist.html#onboarding-process-for-a-new-team-member","title":"Onboarding process for a new team member","text":""},{"location":"onboarding/all.onboarding_checklist.html#meta","title":"Meta","text":""},{"location":"onboarding/all.onboarding_checklist.html#make-on-boarding-automatic","title":"Make on-boarding automatic","text":"<ul> <li>We want to make the onboarding process as automatic as possible</li> </ul>"},{"location":"onboarding/all.onboarding_checklist.html#be-patient","title":"Be patient","text":"<ul> <li>Let's use all the communication tools we have (screen sharing, Google Meet,   phone, Telegram, email) to keep the process smooth and personal</li> <li>There are many (hidden) dependencies in the process, so something will   inevitably go wrong</li> <li>Let's all be patient with each other</li> </ul>"},{"location":"onboarding/all.onboarding_checklist.html#ask-for-confirmation","title":"Ask for confirmation","text":"<ul> <li>Use checklists any time to make sure things get done</li> <li>Ask for confirmation of all the actions, e.g.,</li> <li>\"Does this and that work?\"</li> <li>\"Did you receive the email?\"</li> <li>\"Can you log in?\"</li> <li>Make the new team member follow the instructions so that they can get familiar   with the systems</li> </ul>"},{"location":"onboarding/all.onboarding_checklist.html#make-on-boarding-similar-to-our-work-routine","title":"Make on-boarding similar to our work routine","text":"<ul> <li>Provide tools for interacting with the team</li> <li> <p>During the process the new teammate will ask questions and he / she should     use the appropriate tools for each type of communication</p> <ul> <li>Telegram: only for interactive communications</li> <li>GitHub issues: for coding related things</li> <li>Asana tasks: for business/high-level tasks</li> <li>Email: everything else</li> </ul> </li> <li> <p>We want to nudge a new hire to operate through the tools we use during our   daily work</p> </li> <li>That is why we asking him to create a first issue about his / her experience     with the process</li> <li>He / she should put his / her notes in the GH issue while proceeding during     the onboarding process</li> </ul>"},{"location":"onboarding/all.onboarding_checklist.html#improve-on-boarding-process","title":"Improve on-boarding process","text":"<ul> <li>Review, improve, clarify process</li> <li>We want to point the new team member to a document rather than explaining     over and over the same things</li> <li>Always keep this document updated, ask candidates to follow it carefully,     and update it with a PR</li> <li>We want to use each new team-member experience to improve our workflow</li> <li> <p>We want new team-members to improve the existing documentation using our     standard GitHub process</p> </li> <li> <p>Have the new team member:</p> </li> <li>Open the first GitHub bug on the first day</li> <li>Open the first PR by the second day</li> </ul>"},{"location":"onboarding/all.onboarding_checklist.html#people-involved-in-the-on-boarding","title":"People involved in the on-boarding","text":"<ul> <li>Each task is performed by one of the persons involved in the on-boarding</li> <li>Team leader (e.g., GP, Paul, Grisha, Samarth)</li> <li>Team member (i.e., the person on-boarded)</li> <li>IT (e.g., Shayan)</li> <li>HR (e.g., Rose)</li> </ul>"},{"location":"onboarding/all.onboarding_checklist.html#before-the-start-date","title":"Before the start date","text":"<ul> <li> <p>[ ] Team leader: talk to the teams about the new team member joining</p> </li> <li> <p>[ ] Team leader: Send some information to the new team member as a preview</p> </li> <li> <p>E.g., some PDFs from amp/documentation/general</p> </li> <li> <p>[ ] Team leader: establish contact by Telegram or email with the new hire       with a few words about the next steps</p> </li> <li> <p>[ ] Team leader: do a proper intro with an email to the team:</p> </li> <li>\"Hi team, please join me in welcoming XYZ to the team\"</li> <li>Whom is going to report to</li> <li>What is going to work on initially</li> <li>LinkedIn link</li> </ul>"},{"location":"onboarding/all.onboarding_checklist.html#the-first-day","title":"The first day!","text":""},{"location":"onboarding/all.onboarding_checklist.html#team-member-info","title":"Team member info","text":"<ul> <li>[ ] Team member: send needed information to your team leader</li> <li>Full name:</li> <li>Aka:</li> <li>Personal email:</li> <li>Github user:</li> <li>Telegram handle:</li> <li>Laptop OS: Windows, Linux, or Mac</li> <li>Physical location and timezone</li> <li> <p>User's SSH public key</p> </li> <li> <p>[ ] Team leader: update the       Team member info gdoc</p> </li> </ul>"},{"location":"onboarding/all.onboarding_checklist.html#nda","title":"NDA","text":"<ul> <li> <p>[ ] HR: send the team member an NDA to sign       NDAs Development Team</p> </li> <li> <p>[ ] Team member: send back signed copy of the NDA</p> </li> <li>[ ] HR: store in       signed directory</li> </ul>"},{"location":"onboarding/all.onboarding_checklist.html#hubstaff","title":"Hubstaff","text":"<ul> <li>[ ] HR: Update Hubstaff</li> <li>Add user here</li> <li>We respect our own privacy by not using any of the screenshot or typing     things BS</li> <li> <p>We use it only to track the time automatically and as HR</p> </li> <li> <p>[ ] Team member:</p> </li> <li>Confirm access to Hubstaff</li> <li>Read     Tools - Hubstaff</li> </ul>"},{"location":"onboarding/all.onboarding_checklist.html#it-setup","title":"IT setup","text":"<ul> <li>[ ] Team leader: File an issue with this checklist</li> <li>The title is \"Onboarding {{Name}}\"</li> <li> <p>Copy paste the following checklist</p> </li> <li> <p>[ ] IT: create account info bundle following       New team member addition - admin side tutorial</p> </li> <li> <p>TODO(Shayan): convert this into Markdown</p> </li> <li> <p>[ ] IT: send the following information to team member</p> </li> <li>Linux user: name + first letter of last name</li> <li> <p>Send the account info bundle</p> </li> <li> <p>[ ] IT: create company email <code>@kaizen-tech.io</code></p> </li> <li> <p>Exchange username and password</p> </li> <li> <p>[ ] IT: Add team member to mailing lists</p> </li> <li> <p><code>all@kaizen-tech.io</code> mailing lists</p> <ul> <li>https://groups.google.com/my-groups</li> <li>https://admin.google.com/ac/groups/01qoc8b13iwic0z/info</li> </ul> </li> <li> <p>[ ] Team member: Confirm receipt of emails to <code>@all</code></p> </li> <li> <p>[ ] Team member: Confirm weekly all-hands meeting (usually on Monday       morning)</p> </li> <li>This happens automatically through <code>all@kaizen-tech.io</code></li> <li> <p>https://calendar.google.com/calendar/u/0/r</p> </li> <li> <p>[ ] Team member: Confirm access to the Google Drive documentation</p> </li> <li>This should be automatically granted as being in <code>all@kaizen-tech.io</code></li> <li>Crypto-tech</li> <li> <p>Process</p> </li> <li> <p>[ ] Team member: Join relevant Telegram groups by the links below</p> </li> <li>KT - All</li> <li>KT - Random happy channel</li> <li>KT - Build notifications</li> <li> <p>Channel specifics</p> </li> <li> <p>[ ] Team member: set up laptop to connect to the server following:</p> </li> <li> <p>VPN and dev server access setup</p> </li> <li> <p>[ ] Team member: configure your server environment following:</p> </li> <li> <p>Development - Set-up</p> </li> <li> <p>[ ] Team member: We have implemented a self-registration process for our       Airflow on Kubernetes deployment. Please register and create your personal       accounts as we will no longer be using the shared Airflow Admin user. Make       sure to use the same username that you have on the dev servers.</p> </li> <li>Airflow - Registration</li> <li> <p>TODO(Shayan): Update this</p> </li> <li> <p>[ ] IT: Add team member to all GitHub repos:</p> </li> <li>[ ] Kaizen-ai</li> <li>[ ] dev_tools</li> <li>[ ] cmamp</li> <li>(On per-need basis) [ ]     orange</li> <li> <p>(On per-need basis) [ ]     UMD_data605</p> </li> <li> <p>[ ] Team member: Confirm access to GitHub repos</p> </li> <li> <p>[ ] IT: Add team member to ZenHub</p> </li> <li>After GH access is confirmed</li> <li> <p>Invite</p> </li> <li> <p>[ ] Team member: Confirm access to ZenHub       here</p> </li> <li> <p>[ ] IT: Server set-up</p> </li> <li> <p>We use the personal laptop as a thin client only to connect to the servers,     where the work is typically carried out. We don't check out code on our     laptop</p> </li> <li> <p>[ ] IT: Create the Linux user on all the servers</p> </li> <li> <p>Create a keypair and share it with the new team member</p> </li> <li> <p>[ ] Team member: Check whether he / she can log in on one of the servers</p> </li> <li> <p>TODO(Shayan): How?</p> </li> <li> <p>[ ] IT: Create AWS UI credentials</p> </li> <li> <p>Only if necessary, for developer just starting, this is usually not needed     (CK login)</p> </li> <li> <p>[ ] Team member: check access to AWS on the server</p> </li> <li><code>&gt; aws s3 --profile ck ls s3://cryptokaizen-unit-test/</code></li> <li> <p><code>&gt; aws s3 --profile ck ls s3://cryptokaizen-data/</code></p> </li> <li> <p>[ ] Team member: Customize Gmail, Telegram, and GitHub accounts</p> </li> <li>Please use the same photo in all accounts with a recognizable Gravatar</li> <li>We use the picture to select Issues and conversations</li> <li> <p>Please use a picture where you look somehow professional (e.g., not a     picture of you on the beach taking Tequila shots): the best picture is just     one of your face, so we can virtually get to know each other</p> </li> <li> <p>[ ] Team member: Make sure you have access to the       vacation/OOTO calendar.</p> </li> <li> <p>The link should be accessible and you should see the calendar in the list of     calendars at calendar.google.com (when accessing via your corporate email)</p> </li> <li> <p>[ ] Team member: Add your usual working hours by going to       calendar.google.com (using your corporate email), heading to the settings       section by clicking the gear icon on top right</p> </li> <li> <p></p> </li> <li> <p>[ ] Team member: Confirm you can access the anonymous form to ask anything       https://forms.gle/KMQgobqbyxhoTR9n6</p> </li> <li> <p>[ ] Team member: File first Issue on GitHub</p> </li> <li>It should be called \"Document review while onboarding $TEAM_MEMBER\"</li> <li>Track what is not clear in the onboarding process / documentation and what     should / could be improved</li> </ul>"},{"location":"onboarding/all.onboarding_checklist.html#quick-checklists-to-verify-that-everything-works","title":"Quick checklists to verify that everything works","text":"<ul> <li>Team member</li> <li>[ ] VPN to dev server</li> <li>[ ] ssh into the dev server</li> <li>[ ] Check access to AWS on the server (refer to instructions above)</li> <li>[ ] Clone the code from Git</li> <li>[ ] Connect to server with VisualStudio Code or PyCharm<ul> <li>There is an extension for VSCode, which allows to develop remotely   Remote - SSH</li> <li>Follow this instruction on how to set it up in your   Visual Studio Code</li> </ul> </li> <li>[ ] Run the unit tests and make sure they all pass</li> <li>[ ] Run a docker container     <code>&gt; i docker_bash</code></li> <li>[ ] Run a jupyter notebook<ul> <li>Follow this   instruction   on how to access the Jupyter server running on the remote server through   your local machine ``` <p>i docker_jupyter ```</p> </li> </ul> </li> </ul>"},{"location":"onboarding/all.onboarding_checklist.html#the-second-day","title":"The second day","text":"<ul> <li>[ ] Team member: carefully study all the documents in:       docs/onboarding</li> <li>Read it carefully one by one</li> <li>Ask questions</li> <li>Memorize / internalize all the information</li> <li>Take notes</li> <li>Mark the reading as done</li> <li> <p>Use your first Issue to document all the improvements to the documentation</p> </li> <li> <p>[ ] Team member: Do PRs following the process to improve documentation</p> </li> <li> <p>Do not be shy</p> </li> <li> <p>[ ] Team member: exercise all the important parts of the systems</p> </li> <li>[ ] Create a GitHub issue</li> <li>[ ] Get familiar with         ZenHub doc</li> <li>Check out the code on server</li> <li>Run all regressions on server</li> <li>Create a branch</li> <li>Run the linter.py on server</li> <li>Do a PR</li> </ul>"},{"location":"onboarding/all.organize_email.how_to_guide.html","title":"Organize Email","text":""},{"location":"onboarding/all.organize_email.how_to_guide.html#email","title":"Email","text":""},{"location":"onboarding/all.organize_email.how_to_guide.html#mailing-lists","title":"Mailing lists","text":"<ul> <li><code>@all</code> is the mailing list with everybody at the company</li> <li><code>@contributors</code> is the mailing list with every open-source contributor</li> </ul>"},{"location":"onboarding/all.organize_email.how_to_guide.html#organizing-email-flow","title":"Organizing email flow","text":"<ul> <li>We receive tons of emails, and the inflow is going to keep increasing</li> <li>At a large company you can get 10k emails per day (no kidding)</li> <li>The goal is to read all the emails and always be on top of it</li> <li>How can one do that?</li> <li>As usual the answer is get organized</li> <li>Filter emails in folders</li> <li>Separate emails in folders based on the action that they require (e.g.,     ignore, just read and be aware of it, read and respond)</li> <li>Read email and decide what to do about each of it:<ul> <li>No reply needed</li> <li>Reply right away</li> <li>Follow up later (e.g., to read, reply, think about it)</li> </ul> </li> <li>Use flags to distinguish what needs to be followed up later or if you are     waiting for a response</li> <li>A possible organization in folders is:</li> <li>GitHub<ul> <li>Commits in all the repos (be aware of it)</li> <li>Issue updates (read and respond)</li> <li>PRs</li> <li>Commits directly to <code>master</code> (read and review)</li> <li>Commits into <code>documents</code> dir of <code>master</code> (read and review)</li> <li>Emails generated by my GH activities (ignore and mark as read)</li> </ul> </li> <li>Asana<ul> <li>Daily activities assigned report</li> <li>New comment/activity</li> <li>New task assignment</li> </ul> </li> </ul>"},{"location":"onboarding/all.organize_email.how_to_guide.html#anatomy-of-email-messages-from-infra","title":"Anatomy of email messages from infra","text":"<ul> <li>The goal is to classify emails so that we can filter email effectively</li> </ul>"},{"location":"onboarding/all.organize_email.how_to_guide.html#filtering-emails-with-gmail","title":"Filtering emails with Gmail","text":"<ul> <li>Personally (GP) I prefer an email client (Mozilla Thunderbird and more   recently Apple Mail) rather than using Gmail web interface</li> <li>People are able to use it</li> <li>Personally I prefer to use filters on the Gmail (server) side</li> <li>Pros<ul> <li>I don't get emails on my cell phone continuously</li> <li>The emails are organized as they arrive</li> <li>Folders are on the server side, so my client can simply sync</li> </ul> </li> <li>Cons<ul> <li>The Gmail interface for filtering emails is horrible</li> </ul> </li> <li>The web interface is   https://mail.google.com/mail/u/0/#settings/filters</li> <li>Note that Gmail distinguish different email accounts using different indices,   e.g.,   https://mail.google.com/mail/u//#inbox"},{"location":"onboarding/all.organize_email.how_to_guide.html#notifications-from-github","title":"Notifications from GitHub","text":"<ul> <li>https://help.github.com/en/categories/receiving-notifications-about-activity-on-github</li> </ul>"},{"location":"onboarding/all.organize_email.how_to_guide.html#github-pull-requests","title":"GitHub pull requests","text":"<ul> <li> <p>These emails look like:</p> <pre><code>Samarth KaPatel &lt;notifications@github.com&gt;\nto cryptokaizen/orange, Subscribed\n\ncryptokaizen/cmamp#4765\n- Refactoring amp_path on orange\n- PR in cmamp - cryptokaizen/cmamp#4788\n________________________________________________________________\n\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/cryptokaizen/orange/pull/411\n\nCommit Summary\n- B2b4940 orange fix\n- 850b2b2 amp\n\nFile Changes (2 files)\n- M amp (2)\n- M dataflow_orange/system/Cx/test/test_master_pnl_real_time_observer_notebook.py (5)\n\nPatch Links:\n- Https://github.com/cryptokaizen/orange/pull/411.patch\n- Https://github.com/cryptokaizen/orange/pull/411.diff\n</code></pre> </li> <li> <p>These emails have the words: \"You can view, comment on, or merge this pull   request online at:\" in the body of the email</p> </li> </ul>"},{"location":"onboarding/all.organize_email.how_to_guide.html#github-issue-activity","title":"GitHub issue activity","text":"<ul> <li>These emails look like:<pre><code>Samarth KaPatel &lt;notifications@github.com&gt;\nto Review, kaizen-ai/kaizenflow\n\n@samarth9008 requested your review on: #436 Update Signing_up_for_Kaizenflow.md.\n__\nReply to this email directly, view it on GitHub, or unsubscribe.\nYou are receiving this because your review was requested.\n</code></pre> </li> </ul> <p>or:</p> <pre><code>    Grigorii Pomazkin &lt;notifications@github.com&gt;\n    to cryptokaizen/cmamp, Mention\n\n    @PomazkinG commented on this pull request.\n    ________________________________________________________________________\n\n    In helpers/lib_tasks_pytest.py:\n\n    &gt; @@ -671,17 +671,25 @@ def run_coverage_report(  # type: ignore\n         :param aws_profile: the AWS profile to use for publishing HTML report\n         \"\"\"\n         # TODO(Grisha): allow user to specify which tests to run.\n         ...\n    obsolete, resolving\n    __\n    Reply to this email directly, view it on GitHub, or unsubscribe.\n    You are receiving this because you were mentioned.\n</code></pre> <ul> <li>These emails can be recognized by the fact that have the words \"You are   receiving this because\" in the email body</li> </ul>"},{"location":"onboarding/all.organize_email.how_to_guide.html#commits","title":"Commits","text":"<ul> <li> <p>These emails look like:</p> <pre><code>Grigorii Pomazkin &lt;notifications@github.com&gt;\nto cryptokaizen/cmamp, Push\n\n@PomazkinG pushed 1 commit.\n- 65496bb Merge branch 'master' into CmTask4707_fix_coverage_test\n__\nView it on GitHub or unsubscribe.\nYou are receiving this because you are subscribed to this thread.\n</code></pre> </li> <li> <p>These emails can be recognized by the fact that have the words \"pushed commit\"   in the email body</p> </li> </ul>"},{"location":"onboarding/all.organize_email.how_to_guide.html#gdocs","title":"Gdocs","text":"<ul> <li>These emails have <code>comments-noreply@docs.google.com</code> or (Google Docs) in the   \"subject\" field</li> </ul>"},{"location":"onboarding/all.organize_email.how_to_guide.html#todo-emails","title":"TODO emails","text":"<ul> <li>These emails have TODO in the subject</li> </ul>"},{"location":"onboarding/all.organize_email.how_to_guide.html#asana","title":"Asana","text":"<ul> <li>These emails are received from <code>no-reply@asana.com</code> address</li> <li>These emails may contain:</li> <li>List of tasks assigned to you for today</li> <li>New activities in the tasks assigned to you</li> <li>New task assignment</li> </ul>"},{"location":"onboarding/all.receive_crypto_payment.how_to_guide.html","title":"Receive Crypto Payment","text":""},{"location":"onboarding/all.receive_crypto_payment.how_to_guide.html#choose-a-crypto-wallet-type","title":"Choose a crypto wallet type","text":"<ul> <li>Https://academy.binance.com/en/articles/crypto-wallet-types-explained</li> <li>Https://www.blockchain-council.org/blockchain/types-of-crypto-wallets-explained/</li> </ul>"},{"location":"onboarding/all.receive_crypto_payment.how_to_guide.html#public-private-keys","title":"Public / private keys","text":"<p>See the explanation here</p> <p>TLDR:</p> <ul> <li>Send a public key to receive a transaction, see it as a wallet address</li> <li>Private key is needed to prove ownership or spend the funds associated with   your public address, see it as a password</li> </ul>"},{"location":"onboarding/all.receive_crypto_payment.how_to_guide.html#what-do-people-in-the-team-use","title":"What do people in the team use","text":""},{"location":"onboarding/all.receive_crypto_payment.how_to_guide.html#beginner","title":"Beginner","text":"<p>Mobile / desktop hot wallet Exodus</p> <p>Pros:</p> <ul> <li>Easy to set up</li> <li>Has a mobile app which is easy to use</li> <li>Built-in exchange (e.g., allows to exchange BTC to ETH inside the app)</li> <li>Allows to sell crypto and receive fiat money to a bank account via the app,   see   here</li> <li>Customer support</li> </ul> <p>Cons:</p> <ul> <li>A swap may include a   spread</li> <li>As any hot wallet is vulnerable to hackers attacks</li> <li>It is not recommended to store significant amount of money</li> </ul>"},{"location":"onboarding/all.receive_crypto_payment.how_to_guide.html#advanced","title":"Advanced","text":"<p>If you are experienced and / or own a lot of crypto you might want to have a cold wallet on your laptop, holding the bulk of your bitcoin and transfer among different website on-line wallets.</p> <p>https://bitcoin.org/en/choose-your-wallet</p>"},{"location":"onboarding/all.receive_crypto_payment.how_to_guide.html#choose-a-crypto-currency-to-receive-money-in","title":"Choose a crypto currency to receive money in","text":"<ul> <li>One can either receive a payment in BTC or in one of the Stablecoins (USDT is   a preference)</li> <li>People in the team prefer to use Stablecoins since Stablecoins pursue price   stability</li> </ul>"},{"location":"onboarding/all.receive_crypto_payment.how_to_guide.html#send-a-crypto-wallet-address","title":"Send a crypto wallet address","text":"<p>Send your crypto wallet address to GP together with an invoice via email. E.g.,</p> <pre><code>Hi GP,\n\nPlease find my invoice for the period [a, b] attached. I would like to receive the money in USDT ERC20.\n\nMy crypto wallet address is: 0x5ce3d650703f745B9C0cf20E322204b00bF59205\n</code></pre>"},{"location":"onboarding/all.receive_crypto_payment.how_to_guide.html#do-a-test-transfer","title":"Do a test transfer","text":"<p>For the first time GP sends a test transfer (e.g., 100$) just to confirm that a wallet address provided works.</p> <p>After a wallet address is \"verified\" send the full amount (exluding the test transfer amount).</p>"},{"location":"onboarding/all.receive_crypto_payment.how_to_guide.html#cash-out-crypto","title":"Cash out crypto","text":""},{"location":"onboarding/all.receive_crypto_payment.how_to_guide.html#bank-account","title":"Bank account","text":"<p>There are some 3rd party services that buy crypto money from you and send fiat money to your bank account</p> <p>E.g.,</p> <ul> <li>Built-in Exodus crypto withdrawal service</li> <li>Coinbase cash out serivce</li> <li>Binance P2P</li> </ul>"},{"location":"onboarding/all.receive_crypto_payment.how_to_guide.html#cash","title":"Cash","text":"<p>There are some crypto ATMs in the USA, see here</p>"},{"location":"onboarding/all.track_time_with_hubstaff.how_to_guide.html","title":"Track Time With Hubstaff","text":""},{"location":"onboarding/all.track_time_with_hubstaff.how_to_guide.html#hubstaff","title":"Hubstaff","text":""},{"location":"onboarding/all.track_time_with_hubstaff.how_to_guide.html#general","title":"General","text":"<ul> <li>Hubstaff is a tool for remote working that automates:</li> <li>Time tracking</li> <li>Invoice creation</li> <li>Payment</li> <li>The goal is to replace our   hour log spreadsheet   with Hubstaff and get paid automatically for the worked hours.</li> </ul>"},{"location":"onboarding/all.track_time_with_hubstaff.how_to_guide.html#time-tracking","title":"Time Tracking","text":""},{"location":"onboarding/all.track_time_with_hubstaff.how_to_guide.html#privacy","title":"Privacy","text":"<ul> <li>We have decided to turn off the feature of taking screenshots of the laptop as   proof-as-work, URL, and app tracking.</li> <li></li> <li>Although this seems to be standard practice for remote teams (e.g., it's   default for UpWork), we don't believe in surveilling people, rather we prefer   to assume that people do the right thing (until proven wrong, of course).</li> <li></li> <li></li> <li>We left enabled \"the keyboard and mouse activity\" since this is used to   automatically track the worked time. The tool does not store what you type,   only if you type as a proxy for \"working\".</li> <li>     We let people decide if they want to monitor the usage of programs or not.     Some people like it (I do since it gives an insight on which program I spend     my time)</li> <li></li> <li>You can change your mind over time.</li> </ul>"},{"location":"onboarding/all.track_time_with_hubstaff.how_to_guide.html#tracking-time-automatically","title":"Tracking time automatically","text":"<ul> <li>You can use the desktop App</li> <li>I prefer this since it's completely automated</li> <li>For Linux (Ubuntu) follow this     guide</li> <li>Both options update the time information to Hubstaff automatically.</li> <li></li> <li></li> <li>See   https://hubstaff.com/how-tracking-works</li> </ul>"},{"location":"onboarding/all.track_time_with_hubstaff.how_to_guide.html#tracking-time-manually","title":"Tracking time manually","text":"<ul> <li>You can track time using the   Google Chrome extension:</li> <li>Chrome Time Tracking - Hubstaff Time Tracker</li> <li>Start the timer when you start the work</li> <li>Stop the timer when the work is paused</li> </ul>"},{"location":"onboarding/all.track_time_with_hubstaff.how_to_guide.html#overriding-time-tracking","title":"Overriding time tracking","text":"<ul> <li>In case you forgot to turn on the timer or you are tracking time manually, go   to</li> <li>     and fill the timesheet for the past day</li> <li></li> <li>See the   official guide   for a lot of details</li> </ul>"},{"location":"onboarding/kaizenflow.hiring_meister.how_to_guide.html","title":"General","text":"<ul> <li>The HiringMeister is responsible for testing prospective candidates for   full-time and part-time positions within our organization</li> <li>To see who is the HiringMeister now refer to     Rotation Meisters</li> <li>The HiringMeister:</li> <li>Ensures candidate evaluation through PR and pointing them to documentation</li> <li>Maintains hiring standards</li> <li>Fosters skill assessment through task assignments</li> <li>Continuously improves the recruitment process</li> </ul>"},{"location":"onboarding/kaizenflow.hiring_meister.how_to_guide.html#on-boarding-issue","title":"On-boarding issue","text":"<ul> <li>As the invitation to the repo are accepted by the selected candidates, create   an <code>On-boarding</code> GitHub issue for each candidate</li> <li>The name of the issue must be <code>On-board &lt;Candidate Name&gt;</code> and assignee is the   GitHub username of the candidate</li> <li>The contents of the issue are</li> </ul> <p>```verbatim</p> <p>Please follow this checklist. Mark each item as done once completed.</p> <p>Post any errors you face in this issue.   - [ ] Acknowledge the pledge to put time in the project here   - [ ] Read How to organize your work   - [ ] Read Quick start for developing   - [ ] Make sure the Docker dev container works   - [ ] Make sure the unit tests run successfully   - [ ] Read KaizenFlow Python coding style guide   - [ ] Fork, star, watch the KaizenFlow repo so that GitHub promotes our repo (we gotta work the system)   - [ ] Learn about the Morning Email   - [ ] How to do a review   - [ ] If you are graduating soon and you would like to get a full-time job in one of the companies in the KaizenFlow ecosystem reach out to GP at gp@kaizen-tech.io   - [ ] Get assigned a warm-up issue   ```</p> <ul> <li>A reference issue is   On-boarding</li> <li>Regularly check the updates made by the candidate and help resolving any   errors faced by them</li> </ul>"},{"location":"onboarding/kaizenflow.hiring_meister.how_to_guide.html#create-and-assign-warm-up-issue","title":"Create and Assign warm-up issue","text":""},{"location":"onboarding/kaizenflow.hiring_meister.how_to_guide.html#warm-up-tasks","title":"Warm-up tasks","text":"<ul> <li>Collaborate with the team to identify potential warm-up tasks for candidates   upon completion of their on-boarding process</li> <li>The goal of a warm-up issue is for someone to write a bit of code and show   they can follow the process, the goal is not to check if they can solve a   complex coding problem</li> <li>It should take 1-2 days to get it done</li> <li>This helps us understand if they can</li> <li>Follow the process (or at least show that they read it and somehow     internalized it)</li> <li>Solve a trivial some problems</li> <li>Write Python code</li> <li>Interact on GitHub</li> <li>Interact with the team</li> <li>Ensure that these warm-up tasks are straightforward to integrate, immediately   beneficial, unrelated to new features, and do not rely on the <code>ck</code>   infrastructure</li> <li>As candidates complete their on-boarding checklist, promptly assign them   warm-up tasks from the predetermined list</li> <li>Write specs in a manner that is easily understandable by candidates, address   any queries they may have regarding the task, and regularly follow up for   updates</li> <li>If a candidate shows lack of progress on their assigned warm-up task, ping   them twice for updates. If no progress is made, reassign the task to a more   promising candidate</li> <li>Upon submission of a pull request by the candidate for the task, review it to   ensure adherence to our processes. Provide constructive feedback on areas for   improvement and ascertain if the task's objectives have been fully met</li> <li>Before merging the PR on <code>kaizenflow</code>, create a similar PR on <code>cmamp</code> and merge   both of them together</li> <li>Assign more task to the candidate if required to make a final decision</li> </ul>"},{"location":"onboarding/kaizenflow.hiring_meister.how_to_guide.html#score-candidates","title":"Score candidates","text":"<ul> <li>Score the candidates every two weeks and notify all the team members for   scoring</li> <li>Scoring criteria and template are defined in details in   this doc</li> <li>Not all the criteria are used for scoring the new candidates</li> <li>E.g.     Scoring sheet</li> <li>The scoring should be done by all of the members of the hiring team</li> <li>The final score of the candidate includes the average score of all the team   members</li> <li>The final scored are delivered to the candidates every two weeks</li> <li>The candidate with low score should be dropped</li> </ul>"},{"location":"onboarding/kaizenflow.hiring_meister.how_to_guide.html#suggestions","title":"Suggestions","text":"<ul> <li>In the first couple of weeks we should try to ingrain the following flow into a   new team member\u2019s mind</li> <li>Instead of spending hours coding on their own, apply the following steps:</li> <li>Identify a problem and describe it in the issue</li> <li>Design solution or seek guidance from a mentor</li> <li>Let mentor approve/comment and reach consensus on the solution</li> <li>Write code</li> <li>Stick to smaller PRs</li> <li>It's very important to push frequently and ask for feedback early to avoid     large refactoring</li> </ul>"},{"location":"onboarding/kaizenflow.set_up_development_environment.how_to_guide.html","title":"Set up KaizenFlow development environment","text":""},{"location":"onboarding/kaizenflow.set_up_development_environment.how_to_guide.html#introduction","title":"Introduction","text":"<p>This document outlines the development set up to be followed by KaizenFlow contributors. By documenting the set up, we aim to streamline the information flow and make the contribution process seamless by creating a collaborative and efficient coding environment for all contributors.</p> <p>Happy coding!</p>"},{"location":"onboarding/kaizenflow.set_up_development_environment.how_to_guide.html#technologies-used","title":"Technologies used","text":"<ul> <li>UMD DATA605 Big Data Systems   contains   lectures and   tutorials   about most of the technologies we use in KaizenFlow, e.g., Dask, Docker, Docker   Compose, Git, github, Jupyter, MongoDB, Pandas, Postgres, Apache Spark</li> <li>You can go through the lectures and tutorials on a per-need basis, depending   on what it's useful for you to develop</li> <li>As an additional resource to become proficient in using Linux and shell, you   can refer to   The Missing Semester of Your CS Education</li> </ul>"},{"location":"onboarding/kaizenflow.set_up_development_environment.how_to_guide.html#clone-the-code","title":"Clone the code","text":"<ul> <li> <p>To clone the repo, use the cloning command described in the GitHub official   documentation</p> </li> <li> <p>Example of cloning command:</p> </li> </ul> <p>```bash</p> <p>git clone git@github.com:kaizen-ai/kaizenflow.git ~/src/kaizenflow1   ```   - The previous command might not work sometimes and an alternative command     using HTTP instead of SSH</p> <p>```bash</p> <p>git clone https://github.com/kaizen-ai/kaizenflow.git ~/src/kaizenflow1   ```</p> <ul> <li>All the source code should go under <code>~/src</code> (e.g., <code>/Users/&lt;YOUR_USER&gt;/src</code> on   a Mac)</li> <li>The path to the local repo folder should look like this   <code>~/src/{REPO_NAME}{IDX}</code> where</li> <li><code>REPO_NAME</code> is a name of the repository</li> <li>IDX is an integer</li> </ul>"},{"location":"onboarding/kaizenflow.set_up_development_environment.how_to_guide.html#building-the-thin-environment","title":"Building the thin environment","text":"<ul> <li> <p>Create the \"thin environment\" which contains the minimum set of dependencies   needed for running the KaizenFlow Dev Docker container</p> </li> <li> <p>Build the thin environment; this is done once per client</p> </li> </ul> <p>```bash</p> <p>cd $GIT_ROOT dev_scripts/client_setup/build.sh 2&gt;&amp;1 | tee tmp.build.log   ```</p> <ul> <li> <p>Activate the thin environment; make sure it is always activated   ```</p> <p>source dev_scripts/setenv_amp.sh   ```</p> </li> <li> <p>If you see output like below, your environment is successfully built!   <code>...   alias sp='echo '\\''source ~/.profile'\\''; source ~/.profile'   alias vi='/usr/bin/vi'   alias vim='/usr/bin/vi'   alias vimdiff='/usr/bin/vi -d'   alias vip='vim -c \"source ~/.vimrc_priv\"'   alias w='which'   ==&gt; SUCCESS &lt;==</code></p> </li> <li>If you encounter any issues, please post them by creating a new issue on   GitHub and assign it to the <code>gsaggese</code></li> <li>You should report as much information as possible: what was the command,     what is your platform, output of the command</li> </ul>"},{"location":"onboarding/kaizenflow.set_up_development_environment.how_to_guide.html#install-and-test-docker","title":"Install and test Docker","text":""},{"location":"onboarding/kaizenflow.set_up_development_environment.how_to_guide.html#supported-os","title":"Supported OS","text":"<ul> <li>KaizenFlow supports Mac (both x86 and Apple Silicon) and Linux Ubuntu</li> <li>We do not support Windows and WSL: we have tried several times to port the   tool chain to it, but there are always subtle incompatible behaviors that   drive everyone crazy</li> <li>If you are using Windows, we suggest to use dual boot with Linux or use a     virtual machine with Linux</li> <li>Install VMWare software</li> <li>Reference video for installing     ubuntu     on VMWare software</li> <li>Make sure you set up your git and github</li> <li>Install     docker     on your Ubuntu VM</li> </ul>"},{"location":"onboarding/kaizenflow.set_up_development_environment.how_to_guide.html#install-docker","title":"Install Docker","text":"<ul> <li> <p>Get familiar with Docker if you are not, e.g.,   https://docs.docker.com/get-started/overview/</p> </li> <li> <p>We work in a Docker container that has all the required dependencies installed</p> </li> <li> <p>You can use PyCharm / VS code on your laptop to edit code, but you want to     run code inside the dev container since this makes sure everyone is running     with the same system, and it makes it easy to share code and reproduce     problems</p> </li> <li> <p>Install Docker Desktop on your PC</p> </li> <li> <p>Links:</p> <ul> <li>Mac</li> <li>Linux</li> <li>Windows</li> </ul> </li> <li> <p>Follow https://docs.docker.com/engine/install/</p> </li> <li> <p>For Mac you can also install <code>docker-cli</code> without the GUI using</p> </li> </ul> <p>```bash</p> <p>brew install docker brew link docker brew install colima   ```</p> <ul> <li>After installing make sure Docker works on your laptop (of course the version   will be newer)</li> </ul> <p>```bash</p> <p>docker version   Client:    Cloud integration: v1.0.24    Version:           20.10.17    API version:       1.41    Go version:        go1.17.11    Git commit:        100c701    Built:             Mon Jun  6 23:04:45 2022    OS/Arch:           darwin/amd64    Context:           default    Experimental:      true</p> <p>Server: Docker Desktop 4.10.1 (82475)    Engine:     Version:          20.10.17     API version:      1.41 (minimum version 1.12)     Go version:       go1.17.11     Git commit:       a89b842     Built:            Mon Jun  6 23:01:23 2022     OS/Arch:          linux/amd64     Experimental:     false    containerd:     Version:          1.6.6     GitCommit:        10c12954828e7c7c9b6e0ea9b0c02b01407d3ae1    runc:     Version:          1.1.2     GitCommit:        v1.1.2-0-ga916309    docker-init:     Version:          0.19.0     GitCommit:        de40ad0   ```</p>"},{"location":"onboarding/kaizenflow.set_up_development_environment.how_to_guide.html#checking-docker-installation","title":"Checking Docker installation","text":"<ul> <li>Check the installation by running:   ```bash <p>docker pull hello-world   Using default tag: latest   latest: Pulling from library/hello-world   Digest: sha256:fc6cf906cbfa013e80938cdf0bb199fbdbb86d6e3e013783e5a766f50f5dbce0   Status: Image is up to date for hello-world:latest   docker.io/library/hello-world:latest   ```</p> </li> </ul>"},{"location":"onboarding/kaizenflow.set_up_development_environment.how_to_guide.html#docker-installation-troubleshooting","title":"Docker installation troubleshooting","text":"<ul> <li>Common problems with Docker</li> <li>Mac DNS problem, try step 5 from the     article     and repeat the cmd below:     <code>bash     &gt; docker pull hello-world     Error response from daemon: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)</code></li> <li>Linux sudo problem, see     here     for the solution     <code>bash     &gt; docker pull hello-world     Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get   http://%2Fvar%2Frun%2Fdocker.sock/v1.40/containers/json: dial unix /var/run/docker.sock: connect: permission denied</code></li> </ul>"},{"location":"onboarding/kaizenflow.set_up_development_environment.how_to_guide.html#tmux","title":"Tmux","text":"<ul> <li>To create the standard tmux view on a cloned environment run</li> </ul> <p>```bash</p> <p>go_amp.sh kaizenflow 1   ```</p> <ul> <li>You need to create the tmux environment once per Git client and then you can   re-connect with:</li> </ul> <p>```bash   # Check the available environments.</p> <p>tmux ls   cmamp1: 4 windows (created Fri Dec  3 18:27:09 2021) (attached)</p> <p># Attach an environment.</p> <p>tmux attach -t cmamp1   ```</p> <ul> <li>You can re-run <code>go_amp.sh</code> when your tmux gets corrupted and you want to   restart. Of course this doesn't impact the underlying Git repo</li> </ul>"},{"location":"onboarding/kaizenflow.set_up_development_environment.how_to_guide.html#some-useful-workflows","title":"Some useful workflows","text":"<ul> <li>Check the installation by running:</li> </ul> <p>```bash</p> <p>docker pull hello-world   Using default tag: latest   ```</p> <ul> <li>Pull the latest KaizenFlow image; this is done once</li> </ul> <p>```bash</p> <p>i docker_pull   or   # TODO(Sameep): Update to kaizenflow once docker is updated. docker pull sorrentum/cmamp:latest   ```</p> <ul> <li>Pull the latest <code>dev_tools</code> image containing the linter; this is done once</li> </ul> <p>```bash</p> <p>i docker_pull_dev_tools   or   # TODO(Sameep): Update to kaizenflow once docker is updated. docker pull sorrentum/dev_tools:prod   ```</p> <ul> <li>Get the latest version of <code>master</code></li> </ul> <p>```bash   # To update your feature branch with the latest changes from master run   # the cmd below from a feature branch, i.e. not from master.</p> <p>i git_merge_master   # If you are on <code>master</code> just pull the remote changes. i git_pull   ```</p> <ul> <li>Start a Docker container</li> </ul> <p>```bash</p> <p>i docker_bash   ```</p> <ul> <li>You can ignore all the warnings that do not prevent you from running the   tests, e.g.,</li> </ul> <p>```bash   WARNING: The AM_AWS_ACCESS_KEY_ID variable is not set. Defaulting to a blank string.   WARNING: The AM_AWS_DEFAULT_REGION variable is not set. Defaulting to a blank string.   WARNING: The AM_AWS_SECRET_ACCESS_KEY variable is not set. Defaulting to a blank string.   WARNING: The AM_FORCE_TEST_FAIL variable is not set. Defaulting to a blank string.   WARNING: The CK_AWS_ACCESS_KEY_ID variable is not set. Defaulting to a blank string.   WARNING: The CK_AWS_DEFAULT_REGION variable is not set. Defaulting to a blank string.   WARNING: The CK_AWS_SECRET_ACCESS_KEY variable is not set. Defaulting to a blank string.   WARNING: The CK_TELEGRAM_TOKEN variable is not set. Defaulting to a blank string.</p> <p>This code is not in sync with the container:   code_version='1.4.1' != container_version='1.4.0'</p> <p>You need to:   - merge origin/master into your branch with <code>invoke git_merge_master</code>   - pull the latest container with <code>invoke docker_pull</code>   ```</p> <ul> <li>Start a Jupyter server</li> </ul> <p>```bash</p> <p>i docker_jupyter   ```</p> <ul> <li>To open a Jupyter notebook in a local web-browser:</li> <li>In the output from the cmd above find an assigned port, e.g.,     <code>[I 14:52:26.824 NotebookApp] http://0044e866de8d:10091/</code> -&gt; port is <code>10091</code></li> <li>Add the port to the link like so: <code>http://localhost:10091/</code> or     <code>http://127.0.0.1:10091</code></li> <li>Copy-paste the link into a web-browser and update the page</li> </ul>"},{"location":"onboarding/kaizenflow.set_up_development_environment.how_to_guide.html#coding-style","title":"Coding Style","text":"<ul> <li>Adopt the coding style outlined   here</li> </ul>"},{"location":"onboarding/kaizenflow.set_up_development_environment.how_to_guide.html#linter","title":"Linter","text":"<ul> <li>The linter is in charge of reformatting the code according to our conventions   and reporting potential problems</li> </ul>"},{"location":"onboarding/kaizenflow.set_up_development_environment.how_to_guide.html#run-the-linter-and-check-the-linter-results","title":"Run the linter and check the linter results","text":"<ul> <li>Run the linter against the changed files in the PR branch</li> </ul> <p>```bash</p> <p>invoke lint --files \"file1 file2...\"   ```</p> <ul> <li>More information about Linter -   Link</li> <li>Internalize the guidelines to maintain code consistency</li> </ul>"},{"location":"onboarding/kaizenflow.set_up_development_environment.how_to_guide.html#writing-and-contributing-code","title":"Writing and Contributing Code","text":"<ul> <li>If needed, always start with creating an issue first, providing a summary of   what you want to implement and assign it to yourself and your team</li> <li>Create a branch of your assigned issues/bugs</li> <li>E.g., for a GitHub issue with the name: \"Expose the linter container to     KaizenFlow contributors #63\", The GitHub issue and the branch name should be     called <code>SorrTask63_Expose_the_linter_container_to_Kaizenflow_contributors</code></li> <li>Implement the code based on the requirements in the assigned issue</li> <li>Run the linter on your code before pushing</li> <li>Do <code>git commit</code> and <code>git push</code> together so the latest changes are readily   visible</li> <li>Make sure your branch is up-to-date with the master branch</li> <li>Create a Pull Request (PR) from your branch</li> <li>Add your assigned reviewers for your PR so that they are informed of your PR</li> <li>After being reviewed, the PR will be merged to the master branch by your   reviewers</li> <li>Do not respond to emails for replies to comments in issues or PRs. Use the   GitHub GUI instead, as replying through email adds unwanted information.</li> </ul>"},{"location":"onboarding/kaizenflow.signing_up.how_to_guide.html","title":"Signing up to the project","text":"<ul> <li>Look around the repo and make sure that you are really interested in what we   are doing, and you have time to contribute</li> <li>Ponder on the IMPORTANT note about committing to contribute to the     project here</li> <li> <p>Please fork, star, watch the KaizenFlow repo so that GitHub promotes our repo     (this will help us promote our effort)</p> </li> <li> <p>Fill out the   Contributor Info form.</p> </li> <li>It's meant to just get basic contact info and technical skills about you</li> <li> <p>Don\u2019t worry: by working with us, you will quickly become a coding machine</p> </li> <li> <p>Communication with the project teams happens through:</p> </li> <li>GitHub on the repo like in any     open-source development</li> <li>Telegram IM channel for higher bandwidth discussion</li> <li> <p>Through the KaizenFlow mailing list</p> </li> <li> <p>Accept the invite from GitHub that we will send you</p> </li> <li> <p>Again please don't forget to fork, star, watch the KaizenFlow repo so that     GitHub promotes our repo</p> </li> <li> <p>Accept the invite to join the Google Drive that we will send you</p> </li> <li> <p>Subscribe to the Telegram channel</p> </li> <li> <p>Send a request, you will be added to the group after admin's approval</p> </li> <li> <p>On-boarding Tasks</p> </li> <li>Once the invitation is accepted, an issue will be created by the title     <code>On-board \\&lt;YOUR FIRST NAME LAST NAME\\&gt; Aka &lt;YOUR GITHUB_HANDLE&gt;</code>. If not     assinged, assign it to yourself and go through the particular checklist from     the issue one-by-one, marking each item as done when it's actually done.</li> </ul>"},{"location":"trading_ops/all.how_to_name_objects.explanation.html","title":"How to name objects","text":"<p>When we have lots of accounts, experiments, production systems running at the same time, we need to standardize naming conventions.</p> <p>This document specifies some standard nomenclature for the objects involved in deployment of a system, e.g.,</p> <ul> <li>Airflow DAG names</li> <li>Git branch names</li> <li>Docker image names</li> <li>Directory names</li> <li>Notebook names</li> <li>Telegram channel names</li> <li> <p>Invoke task definition</p> </li> <li> <p>The name of each object needs to have reference to its important metadata</p> </li> <li> <p>The schema and the values of each field should be listed and explained in this   markdown</p> </li> <li> <p>Note that we prefer to spell <code>pre_prod</code> rather than <code>preprod</code>, since it's more   readable</p> </li> </ul>"},{"location":"trading_ops/all.how_to_name_objects.explanation.html#airflow-dags","title":"Airflow DAGs","text":"<ul> <li>The schema is <code>{stage}.{location}.{workload}.{subschema}</code> where:</li> <li><code>stage = {test, pre_prod, prod}</code></li> <li><code>location = {tokyo, stockholm, ...}</code><ul> <li>TODO(gp): It would be better to switch to the AWS region name</li> </ul> </li> <li><code>workload</code> can be:<ul> <li><code>scheduled_trading</code>: a system trading actual capital</li> <li>TODO(gp): We should rename it to something better, e.g., <code>real_trading</code>.     \"Scheduled trading\" is a bad name since we could trade not on a schedule     with real money. \"Prod trading\" is a bad name since prod is a stage</li> <li><code>shadow_trading</code>: a system running a model but saving trades, instead of   sending them to the exchange</li> <li><code>broker_experiment</code>: an experiment running the broker interface to collect   information about market impact</li> <li><code>system_reconciliation</code></li> <li><code>scheduled_trading_system_observer</code>: publish notebooks every N minutes to   track the performance of a <code>scheduled_trading</code> or <code>shadow_trading</code> system</li> <li>TODO(gp): Maybe rename <code>internal_monitoring</code>?</li> <li><code>external_monitoring</code>: publish notebooks every N minutes to represent the   performance of a <code>scheduled_trading</code> or <code>shadow_trading</code> system for   investors</li> </ul> </li> <li> <p><code>subschema</code> depends on the workload</p> <ul> <li>E.g., for <code>*_trading</code> can be <code>{model}.{config}.[optional tag]</code></li> </ul> </li> <li> <p>The path of the DAG in the source tree and in the deployment directory has the   same name as the DAG itself</p> </li> </ul>"},{"location":"trading_ops/all.how_to_name_objects.explanation.html#git-branch-names","title":"Git branch names","text":"<ul> <li>Branch names should follow the schema   <code>{stage}.{workload}.{model}.{config}.{tag}</code></li> <li>E.g., <code>pre_prod.shadow_trading.C11a.config1</code></li> <li>If one branch is used for several values in a field (e.g., for all the   workloads) we use <code>all</code> for the value of that field or just skip it</li> <li>E.g., a Git branch used for all the models in <code>pre_prod</code> <code>scheduled_trading</code>     is called <code>pre_prod.scheduled_trading</code></li> <li>We can add as tag a date in the format <code>YYYYMMDD</code></li> </ul>"},{"location":"trading_ops/all.how_to_name_objects.explanation.html#docker-image-names","title":"Docker image names","text":"<ul> <li>The name should be <code>cmamp.{stage}.{location}.{workload}</code></li> <li>TODO(gp): Right now is <code>cmamp-test-tokyo-full-system</code>   ``` <p>i docker_create_candidate_image --task-definition \"cmamp-test-tokyo-full-system\" --user-tag \"grisha\" --region \"ap-northeast-1\"   ```</p> </li> </ul>"},{"location":"trading_ops/all.how_to_name_objects.explanation.html#directory-names","title":"Directory names","text":"<ul> <li>Derived by the Airflow DAG name with a timestamp</li> </ul>"},{"location":"trading_ops/all.how_to_name_objects.explanation.html#notebook-names","title":"Notebook names","text":"<ul> <li>The schema is <code>{stage}.{location}.{workload}.{model}.{config}.{tag}</code></li> <li>E.g., <code>prod.tokyo.system_observer.C11a.config1.last_5minutes</code></li> <li>Current links are like   http://172.30.2.44/system_reconciliation/C11a.config1.prod.last_5minutes.html</li> </ul>"},{"location":"trading_ops/all.how_to_name_objects.explanation.html#telegram-channels","title":"Telegram channels","text":"<ul> <li>Channels should be names referring to the <code>stage</code> and <code>workload</code> they refer to</li> <li>E.g., <code>KT - pre_prod.scheduled_trading</code></li> </ul>"},{"location":"trading_ops/all.how_to_name_objects.explanation.html#invoke-task-definition","title":"Invoke task definition","text":"<ul> <li>The format is like <code>{repo}.{subschema}</code> where:</li> <li><code>repo</code> is the name of the repo (e.g., <code>cmamp</code>)</li> <li><code>subschema</code> represents the rest of the information</li> <li>E.g., <code>cmamp.pre_prod.tokyo.scheduled_trading</code></li> </ul>"},{"location":"trading_ops/all.scheduled_monitoring.explanation.html","title":"Scheduled trading monitoring","text":"<ul> <li>The data is updated every 5 minutes</li> <li>Scheduled trading monitoring DAGs are stored in   <code>im_v2/airflow/dags/trading/scheduled_trading/monitoring</code></li> </ul>"},{"location":"trading_ops/all.scheduled_monitoring.explanation.html#c11aconfig1","title":"C11a.config1","text":"<ul> <li>DAG URL:   http://internal-a97b7f81b909649218c285140e74f68a-1285736094.eu-north-1.elb.amazonaws.com:8080/dags/preprod.tokyo.scheduled_trading_system_observer.C11a.config1/grid</li> <li>Notebook URL:   http://172.30.2.44/system_reconciliation/C11a.config1.prod.last_5minutes.html</li> </ul>"},{"location":"trading_ops/all.scheduled_monitoring.explanation.html#c11aconfig3","title":"C11a.config3","text":"<ul> <li>DAG URL:   http://internal-a97b7f81b909649218c285140e74f68a-1285736094.eu-north-1.elb.amazonaws.com:8080/dags/preprod.tokyo.scheduled_trading_system_observer.C11a.config3/grid</li> <li>Notebook URL:   http://172.30.2.44/system_reconciliation/C11a.config3.prod.last_5minutes.html</li> </ul>"},{"location":"trading_ops/all.scheduled_monitoring.explanation.html#alerts-conditions","title":"Alerts' conditions","text":"<p>Assertion is raised when:</p> <ul> <li>The current USDT balance is below the threshold.</li> </ul> <p>Currently the threshold is -1000$</p> <ul> <li>Current notional cumulative PnL is below the threshold.</li> </ul> <p>Currently the threshold is -100$</p> <ul> <li>Current notional cumulative PnL as fraction of GMV is below the threshold.</li> </ul> <p>Currently the threshold is -0.1 (i.e. loss is &gt; 10% of avg. GMV)</p> <p>When an assertion is triggered the notebook fails, which means that we stop monitoring the run.</p> <p>#TODO(Grisha): keep publishing the notebook even if it fails.</p>"},{"location":"trading_ops/all.scheduled_monitoring.explanation.html#tg-notifications","title":"TG notifications","text":""},{"location":"trading_ops/all.scheduled_monitoring.explanation.html#scheduled-run-notifications","title":"Scheduled run notifications","text":"<ul> <li>Channel: \"Kaizen Preprod Trading notification\"</li> <li>DAG id: preprod.tokyo.scheduled_trading.C11a.config1</li> </ul> <p>E.g.:</p> <pre><code>DAG 'preprod.tokyo.scheduled_trading.C11a.config1' failed\nRun_mode: scheduled\nDAG start timestamp: 2024-04-19 12:40:00.363734+00:00\n\nLink:\nhttp://internal-a97b7f81b909649218c285140e74f68a-1285736094.eu-north-1.elb.amazonaws.com:8080/dags/preprod.tokyo.scheduled_trading.C11a.config1/grid\n</code></pre>"},{"location":"trading_ops/all.scheduled_monitoring.explanation.html#monitoring-notebook-notifications","title":"Monitoring notebook notifications","text":"<ul> <li>Channel: \"Kaizen Preprod Trading notification\"</li> <li>#TODO(Grisha): should we create a separate channel for scheduled trading?</li> <li>DAG id: preprod.tokyo.scheduled_trading_system_observer.C11a.config1</li> </ul> <p>E.g.:</p> <pre><code>DAG 'preprod.tokyo.scheduled_trading_system_observer.C11a.config1' failed\nRun_mode: scheduled\nDAG start timestamp: 2024-04-19 12:40:00.363734+00:00\n\nLink:\nhttp://internal-a97b7f81b909649218c285140e74f68a-1285736094.eu-north-1.elb.amazonaws.com:8080/dags/preprod.tokyo.scheduled_trading_system_observer.C11a.config1/grid\n</code></pre> <p>N.B. Currently the exact error is not propagated to the TG channel. We only know that the notebook failed but to understand why we need to open AirFlow logs.</p> <p>#TODO(Grisha): propagate the assertion to the TG channel somehow.</p>"},{"location":"trading_ops/all.scheduled_monitoring.explanation.html#logs","title":"Logs","text":"<ul> <li>Access logs by clicking the link from the notification</li> <li>Then choose the failed run</li> </ul> <ul> <li>Then click on the \u201cLogs\u201d tab</li> </ul> <ul> <li>To identify the problem, scroll a bit up and there\u2019s an error message, e.g.,</li> </ul>"},{"location":"trading_ops/all.scheduled_trading.explanation.html","title":"Publishing Experiment Reports","text":""},{"location":"trading_ops/all.scheduled_trading.explanation.html#description","title":"Description","text":"<ul> <li>The architecture automates running, publishing, and accessing analysis   notebooks from the output table, ensuring structured data management for   ongoing and future experiments.</li> </ul>"},{"location":"trading_ops/all.scheduled_trading.explanation.html#detailed-explanation","title":"Detailed Explanation","text":"<ul> <li>When executing a trading experiment, to analyze the results of our trading   experiments we run several notebooks, i.e.</li> <li><code>Master_system_reconciliation_fast</code></li> <li><code>Master_bid_ask_execution_analysis</code></li> <li><code>Master_execution_analysis</code></li> <li><code>Master_broker_debugging</code></li> <li> <p><code>Master_broker_portfolio_reconciliation</code></p> </li> <li> <p>Notebooks are published on S3 as an HTML files and accessed by team members   for analysis</p> </li> <li> <p>To automate the process, we publish a meta notebook which contains links to   all the published analysis notebooks of the latest experiment run:</p> </li> <li>Master_trading_system_report</li> </ul> <p></p> <ul> <li> <p>The links to the S3 files are stored in the CSV file   <code>system_log_dir/analysis_notebooks/analysis_notebooks_links.csv</code></p> </li> <li> <p>Pre-run/post-run scheduled balances (USDT) are stored in   <code>flatten_account.before/</code> and <code>flatten_account.after/</code> directories</p> </li> </ul> <p></p> <ul> <li>Two kinds of the meta notebooks i.e.   <code>Master_trading_system_report</code>   are published:</li> <li>Latest version - Stable link which contains notebooks links of the latest     run. E.g.,     http://172.30.2.44/v2/trading_ops/trading_reports/prod/C11a.config1/2024/08/Master_trading_system_report.latest.html</li> <li> <p>Timestamp version - It contains notebook links of that particular timestamp     run. E.g.,     http://172.30.2.44/v2/trading_ops/trading_reports/prod/C11a.config1/2024/08/Master_trading_system_report.prod.C11a.config1.20240821_210500.20240821_220000.html</p> </li> <li> <p>Here is an example of the directory structure for the trade execution   experiment:   <code>/shared_data/CmTask7852_2/system_reconciliation/C11a/prod/20240419_103500.20240419_113000/system_log_dir.manual/analysis_notebooks/   \\    |-- Master_bid_ask_execution_analysis \\    |-- Master_broker_debugging \\    |-- Master_broker_portfolio_reconciliation \\    |-- Master_execution_analysis \\    |-- analysis_notebooks_links.csv</code></p> </li> </ul>"},{"location":"trading_ops/all.shadow_monitoring.explanation.html","title":"Shadow trading monitoring","text":"<ul> <li><code>C11a.config1</code>:</li> <li>DAG:     http://internal-a97b7f81b909649218c285140e74f68a-1285736094.eu-north-1.elb.amazonaws.com:8080/dags/preprod.tokyo.shadow_trading_system_observer.C11a.config1/grid</li> <li> <p>Stable link:     http://172.30.2.44/system_reconciliation/C11a.config1.shadow_trading.last_5minutes.html</p> </li> <li> <p><code>C11a.config3</code>:</p> </li> <li>DAG:     http://internal-a97b7f81b909649218c285140e74f68a-1285736094.eu-north-1.elb.amazonaws.com:8080/dags/preprod.tokyo.shadow_trading_system_observer.C11a.config3/grid</li> <li> <p>Stable link:     http://172.30.2.44/system_reconciliation/C11a.config3.shadow_trading.last_5minutes.html</p> </li> <li> <p><code>C14a.config1</code>:</p> </li> <li>DAG:     http://internal-a97b7f81b909649218c285140e74f68a-1285736094.eu-north-1.elb.amazonaws.com:8080/dags/preprod.tokyo.shadow_trading_system_observer.C14a.config1/grid</li> <li> <p>Stable link:     http://172.30.2.44/system_reconciliation/C14a.config1.paper_trading.last_5minutes.html</p> </li> <li> <p>Branch: <code>orange/master</code> <code>amp/prod.shadow_trading_system_observer</code></p> </li> </ul> <p>The monitoring site is updating every 5 minutes and can be accessed at: http://kaizentech-public.s3-website.eu-north-1.amazonaws.com/index.html</p>"},{"location":"trading_ops/all.trading_run_summary_sheet.explanation.html","title":"Trading Run Summary Sheet","text":""},{"location":"trading_ops/all.trading_run_summary_sheet.explanation.html#document-description","title":"Document description","text":"<p>Gsheet: 2024Q2/Q3 - Scheduled trading</p> <p>This Gsheet provides a daily summary of trading DAG runs. We run scheduled DAGs and fill the report on the next day after the run start when results are available.</p> <p>The data is entered into the Gsheet every day before 8:30 AM ET.</p>"},{"location":"trading_ops/all.trading_run_summary_sheet.explanation.html#field-descriptions","title":"Field descriptions","text":"<ul> <li> <p>Date: the date of the DAG run in YYYY-MM-DD format</p> </li> <li> <p>Find a date by clicking on the \"system_run\" task</p> <p></p> </li> <li> <p>Day: the day of the week for the DAG run</p> </li> <li> <p>Derived from the Date</p> </li> <li> <p>Account (id): the identifier number of the account used for the DAG run</p> </li> <li> <p>Get the <code>--secret_id</code> value in the \"Rendered Template\" tab of the      \"system_run\" task</p> <p></p> </li> <li> <p>Model/config: the specific model and configuration used for the DAG run,   typically in the format <code>{{model}}.{{config}}</code></p> </li> <li> <p>Scheduled duration (hours): the planned duration of the DAG run in hours</p> </li> <li> <p>Get the <code>--run_duration</code> value in the \"Rendered Template\" tab of the      \"system_run\" task</p> </li> <li> <p>Convert the seconds to hours by dividing by 3600</p> <p></p> </li> <li> <p>Actual duration (hours): the actual time the DAG run lasted, in hours. May   differ from scheduled duration due to early termination or issues</p> </li> <li> <p>Get value from \"Duration\" section in the \"Details\" tab of the \"system_run\"      task</p> <p></p> </li> <li> <p>Pre-run USDT: the amount of USDT available in the account before the   trading run started</p> </li> <li> <p>If the trading notebook was published:</p> <ol> <li>Get Master trading system report link in the \"Logs\" tab of    \"publish_trading_report_notebook\" task</li> <li>Get the value in the \"USDT\" row in the \"Before run\" section</li> </ol> <p>   - If the trading notebook wasn't published:</p> <ol> <li>Get Total balance dictionary in the \"Logs\" tab of    \"flatten_account_before\" task</li> <li>Get the value of the \"USDT\". E.g.: <code>Account flattened. Total balance: {'FDUSD': 0.0, 'BTC': 0.05, 'BNB': 0.0, 'ETH': 0.0, 'USDT': -177.88759239, 'USDC': 0.0}</code></li> </ol> </li> <li> <p>Post-run USDT: the amount of USDT remaining in the account after the   trading run completed</p> </li> <li> <p>If the trading notebook was published:</p> <ol> <li>Get Master trading system report link in the \"Logs\" tab of    \"publish_trading_report_notebook\" task</li> <li>Get the value in the \"USDT\" row in the \"After run\" section</li> </ol> <p>   - If the trading notebook wasn't published:</p> <ol> <li>Get Total balance dictionary in the \"Logs\" tab of \"flatten_account_after\"    task</li> <li>Get the value of the \"USDT\". E.g.: <code>Account flattened. Total balance: {'FDUSD': 0.0, 'BTC': 0.05, 'BNB': 0.0, 'ETH': 0.0, 'USDT': -170.96487017, 'USDC': 0.0}</code></li> </ol> </li> <li> <p>Volume (notional): the total trading volume in notional value for the DAG   run, typically in USDT</p> </li> <li> <p>Get the <code>Master_execution_analysis</code> notebook link from the Master trading      system report</p> </li> <li> <p>Get the combined value of <code>traded_volume_dollars</code> in the \"Fee summary\"      section of <code>Master_execution_analysis</code> notebook</p> <p></p> </li> <li> <p>Trading report: a link to the detailed trading report notebook for the   specific DAG run</p> </li> <li> <p>Find the link in the end of the \"Logs\" tab of      \"publish_trading_report_notebook\" task. E.g.:      <code># To open the notebook from a web-browser open a link:      http://172.30.2.44/notebooks/Master_Analysis/Master_trading_system_report.0.20240711-070816.html</code></p> </li> <li> <p>Notes: additional comments, observations, or explanations about the DAG   run, including any issues or failures encountered. If everything went as   expected during the run, this field is typically left empty</p> </li> <li> <p>PnL (notional): profit and loss in notional value (USDT) for the DAG run</p> </li> <li> <p>Calculated as <code>Post-run (USDT) - Pre-run (USDT)</code></p> </li> <li> <p>PnL (bps over traded volume): profit and loss expressed in basis points   (1/100th of a percent) over the traded volume</p> </li> <li> <p>Calculated as <code>(PnL / Volume) * 10000</code></p> </li> <li> <p>Cumulative PnL (notional): the total accumulated Profit and Loss in   notional value (USDT) up to and including this DAG run</p> </li> <li> <p>Cumulative PnL (bps over volume traded): the total accumulated PnL   expressed in basis points over the total volume traded up to and including   this DAG run</p> </li> <li> <p>Normalized: an adjusted Cumulative PnL (notional) value, potentially used   for comparison or analysis purposes</p> </li> </ul> <p>E.g.</p> Date Day Account (id) Model/config Scheduled duration (hours) Actual duration (hours) Pre-run USDT Post-run USDT Volume (notional) Trading report Notes PnL (notional) PnL (bps over traded volume) Cumulative PnL (notional) Cumulative PnL (bps over volume traded) Normalized 2024-07-08 Monday 9 C11a.config3 22 22 -163.09 -177.89 11441.03 Report -14.80 -12.94 -177.89 -3.30 -83.29 2024-06-10 Monday 9 C11a.config3 22 6 28.83 50.10 4537.60 Report Failed because of https://github.com/cryptokaizen/cmamp/issues/8552 21.28 46.89 50.10 2.34 144.70 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ..."},{"location":"work_organization/Hiring_process.html","title":"HiringMeister","text":""},{"location":"work_organization/Hiring_process.html#find-candidates","title":"Find candidates","text":"<p>We get candidates in several ways</p> <ul> <li>Send emails to CS and related depts</li> <li> <p>Especially for PhD students</p> </li> <li> <p>Use our LinkedIn pipeline to find candidates</p> </li> <li> <p>We are going to start targeting graduates from top schools</p> </li> <li> <p>Post job on LinkedIn</p> </li> <li> <p>Post job on Upwork</p> </li> </ul>"},{"location":"work_organization/Hiring_process.html#initial-screening","title":"Initial screening","text":"<ul> <li>HiringMeister: upon receipt of applications for new positions, track   applicants add task to Asana \"Candidates\"</li> <li> <p>For each task description use the template     https://app.asana.com/0/1204526145297526/1207748235019119/f</p> </li> <li> <p>HiringMeister: send email to gather additional information from candidates</p> </li> <li>Ask candidates to fill out the questionnaire form     here</li> <li>Results are     here</li> </ul>"},{"location":"work_organization/Hiring_process.html#hiring-committee","title":"Hiring committee","text":"<ul> <li>Every 2 weeks the screening committee conducts a meeting to review the   applicants</li> <li>Everyone on the committee collects all the information needed to make a     decision</li> <li> <p>TODO(gp): This needs to be formalized better</p> </li> <li> <p>We want to increase the quality of the collaborators, so if there is a red   flag (e.g., no GitHub, low GPA, undergrad) we can decide to not on-board</p> </li> <li> <p>The goal is to avoid on-boarding people that will likely disappoint us</p> </li> <li> <p>If we receive a single candidate application and find no red flags in the   profile, we should proceed further in the process</p> </li> <li> <p>It's ok to ask more team members to take a look</p> </li> <li> <p>If the candidate is a no-go, GP sends an email of rejection</p> </li> </ul>"},{"location":"work_organization/Hiring_process.html#mentors","title":"Mentors","text":"<ul> <li>Hiring Meisters (e.g., Sonaal and Samarth) supervise the hiring process</li> <li> <p>docs/onboarding/kaizenflow.hiring_meister.how_to_guide.md</p> </li> <li> <p>Coding mentors are:</p> </li> <li>Hiring Meisters</li> <li>Anybody who is not in some critical path (e.g., Dan, Danya, Toma)</li> </ul>"},{"location":"work_organization/Hiring_process.html#onboarding-pipeline","title":"Onboarding pipeline","text":"<ul> <li>Once a candidate is accepted we assign a mentor on Asana</li> <li>The mentor is in charge to intervene when the candidate is blocked</li> <li> <p>We track the mentor in Asana</p> </li> <li> <p>We use Asana tasks to communicate about the candidates</p> </li> <li> <p>There are multiple pipeline stages on Asana that candidates go through</p> </li> <li>Candidates<ul> <li>Anybody who wants to join our project/company</li> </ul> </li> <li>On-boarding on KaizenAI<ul> <li>Goal: set up candidates to work on their laptop on KaizenAI</li> <li>Duration: few days</li> <li>The checklist for onboarding on KaizenAI is   here</li> <li>Once the checklist is complete, candidates move to the next step</li> </ul> </li> <li>Working on KaizenAI<ul> <li>Goal: understand who can/should join the team</li> <li>Duration: 1 month</li> <li>Type of tasks: unit tests</li> <li>Each candidate should work on 2-3 issues to make sure we can judge how   good is a candidate</li> <li>We score candidates every 2 weeks for a few iterations until we can make a   decision</li> <li>Candidates who are good enough can be onboarded on cmamp, otherwise are   dropped</li> </ul> </li> <li>On-boarding on cmamp<ul> <li>Goal: set up candidates to develop on cmamp (only on the server)</li> <li>Duration: few days</li> <li>The checklist for onboarding on cmamp is   here</li> <li>On-boarding GH issue on cmamp</li> </ul> </li> <li>Working on cmamp<ul> <li>Goal: learn our development process</li> <li>Duration: 1 month</li> <li>Type of tasks: unit tests (stuff that doesn\u2019t require design)</li> <li>After that we can assign the best hires to teams</li> </ul> </li> <li>Full-time or interns on probation<ul> <li>Goal: contribute to KaizenAI or cmamp</li> <li>Duration: 3 months</li> <li>Type of tasks: unit test, refactoring, simple non-design tasks</li> <li>Assign them to teams</li> <li>Give them more complex task</li> </ul> </li> <li>Full-time team members<ul> <li>Normal workflow</li> </ul> </li> <li>Team leaders<ul> <li>Type of tasks: in charge of architecture, big picture, real-time systems</li> <li>For the ones that are able to master the process</li> </ul> </li> </ul>"},{"location":"work_organization/Hiring_process.html#onboarding-gh-issues","title":"Onboarding GH Issues","text":"<p>We want to have a pipeline of GH Issues to assign people as soon as they are ready</p> <ul> <li>On-boarding issues</li> </ul>"},{"location":"work_organization/Hiring_process.html#feedback-for-interns","title":"Feedback for interns","text":"<p>The scoring process is described here</p> <p><code>docs/work_organization/all.contributor_scoring.how_to_guide.md</code></p> <p>Internships - Feedback 2024</p>"},{"location":"work_organization/Hiring_process.html#feedback-for-full-timers","title":"Feedback for full-timers","text":"<p>Performance feedback</p>"},{"location":"work_organization/all.buildmeister.how_to_guide.html","title":"Buildmeister","text":""},{"location":"work_organization/all.buildmeister.how_to_guide.html#buildmeister-process","title":"Buildmeister process","text":""},{"location":"work_organization/all.buildmeister.how_to_guide.html#general","title":"General","text":"<ul> <li>The Buildmeister rotates every 2 weeks</li> <li>To see who is the Buildmeister now refer to     Buildmeister gsheet</li> <li>Each rotation should be confirmed by a 'handshake' between the outgoing     Buildmeister and the new one in the related Telegram chat</li> <li>The Buildmeister is responsible for:</li> <li>Check build status using the     buildmeister dashboard everyday</li> <li>Pushing team members to fix broken tests</li> <li>Conducting post-mortem analysis<ul> <li>Why did the break happen?</li> <li>How can we avoid the problem next time, through process and automation?</li> </ul> </li> <li>Refer to <code>.github</code> dir in the repo for update schedule of GH actions</li> <li>Additional information about the   tests</li> </ul>"},{"location":"work_organization/all.buildmeister.how_to_guide.html#notification-system","title":"Notification system","text":"<ul> <li><code>@CK_cmamp_buildbot</code> notifies the team about breaks via Telegram channel   <code>CK build notifications</code></li> <li>A notification contains:</li> <li>Failing tests type: fast/slow/super-slow</li> <li>Repo</li> <li>Branch</li> <li>Event</li> <li>Link to a failing run</li> </ul> <p>Example:</p> <ul> <li></li> </ul>"},{"location":"work_organization/all.buildmeister.how_to_guide.html#buildmeister-instructions","title":"Buildmeister instructions","text":"<ul> <li>You receive a break notification from <code>@CK_cmamp_buildbot</code></li> <li>Have a look at the message</li> <li>Do it right away, this is always your highest priority task</li> <li>Notify the team</li> <li> <p>If the break happened in <code>lemonade</code> repo, ping GP or Paul, since they are     the only ones with write access</p> </li> <li> <p>Post on the <code>CK build notifications</code> Telegram channel what tests broke, e.g.,   <code>FAILED knowledge_graph/vendors/test/test_utils.py::TestClean::test_clean</code></p> </li> <li>If unsure about the cause of failure (there is a chance that a failure is     temporary):<ul> <li>Do a quick run locally for the failed test</li> <li>If the test is specific and can not be run locally, rerun the regressions</li> </ul> </li> <li>Ask if somebody knows what is the problem<ul> <li>If you know who is in charge of that test (you can use <code>git blame</code>) ask   directly</li> </ul> </li> <li>If the offender says that it's fixing the bug right away, let him/her do it</li> <li> <p>Otherwise, file a bug to track the issue</p> </li> <li> <p>File an Issue in GH / ZH to report the failing tests and the errors</p> </li> <li>Example:     https://github.com/cryptokaizen/cmamp/issues/4386</li> <li>Issue title template <code>Build fail - {repo} {test type} ({run number})</code><ul> <li>Example: <code>Build fail - Cmamp fast_tests (1442077107)</code></li> </ul> </li> <li>Paste the URL of the failing run<ul> <li>Example:   https://github.com/alphamatic/dev_tools/actions/runs/1497955663</li> </ul> </li> <li>Provide as much information as possible to give an understanding of the     problem</li> <li>List all the tests with FAILED status in a GitHub run, e.g.,     <code>FAILED knowledge_graph/vendors/test/test_p1_utils.py::TestClean::test_clean     FAILED knowledge_graph/vendors/nbsc/test/test_nbsc_utils.py::TestExposeNBSCMetadata::test_expose_nbsc_metadata</code></li> <li>Stack trace or part of it (if it's too large)     <code>Traceback (most recent call last): File     \"/.../automl/hypotheses/test/test*rh_generator.py\", line 104, in test1     kg_metadata, * = p1ut.load_release(version=\"0.5.2\") File     \"/.../knowledge_graph/vendors/utils.py\", line 53, in load_release % version,     File \"/.../amp/helpers/dbg.py\", line 335, in dassert_dir_exists \\_dfatal(txt,     msg, \\*args) File \"/.../amp/helpers/dbg.py\", line 97, in \\_dfatal     dfatal(dfatal_txt) File \"/.../amp/helpers/dbg.py\", line 48, in dfatal raise     assertion_type(ret) AssertionError:     ##############################################################################     Failed assertion \\*       dir='/fsx/research/data/kg/releases/timeseries_db/v0.5.2' doesn't exist or       it's not a dir The requested version 0.5.2 has directory associated with it.</code></li> <li>Add the issue to the     BUILD - Breaks     Epic so that we can track it</li> <li>If the failures are not connected to each other, file separate issues for     each of the potential root cause</li> <li> <p>Keep issues grouped according to the codebase organization</p> </li> <li> <p>Post the issue reference on Telegram channel CK build notifications</p> </li> <li>You can quickly discuss there who will take care of the broken tests, assign     that person</li> <li> <p>Otherwise, assign it to the person who can reroute</p> </li> <li> <p>Our policy is \"fix it or revert\"</p> </li> <li> <p>The build needs to go back to green within 1 hr</p> <ul> <li>Either the person responsible for the break fixes the issue within 1 hour,   or you need to push the responsible person to disable the test</li> <li>Do not make the decision about disabling the test yourself!</li> <li>First, check with the responsible person, and if he / she is ok with   disabling, do it</li> <li>IMPORTANT: Disabling a test is not the first choice, it's a measure of   last resort!</li> </ul> </li> <li> <p>Regularly check issues that belong to the Epic   BUILD - Breaks.</p> </li> <li>You have to update the break issues if the problem was solved or partially     solved.</li> <li> <p>Pay special attention to the failures which resulted in disabling tests</p> </li> <li> <p>When your time of the Buildmeister duties is over, confirm the rotation with   the next responsible person in the related Telegram chat.</p> </li> </ul>"},{"location":"work_organization/all.buildmeister.how_to_guide.html#update_amp_submodule-fails","title":"<code>update_amp_submodule</code> fails","text":"<ul> <li> <p>When this happens, the first thing to do is attempt to update the <code>amp</code>   pointer manually</p> </li> <li> <p>Instructions:   ```</p> <p>cd src/dev_tools1 git checkout master git pull --recurse-submodules cd amp git checkout master git pull origin master cd .. git add \"amp\" git commit -m \"Update amp pointer\"   ```</p> </li> <li> <p>There is also an invoke target <code>git_roll_amp_forward</code> that does an equivalent   operation</p> </li> </ul>"},{"location":"work_organization/all.buildmeister.how_to_guide.html#buildmeister-dashboard","title":"Buildmeister dashboard","text":"<p>The Buildmeister dashboard is a tool that provides a quick overview of the current state of the results of all GitHub Actions workflows. See run and publish the buildmeister dashboard for detailed information.</p>"},{"location":"work_organization/all.buildmeister.how_to_guide.html#allure-reports-analysis","title":"Allure Reports Analysis","text":"<ul> <li>For a background on Allure, refer to these docs</li> <li>Detailed info can be found in the official     docs</li> <li>Allure Explanantion</li> <li> <p>Allure How to Guide</p> </li> <li> <p>For now, the Buildmeister can get the link to the Allure reports by navigating   GitHub Actions page https://github.com/cryptokaizen/cmamp/actions</p> </li> <li>Select a particular workflow (Allure fast tests, Allure slow tests, Allure     superslow tests) based on the test types</li> <li>Click on the particular run for which to get the report. Latest is on the     top</li> <li>Access the report URL by clicking <code>Report URL</code> in the run link. For e.g.:     https://github.com/cryptokaizen/cmamp/actions/runs/7210433549/job/19643566697</li> <li> <p>The report URL looks like:     http://172.30.2.44/allure_reports/cmamp/fast/report.20231212_013147</p> </li> <li> <p>Once a week the Buildmeister manually inspects   graph section of the   report</p> </li> <li> <p>The overall goal is to:</p> <ul> <li>Monitor the amount of skipped, failed, and broken tests using the   <code>Trend Chart</code>. It shows how a certain value changed over time. Each   vertical line corresponds to a certain version of the test report, with   the last line on the right corresponding to the current version</li> </ul> <p> - Monitor the <code>Duration Trend</code> to check the time taken to the run all tests   comparing to historical trends</p> <p> - Monitor the <code>Duration Distribution</code>, where all the tests are divided into   groups based on how long it took to complete them, and manually compare   with the last week results</p> <p> - Monitor the <code>Retries Trend</code> to check the number of retries occured in a   particular run</p> <p> - The idea is to make sure it doesn't have drastic change in the values</p> </li> <li> <p>Steps to perform if a test fails, timeouts or breaks</p> </li> <li>When a particular test fails, timeouts or breaks, Buildmeister should look     in report for<ul> <li>How long it was the case, e.g., did it occur in the past? Include this   info when filing an issue</li> <li>The very first run when it happened and add that info to the issue. This   could be useful for debugging purposes</li> <li>These info can be extracted by navigating the <code>Packages</code> section of the   report for that test. Any particular test has <code>history</code> and <code>retries</code>   section which shows the history of success and number of retries occured   for that test</li> </ul> </li> <li>The goal here is to provide more context when filing an issue so that we can     make better decisions</li> </ul>"},{"location":"work_organization/all.buildmeister.how_to_guide.html#post-mortem-analysis-tbd","title":"Post-mortem analysis (TBD)","text":"<ul> <li>We want to understand on why builds are broken so that we can improve the   system to make it more robust</li> <li>In order to do that, we need to understand the failure modes of the system</li> <li> <p>For this reason we keep a log of all the issues and what was the root cause</p> </li> <li> <p>After each break fill the   Buildmeister spreadsheet sheet \"Post-mortem breaks analysis\"</p> </li> <li> <p><code>Date</code> column:</p> </li> <li>Enter the date when the break took place</li> <li> <p>Keep the bug ordered in reverse chronological order (i.e., most recent dates     first)</p> </li> <li> <p><code>Repo</code> column:</p> </li> <li> <p>Specify the repo where break occurred</p> <ul> <li><code>amp</code></li> <li>...</li> </ul> </li> <li> <p><code>Test type</code> column:</p> </li> <li> <p>Specify the type of the failing tests</p> <ul> <li>Fast</li> <li>Slow</li> <li>Super-slow</li> </ul> </li> <li> <p><code>Link</code> column:</p> </li> <li> <p>Provide a link to a failing run</p> </li> <li> <p><code>Reason</code> column:</p> </li> <li> <p>Specify the reason of the break</p> <ul> <li>Merged a branch with broken tests</li> <li>Master was not merged in a branch</li> <li>Merged broken slow tests without knowing that</li> <li>Underlying data changed</li> </ul> </li> <li> <p><code>Issue</code> column:</p> </li> <li> <p>Provide the link to the ZH issue with the break description</p> </li> <li> <p><code>Solution</code> column:</p> </li> <li>Provide the solution description of the problem<ul> <li>Problem that led to the break was solved</li> <li>Failing tests were disabled, i.e. problem was not solved</li> </ul> </li> </ul>"},{"location":"work_organization/all.contributor_scoring.how_to_guide.html","title":"Contributor Scoring","text":""},{"location":"work_organization/all.contributor_scoring.how_to_guide.html#scoring-process","title":"Scoring process","text":""},{"location":"work_organization/all.contributor_scoring.how_to_guide.html#general","title":"General","text":"<ul> <li> <p>We want to evaluate and provide feedback to our team members on different   aspects of their work.</p> </li> <li> <p>We don't take non-perfect scores personally but just as a way to understand   what to improve.</p> </li> <li> <p>The scoring template is here   Scoring template   (this is an Excel spreadsheet since you need to upload it and it needs to be a   file and not a Google Sheet).</p> </li> <li> <p>Each metric is scored between 1 (poor), 3 (average) and 5 (excellent)</p> </li> <li> <p>We consider 4 as acceptable, anything less than 4 as problematic and needs     improve</p> </li> <li> <p>We want to score everyone we work with:</p> </li> <li>Initially only people that we supervise, later on anyone</li> <li>Feedback is anonymous</li> <li> <p>At least 2 persons should score everyone</p> </li> <li> <p>Scoring frequency</p> </li> <li>Every 2 weeks for full-time candidates, part-time collaborators</li> <li>Every month for full-time team</li> </ul>"},{"location":"work_organization/all.contributor_scoring.how_to_guide.html#current-process","title":"Current process","text":"<ul> <li>Every scoring needs to happen (e.g., every two weeks):</li> <li>Mentor make a copy of the Excel spreadsheet     Scoring template</li> <li>Rename the template \"Scoring - {Scorer} - {ScoringDate}\" (e.g., \"Scoring -     GP - 2023-09-01\")</li> <li>Fill out the rows for the people that they need to score by looking at the     Mentor column</li> <li>Upload your Scoring Excel file     here<ul> <li>You should see</li> <li></li> <li>(For admin use, the source is   here   and   here)</li> </ul> </li> <li>One of the integrators (GP, Paul, or somebody else) merges all the scoring     template in a single one, and then creates the averaged score for each     person</li> <li>The scores are then distributed anonymously<ul> <li>Scored team members don't know who / how many mentors scored them   (although they have a clue about at least one mentor)</li> </ul> </li> </ul>"},{"location":"work_organization/all.contributor_scoring.how_to_guide.html#scoring-topics","title":"Scoring topics","text":""},{"location":"work_organization/all.contributor_scoring.how_to_guide.html#general_1","title":"General","text":"<ul> <li> <p>Topics should be independent</p> </li> <li> <p>We should provide</p> </li> <li>Concrete questions to assess how people do on each topic</li> <li>Ways to improve the score (e.g., \"read this book!\", \"do more of this and     less of this\")</li> </ul>"},{"location":"work_organization/all.contributor_scoring.how_to_guide.html#current-topics","title":"Current topics","text":"<ul> <li>Scoring table contains the following fields:</li> <li>Quality of code<ul> <li>Writes elegant code?</li> <li>Follows our standards and conventions?</li> <li>Designs beautiful abstractions, when needed?</li> </ul> </li> <li>Quality of design<ul> <li>Designs beautiful but simple abstractions?</li> <li>Adds abstractions only when needed?</li> <li>Orchestrates software components properly?</li> <li>Uses design patterns, when needed?</li> </ul> </li> <li>Attention to details<ul> <li>Thinks in terms of corner cases?</li> <li>Debugs things carefully?</li> <li>Takes pride in well-done product (e.g., code, documentation)?</li> </ul> </li> <li>Productivity<ul> <li>Closes issues effectively without unnecessary iterations?</li> <li>It is a qualitative measure of progress per unit of time</li> </ul> </li> <li>Make and achieve ETAs<ul> <li>Estimates complexity in bugs?</li> <li>Thinks of risks and unknown unknowns, best / average / worst ETAs?</li> <li>Resolves issues in set ETAs?</li> <li>Puts in a sufficient amount of hours to make progress?</li> </ul> </li> <li>Autonomy<ul> <li>Understands specs?</li> <li>Needs a lot of supervision to execute the tasks?</li> <li>Does what's right according to our shared way of doing things without   reminders?</li> </ul> </li> <li>Follow our PR process<ul> <li>Learns from reviews and doesn't make the same mistakes?</li> <li>Runs linter consistently before each iteration?</li> <li>Does a PR / day (even draft)?</li> </ul> </li> <li>Follow our organizational process<ul> <li>Sends a daily TODO email?</li> <li>Updates their issues daily?</li> <li>Curates GitHub / ZenHub?</li> </ul> </li> <li>Team work<ul> <li>Helps others on the team when others need help / supervision?</li> <li>Takes the initiative and goes the extra mile when needed?</li> <li>Sacrifices for the greater good (e.g., doing stuff that is not fun to do)?</li> </ul> </li> <li>Communication<ul> <li>Files issues with clear specs?</li> <li>Explains technical issues and gives updates properly and with clarity?</li> <li>Reports problems and solutions with proper context?</li> <li>Speaks and writes English well?</li> </ul> </li> <li>Ability to run a team<ul> <li>Can juggle multiple topics at once?</li> <li>Can split the work in specs?</li> <li>Is it ok with being interrupted to help team members?</li> </ul> </li> <li>Positive energy<ul> <li>Has an upbeat approach to working even if sh*t doesn't work (since things   never work)?</li> <li>Is a   Negative Nelly?</li> </ul> </li> <li>Dev %, Data scientist %, Devops %<ul> <li>This just measures how much of a role one team member can cover</li> <li>See below</li> </ul> </li> </ul>"},{"location":"work_organization/all.contributor_scoring.how_to_guide.html#roles","title":"Roles","text":"<ul> <li> <p>We want to define how each team-member is comfortable covering several high   level activities.</p> </li> <li> <p>The idea is to understand what roles a new hire can play.</p> </li> <li> <p>Roles are not mutually exclusive</p> </li> <li>E.g., a jack-of-all-trades can be 4 on all topics</li> <li>E.g., XYZ is a data scientist and has data science=5, dev=3, devops=1</li> <li>Data science<ul> <li>Example of activities are:</li> <li>Write notebooks</li> <li>Do research</li> <li>Debug data</li> </ul> </li> <li>Dev<ul> <li>Example of activities are:</li> <li>Write code</li> <li>Refactor code</li> <li>Architecture code</li> <li>Debug code</li> <li>Unit test code</li> </ul> </li> <li>DevOps<ul> <li>Example of activities are:</li> <li>Manage / supervise infra</li> <li>Airflow</li> <li>Docker</li> <li>AWS</li> <li>Administer Linux</li> </ul> </li> </ul>"},{"location":"work_organization/all.contributor_scoring.how_to_guide.html#more-detailed-topics-for-candidate-full-and-part-time","title":"More detailed topics for candidate full- and part-time","text":"<ul> <li>When interviewing a candidate / collaborator we want to have more topics</li> </ul>"},{"location":"work_organization/all.contributor_scoring.how_to_guide.html#technical","title":"Technical","text":"<ul> <li>Unit testing</li> <li>Does he / she write good unit tests?</li> <li>Does he / she use coverage to understand what / how to test?</li> <li>Git / GitHub knowledge</li> <li>Linux knowledge</li> <li>Review churning</li> <li>Do the reviews take too long?</li> <li>Python knowledge / coding ability</li> </ul>"},{"location":"work_organization/all.contributor_scoring.how_to_guide.html#process-related","title":"Process-related","text":"<ul> <li>Send good TODO email</li> <li>Follow our PR process</li> <li>Read the docs with attention</li> <li>Follow our organizational process</li> <li>Make and achieve ETAs</li> <li>Autonomy / Independence</li> <li>Does he / she need a lot of supervision to execute the tasks?</li> <li>Productivity</li> <li>Number of hours</li> <li>This is a minor metric: the number of hours doesn't really matter as long as     stuff is done</li> <li>On the other hand, if somebody consistently doesn't put enough time to get     the needed stuff done, it can become a problem</li> </ul>"},{"location":"work_organization/all.datapull_DAGmeister.how_to_guide.html","title":"Datapull Dagmeister","text":""},{"location":"work_organization/all.datapull_DAGmeister.how_to_guide.html#datapull-dagmeister-process","title":"DataPull DagMeister process","text":""},{"location":"work_organization/all.datapull_DAGmeister.how_to_guide.html#general","title":"General","text":"<ul> <li>The DagMeister rotates every 2 weeks</li> <li>To see who is the DagMeister now refer to     DataPull_DagMeister gsheet</li> <li>Each rotation should be confirmed by a 'handshake' between the outgoing     DagMeister and the new one in the related Telegram chat     <code>Kaizen Preprod Datapull Notifications</code></li> <li>Transfer the assignee of     #8785 to new DagMeister</li> <li>The DagMeister is responsible for:</li> <li>Check the Telegram channel for any failures from preprod DAGs.</li> <li>Raising the issue on Github for that failure by debugging the root cause of     the failure.<ul> <li>If the issue is already raised, comment the link of the failure in the   issue citing same reason.</li> <li>All issues should come under single epic   #8785</li> </ul> </li> <li>Tag team leader in the issue to confirm if the issue needs to be fixed with     highest priority or not.</li> <li>All the failures from region <code>tokyo</code> are of highest priority and needs to be     resolved ASAP.</li> </ul>"},{"location":"work_organization/all.datapull_DAGmeister.how_to_guide.html#notification-system","title":"Notification system","text":"<ul> <li><code>@CK_Airflow_bot</code> notifies the team about breaks via Telegram channel   <code>Kaizen Preprod Datapull Notifications</code></li> <li>A notification contains:</li> <li>DAG start timestamp</li> <li>Link fo broken DAG</li> </ul>"},{"location":"work_organization/all.datapull_DAGmeister.how_to_guide.html#dagmeister-instructions","title":"DagMeister instructions","text":"<ul> <li>You receive a break notification from <code>@CK_Airflow_bot</code></li> <li>Have a look at the message</li> <li>Do it right away, this is always your highest priority task</li> <li>Notify the team</li> <li>If the break happened in <code>tokyo</code> region for <code>bid_ask</code> or <code>OHLCV</code> DAGs ping     the channel by tagging the team leader.</li> <li>Reply on the failure to notify you are already looking into this.</li> <li> <p>After the issue is raised reply back with the issue number.</p> <ul> <li>There could be multiple failure due to the same reason so just reply with   same issue number.</li> </ul> </li> <li> <p>File an Issue in GH / ZH to report the failing tests and the errors</p> </li> <li>Paste the URL of the failing run<ul> <li>Example: #9110</li> </ul> </li> <li>Provide as much information as possible to give an understanding of the     problem</li> <li>Stack trace or part of it (if it's too large)</li> <li>Paste the link of QA notebook if QA failed.</li> <li> <p>Add the issue to the     DATAPULL- Fix failing DAGs     Epic so that we can track it</p> </li> <li> <p>Fixing the issue</p> </li> <li>If the bug is obvious and can be fixed easily. Fix it with highest priority.</li> <li> <p>If fixing will require debugging time, tag the team leader to ask for     priority.</p> <ul> <li>IMPORTANT: Disabling a DAG is not the first choice, it's a measure of last   resort! and should oly be done after the approval from the team leader.</li> </ul> </li> <li> <p>When your time of the DAGMeister duties is over, confirm the rotation with the   next responsible person in the related Telegram chat.</p> </li> </ul>"},{"location":"work_organization/all.epicmeister.how_to_guide.html","title":"Epicmeister","text":""},{"location":"work_organization/all.epicmeister.how_to_guide.html#epicmeister-process","title":"EpicMeister Process","text":""},{"location":"work_organization/all.epicmeister.how_to_guide.html#general","title":"General","text":"<ul> <li>EpicMeister ensures that Epics and issues are well-organized and updated,   providing the team with a clear overview of project progress and priorities</li> <li>By assigning Epics to an issue, the EpicMeister establishes a clear   relationship between the larger project goals and the specific tasks,   facilitating a holistic view of project progress and alignment</li> <li>Refer to this   doc   to have a clear understanding of the workflow using GitHub and ZenHub</li> </ul>"},{"location":"work_organization/all.epicmeister.how_to_guide.html#responsibilities","title":"Responsibilities","text":""},{"location":"work_organization/all.epicmeister.how_to_guide.html#epic-management","title":"Epic Management","text":"<ul> <li>Keep this   document   that lists all existing Epics updated</li> <li>When a new Epic is required, after discussion</li> <li>Create it within ZenHub<ul> <li>Make sure the Epics are alphabetically organized on the ZenHub by   right-click and select to sort them</li> </ul> </li> <li>Update the     document</li> <li>Provide a concise title that reflects the nature of the Epic</li> <li>Craft a description that outlines the goals and scope of the Epic</li> <li>Every two weeks create a checklist of the team members in   this issues to make sure   team members are cleaning up the board.</li> </ul>"},{"location":"work_organization/all.epicmeister.how_to_guide.html#issue-organization","title":"Issue Organization","text":"<ul> <li>Ensure that all Issues in GitHub/ZenHub are well-organized</li> <li>Each Issue should:</li> <li>Be associated with an Epic</li> <li>Have an assigned to a team member (Assignee)</li> <li>Be in the appropriate pipeline</li> <li>Estimate has been set properly</li> <li>Review the board once every week (ideally before the Sprint retrospective on   Friday) to make sure everyone follows the procedure</li> <li>Ping the team members to follow the procedure if they are not following</li> <li>Responsibilities of the Issue Creator<ul> <li>Write well-defined specs</li> <li>Assign it to the team member</li> <li>Assign a pipeline to the issue when defined (e.g., <code>Sprint Backlog (P0)</code>,   <code>Product backlog (P1)</code>)</li> <li>Add an Epic to the issue</li> <li>Add it to the sprint</li> </ul> </li> <li>Responsibilities of the Assignee<ul> <li>Make sure the specs are clear and well-defined</li> <li>Make sure all the required fields are filled out by the creator</li> <li>Assign an estimate to the issue</li> <li>Make sure to change the pipeline as the progress is made</li> </ul> </li> <li>Before closing an issue, confirm that it is associated with a relevant Epic   and all the required fields are filled out. If not, ping/guide the creator to   link the issue appropriately</li> </ul>"},{"location":"work_organization/all.rollout.how_to_guide.html","title":"Rollout","text":""},{"location":"work_organization/all.rollout.how_to_guide.html#roll-out-process","title":"Roll-out process","text":"<ul> <li>Implement</li> <li>Prepare documentation</li> <li>Dogfood by RPs or subset of target audience</li> <li>Initial roll-out</li> <li>Full rollout: Distribute it your team</li> <li>Deprecate the old system</li> <li>The deadline is blah: do it!</li> <li> <p>Shut down from old</p> </li> <li> <p>File an Issue with the content of the email</p> </li> <li>The assignee is the person in charge of making sure the rollout is done</li> <li>Send an ORG email with the same content of the Issue</li> </ul>"},{"location":"work_organization/all.rollout.how_to_guide.html#roll-out-documentation","title":"Roll-out documentation","text":"<ul> <li>A roll-out should address the following points:</li> <li>Short summary</li> <li>Who is the intended audience</li> <li>What you need to do</li> <li>Where is the reference documentation</li> <li>What has changed</li> <li>Why is it important</li> <li>Whom to ask for help</li> </ul>"},{"location":"work_organization/all.rollout.how_to_guide.html#an-example-of-roll-out-email","title":"An example of roll-out email","text":"<pre><code>Hello team,\n\n### Intended audience\n\nAnybody using Jupyter notebooks\n\n### What it is about\n\n- `publish_notebook.py` is a little tool that allows to:\n  1. Opening a notebook in your browser (useful for read-only mode)\n     - E.g., without having to use Jupyter notebook (which modifies the file in\n       your client) or github preview (which is slow or fails when the notebook\n       is too large)\n  2. Sharing a notebook with others in a simple way\n  3. Pointing to detailed documentation in your analysis Google docs\n  4. Reviewing someone's notebook\n  5. Comparing multiple notebooks against each other in different browser\n     windows\n  6. Taking a snapshot / checkpoint of a notebook as a backup or before making\n     changes\n     - This is a lightweight alternative to \"unit testing\" to capture the\n       desired behavior of a notebook\n     - One can take a snapshot and visually compare multiple notebooks\n       side-by-side for changes\n\nYou can get details by running: `dev_scripts/notebooks/publish_notebook.py -h`\n\n### What you need to do\n\nPlease update your branches from the `master` for all the submodules.\n\nYou can use our shortcut:\n\n&gt; make git_pull\n\n### What has changed\n\nWe've deployed the new service for storing notebooks in HTML format. From now on\n`publish_notebook.py` will work from the Docker container. The new version of\n`publish_notebook.py` works using HTTP protocol and does not require ssh key\nauthorization as it was before We've synchronized all documents. So all old docs\nalready available on the new service The old links http://research:8077/...\nwon't work from now on, we need to replace them with the new ones\n(http://notebook-keeper.p1/...) If you see any link starts with\nhttp://research:8077 replace them with http://notebook-keeper.p1 .\n\n### Reference documentation\n//amp/docs/coding/all.publish_notebook.how_to_guide.md\n</code></pre>"},{"location":"work_organization/all.scrum.explanation.html","title":"Scrum","text":""},{"location":"work_organization/all.scrum.explanation.html#scrum-methodology","title":"Scrum Methodology","text":"<ul> <li>From \"Lacey, The Scrum Field Guide: Practical Advice for Your First Year,   2012\"</li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#roles","title":"Roles","text":""},{"location":"work_organization/all.scrum.explanation.html#goal-of-scrum-methodology","title":"Goal of Scrum methodology","text":"<ul> <li>Work in the interests of customers and stakeholders to turn the vision into a   working product</li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#metaphor-for-the-roles-in-terms-of-a-race-car","title":"Metaphor for the roles in terms of a race car","text":"<ul> <li> <p>ProductOwner = driver</p> </li> <li> <p>DevTeam = engine</p> </li> <li> <p>ScrumMaster = lubricants and sensors</p> </li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#scrummaster","title":"ScrumMaster","text":"<ul> <li> <p>Identify when the team is not performing to its ability</p> </li> <li> <p>Assist in correcting the issues</p> </li> <li> <p>Notice non-verbal cues</p> </li> <li> <p>Is comfortable with conflict</p> </li> <li> <p>Can build trust and earn respect</p> </li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#productowner","title":"ProductOwner","text":"<ul> <li> <p>Represent the customers</p> </li> <li> <p>Point the car in the correct direction</p> </li> <li> <p>Adjust the car direction to stay on course</p> </li> <li> <p>Make decisions about official release</p> </li> <li> <p>Ultimately he is responsible for success or failure of the projects</p> </li> <li> <p>Decide:</p> </li> <li>What is developed</li> <li>When it is developed</li> <li>Whether the product meets expectations</li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#devteam","title":"DevTeam","text":"<ul> <li> <p>Aka Team, Development team, Core team</p> </li> <li> <p>Developers, testers, architects, designers</p> </li> <li> <p>Cross-functionality is a good thing</p> </li> <li> <p>The ideal team size is 6 plus / minus 2</p> </li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#artifacts","title":"Artifacts","text":""},{"location":"work_organization/all.scrum.explanation.html#product-backlog","title":"Product backlog","text":"<ul> <li> <p>= master list of all features and functionalities needed to implement the   vision into the product</p> </li> <li> <p>The ProductOwner keeps the backlog:</p> </li> <li>Prioritized</li> <li>Up to date</li> <li> <p>Clear</p> </li> <li> <p>The backlog is never complete:</p> </li> <li>Items are added and removed</li> <li>Reordered based on priority, value, or risk</li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#product-backlog-items","title":"Product backlog items","text":"<ul> <li> <p>Aka PBI</p> </li> <li> <p>E.g., bugs, features, enhancements, non-functional requirements</p> </li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#complexity-of-pbi","title":"Complexity of PBI","text":"<ul> <li> <p>ProductOwner and the DevTeam estimate the size of each task</p> </li> <li> <p>The complexity of each task can be expressed in different ways:</p> </li> <li>Points</li> <li>T-shirt size (S, M, L, XL)</li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#high-priority-vs-lower-priority-tasks","title":"High-priority vs lower-priority tasks","text":"<ul> <li>High-priority stories should be small and clear</li> <li> <p>So they can be brought into the sprint</p> </li> <li> <p>Lower-priority items can be large and fuzzy</p> </li> <li>Bigger stories are decomposed into smaller chunks</li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#sprint-backlog","title":"Sprint backlog","text":"<ul> <li> <p>= output of the planning meeting</p> </li> <li> <p>List of tasks that need to complete during the sprint</p> </li> <li> <p>Sprint backlog tasks have an estimate in hours</p> </li> <li> <p>The DevTeam keeps the sprint backlog up to date</p> </li> <li> <p>During a sprint</p> </li> <li>New tasks are discovered</li> <li>Tasks are adjusted (in terms of description or estimated hours)</li> <li>Tasks are marked as done</li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#the-burndown","title":"The burndown","text":"<ul> <li> <p>Communicate how much work is remaining and what is the team velocity</p> </li> <li> <p>It is updated at the end of each day</p> </li> <li> <p>Plot the number of hours remaining (y-axis) against the number of days   remaining (x-axis)</p> </li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#the-meetings","title":"The meetings","text":""},{"location":"work_organization/all.scrum.explanation.html#planning-meeting","title":"Planning meeting","text":"<ul> <li>Each sprint begins with a sprint planning attended by the team, ScrumMaster,   ProductOwner</li> <li>Typically one needs two hours per number of weeks to plan the sprint</li> <li>For a 1-month sprint, 8 hours of meeting</li> <li>For 2-week sprint, 4 hours of meeting</li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#part-one-of-sprint-planning-meeting","title":"Part one of sprint planning meeting","text":"<ul> <li> <p>Review of potential product backlog items for the sprint</p> </li> <li> <p>ProductOwner describes what the goal of the meeting is</p> </li> <li> <p>DevTeam asks questions to drive away ambiguity</p> </li> <li> <p>Outcome is one-sentence description of the desired outcome of the sprint</p> </li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#part-two-of-sprint-planning-meeting","title":"Part two of sprint planning meeting","text":"<ul> <li> <p>Many DevTeams discuss how to implement the tasks</p> </li> <li> <p>The ProductOwner doesn't need to be present</p> </li> <li> <p>The ScrumMaster can be present facilitating the process</p> </li> <li> <p>The DevTeam discusses and decides the implementation of the tasks</p> </li> <li> <p>Decompose backlog items into work tasks</p> </li> <li> <p>Estimate tasks in terms of hours</p> </li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#daily-scrum","title":"Daily scrum","text":"<ul> <li> <p>Aka daily stand-up</p> </li> <li> <p>Give the DevTeam the opportunity to sync daily, at the same time, and at the   same place</p> </li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#daily-scrum-questions","title":"Daily scrum: questions","text":"<ul> <li>The 3 most frequent questions are:</li> <li>What have you accomplished since the last meeting?</li> <li>What will you accomplish today?</li> <li>What obstacles are in your way?</li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#what-the-daily-scrum-is-not","title":"What the daily scrum is not","text":"<ul> <li>The daily scrum is not a deep-dive problem-solving meeting</li> <li> <p>Any other issues need to be taken offline</p> </li> <li> <p>It is not a status report meeting to the ScrumMaster</p> </li> <li> <p>The purpose is for the DevTeam members to talk to each other</p> </li> <li> <p>The ProductOwner is in \"listen-only\" mode</p> </li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#sprint-review","title":"Sprint review","text":"<ul> <li> <p>On the last day of the sprint, the DevTeam holds a sprint review</p> </li> <li> <p>Everybody should join</p> </li> <li>ScrumMaster</li> <li>ProductOwner</li> <li>DevTeam</li> <li>Customers, key stakeholders</li> <li> <p>Executives</p> </li> <li> <p>DevTeam</p> </li> <li>Recaps the goal of the sprint</li> <li> <p>Presents the work done</p> </li> <li> <p>Customers</p> </li> <li>Review the progress made on the project</li> <li>Accept changes</li> <li>Ask for changes</li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#sprint-retrospective","title":"Sprint retrospective","text":"<ul> <li>After the sprint review, the retrospective is a way to identify how to improve   process and execution</li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#sprint-retrospective-questions","title":"Sprint retrospective: questions","text":"<ul> <li> <p>What went well during the sprint?</p> </li> <li> <p>What could be improved in the next sprint?</p> </li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html","title":"Team Collaboration","text":""},{"location":"work_organization/all.team_collaboration.how_to_guide.html#general-rules-of-collaboration","title":"General Rules of Collaboration","text":""},{"location":"work_organization/all.team_collaboration.how_to_guide.html#ask-somebody-if-you-have-any-doubts","title":"Ask somebody if you have any doubts","text":"<ul> <li>If you have doubts on how to do something you want to do:</li> <li>Look in the     documentation and     our     Google drive</li> <li>Google search is your friend</li> <li>Ask your team-members<ul> <li>Learn   how to ask questions   first</li> <li>Note that often people tell you his / her interpretation or their   workaround for a problem, which might not be the best approach, so be   careful and always think for yourself</li> <li>Don't hesitate to ask anyone, even GP &amp; Paul</li> </ul> </li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#ping-team-leaders-when-you-are-out-of-tasks","title":"Ping Team Leaders when you are out of tasks","text":"<ul> <li>When you're close to being out of tasks or all your ongoing PRs are waiting   for review and are close to being merged, feel free to ping us in the Telegram   chat to ask for more issues</li> <li>In this way, Team Leaders can quickly assign you another issue, before you     run out of work</li> <li>The goal is for everyone to have 2 issues to work on at the same time to     avoid getting blocked on us</li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#collaboration","title":"Collaboration","text":""},{"location":"work_organization/all.team_collaboration.how_to_guide.html#why-do-we-need-to-follow-this-handbook","title":"Why do we need to follow this handbook?","text":""},{"location":"work_organization/all.team_collaboration.how_to_guide.html#learning-from-each-other","title":"Learning from each other","text":"<ul> <li>Proper research and software engineering practices allow us to:</li> <li>Learn from each other</li> <li>Accumulate and distill the wisdom of experts</li> <li>Share lessons learned from our mistakes along the way</li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#consistency-and-process","title":"Consistency and process","text":"<ul> <li>Consistency is a crucial enabler to make teams faster</li> <li>Productivity increases when team members \"work in the same way\", i.e., there   is a single official way of performing a task, so that it's possible and easy   to:</li> <li>Re-use research and software components</li> <li>Help each other in debugging issues</li> <li>Add/transfer new people to teams</li> <li>Work on multiple projects simultaneously</li> <li>Learn from each other's experience and mistakes</li> <li>Review each other's work looking for errors and improvements</li> <li>...</li> <li>We are not going to discuss and debate the rationale, but instead assume the   above as self-evident truth</li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#sync-ups","title":"Sync-ups","text":"<ul> <li>We meet regularly every week and with different audiences to check on the   progress of the many projects we work on</li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#all-hands-meetings","title":"All-hands meetings","text":"<ul> <li>All-hands meeting on Mondays has the following goals:</li> <li>Summarize ongoing projects and their status<ul> <li>Review the ZenHub board and share the achieved milestones</li> <li>Discuss blocking tasks across projects</li> </ul> </li> <li>Discuss topics of general interest<ul> <li>E.g., organization, process</li> </ul> </li> <li>Talk about the team, hiring, customers</li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#technical-sync-ups","title":"Technical sync-ups","text":"<ul> <li>We meet one or two times per week for each of the projects (e.g., IM, WEB3)</li> <li>Please check your calendar to make sure the times work and the invited   participants are correct</li> <li>The people running the day-to-day project should update the meeting agenda in   the Gdoc</li> <li>Try to do it one day before so that everybody knows ahead of time what we     need to talk about and can come prepared</li> <li>Typically 2-3 issues are more than enough to fill one hour of discussion</li> <li>Give priority to tasks that are controversial, blocking, or finished</li> <li>No reason to linger on the successes or the easy stuff</li> <li>Send an email or tag a comment to Gdocs to broadcast the agenda</li> <li>It's ok to skip a meeting when the agenda is empty, or keep it short when   there is not much to discuss</li> <li>We don't have to fill one hour every time</li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#ad-hoc-meetings","title":"Ad-hoc meetings","text":"<ul> <li>Don't hesitate to ask for a quick meeting if you are unsure about:</li> <li>What exactly needs to be done in a GitHub Issue</li> <li>How to set-up something (e.g., environment, docker)</li> <li>Better safe than sorry</li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#org-emails","title":"Org emails","text":"<ul> <li>GP &amp; Paul may send emails with the subject starting with \"ORG:\" pointing to   interesting docs that are of general interest and relevance</li> <li>Please make sure to read the docs carefully and internalize what we suggest to   do and, especially, the rationale of the proposed solutions</li> <li>It's ok to acknowledge the email replying to <code>all@kaizen-tech.io</code></li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#synchronization-point","title":"Synchronization point","text":"<ul> <li>We understand that most of the time everybody is head-down making progress on   their tasks</li> <li>This is great!</li> <li>However, sometimes we need synchronization:</li> <li>We need to stop the progress for a bit when requested</li> <li>Do an urgent task</li> <li>Acknowledge that the task is done</li> <li>Go back to pushing</li> <li>The procedure is:</li> <li>One of us (e.g., GP or Paul) creates a GitHub task, with:<ul> <li>Detailed instructions</li> <li>The list of all of the persons in charge of executing the task</li> </ul> </li> <li>Send a ping with the link on Telegram if the task is urgent</li> <li>Everybody does what's asked</li> <li>Mark on the GitHub task your name</li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#morning-todo-email","title":"Morning TODO email","text":"<p>The idea is to send a morning TODO email to broadcast:</p> <ul> <li>Issues you will be working on</li> <li>Working hours</li> <li>Blocking issues/PRs</li> </ul> <p>E-mail template:</p> <pre><code>To: all@kaizen-tech.io\n\nSubject: TODO\n\nHi all,\n\nToday I am going to work\n\nHours:\n- ...\n\nIssues:\n- GitHub issue title and number with a hyperlink\n   - Original ETA: YYYY-MM-DD\n   - ETA: YYYY-MM-DD\n   - Reason for ETA update: ...\n   - Blocked on: ... (ok to omit if not blocked)\n- GitHub issue title and number with a hyperlink\n   - Original ETA: YYYY-MM-DD\n   - ETA: YYYY-MM-DD\n   - Reason for ETA update: ...\n   - Blocked on: ... (ok to omit if not blocked)\n</code></pre> <p>Good example:</p> <pre><code>To: all@kaizen-tech.io\n\nSubject: TODO\n\nHi all,\n\nToday I am going to work\n\nHours:\n- 8\n\nIssues:\n- Create matching service #261\n    - ETA: today (2023-05-25)\n    - Original ETA: yesterday (2023-05-24)\n    - Reason for ETA update: it was more complex than what we thought\n    - Blocked on: PR review from Grisha\n- Create linear supply/demand curves #177\n    - ETA: tomorrow (2023-05-26)\n- Unit test `compute_share_prices_and_slippage()` #8482\n    - ETA Wednesday (06-19-2024)\n</code></pre> <p>Bad example:</p> <pre><code>To: all@kaizen-tech.io\n\nSubject: to-do\n\nHi all,\n\nToday I am going to work 2-6 hours.\n\n- Calls\n- PR reviews\n- Finish the trading report analysis\n</code></pre> <ul> <li>The goal is:</li> <li>Think about what you are going to work on for the day, so you have a clear     plan</li> <li>Let Team Leaders know that you're going work today and what is your workload</li> <li>Make sure people blocked on your tasks know that / whether you are working     on those tasks</li> <li>Broadcast if you are blocked or if you don't have tasks</li> <li>A TODO email replaces stand-up meetings</li> <li>When to send an email:</li> <li>Within the first hour of beginning the day</li> <li>It is recommended to check the inbox (email, GitHub) first to reflect all     the update in the plan for the day</li> <li>Send an email to:</li> <li>For full-time employees/contractors: <code>all@kaizen-tech.io</code></li> <li>For interns or collaborators: <code>contributors@crypto-kaizen.com</code></li> <li>Email subject:</li> <li><code>TODO</code> (all caps)</li> <li>It easier to filter emails by subject</li> <li>Reply to your previous TODO email so that at least last week of TODOs is   included</li> <li>Specify how many hours are you going to work today</li> <li>On Monday we also specify an estimation for a week</li> <li>No need to be too specific, give just an approximation</li> <li>List all the tasks you're going to work during the day in priority order</li> <li>Add a hyperlink to the corresponding GitHub issue to each task in order to     ease the navigation     </li> <li>For each task provide an ETA<ul> <li>No reason to be optimistic: complex things take time to be done correctly</li> <li>Use a date in the usual format, e.g. 2023-05-10</li> <li>Add \"today\", \"yesterday\", \"tomorrow\", \"end of week\" so that it's easier to   parse</li> <li>If your original ETA needs to be updated (e.g., you thought that you would   have finished a task by yesterday, but it's taking longer) keep the older   ETA and add the new one</li> </ul> </li> <li>Report the PR/issue blocked</li> </ul> <p>If you are a collaborator or intern, follow the steps to join the mailing group</p> <ul> <li>Visit the   group</li> <li>Click \u201cask to join group\u201d</li> </ul> <p></p> <ul> <li>Choose the following settings</li> </ul> <p></p> <ul> <li>Wait for the confirmation e-mail, one for the group managers will approve your   request. It should look like this:   </li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#communication","title":"Communication","text":""},{"location":"work_organization/all.team_collaboration.how_to_guide.html#use-the-right-form-of-communication","title":"Use the right form of communication","text":"<ul> <li>GitHub</li> <li>This is a major form of communication about technical details, so if you     have any questions about any particular issue or PR, discuss it there first,     e.g.:<ul> <li>Clarify issue specs</li> <li>Ask for help with debugging</li> <li>PR reviews</li> </ul> </li> <li>Asana</li> <li>Is concerned with all the non-technical issues in general, e.g.:<ul> <li>Work organization tasks</li> <li>Marketing and funding</li> <li>On-boarding process</li> </ul> </li> <li>Telegram</li> <li>This is our messenger for tight interaction (like a debug session) or     immediacy (e.g., \"are you ready for the sync up?\")</li> <li>Please, avoid discussing anything that can be discussed at GitHub or Asana<ul> <li>You often need to reference some comments and ideas in other places like   issues or messages and it is impossible to reference a Telegram message   outside of it</li> <li>It is much easier for all reviewers to catch up with all the thinking   process if it is logged at one place - Telegram is never a place for this</li> </ul> </li> <li>Jupyter notebooks</li> <li>Generally used to implement and describe research in detail<ul> <li><code>Master</code> notebooks are intended to be used as tools for demonstrative   analysis with visible stats and plots</li> </ul> </li> <li>Markdown files</li> <li>Document instructions, process, design closely related to code</li> <li>Notes that need to be close to the code itself</li> <li>Documents that need to be authoritative and long-term (e.g., reviewed,     tracked carefully)</li> <li>Google docs</li> <li>Document research in a descriptive way</li> <li>Explain what are the results independently on how they were reached</li> <li>Emails</li> <li><code>TODO</code>s primarily</li> <li>Rarely used for any other purpose</li> <li>Exceptions are to send non-urgent information to everybody</li> <li>There should be little replication among these forms of documentation</li> <li>It's not ok to file a bug and then ping on Telegram unless it's urgent</li> <li>Google Form</li> <li>When you want to ask a question anonymously use     https://forms.gle/KMQgobqbyxhoTR9n6</li> <li>The question will be discussed at the all hands meeting</li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#dry-also-applies-to-documentation","title":"DRY also applies to documentation","text":"<ul> <li>DRY! Do not Repeat   Yourself</li> <li>E.g., it's not a good idea to cut &amp; paste pieces of Gdocs in a GitHub bug,   rather just point to the relevant session on Gdocs from the GitHub bug</li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#avoid-write-once-code-and-research","title":"Avoid write-once code and research","text":"<ul> <li>Code and research is:</li> <li>Written once by a few people</li> <li>Read many times by many people</li> <li>Therefore it is essential to invest in the process of writing it heavily</li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#consistency","title":"Consistency","text":"<ul> <li>Coding/research across our group is done with consistent procedures, code   layout, and naming conventions</li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#training-period","title":"Training period","text":"<ul> <li>When you start working with us, you need to go through a period of training in   following the procedures and conventions described in this handbook</li> <li>We understand that this is a painful process for you:</li> <li>You need to change your old habits for new habits that you might disagree     with, or not comprehend</li> <li>You need to rework your code/notebooks that are already correctly working     until it adheres to the new conventions</li> <li>Understand that this is also a painful process for the reviewers:</li> <li>On top of their usual workload, they need to:<ul> <li>Invest time to explain to you how we do things</li> <li>Answer your questions</li> <li>Try to convey the sense of why these procedures are important</li> </ul> </li> <li>In a few words, nobody enjoys this process, and yet it is necessary,   mandatory, and even beneficial</li> <li>The process acquaintance can take several days if you are open and patient,   but months if you resist or treat it as an afterthought</li> <li>Our suggestion is to accept these rules as the existence of gravity</li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#go-slowly-to-go-faster","title":"Go slowly to go faster","text":"<ul> <li>Once you reach proficiency, you will be moving much faster and make up for the   invested time</li> <li>In fact, everyone will be much quicker, because everyone will be able to     look at any part of the codebase or any notebook and get oriented quickly</li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#vacationsooto-time","title":"Vacations/OOTO time","text":"<ul> <li>We use   vacation calendar   to announce time off</li> <li>If you are a part of @all mailing group you should be able to access this   calendar with your company email</li> <li>Create an event in it, whenever you have planned time off in order to let your   colleagues know in advance</li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#improve-your-english","title":"Improve your English!","text":"<ul> <li>Make sure you have English checker in all your tools:</li> <li>Pycharm: you can use     this plugin</li> <li>Vim: <code>set spell</code></li> <li>Google docs: Grammarly</li> <li>GitHub and web: Grammarly</li> <li>Email client: TBD</li> <li>These tools are going to help you improve since you can see the mistake as you   go</li> <li>Feel free to use Google Translate when you   are not sure about a word or a phrase</li> <li>What's the point of doing an excellent job if you can't communicate it?</li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#study-an-english-grammar-book","title":"Study an English grammar book","text":"<ul> <li>I used   this   when I learned English (late in life at 25 starting from   no-English-whatsoever, so you can do it too)</li> </ul>"},{"location":"work_organization/all.use_github_and_zenhub.how_to_guide.html","title":"Use Github And Zenhub","text":""},{"location":"work_organization/all.use_github_and_zenhub.how_to_guide.html#introduction","title":"Introduction","text":"<ul> <li>In the following we use the abbreviations below:</li> <li>GH = GitHub</li> <li>ZH = ZenHub</li> <li>PR = Pull Request</li> <li>RP = Responsible party (aka Team Leader)</li> <li>Everything we work on comes in the form of a GH Issues</li> <li>We call GH Issues \"issues\", and \"tasks\", (sometimes \"tickets\")     interchangeably</li> <li>We avoid to call them bugs since many times we use GH to track ideas,     activities, and improvements, and not only defects in the code</li> <li>We file tasks, prioritize them, and distribute the workload across the team</li> <li>We try to always work on high priority (aka, P0) tasks</li> <li>We use   ZenHub   as project management layer on top of GH</li> <li>Please install the ZH extension for GH,     since it is going to make your life easier</li> </ul>"},{"location":"work_organization/all.use_github_and_zenhub.how_to_guide.html#concepts","title":"Concepts","text":""},{"location":"work_organization/all.use_github_and_zenhub.how_to_guide.html#sprints","title":"Sprints","text":"<ul> <li>Sprints are weekly, Monday - Friday and consist of the Issues worked on during   the week</li> <li>Sprints help us answer the questions</li> <li>What work should the team be focusing during this week?</li> <li>What did the team achieve last week?</li> <li>Anything worked on during a week is added to that week's Sprint</li> <li>Issues added to a Sprint but not worked on or completed during the week should   not be removed (Issues can belong to more than one Sprint, and not removing   helps measure how \u201coverloaded\u201d a Sprint was)</li> <li>Each week's Sprint has Issues added to it by Team Leaders before Monday's work   begins</li> <li>Every Issue in a Sprint should have</li> <li>A point estimate</li> <li>An Epic</li> <li>The Team Member working on the Issue sets the point estimate by themselves or   together with the Team Leader</li> <li>Each sprint should have limits based on the estimates. E.g., a sprint cannot   have issues worth more than 30 (arbitrary hyper-parameter) story points</li> <li>To support adaptability and flexibility, Issues may be added to a Sprint   mid-week (but Issues should not be removed). While this may require   sacrificing other Issues in the Sprint, the point is to make the trade-off   apparent</li> </ul>"},{"location":"work_organization/all.use_github_and_zenhub.how_to_guide.html#epics","title":"Epics","text":"<ul> <li>Epics are thematic groups of Issues that are somehow related by their topic</li> <li>It may take multiple Sprints to complete all the Issues in an Epic</li> <li>Most Epics are created around software components or user workflows (which     may cross software components)</li> <li>Epics help us answer the questions</li> <li>What features is the team implementing?</li> <li>What is the team's current focus?</li> <li>Epics with a high level of activity are versioned and have a well-defined   purpose</li> <li>We distinguish Master Epics (e.g., <code>WEB3</code>) and sub-Epics (e.g.,     <code>WEB3 - DaoCross v0.1</code>)</li> <li>We maintain all the information about what the Epic is about in its   description</li> <li>We keep the Epics alphabetized on the board under the pipeline \"EPICs\"</li> </ul>"},{"location":"work_organization/all.use_github_and_zenhub.how_to_guide.html#master-epics","title":"Master Epics","text":"<ul> <li>Master Epics are long-running Epics (i.e., projects)</li> <li>E.g., <code>WEB3</code></li> <li>They are capitalized (e.g., <code>WEB3</code> and not <code>web3</code> or <code>Web3</code>)</li> <li>Each Issue should belong to at least one Epic, either a sub-Epic or a master   Epic</li> <li>There is no need to add an Issue to a Master Epic if it is already added to     a sub-Epic, since this is handled automatically by ZH</li> </ul>"},{"location":"work_organization/all.use_github_and_zenhub.how_to_guide.html#sub-epics","title":"Sub-Epics","text":"<ul> <li>Master Epics can be broken down into smaller Epics, called \"sub-Epics\"</li> <li>E.g., <code>WEB3 - DaoCross v0.1</code></li> <li>Their titles should follow the pattern: <code>XYZ - ABC vM.N</code>, where:</li> <li><code>XYZ</code>: master Epic title (capitalized)</li> <li><code>ABC</code>: sub-Epic title (camel case or snake case)</li> <li><code>vM.N</code>: version</li> <li>Sub-Epics should have a short title and a smaller scope</li> <li>Some sub-Epics are related to short term milestones or releases (e.g.,   <code>WEB3 - DaoCross v0.1</code>), other sub-Epics are for long-running activities   (e.g., <code>WEB3 - DaoCross</code>)</li> <li>Sub-Epics should belong to a Master Epic in ZH so that querying by Epic and   sub-epics is simplified</li> </ul>"},{"location":"work_organization/all.use_github_and_zenhub.how_to_guide.html#list-of-epics","title":"List of Epics","text":"<ul> <li>Below there is a list of the Epics and sub-Epics</li> <li> <p>For simplicity we keep the information about the Epics here, instead of     ZenHub</p> </li> <li> <p>Epics can be determined based on:</p> </li> <li>Software components<ul> <li>E.g., <code>CORE</code> covers code under <code>//amp/core</code></li> <li>We should try to have a direct mapping between software components and   directories</li> </ul> </li> <li>Theme<ul> <li>E.g., <code>TRADE_EXECUTION</code> covers code related to multiple software   components</li> </ul> </li> <li>Sometimes an Issue could go in different Epics</li> <li>Typically from the Epic should be easy to find the assignee in terms of team   leader that can then re-route the Issue to somebody on their team</li> <li>Epics can be qualified as <code>Dev</code> or <code>Research</code> depending if they are related   more to implementation or to research / experimentation</li> </ul> <p>TODO(all): Review and update</p> <ul> <li><code>BUILD</code></li> <li>Description: anything related to the build / regression system</li> <li><code>Breaks</code><ul> <li>Description: Report breaks in the GH action system or locally</li> <li>We follow the Buildmeister procedures</li> </ul> </li> <li><code>GH Actions</code><ul> <li>Description: Improvements to the build system</li> </ul> </li> <li><code>CODEBASE</code></li> <li>Description: anything that relates to the entire code base</li> <li>E.g., a global replace of <code>act</code> and <code>exp</code></li> <li><code>CONFIG</code></li> <li>Description: Anything related to <code>config</code> implementation</li> <li><code>CORE</code></li> <li>Description: Anything in <code>//amp/core</code> (but not dataflow)</li> <li><code>DATAFLOW</code></li> <li>Description: Anything related to <code>//amp/core/dataflow</code></li> <li><code>DATAPULL</code></li> <li>Description: Everything related to Instrument Master, data extraction, data     on-boarding</li> <li><code>DEV_TOOLS</code></li> <li>Description: Anything related to (non-research) tools we use</li> <li><code>Docker</code><ul> <li>Anything related to improvements and fixes to the docker workflow</li> <li>Example: #19,   #28,   #30</li> </ul> </li> <li><code>Thin_env</code><ul> <li>Description: Anything related to improving the thin env in   <code>dev_scripts/client_setup/</code></li> </ul> </li> <li><code>DOCS</code></li> <li>Description: Anything related to documentation</li> <li><code>Publish documentation</code><ul> <li>Description: Anything related to publishing Google Docs, Markdown to repo</li> </ul> </li> <li><code>HELPERS</code></li> <li>Description: Anything in <code>//amp/helper</code></li> <li><code>HIRING</code></li> <li>Anything related to:<ul> <li>Screening candidates</li> <li>Running interviews</li> <li>Creating job descriptions</li> <li>Finding channels to propagate job descriptions</li> </ul> </li> <li><code>INFRA/DevOps</code></li> <li>Description: Anything related to IT/Cloud infrastructure, code deployment,     CI/CD, monitoring, server administration etc.</li> <li><code>INVOKE</code></li> <li>Description: Anthing related to invoke tasks, their unit testing comes here</li> <li><code>LINTER</code></li> <li>Description: Issues related to the <code>linter</code></li> <li><code>MARKETING</code></li> <li>Description: Issues related to promoting our product, e.g.,<ul> <li>Prepare a demo of the production system</li> <li>Prepare pnl / trades data and send it to investors</li> <li>Prepare a power-point presentation for investors</li> </ul> </li> <li><code>MARKET_DATA</code></li> <li>Description: anything related to MarketData</li> <li><code>MODEL</code></li> <li><code>Alpha research</code><ul> <li>Specific research tasks that we want to perform (e.g., \"test the foobar   hypothesis\")</li> <li>Multiple implementation issues can be filed to support performing a   certain research task (e.g., \"implement this and that\")</li> </ul> </li> <li><code>Backtesting</code><ul> <li>Run experiments about existing alpha models (e.g., tuning parameters,   portfolio construction)</li> <li> </li> <li>Flow for backtesting portfolio construction</li> <li>Backtests and analysis for candidate models</li> </ul> </li> <li><code>Deployment</code><ul> <li>It includes reconciliation, monitoring</li> <li>Flows for deploying models from research to production</li> <li>Support for testing model config changes in production</li> <li>Model reconciliation</li> <li>Pipeline failure debugging</li> </ul> </li> <li><code>Exploratory research</code><ul> <li>E.g., build a notebook about some stylized facts about prices, model</li> </ul> </li> <li><code>ON-BOARDING</code></li> <li>Description: Issues related to on-boarding new team members and improvements     to the on-boarding process</li> <li><code>OPTIMIZER</code></li> <li><code>Dev</code><ul> <li>Features and config for optimizer</li> <li>Optimizer testing</li> </ul> </li> <li><code>Research</code></li> <li><code>PROCESS</code></li> <li>Description: Anything meta (e.g., conventions)</li> <li><code>EpicMeister</code><ul> <li>Description: Anything related to improving the epicmeister process</li> </ul> </li> <li><code>PROD</code></li> <li>Description: Anything related to releasing to production</li> <li><code>TRADE_EXECUTION</code></li> <li><code>Dev</code><ul> <li>Description: it's about anything related to software engineering (e.g.,   implementation of new features, adding testing, fixing bugs, refactoring)</li> <li>E.g., create a new broker</li> </ul> </li> <li><code>Research</code><ul> <li>Description: i.e., data science on execution experiment</li> <li>E.g., notebooks about bid / ask</li> </ul> </li> <li><code>UNIVERSE</code></li> <li>Description: Related to on-boarding / exploring a new universe</li> <li><code>WEB3</code></li> <li>Description: Anything related to Web3 tasks</li> <li><code>Arbitrage</code></li> <li><code>Tulip</code></li> <li><code>Utils</code></li> </ul>"},{"location":"work_organization/all.use_github_and_zenhub.how_to_guide.html#flows-for-backtesting-alpha","title":"Flows for backtesting alpha \u203a\u203a","text":""},{"location":"work_organization/all.use_github_and_zenhub.how_to_guide.html#issue","title":"Issue","text":"<ul> <li>Each Issue is a piece of work to be done</li> <li>Issues are combined into Epics by topic</li> <li>An Issue has certain characteristics, i.e. labels</li> <li>An Issue has a progress status, i.e. ZH pipeline (e.g.,   <code>Product backlog (P1)</code>, <code>In progress</code>, <code>Done/Pre-prod</code>)</li> <li>PRs are linked to work needed to complete an Issue</li> <li>An issue might not have an assignee and estimate if it is not inside an epic   but before execution of course it needs to be resolved</li> </ul>"},{"location":"work_organization/all.use_github_and_zenhub.how_to_guide.html#milestone","title":"Milestone","text":"<ul> <li>Milestone consist of group of tasks we want to accomplish during certain   period</li> <li>We plan for milestone in advance and every milestone is 4 to 5 months long</li> <li>We select Epics to work on in the particular milestone, add issues to it and   assign a start date and an estimated end date to the epic</li> <li>Every Epic, pre-planned or newly filed, must have an estimated start and end   date if it belongs to current milestone</li> <li>Epics can also have dates from future as well to get clear estimation of the     milestone</li> <li>Not having a start and end date to an Epic is fine if it does not belong to     the current milestone</li> </ul>"},{"location":"work_organization/all.use_github_and_zenhub.how_to_guide.html#label","title":"Label","text":"<ul> <li>Labels are attributes of an issue (or PR), e.g., <code>good first issue</code>,   <code>PR_for_reviewers</code>, <code>duplicate</code>, etc.</li> <li>See the current list of labels and their descriptions are   Sorrentum and   cmamp</li> <li>The repos should always have labels in sync</li> </ul>"},{"location":"work_organization/all.use_github_and_zenhub.how_to_guide.html#list-of-labels","title":"List of labels","text":"<ul> <li><code>Blocking</code>: This issue needs to be worked on immediately</li> <li><code>Bug</code>: Something isn't working</li> <li><code>Cleanup</code>:</li> <li><code>Design</code></li> <li><code>Documentation</code>: Improvements or additions to documentation</li> <li><code>Enhancement</code>: New feature or request</li> <li><code>Epic</code></li> <li><code>good first issue</code>: Good for newcomers <p>TODO(gp): <code>Good_first_issue</code></p> </li> <li><code>Outsource</code>: Anybody can do it</li> <li><code>P0</code>: Issue with a high priority</li> <li><code>P1</code>: Issue is important but has less priority</li> <li><code>P2</code>: Issue whcih is not urgent and has the lowest priorty</li> <li><code>Paused</code>: An issue was started and then stopped</li> <li><code>PR_for_authors</code>: The PR needs authors to make changes</li> <li><code>PR_for_reviewers</code>: The PR needs to be reviewed by team leaders <p>TODO(gp): -&gt; <code>PR_for_team_leaders</code></p> </li> <li><code>PR_for_integrators</code>: The PR needs to be reviewed by Integrators and possibly   merged</li> <li><code>Readings</code>: Reading a book, article and getting familiar with code</li> <li><code>To close</code>: An issue can be potentially closed <p>TODO(gp): -&gt; To_close</p> </li> </ul>"},{"location":"work_organization/all.use_github_and_zenhub.how_to_guide.html#pipeline","title":"Pipeline","text":"<ul> <li>A ZH Pipeline represents the \"progress\" status of an Issue in our process</li> <li>We have the following Pipelines on the ZH board:</li> <li><code>Product Backlog</code><ul> <li>Issues which are new and do not belong to current sprint</li> <li><code>estimate</code> is not mandatory</li> <li>It should at least have Master <code>EPIC</code> (e.g., <code>INFRA</code>), not necessarily but   preferred sub-EPIC (e.g., <code>INFRA - DevOps</code>)</li> <li><code>assignee</code> should be the tech lead of that area</li> </ul> </li> <li><code>Sprint Backlog</code><ul> <li>Issues that should be executed in the this sprint</li> <li>It must have an <code>assignee</code>, <code>epic</code>, <code>sprint</code> and <code>estimate</code></li> </ul> </li> <li><code>In Progress</code><ul> <li>Issues that we are currently working on</li> </ul> </li> <li><code>Review/QA</code>     &gt; TODO(gp): Let's split Review and QA?<ul> <li>Issues opened for review and testing</li> <li>Code is ready to be merged pending feedback</li> <li>Code is ready to be deployed to pre-production</li> </ul> </li> <li><code>Done/Pre-prod</code>     &gt; TODO(gp): Let's split Done and Pre-prod?</li> <li><code>Done/Pre-prod</code> - Issues that are done and are waiting for closing</li> <li><code>Epics</code><ul> <li>Both Master Epics and Sub-Epics</li> </ul> </li> <li><code>Closed</code><ul> <li>Issues that are done and don't need a follow-up</li> <li>Integrators / team leaders are responsible for closing</li> </ul> </li> </ul> <pre><code>stateDiagram\n    [*] --&gt; Product_Backlog\n    Product_Backlog --&gt; Sprint_Backlog\n    Sprint_Backlog --&gt; In_Progress\n    In_Progress --&gt; QA\n    QA --&gt; Done\n    In_Progress --&gt; Pre_prod\n    Pre_prod --&gt; Prod\n    Prod --&gt; Done\n    Done --&gt; [*]\n</code></pre>"},{"location":"work_organization/all.use_github_and_zenhub.how_to_guide.html#issue-estimate","title":"Issue Estimate","text":"<ul> <li>The Issue estimates ranges from 1 to 5:</li> <li>1 (e.g., a function rename, updating the entire code base and the unit     tests)</li> <li>2 (e.g., factoring out a method)</li> <li>3 (e.g., write a unit test for an existing function)</li> <li>4 (e.g., implement a new feature, where the solution is clear in advance)</li> <li>5 (e.g., implement a new feature, where the solution is complex)</li> </ul>"},{"location":"work_organization/all.use_github_and_zenhub.how_to_guide.html#pr","title":"PR","text":"<ul> <li>A pull request is an event where a contributor asks to review code they want   to merge into a project</li> </ul>"},{"location":"work_organization/all.use_github_and_zenhub.how_to_guide.html#issue-workflows","title":"Issue workflows","text":""},{"location":"work_organization/all.use_github_and_zenhub.how_to_guide.html#naming-an-issue","title":"Naming an Issue","text":"<ul> <li>Use an informative description, typically in the form an action</li> <li>E.g., \"Do this and that\"</li> <li>We don't use a period at the end of the title</li> <li>We prefer to avoid too much capitalization to make the Issue title easy to   read and for consistency with the rest of the bugs</li> </ul> <p>Good <code>Optimize Prometheus configuration for enhanced Kubernetes monitoring</code></p> <p>Bad <code>Optimize Prometheus Configuration for Enhanced Kubernetes Monitoring</code></p> <ul> <li>They are equivalent, but the first one is more readable</li> </ul>"},{"location":"work_organization/all.use_github_and_zenhub.how_to_guide.html#filing-a-new-issue","title":"Filing a new issue","text":"<ul> <li>If it is a \"serious\" problem (bug) put as much information about the Issue as   possible, e.g.,:</li> <li>What you are trying to achieve</li> <li>Command line you ran, e.g.,     <code>&gt; i lint -f defi/tulip/test/test_dao_cross_sol.py</code></li> <li>Copy-paste the error and the stack trace from the command line, no     screenshots, e.g.,     <code>Traceback (most recent call last):       File \"/venv/bin/invoke\", line 8, in &lt;module&gt;         sys.exit(program.run())       File \"/venv/lib/python3.8/site-packages/invoke/program.py\", line 373, in run         self.parse_collection()     ValueError: One and only one set-up config should be true:</code></li> <li>The log of the run<ul> <li>Maybe the same run using <code>-v DEBUG</code> to get more info on the problem</li> </ul> </li> <li>What the problem is</li> <li>Why the outcome is different from what you expected</li> <li>Issue in the current sprint cannot be <code>P1</code> because <code>P0</code> is a sprint backlog   and an issue is not in a sprint backlog, it should not be in a sprint.</li> <li>Use check boxes for \"small\" actions that need to be tracked in the Issue (not   worth their own Issue)</li> <li>An issue should be closed only after all the checkboxes have been addressed     OR the remaining checkboxes were either transformed into new issues in their     own right (e.g. if the implementation turned out to be more complex than     initially thought) OR have a reason for not being implemented</li> <li>These things should be mentioned explictly before closing the issue (Element     of Least Surprise.).</li> <li>We use the <code>FYI @...</code> syntax to add \"watchers\"</li> <li>E.g., <code>FYI @cryptomtc</code> so that he receives notifications for this issue</li> <li>Authors and assignees receive all the emails in any case</li> <li>In general everybody should be subscribed to receiving all the notifications     and you can quickly go through them to know what's happening around you</li> <li>Assign an Issue to the right person for re-routing</li> <li>There should be a single assignee to a Issue so we know who needs to do the     work</li> <li>Assign Integrators / Team leaders if not sure</li> <li>Assign an Issue to one of the pipelines, ideally based on the urgency</li> <li>If you are not sure, leave it unassigned but <code>@tag</code> Integrators / team leaders   to make sure we can take care of it</li> <li>Assign an Issue to the right Epic and Label</li> <li>Use <code>Blocking</code> label when an issue needs to be handled immediately, i.e. it     prevents you from making progress</li> <li>If you are unsure then you can leave it empty, but <code>@tag</code> Integrator / team     leaders to make sure we can re-route and improve the Epics/Labels</li> </ul>"},{"location":"work_organization/all.use_github_and_zenhub.how_to_guide.html#updating-an-issue","title":"Updating an issue","text":"<ul> <li>For large or complex Issues, there should be a design phase (in the form of GH   Issue, Google Doc, or design PR) before starting to write a code</li> <li>A team leader / integrator should review the design</li> <li>When you start working on an Issue, move it to the <code>In Progress</code> pipeline on   ZH</li> <li>Try to use <code>In Progress</code> only for Issues you are actively working on</li> <li>A rule of thumb is that you should not have more than 2-3 <code>In Progress</code>     Issues</li> <li>Give priority to Issues that are close to being completed, rather than     starting a new Issue</li> <li>Update an Issue on GH often, like at least once a day of work</li> <li>Show the progress to the team with quick updates</li> <li>Update your Issue with pointers to gdocs, PRs, notebooks</li> <li>If you have questions, post them on the bug and tag people</li> <li>Once the task, in your opinion, is done, move an issue to <code>Review/QA</code> pipeline   so that Integrator / team leaders can review it</li> <li>If we decide to stop the work, add a <code>Paused</code> label and move it back to the   backlog, e.g., <code>Sprint backlog (P0)</code>, <code>Product backlog (P1)</code>, <code>Icebox (P2)</code></li> </ul>"},{"location":"work_organization/all.use_github_and_zenhub.how_to_guide.html#closing-an-issue","title":"Closing an issue","text":"<ul> <li>A task is closed when PR has been reviewed and merged into <code>master</code></li> <li>When, in your opinion, there is no more work to be done on your side on an   Issue, please move it to the <code>Done/Pre-prod</code> or <code>Review/QA</code> pipeline, but do   not close it</li> <li>Integrators / team leaders will close it after review</li> <li>If you made specific assumptions, or if there are loose ends, etc., add a   <code>TODO(user)</code>or file a follow-up Issue</li> <li>Done means that something is \"DONE\", not \"99% done\"</li> <li>\"DONE\" means that the code is tested, readable, and usable by other     teammates</li> <li>Together we can decide that 99% done is good enough, but it should be a   conscious decision and not comes as a surprise</li> <li>There should be a reason when closing an issue</li> <li>E.g. - closing as PR is merged</li> <li>E.g. - closing since obsolete</li> </ul>"},{"location":"work_organization/all.use_github_and_zenhub.how_to_guide.html#pr-workflows","title":"PR workflows","text":""},{"location":"work_organization/all.use_github_and_zenhub.how_to_guide.html#pr-labels","title":"PR labels","text":"<ul> <li><code>PR_for_authors</code></li> <li>There are changes to be addressed by an author of a PR</li> <li><code>PR_for_reviewers</code></li> <li>PR is ready for review by team leaders</li> <li><code>PR_for_integrators</code></li> <li>PR is ready for the final round of review by Integrators, i.e. close to     merge</li> </ul>"},{"location":"work_organization/all.use_github_and_zenhub.how_to_guide.html#filing-a-new-pr","title":"Filing a new PR","text":""},{"location":"work_organization/all.use_github_and_zenhub.how_to_guide.html#general-tips","title":"General tips","text":"<ul> <li>Implement a feature in a branch (not <code>master</code>), once it is ready for review   push it and file a PR via GH interface</li> <li>We have <code>invoke</code> tasks to automate some of these tasks:   ``` <p>i git_branch_create -i 828 i git_branch_create -b Cmamp723_hello_world i gh_create_pr   ```</p> </li> <li>If you want to make sure you are going in a right direction or just to confirm   the interfaces you can also file a PR to discuss</li> <li>Mark PR as draft if it is not ready, use the <code>convert to draft</code> button</li> <li> <p>Draft PR should be filed when there is something to discuss with and     demonstrate to the reviewer, but the feature is not completely implemented</p> <p></p> </li> </ul>"},{"location":"work_organization/all.use_github_and_zenhub.how_to_guide.html#filing-process","title":"Filing process","text":"<ul> <li>Add a description to help reviewers to understand what it is about and what   you want the focus to be</li> <li>Add a pointer in the description to the issue that PR is related to - this     will ease the GH navigation for you and reviewers</li> <li>Leave the assignee field empty</li> <li>This will be done by team leaders</li> <li>Add reviewers to the reviewers list</li> <li>For optional review just do <code>@FYI</code> <code>person_name</code> in the description</li> <li>Add a corresponding label</li> <li>Usually the first label in the filed PR is <code>PR_for_reviewers</code></li> <li>If it is urgent/blocking, use the <code>Blocking</code> label</li> <li>Make sure that the corresponding tests pass</li> <li>Always lint before asking for a review</li> <li>Link a PR to an issue via ZH plugin feature   </li> <li>If the output is a notebook:</li> <li>Publish a notebook, see     here</li> <li>Attach a command line to open a published notebook, see     here</li> </ul>"},{"location":"work_organization/all.use_github_and_zenhub.how_to_guide.html#review","title":"Review","text":"<ul> <li>A reviewer should check the code:</li> <li>Architecture</li> <li>Conformity with specs</li> <li>Code style conventions</li> <li>Interfaces</li> <li>Mistakes</li> <li>Readability</li> <li>There are 2 possible outcomes of a review:</li> <li>There are changes to be addressed by author<ul> <li>A reviewer leaves comments to the code</li> <li>Marks PR as <code>PR_for_authors</code></li> </ul> </li> <li>A PR is ready to be merged:<ul> <li>Pass it to integrators and mark it as <code>PR_for_integrators</code></li> <li>Usually is placed by team leaders after they approve PR</li> </ul> </li> </ul>"},{"location":"work_organization/all.use_github_and_zenhub.how_to_guide.html#addressing-comment","title":"Addressing comment","text":"<ul> <li>If the reviewer's comment is clear to the author and agreed upon:</li> <li>The author addresses the comment with a code change and after changing the     code (everywhere the comment it applies) marks it as <code>RESOLVED</code> on the GH     interface</li> <li>Here we trust the authors to do a good job and to not skip / lose comments</li> <li>If the comment needs further discussion, the author adds a note explaining     why he/she disagrees and the discussion continues until consensus is reached</li> <li>Once all comments are addressed:</li> <li>Re-request the review</li> <li>Mark it as <code>PR_for_reviewers</code></li> </ul>"},{"location":"work_organization/all.use_github_and_zenhub.how_to_guide.html#coverage-reports-in-prs-discussion","title":"Coverage reports in PRs - discussion","text":"<ul> <li> <p>We should start posting coverage reports in PRs.</p> </li> <li> <p>The suggested process is:</p> </li> <li>PR's author posts coverage stats before (from master) and after the changes     in the format below. The report should contain only the files that were     touched in a PR.<ul> <li>We have <code>run_coverage_report</code> invoke</li> <li><code>TODO(*): Enable for Sorrentum and add usage examples.</code></li> <li>Maybe we can automate it somehow, e.g., with GH actions. But we need to   start from something.   ```   Name                                    Stmts   Miss Branch BrPart  Cover</li> </ul> <p>oms/locates.py                              7      7      2      0     0%   oms/oms_utils.py                           34     34      6      0     0%   oms/tasks.py                                3      3      0      0     0%   oms/oms_lib_tasks.py                       64     39      2      0    38%   oms/order.py                              101     30     22      0    64%   oms/test/oms_db_helper.py                  29     11      2      0    65%   oms/api.py                                154     47     36      2    70%   oms/broker.py                             200     31     50      9    81%   oms/pnl_simulator.py                      326     42     68      8    83%   oms/place_orders.py                       121      8     18      6    90%   oms/portfolio.py                          309     21     22      0    92%   oms/oms_db.py                              47      0     10      3    95%   oms/broker_example.py                      23      0      4      1    96%   oms/mr_market.py                           55      1     10      1    97%   oms/init.py                             0      0      0      0   100%   oms/call_optimizer.py                      31      0      0      0   100%   oms/devops/init.py                      0      0      0      0   100%   oms/devops/docker_scripts/init.py       0      0      0      0   100%   oms/order_example.py                       26      0      0      0   100%   oms/portfolio_example.py                   32      0      0      0   100%</p> <p>TOTAL                                    1562    274    252     30    80%   ```</p> </li> <li>PR's author also sends a link to S3 with the full html report so that a   reviewer can check that the new lines added are covered by the tests</li> </ul>"},{"location":"work_organization/kaizenflow.organize_your_work.how_to_guide.html","title":"Golden rules","text":"<ul> <li>Please <code>watch</code>, <code>star</code> and <code>fork</code> the repo.</li> <li>\"Remember to treat others the way you want to be treated\"</li> <li>Everybody comes from a different place and different skill-level, somebody has   a job, somebody has a full-time work</li> <li>Let's be polite, helpful, and supportive to each other</li> <li>I'll remind you an adage that I tell myself when my teammates drive me crazy:   \"If you want to go fast, go alone; if you want to go far, go together\"</li> <li>While being assigned a warm-up issue:</li> <li>We highly value the importance of following instructions to ensure efficient     task completion. We intentionally select straightforward and simple issues     as the first tasks for you. So as your first step you should put primary     focus on following the rules. Failure to do so can result in wasting more of     our time than saving it.</li> <li>A pledge to put in the time you committed and not disappear</li> <li>If we don't find that you can meaningfully contribute, we will give you a     warning and help you improve, we will suggest you how to fill in your     knowledge gaps, and start not putting the time to help since we don't have     enough resources</li> <li>If we decide that you are not ready to contribute, we will drop you, without     discussions: this is our sole decision for the good of the project and the     other contributors</li> <li>You can study and try again in 6 months</li> </ul>"},{"location":"work_organization/kaizenflow.organize_your_work.how_to_guide.html#roles","title":"Roles","text":""},{"location":"work_organization/kaizenflow.organize_your_work.how_to_guide.html#tech-leads","title":"Tech leads","text":"<ul> <li> <p>Tech leads are contributors that have put more effort in KaizenFlow and / or   have more experience in research and development</p> </li> <li> <p>The same project (e.g., Web3, Model, Arbitrage, NLP) can have multiple tech   leads</p> </li> <li> <p>One becomes a tech lead by technical excellence and dedication, and for no     other reason (e.g., seniority or degree)</p> </li> <li> <p>What tech leads do:</p> </li> <li> <p>Organize the work in their specific project together with GP / Paul and the     other full-time contributors</p> </li> <li>Help young team members, e.g., answering their questions, showing how we do     things</li> <li>Answer other team members questions, even in different projects</li> <li>Following our process, advertising it, and explaining it to new team members</li> <li>Understand what other people in your team are doing and coordinate with them</li> <li> <p>Make sure code, notebooks, and data are reused and shared</p> </li> <li> <p>What tech leads gain from being a tech lead:</p> </li> <li>Learn how to run a research / dev team</li> <li>Practice leadership skills (which mainly entail technical excellence and     patience)</li> <li>Work closely with GP / Paul</li> <li>Material benefits (e.g., higher KaizenFlow bonuses once the reward system is     in place)</li> </ul>"},{"location":"work_organization/kaizenflow.organize_your_work.how_to_guide.html#developers","title":"Developers","text":"<ul> <li>People on teams working on the same project or on related projects can:</li> <li>work together on the same tasks (you can coordinate on GitHub, Telegram, or      do a Zoom)</li> <li>work together and split the work (still you need coordination)</li> <li>each can \"replicate\" the work so that both of you understand (no      coordination)</li> </ul>"},{"location":"work_organization/kaizenflow.organize_your_work.how_to_guide.html#how-to-organize-your-research","title":"How to organize your research","text":"<ul> <li> <p>The GH issues are for:</p> </li> <li> <p>reporting issues (e.g., \"I am getting this error. What should I do?\")</p> </li> <li> <p>updates (e.g., \"I've made a bunch of progress and the details are in the     gdoc )</p> </li> <li> <p>Commit the code in a PR (even the same PR) so that it's easier to review the   details together</p> </li> <li> <p>You can say \"this is what I'm doing, what do you think?\"</p> </li> <li>E.g., create a dir like 'SorrIssueXYZ_...' under     https://github.com/kaizen-ai/kaizenflow/tree/master/sorrentum_sandbox/examples/ml_projects</li> <li> <p>E.g., see the PR under https://github.com/kaizen-ai/kaizenflow/pull/31</p> </li> <li> <p>Work in the Docker container so that you use the standard flow and it's easy   to collaborate</p> </li> <li> <p>It's ok if you want to use your different flow, but still try to commit     often</p> </li> <li> <p>Save all the files in   Project dir</p> </li> <li> <p>You can use Google Desktop to keep     the dir synced locally on your computer so when you work on your project the     data is always in sync with what others view</p> </li> <li> <p>Keep a detailed research log in a Google Doc in the   Project dir</p> </li> <li> <p>Document what you are doing, take screenshot, explain the results</p> </li> <li> <p>You can make progress only if you are organized and consistent</p> </li> <li> <p>Avoid emails any time possible</p> </li> <li> <p>Communication should happen on GitHub around specific Issues</p> </li> <li> <p>Read General Rules of Collaboration for     more details</p> </li> <li> <p>Use Telegram chat when you are blocked on something</p> </li> <li> <p>Try to use GitHub Issues when not urgent to provide context</p> </li> <li> <p>Don't abuse Telegram, learning what is urgent and what's not</p> </li> <li> <p>On GitHub</p> </li> <li> <p>Provide context about the problem</p> </li> <li>Explain clearly what is the problem and what you are trying to do</li> <li> <p>Writing good bug updates is an art that takes time</p> </li> <li> <p>When you are done with your task and you need more work ping your project chat</p> </li> <li> <p>Work with your team (especially if you are a tech lead)</p> </li> <li>Answer other people questions</li> <li>Understand what other people in your team are doing</li> <li>Coordinate with them</li> <li>We can only make progress together as a team and not as a single person (get     over being shy)</li> </ul>"},{"location":"work_organization/kaizenflow.organize_your_work.how_to_guide.html#about-the-project-you-choose","title":"About the project you choose","text":"<ul> <li>We let you pick the project you like because we believe that one needs to be   excited about something to work on that through the late night, on Sat and   Sunday</li> <li> <p>On the other side, in 15 years of leading, mentoring, hiring, and firing   people I've never found one person not complaining about their project being   \"boring\"</p> </li> <li> <p>People working on exploratory analysis want to do machine learning</p> </li> <li>People want to do \"machine learning\" when 95% of the machine learning work     is building data pipelines and doing exploratory analysis</li> <li>People running models complain about that being uninspiring and would rather     to do exploratory analysis</li> <li>People writing code want to do machine learning (since that's the cool     stuff!)</li> <li> <p>Software engineers would like to be lawyers (since they want to interact     with people and computers), and lawyers want to be engineers (since they     can't take dealing with people anymore)</p> </li> <li> <p>The problem is that everything is interesting and boring at the same time. It   depends on your mindset</p> </li> <li>If you say \"this is boring\", it will be boring for sure</li> <li>If you look for finding interesting pieces in what you need to do, it will     become suddenly super interesting</li> <li>The people that succeed are the ones that do their best at their job no     matter what, that find joy in solving whatever problem brings value</li> </ul>"},{"location":"work_tools/all.add_toc_to_notebook.how_to_guide.html","title":"Adding a table of contents to a notebook","text":""},{"location":"work_tools/all.add_toc_to_notebook.how_to_guide.html#problem","title":"Problem","text":"<ul> <li>The Jupyter Notebook's extension that adds a table of contents (TOC) to a   notebook only works on the Jupyter server. When the notebook is published as   HTML or rendered on GitHub, the TOC added by this extension is not displayed</li> <li>We want to be able to add a TOC to a notebook in a way that is also visible in   these environments</li> </ul>"},{"location":"work_tools/all.add_toc_to_notebook.how_to_guide.html#solution","title":"Solution","text":"<ul> <li>Create a custom script that adds a TOC to a notebook by parsing its contents</li> <li>The script:   /dev_scripts/notebooks/add_toc_to_notebook.py</li> </ul>"},{"location":"work_tools/all.add_toc_to_notebook.how_to_guide.html#how-it-works","title":"How it works","text":"<ul> <li>Parse the JSON representation of the notebook's contents</li> <li>Identify cells that contain headings</li> <li>Headings are located in Markdown-type cells, start at the beginning of a     line and consist of one or more hash signs, followed by at least one     whitespace and then the heading text</li> <li>E.g., \"# First heading\"</li> <li>Add HTML anchors to the cells where headings were found</li> <li>E.g.,   <code>html   &lt;a name=\"first-heading\"&gt;&lt;/a&gt;</code></li> <li>Compose a TOC from the extracted headings, using anchors to create internal   links to the notebook cells</li> <li>Add the TOC to the first cell of the notebook</li> <li> <p>If there is already a TOC in that cell, replace it with the new one to     ensure the TOC is up-to-date</p> </li> <li> <p>The flow of the script resembles   this 3rd-party solution.   The principal difference is that we are not using the <code>nbformat</code> library and   instead parse the JSON representation of the notebook's code directly</p> </li> <li>The reason is that we prioritize transparency in parsing and prefer not to   rely on external libraries that may unexpectedly change their behavior in the   future</li> </ul>"},{"location":"work_tools/all.add_toc_to_notebook.how_to_guide.html#how-to-use","title":"How to use","text":"<ul> <li>Run on one or more notebooks:</li> </ul> <pre><code>&gt; dev_scripts/notebooks/add_toc_to_notebook.py \\\n    --input_files dir1/file1.ipynb dir2/file2.ipynb\n</code></pre> <ul> <li>Run on a directory to process all the notebooks in it:</li> </ul> <pre><code>&gt; dev_scripts/notebooks/add_toc_to_notebook.py \\\n    --input_dir dir1/\n</code></pre>"},{"location":"work_tools/all.add_toc_to_notebook.how_to_guide.html#limitations","title":"Limitations","text":"<ul> <li>If a notebook with the TOC added by the script is rendered on GitHub, the TOC   is displayed but its links are broken (they do not lead to the cells with the   corresponding headings)</li> <li>This is due to a known issue with how GitHub renders notebooks, see, for   example, discussions   here   and   here</li> <li>If the notebook is published as HTML, the TOC is displayed and its links work   as expected</li> </ul>"},{"location":"work_tools/all.bfg_repo_cleaner.how_to_guide.html","title":"BFG Repo-Cleaner","text":"<p>BFG dockerized.</p>"},{"location":"work_tools/all.bfg_repo_cleaner.how_to_guide.html#build","title":"Build","text":"<pre><code>docker build . --tag bfg\n</code></pre>"},{"location":"work_tools/all.bfg_repo_cleaner.how_to_guide.html#usage","title":"Usage","text":"<p>You could run BFG in a container by executing the following <code>docker</code> command.</p> <pre><code>docker run -it --rm \\\n  --volume \"$PWD:/home/bfg/workspace\" \\\n  koenrh/bfg \\\n  --delete-files id_rsa\n</code></pre> <p>You could make this command more easily accessible by putting it in an executable, and make sure that it is available in your <code>$PATH</code>. Alternatively, you could create wrapper functions for your <code>docker run</code> commands (example).</p> <pre><code>bfg() {\n  docker run -it --rm \\\n    --volume \"$PWD:/home/bfg/workspace\" \\\n    --name bfg \\\n    koenrh/bfg \"$@\"\n}\n</code></pre>"},{"location":"work_tools/all.chatgpt_api.how_to_guide.html","title":"Chatgpt Api","text":""},{"location":"work_tools/all.chatgpt_api.how_to_guide.html#openai-assistant-runner-manager","title":"OpenAI Assistant Runner &amp; Manager","text":""},{"location":"work_tools/all.chatgpt_api.how_to_guide.html#what-is-openai-assistant","title":"What is OpenAI Assistant","text":"<ul> <li>An assistant is similar to a modified GPT that has mastered some knowledge and   is able to use that knowledge for future tasks</li> <li> <p>The official OpenAI documentation is at   https://platform.openai.com/docs/assistants/overview</p> </li> <li> <p>Normally you \"chat\" with ChatGPT, that means anything you send to it is   treated as input</p> </li> <li>GPT will forget what you previously said or files uploaded as the total length   of the conversation grows</li> <li>By creating an Assistant, you build a new \"instance\" of ChatGPT and can give   it some knowledge to learn</li> <li>This knowledge can be in many formats and up to 20 files (512MB each) at a   time</li> <li>With an instruction string, you define its behavior about how it should make   use of those knowledge</li> <li>When talking to an assistant, you can still add files in the message</li> <li>These files does not count towards its 20 files' knowledge limit, as they are   considered as input and will be forgotten eventually</li> </ul>"},{"location":"work_tools/all.chatgpt_api.how_to_guide.html#general-pattern","title":"General pattern","text":"<ul> <li>Creation:</li> <li>Send some guideline or example files for one type of task to the Assistant     itself as knowledge</li> <li>Send an instruction about how it should deal with tasks using those     knowledge</li> <li>Running:</li> <li>Whenever a specific task comes, send the task and its data files in the     message as input</li> <li>Let the assistant solve the task based on the knowledge it learned</li> <li>It will forget everything in this task, and be ready for the next task like     this one never happens</li> <li>Chatting:</li> <li>You can continue the conversation if you are not satisfied with its reply</li> <li>Chatting is not yet implemented in our code, since command line scripts     cannot save conversations.</li> </ul>"},{"location":"work_tools/all.chatgpt_api.how_to_guide.html#code-organization","title":"Code organization","text":"<ul> <li>Libraries are under <code>helpers</code>, e.g.,</li> <li><code>helpers/hchatgpt.py</code></li> <li> <p><code>helpers/hchatgpt_instructions.py</code></p> </li> <li> <p>Scripts are under <code>dev_scripts/chatgpt</code>, e.g.,</p> </li> <li><code>dev_scripts/chatgpt/manage_chatgpt_assistant.py</code></li> <li><code>dev_scripts/chatgpt/run_chatgpt.py</code></li> </ul>"},{"location":"work_tools/all.chatgpt_api.how_to_guide.html#how-to-use","title":"How to use","text":"<ul> <li>Add the API KEY   ```bash <p>export OPENAI_API_KEY=   ``` <li>Each API key can be bound to an OpenAI Organization</li> <li>Assistants are organization-wide, an assistant created under our Org can be   accessed by any API key that belongs to our Org</li>"},{"location":"work_tools/all.chatgpt_api.how_to_guide.html#assistant-manager","title":"Assistant Manager","text":"<ul> <li>The interface is like:</li> </ul> <p>```bash</p> <p>manage_chatgpt_assistant.py -h   Manage the ChatGPT Assistants in our OpenAI Organization</p> <p>optional arguments:     -h, --help            show this help message and exit     --create CREATE_NAME  Name of the assistant to be created     --edit EDIT_NAME      Name of the assistant to be edited     --delete DELETE_NAME  Name of the assistant to be deleted, will ignore all other arguments     --new_name NEW_NAME   New name for the assistant, only used in -e     --model MODEL         Model used by the assistant     --instruction_name INSTRUCTION_NAME                           Name of the instruction for the assistant, as shown in helpers.hchatgpt_instructions     --input_files [INPUT_FILE_PATHS ...]                           Files needed for the assistant, use relative path from project root     --retrieval_tool, --no-retrieval_tool                           Enable the retrieval tool. Use --no-r to disable     --code_tool, --no-code_tool                           Enable the code_interpreter tool. Use --no-c to disable     --function FUNCTION   Apply certain function tool to the assistant, not implemented yet     -v {TRACE,DEBUG,INFO,WARNING,ERROR,CRITICAL}                           Set the logging level   ```</p> <ul> <li>Use this script to create, modify, or delete an assistant in our Org</li> <li>A set of instructions are in <code>helpers/hchatgpt_instructions.py</code></li> <li>Feel free to add more instructions for different tasks here</li> <li> <p>For better understanding, the name of an assistant created should be related   to its instruction name</p> </li> <li> <p>E.g., create a doc writer assistant with name <code>DocWriter-1</code> and use   <code>instruction=DocWriter</code> from <code>helpers/hchatgpt_instructions.py</code>   ```bash</p> <p>manage_chatgpt_assistant.py \\       --create DocWriter-1 \\       --model \"gpt-4-1106-preview\" \\       --instruction_name DocWriter \\       --input_files docs/work_tools/all.bfg_repo_cleaner.how_to_guide.md docs/marketing/dropcontact.how_to_guide.md docs/coding/all.hplayback.how_to_guide.md \\       --retrieval_tool --code_tool   ```</p> </li> </ul>"},{"location":"work_tools/all.chatgpt_api.how_to_guide.html#running-assistant","title":"Running Assistant","text":"<ul> <li> <p>The script <code>dev_scripts/chatgpt/run_chatgpt.py</code> runs an assistant</p> </li> <li> <p>The interface is like:</p> </li> </ul> <p>```bash</p> <p>run_chatgpt.py -h   Use ChatGPT Assistant to process a file or certain text.</p> <p>optional arguments:     -h, --help            show this help message and exit     --list                Show all currently available assistants and exit     --assistant_name ASSISTANT_NAME                           Name of the assistant to be used     --input_files [INPUT_FILE_PATHS ...]                           Files needed in this run, use relative path from project root     --model MODEL         Use specific model for this run, overriding existing assistant config     --output_file OUTPUT_FILE                           Redirect the output to the given file     --input_text INPUT_TEXT                           Take a text input from command line as detailed instruction     --vim                 Disable -i (but not -o), take input from stdin and output to stdout forcely     -v {TRACE,DEBUG,INFO,WARNING,ERROR,CRITICAL}                           Set the logging level   ```</p> <ul> <li><code>dev_scripts/run_chatgpt.py -l</code> will show all the available assistants in our   Org.</li> <li>Refer to <code>helpers/hchatgpt_instructions.py</code> to see how they are instructed</li> </ul> <p>```bash</p> <p>python dev_scripts/chatgpt/run_chatgpt.py \\     -n MarkdownLinter \\ # Use the assistant \"MarkdownLinter\"     -f dev_scripts/chatgpt/example_data/corrupted_dropcontact.how_to_guide.md \\ # Give this corrupted markdown file     -o dev_scripts/chatgpt/example_data/gpt_linted_dropcontact.how_to_guide.md # Redirect its output to this file   ```</p>"},{"location":"work_tools/all.chatgpt_api.how_to_guide.html#api-library","title":"API library","text":"<ul> <li><code>helpers/hchatgpt.py</code> provides methods that wrap and interact with OpenAI API</li> <li> <p>By using these methods, you can easily build an assistant and chat to it with   our files</p> </li> <li> <p>Key functionalities include:</p> </li> <li>Uploading and removing files from OpenAI</li> <li>Adding and removing files for an assistant</li> <li>Creating threads and messages for user input</li> <li>Running threads with certain assistants</li> <li>End-to-end communication method between users and the assistant</li> </ul>"},{"location":"work_tools/all.chatgpt_api.how_to_guide.html#usage","title":"Usage","text":"<p>The following snippets provide a basic overview of the code usage.</p>"},{"location":"work_tools/all.chatgpt_api.how_to_guide.html#file-structure","title":"File structure","text":"<ul> <li>Since OpenAI File manager does not hold folder structure, you use a cache   dictionary to save the relation between our file (with folder) and OpenAI File   IDs</li> <li>This dictionary will be constantly accessed and saved back to   <code>project_root/gpt_id.json</code></li> <li>If you find anything buggy, try deleting this cache file and rerun the code so   that it can be regenerated from scratch</li> </ul>"},{"location":"work_tools/all.chatgpt_api.how_to_guide.html#uploading-and-retrieving-files","title":"Uploading and retrieving Files","text":"<ul> <li>To upload a file to OpenAI, which you can later attach to messages/assistants:</li> </ul> <p><code>python   file_id = upload_to_gpt('path_to_your_file')</code></p> <ul> <li>If you want to retrieve a file that has been uploaded to OpenAI by its path or   ID:   <code>python   file_object = get_gpt_file_from_path('path_to_your_file')</code>   or   <code>python   file_object = get_gpt_file_from_id(file_id)</code></li> </ul>"},{"location":"work_tools/all.chatgpt_api.how_to_guide.html#managing-assistants","title":"Managing Assistants","text":"<p>You can specify files an assistant should constantly use (like guidelines):</p> <pre><code>set_assistant_files_by_name('assistant_name', ['file_path_1', 'file_path_2'])\n</code></pre> <p>Add or remove specific files to/from an existing assistant by file paths or IDs:</p> <pre><code>add_files_to_assistant_by_name('assistant_name', ['new_file_path'])\ndelete_file_from_assistant_by_name('assistant_name', 'file_path_to_remove')\n</code></pre>"},{"location":"work_tools/all.chatgpt_api.how_to_guide.html#chatgpt-communication","title":"ChatGPT communication","text":"<ul> <li>Create a thread and send a message, with or without attaching files:</li> </ul> <p><code>python   thread_id = create_thread()   message_id = create_message_on_thread_with_file_names(thread_id,     'Your message content',     ['file_name_1'])</code></p> <ul> <li>Run a thread on an assistant to get the Assistant's response:   <code>python   run_id = run_thread_on_assistant_by_name('assistant_name', thread_id)   response_messages = wait_for_run_result(thread_id, run_id)</code></li> </ul>"},{"location":"work_tools/all.chatgpt_api.how_to_guide.html#e2e-assistant-runner","title":"E2E assistant runner","text":"<ul> <li>Interact with an assistant conveniently with the <code>e2e_assistant_runner</code>   function</li> <li>This function can take user input, send it to the assistant, and manage file   attachments in one call:   <code>python   response = e2e_assistant_runner(       'assistant_name',       'Your question or statement here',       input_file_names=['file_name_1'])   # Outputs the assistant's response.   print(response)</code></li> </ul>"},{"location":"work_tools/all.codebase_clean_up.how_to_guide.html","title":"Codebase Clean Up","text":""},{"location":"work_tools/all.codebase_clean_up.how_to_guide.html#codebase-clean-up-scripts","title":"Codebase clean-up scripts","text":""},{"location":"work_tools/all.codebase_clean_up.how_to_guide.html#problem","title":"Problem","text":"<ol> <li>Since we have multiple repos, we can't always easily replace code in one repo    (e.g., with PyCharm) and have all the other repos work properly</li> <li>Sometimes it is required to rename files and replace text in files at the    same time (e.g., when renaming an import)</li> <li> <p>While developing a change for the entire repo, we want to be able to \"reset\"    the work and apply the change from scratch</p> </li> <li> <p>E.g., during the review of the PR applying lots of text replaces:</p> </li> <li>The code in master might be changing creating conflicts</li> <li>The reviewers might ask some changes</li> <li>This creates a lot of manual changes</li> </ol>"},{"location":"work_tools/all.codebase_clean_up.how_to_guide.html#solution-script-approach","title":"Solution: script approach","text":"<ul> <li>Create a shell <code>sh</code> script that applies the correct changes to all the repos   using /dev_scripts/replace_text.py</li> <li> <p>For more complex problems we can extend <code>replace_text.py</code> with custom   regex-based operations</p> </li> <li> <p>The script approach solves all the problems above</p> </li> <li> <p>We apply the script to all the repos</p> </li> <li>The script can rename text and files at the same time</li> <li>We can check out a clean master, run the script to apply the changes    automatically, regress and merge</li> </ul>"},{"location":"work_tools/all.codebase_clean_up.how_to_guide.html#using-the-script-approach","title":"Using the script approach","text":"<ul> <li> <p>We want to apply clean-up changes to the code base with a script</p> </li> <li> <p>Ideally we would like to apply all the changes automatically through the   script</p> </li> <li>E.g., in SorrTask258 a     script that replaces <code>pytest.raises</code> with <code>self.assertRaises</code> everywhere in     the code</li> <li> <p>We are ok to make the vast majority (like 95%) of the changes automatically,   and the rest manually</p> </li> <li> <p>We want to keep together in a single PR</p> </li> <li>The script performing automatically the changes; and</li> <li> <p>The manual changes from the outcome of the script</p> </li> <li> <p>We want to create a separate PR to communicate with the reviewers the output   of running the script</p> </li> <li>The author/reviewers should run the script on all the repos, run the unit     tests, and merge (through a PR as usual)</li> </ul>"},{"location":"work_tools/all.codebase_clean_up.how_to_guide.html#how-to-use-replace_textpy","title":"How to use <code>replace_text.py</code>","text":"<ul> <li> <p>See <code>-h</code> for updated list of options</p> </li> <li> <p>Replace an instance of text in the content of all the files with extensions:   <code>.py</code>, <code>.ipynb</code>, <code>.txt</code>, <code>.md</code></p> </li> <li><code>--old</code>: regular expression or string that should be replaced with <code>--new</code></li> <li><code>--new</code>: regular expression or string that should replace <code>--old</code></li> <li><code>--preview</code>: see script result without making actual changes</li> <li><code>--only_dirs</code>: provide space-separated list of directories to process only</li> <li><code>--only_files</code>: provide space-separated list of files to process only</li> <li><code>--exclude_files</code>: provide space-separated list of files to exclude from   replacements</li> <li><code>--exclude_dirs</code>: provide space-separated list of dir to exclude from   replacements</li> <li><code>--ext</code>: process files with specified extensions</li> <li>Defaults are <code>py, ipynb, txt, sh</code></li> <li> <p>Use <code>_all_</code> for all files</p> </li> <li> <p>The goal of the script is to replace an instance of text in the content of all   the files with extensions <code>.py</code>, <code>.ipynb</code>, <code>.txt</code>, <code>.md</code> and to do a <code>git mv</code>   for files based on certain criteria</p> </li> </ul>"},{"location":"work_tools/all.codebase_clean_up.how_to_guide.html#rename-a-file","title":"Rename a file","text":"<ul> <li>Preview the change   ```bash <p>replace_text.py \\       --old research_backtest_utils \\       --new backtest_api \\       --preview   ```</p> </li> <li>Then you can look at the changes to be performed with <code>vic</code> /   <code>vim -c \"cfile cfile\"</code></li> <li>If you are satisfied you can re-run the command with <code>--preview</code> to apply the   change</li> <li>Rename file   ```bash <p>git mv ./dataflow/backtest/{research_backtest_utils.py,backtest_api.py}   ```</p> </li> </ul>"},{"location":"work_tools/all.codebase_clean_up.how_to_guide.html#replace-an-import-with-a-new-one","title":"Replace an import with a new one","text":"<pre><code>&gt; replace_text.py \\\n    --old \"import core.fin\" \\\n    --new \"import core.finance\"\n</code></pre>"},{"location":"work_tools/all.codebase_clean_up.how_to_guide.html#replace-text-in-a-specific-directory","title":"Replace text in a specific directory","text":"<pre><code>&gt; replace_text.py \\\n    --old \"exec \" \\\n    --new \"execute \" \\\n    --preview \\\n    --dirs dev_scripts \\\n    --exts None\n</code></pre>"},{"location":"work_tools/all.codebase_clean_up.how_to_guide.html#revert-all-files-but-this-one","title":"Revert all files but this one","text":"<ul> <li>There is an option <code>--revert_all</code> to apply this before the script   ```bash <p>gs -s | \\       grep -v dev_scripts/replace_text.py | \\       grep -v \"\\?\" | \\       awk '{print $2}' | \\       xargs git checkout --   ```</p> </li> </ul>"},{"location":"work_tools/all.codebase_clean_up.how_to_guide.html#custom-flows","title":"Custom flows","text":"<pre><code>&gt; replace_text.py --custom_flow _custom1\n</code></pre> <ul> <li>Custom flow for AmpTask14   ```bash <p>replace_text.py --custom_flow _custom2 --revert_all   ```</p> </li> </ul>"},{"location":"work_tools/all.codebase_clean_up.how_to_guide.html#usage-examples","title":"Usage examples","text":"<ul> <li>See SorrIssue259 and the   related PR for reference</li> <li>We wanted to make <code>_to_multiline_cmd()</code> from <code>helpers/lib_tasks_utils.py</code> a     public function</li> <li>This would require to rename <code>_to_multiline_cmd()</code> to <code>to_multiline_cmd()</code>     with the script</li> <li> <p>This     script     will make the replacement smoothly everywhere in the code except for the     dirs specified <code>--exclude_dirs</code> flag.</p> </li> <li> <p>See SorrIssue258 and the   related PR for reference</p> </li> <li>We wanted to replace <code>pytest.raises</code> with <code>self.assertRaises</code></li> <li>This     script     will replace it everywhere in the code</li> <li>Note the use of <code>--ext</code> flag to specify the file extentions the script     should work on</li> <li>Of course the changes need to be applied in one repo and then propagated to   all the other repos if the tests are successful</li> </ul>"},{"location":"work_tools/all.codebase_clean_up.how_to_guide.html#instructions-for-the-pr-author","title":"Instructions for the PR author","text":"<ul> <li>Create a local branch called <code>...TaskXYZ_..._script</code> containing:</li> <li>The code that needs to be changed manually<ul> <li>E.g.: Replacing <code>pytest.raises</code> with <code>self.assertRaises</code></li> </ul> </li> <li>More contextual changes<ul> <li>E.g.: Adding unit tests to the new functions</li> </ul> </li> <li>The script for the replacement of the caller named after the GH issue<ul> <li>The script should:</li> <li>Prepare the target Git client</li> <li>Merge this <code>script</code> branch with the manual changes</li> <li>Make the automated changes<ul> <li>E.g.: Rename a function or replace certain word in comments /   docstring</li> </ul> </li> </ul> </li> <li>Notes in the files that need to be changed manually after the automatic     script</li> <li>Run from scratch the script getting the regression to pass</li> <li>Any time there is a change needed by hand, the change should be added to the     script branch</li> <li>The goal is to be able to run the script</li> <li>File a PR of the <code>...TaskXYZ_..._script</code></li> <li>(Optional) Create a PR with the result of the script</li> <li>The author can request a review on this PR, but still the goal is to     automate as much as possible</li> <li>Finally, the PR author merges the PR with the results of the script</li> </ul>"},{"location":"work_tools/all.codebase_clean_up.how_to_guide.html#example","title":"Example","text":"<ul> <li>The name of script should be related to the task. E.g:   <code>SorrTask259_Make_to_multi_line_cmd_public.sh</code></li> <li>The script should have a system call to <code>replace_text.py</code> to execute the   required functionality as provided in the above examples</li> <li>Create a PR only with the script and the changes</li> </ul>"},{"location":"work_tools/all.codebase_clean_up.how_to_guide.html#instructions-for-the-subrepo-integrator","title":"Instructions for the subrepo integrator","text":"<ul> <li>Do a <code>git checkout</code> of the <code>...TaskXYZ_..._script</code></li> <li>Run the script</li> <li>Review carefully the changes to make sure we are not screwing things up</li> <li>Run the regressions</li> <li>Merge the resulting <code>...TaskXYZ...</code> PR</li> <li>Ensure that the <code>...TaskXYZ_..._script</code> is merged in <code>master</code></li> <li>Delete the <code>...TaskXYZ_..._script</code></li> </ul>"},{"location":"work_tools/all.conda_environment_obsolete.how_to_guide.html","title":"Conda Environment Obsolete","text":"<p>THIS IS OBSOLETE AFTER DOCKER DEV CONTAINER</p>"},{"location":"work_tools/all.conda_environment_obsolete.how_to_guide.html#conda-flow","title":"Conda flow","text":""},{"location":"work_tools/all.conda_environment_obsolete.how_to_guide.html#optional-install-anaconda","title":"(optional) Install anaconda","text":"<ul> <li>For the AWS machine there is already a central conda, so there is no need for   users to install</li> <li>For a laptop you need to install it yourself</li> <li>You need anaconda3</li> </ul>"},{"location":"work_tools/all.conda_environment_obsolete.how_to_guide.html#configure-anaconda","title":"Configure anaconda","text":"<ul> <li>Configure anaconda for your shell using:   ```bash <p>conda init bash   ```</p> </li> <li>Anaconda3 adds a snippet of code in your <code>.bashrc</code></li> </ul>"},{"location":"work_tools/all.conda_environment_obsolete.how_to_guide.html#create-conda-environment","title":"Create conda environment","text":"<ul> <li>This is needed to install all the packages that are required for development:   ```bash <p>cd $DST_DIR ./dev_scripts/create_conda.develop.sh   ```</p> </li> <li> <p>This script takes 5 mins to run</p> </li> <li> <p>You need to create an environment for every server you use (e.g., for the AWS   server <code>research</code>, for your laptop)</p> </li> <li>You can reuse the same environment for multiple Git clients</li> </ul>"},{"location":"work_tools/all.conda_environment_obsolete.how_to_guide.html#check-conda-environment","title":"Check conda environment","text":"<ul> <li>Check that your conda environment exists:   ```bash <p>conda info --envs   # conda environments:   #   base                     /anaconda3   develop            *  /home//.conda/envs/develop   ```"},{"location":"work_tools/all.conda_environment_obsolete.how_to_guide.html#configure-conda-environment","title":"Configure conda environment","text":"<ul> <li>Every time you open a shell you need to activate the development environment   run:</li> </ul> <p>```bash</p> <p>source dev_scripts/setenv_....sh   ```</p> <ul> <li>This script:</li> <li>Activates the conda environment</li> <li>Sets environment variables</li> <li>Makes sure things are working properly</li> </ul>"},{"location":"work_tools/all.conda_environment_obsolete.how_to_guide.html#delete-recreate-environment","title":"Delete / recreate environment","text":""},{"location":"work_tools/all.conda_environment_obsolete.how_to_guide.html#overwrite-a-conda-environment-with-create_condapy","title":"Overwrite a conda environment with <code>create_conda.py</code>","text":"<ul> <li>You can use the option <code>--delete_env_if_exists</code> to overwrite a conda env,   creating it from scratch</li> <li> <p>This is the typical approach</p> </li> <li> <p>There are some pre-packaged command lines to create the standard environments,   e.g., <code>./dev_scripts/create_conda.develop.sh</code></p> </li> <li> <p>The <code>create_conda.py</code> help as some useful examples of command lines, see the   help:   ```bash</p> <p>create_conda.py -h   ```</p> </li> </ul>"},{"location":"work_tools/all.conda_environment_obsolete.how_to_guide.html#manually-delete-a-conda-environment","title":"Manually delete a conda environment","text":"<ul> <li>You can delete a conda environment by simply deleting the corresponding   directory</li> <li>The conda command tries to be smart removing the packages and leaving the dir,   but IMO it doesn't always work</li> <li>You look at the environments with:   ```bash <p>conda info --envs   # conda environments:   #   ...   develop               *  /Users//.conda/envs/develop   ...   ``` <li>Then you can delete with:   ```bash <p>rm -rf /Users//.conda/envs/develop   ``` <li>It's a good idea to move it so you can resume it if something goes wrong:   ```bash <p>mv /Users//.conda/envs/develop &gt; /Users//.conda/envs/develop.OLD   ``` <li>Note that <code>develop.OLD</code> might not work anymore since all the links are     broken by the move</li>"},{"location":"work_tools/all.conda_environment_obsolete.how_to_guide.html#to-delete-the-entire-conda-installation-advanced-users","title":"To delete the entire conda installation (advanced users)","text":"<ul> <li>This is a dangerous operation, since it deletes the executable <code>conda</code></li> <li>You want to do this only when your environment is screwed up: a more expert     team member can help you diagnose it</li> <li>If you want to delete your conda installation, find the base env   ```bash <p>conda info --envs   base                     /anaconda3   ...   ```</p> </li> <li>Run <code>rm -rf /anaconda3</code></li> <li>A good idea is to move it so you can resume the state</li> </ul>"},{"location":"work_tools/all.conda_environment_obsolete.how_to_guide.html#update-anaconda","title":"Update anaconda","text":"<ul> <li>To update anaconda (i.e., the framework that manages conda packages and   <code>conda</code> executable)</li> </ul> <p>```bash</p> <p>conda activate base   # Remove index cache, lock files, tarballs, unused cache packages, and source   # cache. conda clean --all conda update conda conda update anaconda conda -V   conda 4.7.12   ```</p> <ul> <li>You can try to activate one environment   ```bash <p>conda activate amp_develop which python   /Users/saggese/.conda/envs/amp_develop/bin/python   ```</p> </li> </ul>"},{"location":"work_tools/all.conda_environment_obsolete.how_to_guide.html#configure-user-credentials","title":"Configure user credentials","text":"<ul> <li> <p>For now this topic is obsolete. All development with AWS is running on a   server side (or locally) in a docker container. Here you can find the   documentation   the link</p> </li> <li> <p>Update the user credential files in <code>amp/helpers/user_credentials.py</code></p> </li> <li>Commit this so all your clients are configured</li> <li>Typically you can just copy-paste a portion of the configuration of another   user</li> </ul>"},{"location":"work_tools/all.conda_environment_obsolete.how_to_guide.html#be-patient","title":"Be patient","text":"<ul> <li>The <code>create_conda.py</code> flow is designed to make our projects portable across:</li> <li>Platforms (e.g., macOS, Linux)</li> <li>Different installation of OSes (e.g., GP's laptop vs Paul's laptop) with all     the peculiar ways we install and manage servers and laptops</li> <li>Different versions of conda</li> <li>Different versions of python 3.x</li> <li> <p>Different versions of python packages</p> </li> <li> <p>There is no easy way to make sure that <code>create_conda.py</code> works for everybody</p> </li> <li>We can only make sure that Jenkins builds the environment correctly in its     set-up by following the process described above</li> <li>Try to follow the steps one by one, using a clean shell, cutting and pasting     commands</li> <li>If you hit a problem, be patient, ping GP / Paul, and we will extend the     script to handle the quirks of your set-up</li> </ul>"},{"location":"work_tools/all.conda_environment_obsolete.how_to_guide.html#conda-bloat","title":"Conda bloat","text":"<ul> <li> <p>\"Conda bloat\" refers to the situation when there are more packages in the   conda recipe than what strictly needed to allow us to make progress.</p> </li> <li> <p>The effects of conda bloat are:</p> </li> <li>Longer time for <code>conda</code> to solve the dependencies, download the packages,     and update the environment</li> <li>Additional dependencies forcing <code>conda</code> to install older packages that can     create subtle issues in our environment (we want our code always to be     forward compatible, but we can't afford our code to be back compatible)</li> <li>People starting pinning down packages to get deterministic library behaviors</li> <li> <p>In the worst case, not being able to install the environment at all due to     conflicting <code>conda</code> requirements</p> </li> <li> <p>On the one side, we want to minimize \"conda bloat\".</p> </li> <li>On the other side, we want to be able to experiment with packages.</li> </ul>"},{"location":"work_tools/all.conda_environment_obsolete.how_to_guide.html#minimize-conda-bloat","title":"Minimize conda bloat","text":"<ul> <li>To minimize conda bloat, our process consists of adding a package to the conda   recipe when a new package is actually needed by code and to run unit tests</li> <li>Having code using the library and running unit tests for that code should     happen together</li> <li>The rule is \"Important code should be tested, and if code is not important     it should not be in the repo at all\"</li> <li>Thus a corollary is that all code in the repo should be tested</li> </ul>"},{"location":"work_tools/all.conda_environment_obsolete.how_to_guide.html#conda-environment-lifecycle","title":"Conda environment lifecycle","text":""},{"location":"work_tools/all.conda_environment_obsolete.how_to_guide.html#experimental-conda-environment","title":"Experimental conda environment","text":"<ul> <li> <p>On the other side we want to be free to experiment with a package that can   save us tons of development time.</p> </li> <li> <p>The proposed approach is:</p> </li> <li>Install the package in whatever way you want on top of your standard conda     environment (<code>conda</code>, <code>pip</code>, whatever)</li> <li>Be aware that some packages are not easy to add to our production system     (e.g. packages written by students trying to graduate, non-coding savvy PhDs     and professors)</li> <li>Create a new local experimental conda environment with:     <code>create_conda.py --env_name develop.nlp</code></li> <li> <p>If you want to reproduce your environment or share it (e.g., among the NLP     team) you can branch <code>dev_scripts/install/requirements/develop.yaml</code> and     modify it, then you can create an environment programmatically with:     <code>create_conda.py --env_name develop.nlp --req_file dev_scripts/install/requirements/develop.nlp.yaml</code></p> </li> <li> <p>We can make this process more automated by generalizing the scripts we already   have.</p> </li> </ul>"},{"location":"work_tools/all.conda_environment_obsolete.how_to_guide.html#releasing-a-new-conda-environment","title":"Releasing a new conda environment","text":"<ul> <li>Once the new package is added to the official conda environment, we should:</li> <li>Test the new conda environment locally, by creating a fresh environment and     running all the tests</li> <li>Compare the list of packages installed before / after the change:     <code>git diff amp/dev_scripts/install/conda_envs/amp_develop.saggese.Darwin.gpmac.lan.txt</code><ul> <li>E.g., Check whether installed pandas went from 0.25 to 0.14.1</li> </ul> </li> <li>Use the <code>dev</code> build to make sure Jenkins is happy with it, especially when     we develop on a different OS than Linux</li> <li> <p>Send an email to the team asking them to recreate the environment</p> </li> <li> <p>Typically GP takes care of getting all this fun stuff to work, but you are   welcome to try locally to minimize surprises.</p> </li> </ul>"},{"location":"work_tools/all.conda_environment_obsolete.how_to_guide.html#conda-maintenance-only-for-admins","title":"Conda maintenance (only for admins)","text":""},{"location":"work_tools/all.conda_environment_obsolete.how_to_guide.html#updating-conda-itself","title":"Updating conda itself","text":"<ul> <li>To update conda itself you can run:</li> </ul> <pre><code>&gt; conda activate base\n\n&gt; conda --version\n3.7.1\n\n&gt; conda update anaconda\n\n&gt; conda --version\n3.8.0\n</code></pre>"},{"location":"work_tools/all.conda_environment_obsolete.how_to_guide.html#cleaning-conda-packages","title":"Cleaning conda packages","text":"<ul> <li>One can clean up the entire cache of packages with:</li> </ul> <pre><code>&gt; conda clean --yes --all\n</code></pre> <ul> <li>This operation:</li> <li>Affects the conda system for all the users</li> <li>Is just about deleting cached artifacts so it should not have destructive     effects</li> </ul>"},{"location":"work_tools/all.development.how_to_guide.html","title":"Development","text":""},{"location":"work_tools/all.development.how_to_guide.html#setting-up-git-credentials","title":"Setting up Git credentials","text":""},{"location":"work_tools/all.development.how_to_guide.html#preamble","title":"Preamble","text":"<ul> <li>Git allows setting credentials at different \"levels\":</li> <li>System (set for all the users in <code>/etc/git</code>)</li> <li>Global (set for a single user in <code>$HOME/.gitconfig</code> or     <code>$HOME/.config/git/config</code>)</li> <li>Local (set on a per client basis in <code>.git/config</code> in the repo root)</li> <li>Git uses a hierarchical config approach in which settings of a broader scope     are inherited if not overridden.</li> <li>Refs:</li> <li>How to customize Git:     https://git-scm.com/book/en/v2/Customizing-Git-Git-Configuration</li> <li>Details on <code>git config</code>: https://git-scm.com/docs/git-config</li> </ul>"},{"location":"work_tools/all.development.how_to_guide.html#check-git-credentials","title":"Check Git credentials","text":"<ul> <li>You can check the Git credentials that will be used to commit in a client by   running:   ```bash <p>git config -l | grep user   user.name=saggese   user.email=saggese@gmail.com   github.user=gpsaggese   ```</p> </li> <li>To know at which level each variable is defined, run   ```bash <p>git config --show-origin user.name   file:/Users/saggese/.gitconfig saggese   ```</p> </li> <li>You can see all the <code>Authors</code> in a Git repo history with:   ```bash <p>git log | grep -i Author | sort | uniq   ...   ```</p> </li> <li>Git doesn't do any validation of <code>user.name</code> and <code>user.email</code> but it just uses   these values to compose a commit message like:</li> </ul> <p>```bash</p> <p>git log -2   commit 31052d05c226b1c9834d954e0c3d5586ed35f41e (HEAD -&gt;   AmpTask1290_Avoid_committing_to_master_by_mistake)   Author: saggese saggese@gmail.com   Date: Mon Jun 21 16:22:25 2021</p> <p>Update hooks   ```</p>"},{"location":"work_tools/all.development.how_to_guide.html#setting-git-credentials","title":"Setting Git credentials","text":"<ul> <li>To keep things simple and avoid variability, our convention is to use:</li> <li>As <code>user.name</code> our Linux user name on the local computer we are using to     commit which is returned by <code>whoami</code> (e.g., <code>user.name=saggese</code>)</li> <li>As <code>user.email</code> the email that corresponds to that user (e.g,.     <code>user.email=saggese@gmail.com</code>)</li> <li>To accomplish the set-up above you can:</li> <li>Use in <code>/Users/saggese/.gitconfig</code> the values for our open-source account,     so that they are used by default   ```bash <p>git config --global user.name $(whoami) git config --global user.email YOUR_EMAIL   ```</p> </li> <li>Use the correct user / email in the repos that are not open-source   ```bash <p>cd $GIT_ROOT git config --local user.name $(whoami) git config --local user.email YOUR_EMAIL   ```</p> </li> <li>Note that you need to set these local values on each Git client that you have   cloned, since Git doesn't version control these values</li> </ul>"},{"location":"work_tools/all.development.how_to_guide.html#enforcing-git-credentials","title":"Enforcing Git credentials","text":"<ul> <li>We use Git hooks to enforce that certain emails are used for certain repos   (e.g., we should commit to our open-source repos only using our personal   non-corporate email).</li> <li>You need to install the hooks in each Git client that you use. Conceptually   this step is part of <code>git clone</code>: every time you clone a repo locally you need   to set the hooks.</li> <li>TODO(gp): We could create a script to automate cloning a repo and setting it   up.</li> </ul> <p>```bash</p> <p>cd //amp ./dev_scripts/git/git_hooks/install_hooks.py --action install cd //lem ./amp/dev_scripts/git/git_hooks/install_hooks.py --action install   ```</p> <ul> <li>This procedure creates some links from <code>.git/hook</code> to the scripts in the repo.</li> <li>You can also use the action <code>status</code> to see the status and <code>remove</code> to the   hooks.</li> </ul>"},{"location":"work_tools/all.development.how_to_guide.html#create-the-thin-env","title":"Create the thin env","text":"<ul> <li>You can follow the</li> </ul> <p>```bash   # Build the client env.</p> <p>dev_scripts/client_setup/build.sh 2&gt;&amp;1 | tee tmp.build.log source dev_scripts/setenv_amp.sh   ```</p> <ul> <li>The installation is successful if you see at the end of the output</li> </ul> <p>```verbatim   ...   # Installation   # Configure your client with:</p> <p>source dev_scripts/setenv_amp.sh   ```</p> <ul> <li>To configure each shell, you should run:   ```bash <p>source dev_scripts/setenv_amp.sh   <code>which should output</code>verbatim   ...   alias w='which'   # Enable invoke autocompletion.   ==&gt; SUCCESS &lt;==   ```</p> </li> </ul>"},{"location":"work_tools/all.development.how_to_guide.html#publish-a-notebook","title":"Publish a notebook","text":"<ul> <li><code>publish_notebook.py</code> is a little tool that allows to:</li> <li>Opening a notebook in your browser (useful for read-only mode)<ul> <li>E.g., without having to use Jupyter notebook (which modifies the file in    your client) or github preview (which is slow or fails when the notebook    is too large)</li> </ul> </li> <li>Sharing a notebook with others in a simple way</li> <li>Pointing to detailed documentation in your analysis Google docs</li> <li>Reviewing someone's notebook</li> <li>Comparing multiple notebooks against each other in different browser      windows</li> <li>Taking a snapshot / checkpoint of a notebook as a backup or before making      changes<ul> <li>This is a lightweight alternative to \"unit testing\" to capture the    desired behavior of a notebook</li> <li>One can take a snapshot and visually compare multiple notebooks    side-by-side for changes</li> </ul> </li> </ul>"},{"location":"work_tools/all.development.how_to_guide.html#detailed-instructions","title":"Detailed instructions","text":"<ul> <li>You can get details by running:</li> </ul> <p>```bash</p> <p>dev_scripts/notebooks/publish_notebook.py -h   ```</p> <ul> <li>Plug-in for Chrome   my-s3-browser</li> </ul>"},{"location":"work_tools/all.development.how_to_guide.html#publish-notebooks","title":"Publish notebooks","text":"<ul> <li>Make sure that your environment is set up properly</li> </ul> <p>```bash</p> <p>more ~/.aws/credentials   [am]   aws_access_key_id=   aws_secret_access_key=   aws_s3_bucket=alphamatic-data</p> <p>printenv | grep AM_   AM_AWS_PROFILE=am   ```</p> <ul> <li>If you don't have them, you need to re-run <code>source dev_scripts/setenv.sh</code> in   all the shells. It might be easier to kill that tmux session and restart it</li> </ul> <p>```bash</p> <p>tmux kill-session --t limeXYZ</p> <p>~/go_lem.sh XYZ   ```</p> <ul> <li>Inside or outside a Docker bash run</li> </ul> <p>```bash</p> <p>publish_notebook.py --file http://127.0.0.1:2908/notebooks/notebooks/Task40_Optimizer.ipynb --action publish_on_s3   ```</p> <ul> <li>The file is copied to S3</li> </ul> <p><code>bash   Copying './Task40_Optimizer.20210717_010806.html' to   's3://alphamatic-data/notebooks/Task40_Optimizer.20210717_010806.html'</code></p> <ul> <li>You can also save the data locally:</li> </ul> <p>```bash</p> <p>publish_notebook.py --file   amp/oms/notebooks/Master_forecast_processor_reader.ipynb --action publish_on_s3   --aws_profile saml-spm-sasm   ```</p> <ul> <li>You can also use a different path or profile by specifying it directly</li> </ul> <p>```bash</p> <p>publish_notebook.py \\   --file http://127.0.0.1:2908/notebooks/notebooks/Task40_Optimizer.ipynb \\   --action publish_on_s3 \\   --s3_path s3://alphamatic-data/notebooks \\   --aws_profile am   ```</p>"},{"location":"work_tools/all.development.how_to_guide.html#open-a-published-notebook","title":"Open a published notebook","text":""},{"location":"work_tools/all.development.how_to_guide.html#start-a-server","title":"Start a server","text":"<ul> <li> <p><code>(cd /local/home/share/html/published_notebooks; python3 -m http.server 8000)</code></p> </li> <li> <p>Go to the page in the local browser</p> </li> </ul>"},{"location":"work_tools/all.development.how_to_guide.html#using-the-dev-box","title":"Using the dev box","text":"<ul> <li>To open a notebook saved on S3, *outside* a Docker container run:</li> </ul> <p>```bash</p> <p>publish_notebook.py --action open --file   s3://alphamatic-data/notebooks/Task40_Optimizer.20210717_010806.html   ```</p> <ul> <li>This opens a Chrome window through X-windows.</li> <li>To open files faster you can open a Chrome window in background with</li> </ul> <p>```bash</p> <p>google-chrome   ```</p> <ul> <li>And then navigate to the path (e.g.,   <code>/local/home/share/html/published_notebooks/Master_forecast_processor_reader.20220810-112328.html</code>)</li> </ul>"},{"location":"work_tools/all.development.how_to_guide.html#using-windows-browser","title":"Using Windows browser","text":"<ul> <li>Another approach is:</li> </ul> <p>```bash</p> <p>aws s3 presign --expires-in 36000   s3://alphamatic-data/notebooks/Task40_Optimizer.20210716_194400.html | xclip   ```</p> <ul> <li>Open the link saved in the clipboard in the Windows browser</li> <li>For some reason, Chrome saves the link instead of opening, so you need to   click on the saved link</li> </ul>"},{"location":"work_tools/all.development.how_to_guide.html#how-to-create-a-private-fork","title":"How to create a private fork","text":"<ul> <li>Https://stackoverflow.com/questions/10065526/github-how-to-make-a-fork-of-public-repository-private</li> <li>From   https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/creating-a-repository-on-github/duplicating-a-repository</li> </ul> <p>```bash</p> <p>git clone --bare git@github.com:alphamatic/amp.git amp_bare</p> <p>git push --mirror https://github.com/cryptomtc/cmamp.git   ```</p> <ul> <li>It worked only as cryptomtc, but not using my key</li> </ul>"},{"location":"work_tools/all.development.how_to_guide.html#integrate-public-to-private-amp-cmamp","title":"Integrate public to private: <code>amp</code> -&gt; <code>cmamp</code>","text":""},{"location":"work_tools/all.development.how_to_guide.html#set-up","title":"Set-up","text":"<pre><code>&gt; git remote add public git@github.com:alphamatic/amp\n\n## Go to cmamp\n&gt; cd /data/saggese/src/cmamp1\n&gt; cd /Users/saggese/src/cmamp1\n\n## Add the remote\n## git remote add public https://github.com/exampleuser/public-repo.git\n&gt; git remote add public git@github.com:alphamatic/amp\n\n&gt; git remote -v\norigin https://github.com/cryptomtc/cmamp.git (fetch)\norigin https://github.com/cryptomtc/cmamp.git (push)\npublic git@github.com:alphamatic/amp (fetch)\npublic git@github.com:alphamatic/amp(push)\n</code></pre>"},{"location":"work_tools/all.development.how_to_guide.html#ours-vs-theirs","title":"Ours vs theirs","text":"<ul> <li> <p>From   https://stackoverflow.com/questions/25576415/what-is-the-precise-meaning-of-ours-and-theirs-in-git/25576672</p> </li> <li> <p>When merging:</p> </li> <li>Ours = branch checked out (git checkout *ours*)</li> <li>Theirs = branch being merged (git merge *theirs*)</li> <li>When rebasing the role is swapped</li> <li>Ours = branch being rebased onto (e.g., master)</li> <li>Theirs = branch being rebased (e.g., feature)</li> </ul>"},{"location":"work_tools/all.development.how_to_guide.html#sync-the-repos-after-double-integration","title":"Sync the repos (after double integration)","text":"<pre><code>&gt; git fetch origin; git fetch public\n\n## Pull from both repos\n\n&gt; git pull public master -X ours\n\n## You might want to use `git pull -X theirs` or `ours`\n\n&gt; git pull -X theirs\n\n&gt; git pull public master -s recursive -X ours\n\n## When there is a file added it is better to add\n\n&gt; git diff --name-status --diff-filter=U | awk '{print $2}'\n\nim/ccxt/db/test/test_ccxt_db_utils.py\n\n## Merge branch\n\n&gt; gs\n+ git status\nOn branch AmpTask1786_Integrate_20211128_02 Your branch and 'origin/AmpTask1786_Integrate_20211128_02' have diverged, and have 861 and 489\ndifferent commits each, respectively. (use \"git pull\" to merge the remote branch into yours)\n\nYou are in a sparse checkout with 100% of tracked files present.\n\nnothing to commit, working tree clean\n\n&gt; git pull -X ours\n\n### Make sure it's synced at ToT\n\n&gt; rsync --delete -r /Users/saggese/src/cmamp2/ /Users/saggese/src/cmamp1\n--exclude='.git/'\n\n&gt; diff -r --brief /Users/saggese/src/cmamp1 /Users/saggese/src/cmamp2 | grep -v \\.git\n</code></pre>"},{"location":"work_tools/all.development.how_to_guide.html#updated-sync","title":"Updated sync","text":"<pre><code>&gt; git fetch origin; git fetch public\n</code></pre>"},{"location":"work_tools/all.development.how_to_guide.html#check-that-things-are-fine","title":"Check that things are fine","text":"<pre><code>&gt; git diff origin/master... &gt;patch.txt\n\n&gt; cd /Users/saggese/src/cmamp2\n\n## Create a branch\n\n&gt; git checkout -b Cmamp114_Integrate_amp_cmamp_20210928\n&gt; git apply patch.txt\n\n## Compare branch with references\n\n&gt; dev_scripts/diff_to_vimdiff.py --dir1 /Users/saggese/src/cmamp1/im --dir2\n/Users/saggese/src/cmamp2/im\n\n&gt; diff -r --brief /Users/saggese/src/lemonade3/amp \\~/src/cmamp2 | grep -v \"/im\"\n\n## Creates a merge commit\n&gt; git push origin master\n</code></pre>"},{"location":"work_tools/all.development.how_to_guide.html#integrate-private-to-public-cmamp-amp","title":"Integrate private to public: <code>cmamp</code> -&gt; <code>amp</code>","text":"<pre><code>&gt; cd /data/saggese/src/cmamp1\n&gt; tar cvzf patch.tgz $(git diff --name-onlyorigin/master public/master | grep -v repo_config.py)\n\n&gt; cd /Users/saggese/src/amp1 git remote add cmamp\n&gt; git@github.com:cryptomtc/cmamp.git\n\n&gt; GIT_SSH_COMMAND=\"ssh -i \\~/.ssh/cryptomatic/id_rsa.cryptomtc.github\" git fetch\n&gt; git@github.com:cryptomtc/cmamp.git\n\n&gt; git checkout -b Integrate_20210928\n\n&gt; GIT_SSH_COMMAND=\"ssh -i \\~/.ssh/cryptomatic/id_rsa.cryptomtc.github\" git pull\n&gt; cmamp master -X ours\n</code></pre>"},{"location":"work_tools/all.development.how_to_guide.html#squash-commit-of-everything-in-the-branch","title":"Squash commit of everything in the branch","text":"<ul> <li>From   https://stackoverflow.com/questions/25356810/git-how-to-squash-all-commits-on-branch</li> </ul> <p>```bash</p> <p>git checkout yourBranch git reset $(git merge-base master $(git branch   --show-current)) git add -A git commit -m \"Squash\" git push --force   ```</p>"},{"location":"work_tools/all.development.how_to_guide.html#double-integration-cmamp-amp","title":"Double integration <code>cmamp</code> &lt; -- &gt; <code>amp</code>","text":"<ul> <li>The bug is https://github.com/alphamatic/amp/issues/1786</li> </ul>"},{"location":"work_tools/all.development.how_to_guide.html#script-set-up","title":"Script set-up","text":"<pre><code>&gt; vi /Users/saggese/src/amp1/dev_scripts/integrate_repos/setup.sh\nUpdate the date\n\n&gt; vi /Users/saggese/src/amp1/dev_scripts/integrate_repos/*\n\n&gt; cd \\~/src/amp1\n&gt; source /Users/saggese/src/amp1/dev_scripts/integrate_repos/setup.sh\n\n&gt; cd \\~/src/cmamp1\n&gt; source /Users/saggese/src/amp1/dev_scripts/integrate_repos/setup.sh\n</code></pre>"},{"location":"work_tools/all.development.how_to_guide.html#manual-set-up-branches","title":"Manual set-up branches","text":"<pre><code>## Go to cmamp1\n&gt; go_amp.sh cmamp 1\n\n## Set up the env vars in both clients\n&gt; export AMP_DIR=/Users/saggese/src/amp1; export\nCMAMP_DIR=/Users/saggese/src/cmamp1; echo \"$AMP_DIR\"; ls\n$AMP_DIR; echo \"$CMAMP_DIR\"; ls $CMAMP_DIR\n\n## Create two branches\n&gt; export BRANCH_NAME=AmpTask1786_Integrate_20211010 export BRANCH_NAME=AmpTask1786_Integrate_2021117\n...\n&gt; cd $AMP_DIR\n\n## Create automatically\n&gt; i git_create_branch -b $BRANCH_NAME\n\n## Create manually\n&gt; git checkout -b $BRANCH_NAME\n&gt; git push --set-upstream origin $BRANCH_NAME\n\n&gt; cd $CMAMP_DIR\n&gt; i git_create_branch -b $BRANCH_NAME\n</code></pre>"},{"location":"work_tools/all.development.how_to_guide.html#high-level-plan","title":"High-level plan","text":"<ul> <li>SUBDIR=im</li> <li>Typically <code>cmamp</code> is copied on top of <code>amp</code></li> <li>SUBDIR=devops</li> <li><code>cmamp</code> and <code>amp</code> need to be different (until we unify the Docker flow)</li> <li>Everything else</li> <li>Typically <code>amp</code> -&gt; <code>cmamp</code></li> </ul>"},{"location":"work_tools/all.development.how_to_guide.html#sync-im-cmamp-amp","title":"Sync <code>im</code> <code>cmamp</code> -&gt; <code>amp</code>","text":"<pre><code>SUBDIR=im\n\n## Check different files\n&gt; diff -r --brief $AMP_DIR/$SUBDIR $CMAMP_DIR/$SUBDIR | grep -v .git\n\n## Diff the entire dirs with vimdiff\n&gt; dev_scripts/diff_to_vimdiff.py --dir1 $AMP_DIR/$SUBDIR --dir2 $CMAMP_DIR/$SUBDIR\n\n## Find different files\n&gt; find $AMP_DIR/$SUBDIR -name \"*\"; find $CMAMP_DIR/$SUBDIR -name \"*\" sdiff\n/tmp/dir1 /tmp/dir2\n\n## Copy cmamp -&gt; amp\n&gt; rsync --delete -au $CMAMP_DIR/$SUBDIR/ $AMP_DIR/$SUBDIR\n-a = archive\n-u = ignore newer\n\n## Add all the untracked files\n&gt; cd $AMP_DIR/$SUBDIR &amp;&amp; git add $(git ls-files -o --exclude-standard)\n\n## Check that there are no differences after copying\n&gt; dev_scripts/diff_to_vimdiff.py --dir1 $AMP_DIR/$SUBDIR --dir2 $CMAMP_DIR/$SUBDIR\n\n==========\n\n&gt; rsync --delete -rtu $AMP_DIR/$SUBDIR/ $CMAMP_DIR/$SUBDIR\n\n&gt; rsync --dry-run -rtui --delete $AMP_DIR/$SUBDIR/ $CMAMP_DIR/$SUBDIR/ .d..t.... ./\n&gt; f..t.... __init__.py\ncd+++++++ features/\n&gt; f+++++++ features/__init__.py\n&gt; f+++++++ features/pipeline.py\ncd+++++++ features/test/\n&gt; f+++++++ features/test/test_feature_pipeline.py\ncd+++++++ features/test/TestFeaturePipeline.test1/\ncd+++++++ features/test/TestFeaturePipeline.test1/output/\n&gt; f+++++++ features/test/TestFeaturePipeline.test1/output/test.txt\n.d..t.... price/\n.d..t.... real_time/\n&gt; f..t.... real_time/__init__.py\n.d..t.... real_time/notebooks/\n&gt; f..t.... real_time/notebooks/Implement_RT_interface.ipynb\n&gt; f..t.... real_time/notebooks/Implement_RT_interface.py\n.d..t.... real_time/test/\ncd+++++++ real_time/test/TestRealTimeReturnPipeline1.test1/\ncd+++++++ real_time/test/TestRealTimeReturnPipeline1.test1/output/\n&gt; f+++++++ real_time/test/TestRealTimeReturnPipeline1.test1/output/test.txt\n.d..t.... returns/\n&gt; f..t.... returns/__init__.py\n&gt; f..t.... returns/pipeline.py\n.d..t.... returns/test/\n&gt; f..t.... returns/test/test_returns_pipeline.py\n.d..t.... returns/test/TestReturnsBuilder.test_equities1/\n.d..t.... returns/test/TestReturnsBuilder.test_equities1/output/\n.d..t.... returns/test/TestReturnsBuilder.test_futures1/\n.d..t.... returns/test/TestReturnsBuilder.test_futures1/output/\n\n&gt; rsync --dry-run -rtui --delete $CMAMP_DIR/$SUBDIR/ $AMP_DIR/$SUBDIR/\n&gt; f..t.... price/__init__.py\n&gt; f..t.... price/pipeline.py\n&gt; f..t.... real_time/pipeline.py\n&gt; f..t.... real_time/test/test_dataflow_amp_real_time_pipeline.py\n&gt; f..t.... returns/test/TestReturnsBuilder.test_equities1/output/test.txt\n&gt; f..t.... returns/test/TestReturnsBuilder.test_futures1/output/test.txt\n</code></pre>"},{"location":"work_tools/all.development.how_to_guide.html#sync-everything","title":"Sync everything","text":"<pre><code>## Check if there is anything in cmamp more recent than amp\n&gt; rsync -au --exclude='.git' --exclude='devops' $CMAMP_DIR/ $AMP_DIR\n\n## vimdiff\n&gt; dev_scripts/diff_to_vimdiff.py --dir1 $AMP_DIR --dir2\n$CMAMP_DIR\n\nF1: skip\nF9: choose left (i.e., amp)\nF10: choose right (i.e,. cmamp)\n\n## Copy\n\n&gt; rsync -au --delete --exclude='.git' --exclude='devops' --exclude='im'\n$AMP_DIR/\n$CMAMP_DIR\n\n## Add all the untracked files\n\n&gt; (cd $CMAMP_DIR/$SUBDIR &amp;&amp; git add $(git ls-files -o --exclude-standard))\n\n&gt; diff -r --brief $AMP_DIR $CMAMP_DIR | grep -v .git | grep Only\n</code></pre>"},{"location":"work_tools/all.development.how_to_guide.html#files-that-need-to-be-different","title":"Files that need to be different","text":"<ul> <li> <p><code>amp</code> needs an <code>if False</code> <code>helpers/lib_tasks.py</code></p> </li> <li> <p><code>amp</code> needs two tests disabled <code>im/ccxt/data/load/test/test_loader.py</code></p> </li> </ul> <p><code>im/ccxt/data/load/test/test_loader.py</code></p> <p>TODO(gp): How to copy files in vimdiff including last line?</p> <ul> <li>Have a script to remove all the last lines</li> <li>Some files end with an <code>0x0a</code></li> <li><code>tr -d '\\\\r'</code></li> </ul> <p>```bash</p> <p>find . -name \"*.txt\" | xargs perl -pi -e 's/\\r\\n/\\n/g'   # Remove <code>No newline at end of file</code> find . -name \"*.txt\" | xargs perl -pi -e 'chomp if eof'   ```</p>"},{"location":"work_tools/all.development.how_to_guide.html#testing","title":"Testing","text":"<ul> <li>Run <code>amp</code> on my laptop (or on the server)</li> <li>IN PROGRESS: Get <code>amp</code> PR to pass on GH</li> <li>IN PROGRESS: Run lemonade on my laptop</li> <li>Run <code>cmamp</code> on the dev server</li> <li>Get <code>cmamp</code> PR to pass on GH</li> <li>Run dev_tools on the dev server</li> </ul>"},{"location":"work_tools/all.dind_and_sibling_containers.how_to_guide.html","title":"Docker-in-docker (dind)","text":"<ul> <li>It is possible to install a Docker engine inside a Docker container so that   one can run Docker container (e.g., OMS or IM) inside an isolated <code>amp</code>   container.</li> <li>The problems with this approach are:</li> <li>Dind requires to run the external container in privileged mode, which might     not be possible due to security concerns</li> <li>The Docker / build cache is not shared across parent and children     containers, so one needs to pull / build an image every time the outermost     container is restarted</li> <li>An alternative approach is the \"sibling container\" approach</li> </ul>"},{"location":"work_tools/all.dind_and_sibling_containers.how_to_guide.html#sibling-container-approach","title":"Sibling container approach","text":"<ul> <li>Refs:</li> <li>Can I run Docker-in-Docker without using the --privileged flag - Stack Overflow</li> <li>https://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/</li> <li>Often what's really needed is the ability to build / run a container from   another container (e.g., CI or unit test). This can be achieved by mounting   the Docker socket <code>/var/run/docker.sock</code> to the container, so that a container   can talk to Docker Engine.</li> <li>This approach allows reuse of the build cache across the sibling containers.</li> <li>The downside is less isolation from the external container, e.g., spawned   containers can be left hanging or can collide.</li> <li>E.g.,   <code>``   # Run</code>docker ps` in a container, showing the containers running in the main   container <p>docker run -ti --rm \\         -v /var/run/docker.sock:/var/run/docker.sock \\         dindtest \\         docker ps</p> </li> </ul> <p># Start a sibling hello world container:</p> <p>docker run -it --rm \\         -v /var/run/docker.sock:/var/run/docker.sock \\         dindtest \\         docker run -ti --rm hello-world   ```</p>"},{"location":"work_tools/all.dind_and_sibling_containers.how_to_guide.html#connecting-to-postgres-instance-using-sibling-containers","title":"Connecting to Postgres instance using sibling containers","text":"<ul> <li>We can start the Docker container with Postgres as a service from outside the   container.   ``` <p>(cd oms;  i oms_docker_up -s local)   INFO: &gt; cmd='/local/home/gsaggese/src/venv/amp.client_venv/bin/invoke oms_docker_up -s local'   report_memory_usage=False report_cpu_usage=False   docker-compose \\   --file /local/home/gsaggese/src/sasm-lime4/amp/oms/devops/compose/docker-compose.yml \\   --env-file /local/home/gsaggese/src/sasm-lime4/amp/oms/devops/env/local.oms_db_config.env \\   up \\   oms_postgres   Creating compose_oms_postgres_1 ... done   Attaching to compose_oms_postgres_1   oms_postgres_1  |   oms_postgres_1  | PostgreSQL Database directory appears to contain a database; Skipping initialization   oms_postgres_1  |   oms_postgres_1  | 2022-05-19 22:57:15.659 UTC [1] LOG:  starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit   oms_postgres_1  | 2022-05-19 22:57:15.659 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432   oms_postgres_1  | 2022-05-19 22:57:15.659 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432   oms_postgres_1  | 2022-05-19 22:57:15.663 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"   oms_postgres_1  | 2022-05-19 22:57:15.670 UTC [25] LOG:  database system was shut down at 2022-05-19 22:56:50 UTC   oms_postgres_1  | 2022-05-19 22:57:15.674 UTC [1] LOG:  database system is ready to accept connections   ```</p> </li> <li>Note that Postgres needs to be</li> <li>Start a container able to</li> <li>From inside a container I launch postgres through the /var/...   ``` <p>docker ps | grep postgres   CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES   83bba0818c74 postgres:13 \"docker-entrypoint.s...\" 6 minutes ago Up 6 minutes   0.0.0.0:5432-&gt;5432/tcp compose-oms_postgres-1   ```</p> </li> <li>Test connection to the DB from outside the container   ``` <p>psql --host=cf-spm-dev4 --port=5432 --user aljsdalsd -d oms_postgres_db_local   Password for user aljsdalsd:   psql (9.5.25, server 13.5 (Debian 13.5-1.pgdg110+1))   WARNING: psql major version 9.5, server major version 13.           Some psql features might not work.   Type \"help\" for help.   oms_postgres_db_local=#   ```</p> </li> <li>Test connection to the DB from inside the container   ``` <p>psql --host=cf-spm-dev4 --port=5432 --user aljsdalsd -d oms_postgres_db_local   ...   ```</p> </li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html","title":"Docker","text":""},{"location":"work_tools/all.docker.how_to_guide.html#introduction","title":"Introduction","text":"<ul> <li>Docker is an open-source tool designed to make our life typically easier   (although sometimes it makes it harder) when creating, building, deploying,   and running software applications.</li> <li>Docker can package an application and its dependencies in a virtual container   that can run on any Linux, Windows, or macOS computer.</li> <li>Our Docker containers have everything required (e.g. OS packages, Python   packages) inside to run certain applications/code.</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#concepts","title":"Concepts","text":""},{"location":"work_tools/all.docker.how_to_guide.html#docker-image","title":"Docker image","text":"<ul> <li>A Docker image is a read-only template with instructions for creating a Docker   container</li> <li>Typically instructions include information about which packages and their   versions to install, e.g. list of python packages and their corresponding   versions</li> <li>All steps needed to create the image and run it are defined in a Dockerfile,   e.g. <code>dev_tools/devops/docker_build/dev.Dockerfile</code></li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#docker-container","title":"Docker container","text":"<ul> <li>A Docker container is a runnable instance of an image. One can run code inside   a docker container having all requirements installed.</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#docker-registry","title":"Docker registry","text":"<ul> <li>A Docker registry stores docker images. In other words, Docker registry for   docker images is like GitHub for code.</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#high-level-philosophy","title":"High level philosophy","text":"<ul> <li>We always want to separate things that don't need to run together in different   containers (e.g., <code>dev / prod cmamp</code>, <code>optimizer</code>, <code>im</code>, <code>oms</code>, <code>dev_tools</code>),   along a logic of \"independently runnable / deployable directories\".</li> <li>The problem is that when we put too many dependencies in a single container,   trying to simplify the release approach we start having huge containers that   are difficult to deploy and are unstable in terms of building even using   <code>poetry</code>.</li> <li>Each dir that can be \"deployed\" and run should have a <code>devops</code> dir to build /   qa / release containers with all the needed dependencies</li> <li>Certain containers that need to be widely available to the team and deployed   go through the release process and ECR</li> <li>Other containers that are lightweight and used only by one person (e.g., the   <code>infra</code> container) can be built on the fly using   <code>docker compose</code>/<code>docker build</code>.</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#thin-client","title":"Thin client","text":"<ul> <li>To bootstrap the system we use a \"thin client\" which installs in a virtual env   the minimum set of packages to run (e.g., installs <code>invoke</code>, <code>docker</code>, etc).</li> <li>TODO(gp): Audit / make sure we can simplify the thin env</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#amp-cmamp-container","title":"amp / cmamp container","text":"<ul> <li>The <code>dev</code> version is used to develop</li> <li>The <code>prod</code> version can be used for deployment as shortcut to creating a   smaller container with only the strictly needed dependencies</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#prod-container","title":"Prod container","text":"<ul> <li>In order to avoid shipping the monster cmamp dev / prod container, we want to   start building smaller containers with only the dependencies that specific   prod scripts need</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#infra-container","title":"Infra container","text":"<ol> <li>To run infra script, if we only need <code>boto3</code> and <code>moto</code>, we can</li> <li>Create a Python library</li> <li>Create a script interface</li> <li>Create an <code>invoke</code> task that calls <code>i docker_cmd --cmd ...</code> reusing the      cmamp container, (since that container already has <code>boto3</code> and <code>moto</code> that      are dependencies we can't remove)<ul> <li>This approach is similar to calling the <code>linter</code></li> </ul> </li> <li>If we think we need to add new packages only for running infra scripts then    we will create a new <code>infra</code> container.</li> <li> <p>We can build on the fly and not release through ECR</p> </li> <li> <p>We can start with approach 1, which will also allow us to transition to 2   transparently, if needed</p> </li> </ol>"},{"location":"work_tools/all.docker.how_to_guide.html#relevant-bugs","title":"Relevant bugs","text":"<ul> <li>https://github.com/cryptokaizen/cmamp/issues/1060</li> <li>Tool to extract the dependency from a project #1038</li> <li>Create tool for poetry debugging #1026</li> <li>Fix tests that fail due to pandas update and release cmamp image #1002</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#poetry","title":"Poetry","text":"<ul> <li>Poetry is a tool for managing Python packages and dependencies:</li> <li>List packages you want to install with some constraints, e.g., <code>pandas</code> must     be above 1.0 in <code>devops/docker_build/pyproject.toml</code></li> <li>Given a list of packages you need to install to get the desired environment,     you want <code>poetry</code> to \"optimize\" the packages and generate     <code>devops/docker_build/poetry.lock</code>, which contains the list of versions of     the packages to install</li> <li>If there is a new version of a package re-running <code>poetry</code> might give you an     updated list of packages to install</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#build-a-docker-image","title":"Build a Docker image","text":""},{"location":"work_tools/all.docker.how_to_guide.html#general","title":"General","text":"<ul> <li>A docker image is built from a <code>Dockerfile</code>. The image is then used to run a   Docker container.</li> </ul> <ul> <li>There is <code>/devops</code> dir under a project's dir that contains Docker-related   files, e.g. <code>cmamp/devops</code>.</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#dockerfile","title":"Dockerfile","text":"<ul> <li>A <code>Dockerfile</code> is a text document that contains all the commands to call on   the command line to assemble an image. E.g.   <code>cmamp/devops/docker_build/dev.Dockerfile</code>.</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#base-image","title":"Base image","text":"<ul> <li>A <code>Dockerfile</code> should start with specifying a base image.</li> <li>Base image is an image that a new image is built from. A new Docker image will   have all the packages/dependencies that are installed in the base image.</li> <li>Use <code>FROM</code> statement to specify a base image, e.g.   <code>FROM ubuntu:20.4</code></li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#copy-files","title":"Copy files","text":"<ul> <li>Copy files that are required to build a Docker image to the Docker filesystem.</li> <li>To copy a file from <code>/source_dir</code> (your filesystem) to <code>/dst_dir</code> (Docker   filesystem) do:   <code>COPY source_dir/file dst_dir</code></li> <li>E.g., the command below will copy <code>install_packages.sh</code> from   <code>devops/docker_build</code> to the Docker's root directory so that   <code>install_packages.sh</code> can be accessed by Docker.   <code>COPY devops/docker_build/install_packages.sh .</code></li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#install-os-packages","title":"Install OS packages","text":"<ul> <li>Install OS packages that are needed for a Docker app, but that are not   installed for a base image.</li> <li>Use <code>RUN</code> instruction to install a package, e.g.   <code>RUN apt-get install postgresql-client</code></li> <li>Alternatively you can package all installation instructions in a <code>.sh</code> file   and run it. Do not forget to copy a <code>.sh</code> file to the Docker filesystem so   that Docker can see it. E.g.,   <code>COPY devops/docker_build/install_packages.sh .   RUN /bin/sh -c \"./install_packages.sh\"</code></li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#install-python-packages","title":"Install Python packages","text":"<ul> <li>We prefer to install Python packages with <code>poetry</code>.</li> <li>Make sure that there is instruction to install <code>pip3</code> and <code>poetry</code>. You can   either put it in a <code>Dockerfile</code> or in a separate file like   <code>install_packages.sh</code>.   <code>RUN apt-get install python3-pip   RUN pip3 install poetry</code></li> <li>Copy poetry-related files to the Docker filesystem so that files can be   accessed by Docker   <code>COPY devops/docker_build/poetry.toml   COPY devops/docker_build/poetry.lock</code></li> <li>Update Python packages   <code>RUN poetry install</code></li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#build-an-image-from-a-dockerfile","title":"Build an image from a Dockerfile","text":"<ul> <li>To build an image from a <code>Dockerfile</code> run:   ``` <p>docker build .   ```</p> </li> <li>The <code>Dockerfile</code> must be called <code>Dockerfile</code> and located in the root of the   context.</li> <li>You can point to any <code>Dockerfile</code> by using <code>-f</code>:   ``` <p>docker build -f /path_to_dockerfile/dockerfile_name   ```</p> </li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#run-multi-container-docker-application","title":"Run multi-container Docker application","text":"<ul> <li>Compose is a tool for defining and running multi-container Docker   applications. With Compose, you use a <code>YAML</code> file to configure your   application's services.</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#version","title":"Version","text":"<ul> <li>At the beginning of a <code>docker-compose.yaml</code> file specify the <code>docker-compose</code>   version. We use the version <code>3.0</code>, for more information see   the official documents.   <code>version: \"3.0\"</code></li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#images","title":"Images","text":"<ul> <li>You can either re-use a public image or build a new one from a <code>Dockerfile</code>.</li> <li>The <code>app</code> service below uses the image that is built from the   <code>dev.Dockerfile</code>.   <code>app:     build:       context: .       dockerfile: dev.Dockerfile</code></li> <li>The <code>im_postgres_local</code> service below uses the public <code>postgres</code> image pulled   from the Docker hub registry.   <code>im_postgres_local:     image: postgres: 13</code></li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#bind-mount","title":"Bind mount","text":"<ul> <li>If you want to be able to access code inside a Docker container, you should   bind-mount a directory with the code on the host.</li> <li>Mount a directory on the host inside a Docker container, e.g. mount current   directory to <code>/app</code> dir inside a Docker container:   ```   app:     volumes:<ul> <li>.:/app   ```</li> </ul> </li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#environment-variables","title":"Environment variables","text":"<ul> <li>You can either use variables directly from the environment or pass them in a   <code>docker-compose.yaml</code> file.</li> <li>It is supposed that <code>POSTGRES_VERSION</code> is already defined in the shell.   <code>db:     image: \"postgres:${POSTGRES_VERSION}\"</code></li> <li>Set environment variable in a service's container   ```   db:     environment:<ul> <li>POSTGRES_VERSION=13   image: \"postgres:${POSTGRES_VERSION}\"   ```</li> </ul> </li> <li>Set environment variable with <code>.env</code> file   ```   db:     env_file:<ul> <li>./postgres_env.env   image: \"postgres:${POSTGRES_VERSION}\"   ```</li> </ul> </li> <li>File <code>postgres_env.env</code>   ``` <p>cat ./postgres_env.env   POSTGRES_VERSION=13   ```</p> </li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#basic-commands","title":"Basic commands","text":"<ul> <li>To check more advanced usage, please see   the official documentation.</li> <li>Build, (re)create, start, and attach to containers for a service. It is   assumed that a <code>docker-compose</code> file has the name <code>docker-compose.yaml</code> and is   located in the current dir.   ``` <p>docker-compose up   ```</p> </li> <li>List containers   ``` <p>docker-compose ps   ```</p> </li> <li>Stop containers created by <code>down</code>.   ``` <p>docker-compose down   ```</p> </li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#how-to-test-a-package-in-a-docker-container","title":"How to test a package in a Docker container","text":"<pre><code>&gt; sudo /bin/bash -c \"(source /venv/bin/activate; pip install yfinance)\"\n&gt; python -c \"import finance\"\n</code></pre>"},{"location":"work_tools/all.docker.how_to_guide.html#hacky-approach-to-patch-up-a-container","title":"Hacky approach to patch up a container","text":"<pre><code># After install create a new version of the container\n&gt; docker commit d2916dd5f122\n&gt; 623860924167.dkr.ecr.eu-north-1.amazonaws.com/cmamp:dev_ccxtpr\n\n# Push to the repo\n&gt; docker push 623860924167.dkr.ecr.eu-north-1.amazonaws.com/cmamp:dev_ccxtpro\n\n# Then you can push and pull on different machines\n&gt; docker pull 623860924167.dkr.ecr.eu-north-1.amazonaws.com/cmamp:dev_ccxtpro\n\n# To use `docker_bash` you might need to retag it to match what the system expects\n&gt; docker tag 623860924167.dkr.ecr.eu-north-1.amazonaws.com/cmamp:dev_ccxtpro\n</code></pre>"},{"location":"work_tools/all.docker.how_to_guide.html#how-to-release-a-docker-image","title":"How to release a Docker image","text":"<ul> <li>All the <code>invoke</code> tasks to run the release flow are in   <code>//amp/helpers/lib_tasks.py</code>.</li> <li>Depending on the type of changes sometimes one needs to rebuild only the   <code>prod</code> image, other times one needs to rebuild also the <code>dev</code> image.</li> <li>E.g.,</li> <li>If you change Docker build-related things (e.g., add a Python package), you     need to rebuild the <code>dev</code> image and then the <code>prod</code> image from the <code>dev</code>     image</li> <li>If you change the code for a production system, you need to create a new     <code>prod</code> image</li> <li>We try to use the same flow, conventions, and code for all the containers   (e.g., amp, cmamp, dev_tools, opt).</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#multi-architecture-build","title":"Multi-architecture build","text":"<ul> <li>To build multi-arch (e.g., <code>x86</code>, <code>arm</code>) docker image using   <code>docker_build_local_image</code> we should use <code>--multi-build</code> flag</li> <li>To build for specific platforms specify the platform name:<ul> <li>For <code>x86</code> - <code>linux/amd64</code></li> <li>For <code>arm</code> - <code>linux/arm64</code>   ``` <p>i docker_build_local_image --version  --multi-build --platform    ``` <li>To build for both <code>arm</code> and <code>x86</code> architectures:     <code>&gt; i docker_build_local_image --version &lt;VERSION&gt; --multi-build --platform linux/amd64,linux/arm64</code></li> <li>Multi-arch images are built using <code>docker buildx</code> which do not generate any     local image by default</li> <li>Images are pushed to the remote registry and pulled for testing and usage</li> <li>To tag the local image as dev and push it to the target registry: e.g.,     <code>aws_ecr.ck</code> or <code>dockerhub.sorrentum</code> , use     <code>&gt; i docker_tag_push_multi_build_local_image_as_dev --version &lt;VERSION&gt; --target &lt;TARGET&gt;</code></li> <li>Once the image has been successfully pushed to both ECR and DockerHub     registries, the subsequent step involves pushing the <code>dev</code> image to GHCR     registry. However, this action currently requires manual execution due to     restricted access<ul> <li>Access to the <code>cryptokaizen</code> packages is limited. To gain access, kindly   reach out to GP or Juraj</li> <li>To proceed, perform a Docker login using your GitHub username and PAT   (Personal Access Token):   ```bash <p>docker login ghcr.io -u    ``` <li>Tag the <code>dev</code> image to the GHCR namespace:   ```bash <p>docker tag 623860924167.dkr.ecr.eu-north-1.amazonaws.com/cmamp:dev ghcr.io/cryptokaizen/cmamp:dev   ```</p> </li> <li>Push the tagged image to the GHCR registry:   ```bash <p>docker push ghcr.io/cryptokaizen/cmamp:dev   ```</p> </li>"},{"location":"work_tools/all.docker.how_to_guide.html#run-the-dev-multi-architecture-image-release-end-to-end","title":"Run the dev multi-architecture image release end-to-end","text":""},{"location":"work_tools/all.docker.how_to_guide.html#overview","title":"Overview","text":"<ul> <li>Update the <code>changelog.txt</code> file with description of new version</li> <li>Build \"local\" image remotely in the CK AWS ECR registry and pull once it is   built</li> <li>Run the <code>cmamp</code> regressions using a local image</li> <li>Run QA tests using a local image</li> <li>Tag the image as dev image and push it to the target Docker registries</li> <li>Tag the new <code>dev</code> image to GHCR namespace and push it to GHCR registry</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#pre-release-check-list","title":"Pre-release check-list","text":"<p>Prerequisites:</p> <ul> <li>The new image is built locally</li> </ul> <p>Check-list:</p> <p>Make sure that the regressions are passing when being run using the local image because we run the regressions as part of the official release flow, i.e. via <code>docker_release_multi_build_dev_image()</code>.</p> <ul> <li><code>cmamp</code></li> <li>[ ] Update the <code>changelog.txt</code> file</li> <li>[ ] Fast tests</li> <li>[ ] Slow tests</li> <li>[ ] Super-slow test</li> <li>[ ] QA tests</li> </ul> <p>Running regressions in the <code>orange</code> repository is not a part of the official image release flow so run them separately.</p> <ul> <li><code>orange</code></li> <li>[ ] Update the <code>changelog.txt</code> file</li> <li>[ ] Fast tests</li> <li>[ ] Slow tests</li> <li>[ ] Super-slow test</li> </ul> <p>Example:</p> <pre><code>i run_fast_tests --version 1.10.0 --stage local\n</code></pre> <p>Where <code>1.10.0</code> is the new version of the image with stage as local.</p>"},{"location":"work_tools/all.docker.how_to_guide.html#command-to-run-the-release-flow","title":"Command to run the release flow:","text":"<pre><code>&gt; i docker_release_multi_build_dev_image --version &lt;VERSION&gt; --platform &lt;PLATFORM&gt; --target-registries &lt;TARGET_REGISTRIES&gt;\n</code></pre> <p>E.g.,</p> <pre><code>i docker_release_multi_build_dev_image --version 1.6.1 --platform linux/amd64,linux/arm64 --target-registries aws_ecr.ck,dockerhub.sorrentum\n</code></pre> <p>TARGET_REGISTRIES: list of target registries to push the image to.</p> <p>E.g.,</p> <ul> <li><code>aws_ecr.ck</code> -- private CK AWS Docker registry</li> <li><code>dockerhub.sorrentum</code> -- public Dockerhub registry</li> </ul> <p>All other options are the same as for the <code>docker_release_dev_image</code> end-to-end flow.</p> <ul> <li>End-to-end flow for <code>dev</code> image</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#post-release-check-list","title":"Post-release check-list","text":"<ul> <li>[ ] Make an integration with the <code>sorrentum</code> repository in order to copy all       the changes from the <code>cmamp</code> repository</li> <li>[ ] Tag the new <code>dev</code> image to GHCR namespace and push it to GHCR registry</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#stages","title":"Stages","text":"<ul> <li>A \"stage\" is a step (e.g., local, dev, prod) in our release workflow of Docker   images, code, or infrastructure.</li> <li>To run a Docker container in a certain stage use the <code>stage</code> parameter</li> <li>E.g. <code>i docker_bash --stage=\"local\"</code> creates a bash session inside the local     docker <code>amp</code> container</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#local","title":"Local","text":"<ul> <li>A <code>local</code> image is used to develop and test an update to the Docker container,   e.g. after updating a package, installing a new package, etc.</li> <li>Local images can only be accessed locally by a developer, i.e. the team   members can not / should not use local images. In practice <code>local</code> images are   like <code>dev</code> images but private to users and servers.</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#dev","title":"Dev","text":"<ul> <li>A <code>dev</code> image is used by our team to develop our systems (e.g., to add new   functionalities to the <code>dev_tools</code> code).</li> <li>Typically the source code is mounted through a bind mount in Docker so that   one can change the code and execute it in Docker.</li> <li>The image is tested, blessed, and released so that users and CI can use it   without worries. Once a <code>dev</code> image is pushed to the docker registry it can be   pulled and used by the team members.</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#prod","title":"Prod","text":"<ul> <li>A <code>prod</code> image is used to run a system by final users. E.g., the linter inside   <code>dev_tools</code>, some prod system inside Airflow.</li> <li>It is self-contained (it should have no dependencies) since it has everything   required to run a system installed inside it, e.g., code (e.g., the linter),   Python packages.</li> <li>It is typically created from the <code>dev</code> image by copying the released code   inside the <code>prod</code> image.</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#overview-of-how-to-release-an-image","title":"Overview of how to release an image","text":"<ul> <li>The release flow consists of the following phases</li> <li>Make changes to the image<ul> <li>E.g., add Python package</li> <li>Update the changelog</li> </ul> </li> <li>Build a local image<ul> <li>Run specific tests (e.g., make sure that the new packages are installed)</li> <li>Run unit tests</li> <li>Run QA tests</li> </ul> </li> <li>Tag local image as dev image</li> <li>Push dev image to ECR</li> <li>Push the image to GHCR</li> <li>If there is also an associated prod image</li> <li>Build prod image from dev image<ul> <li>Run unit / QA tests</li> </ul> </li> <li>Push prod image to ECR</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#how-to-add-a-python-package-to-docker-image","title":"How to add a Python package to Docker image","text":"<ul> <li>To add a new Python package to a Docker image you need to update <code>poetry</code>   files and release a new image:</li> <li>Add a new package to <code>amp/devops/docker_build/pyproject.toml</code> file to the     <code>[tool.poetry.dependencies]</code> section E.g., to add <code>pytest-timeout</code> do:     <code>[tool.poetry.dependencies]     ...     pytest-timeout = \"*\"     ...</code></li> <li>In general we use the latest version of a package (<code>*</code>) until the tests fail     or the system stops working<ul> <li>If the system fails, we freeze the version of the problematic packages to   a known-good version to get the tests back to green until the problem is   solved. We switch back to the latest version once the problem is fixed</li> <li>If you need to put a constraint on the package version, follow the   official docs,   and explain in a comment why this is needed making reference to GitHub   issues</li> </ul> </li> <li>To verify that package is installed correctly one can<ul> <li>Build a local image. There are two options:</li> <li>Update poetry and upgrade all packages to the latest versions     <code>&gt; i docker_build_local_image --version {new version} --update-poetry</code></li> <li>Refresh the lock file (e.g., install / update / remove a single package)     without upgrading all the packages     Link to the poetry docs.     <code>&gt; i docker_build_local_image --version {new version} --update-poetry --refresh-only-poetry</code></li> <li>Run a docker container based on the local image   ``` <p>i docker_bash --stage local --version {new version}   ```</p> </li> <li>Verify what package was installed with <code>pip show {package name}</code>, e.g.,   ``` <p>pip show pytest-rerunfailures   Name: pytest-rerunfailures   Version: 10.2   Summary: pytest plugin to re-run tests to eliminate flaky failures   ...   Location: /venv/lib/python3.8/site-packages   Requires: pytest, setuptools   Required-by:   ```</p> </li> <li>Run regressions for the local image, i.e.   ``` <p>i run_fast_tests --stage local --version {new version} i run_slow_tests --stage local --version {new version}   ```</p> </li> </ul> </li> <li>Update the changelog describing the new version</li> <li>Send a PR with the updated poetry files and any other change needed to make     the tests pass</li> <li>Release the new image. To do so follow the     # Release a Docker image     section, use <code>--update-poetry</code> flag to resolve the dependencies</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#how-to-find-unused-packages","title":"How to find unused packages","text":"<ul> <li>While installing Python packages we need to make sure that we do not install   packages that we do not use</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#import-based-approach-using-pipreqs","title":"Import-based approach using <code>pipreqs</code>","text":""},{"location":"work_tools/all.docker.how_to_guide.html#how-it-works","title":"How it works","text":"<ul> <li>To do so we use an import-based approach provided by   <code>pipreqs</code>. Under the hood it uses the regex   below and <code>os.walk</code> for selected dir:   <code>REGEXP = [       re.compile(r'^import (.+)$'),       re.compile(r'^from ((?!\\.+).*?) import (?:.*)$')   ]</code></li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#limitations","title":"Limitations","text":"<ul> <li>Not all packages that we use are necessarily imported, e.g. <code>awscli</code>,   <code>jupyter</code>, <code>pytest-cov</code>, etc. -&gt; <code>pipreqs</code> won't find these packages</li> <li>The import name is not always equal to the package actual name, see the   mapping here</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#usage","title":"Usage","text":"<ul> <li>See the official docs for the advanced   usage.</li> <li>Run a bash session inside a Docker container</li> <li>Install <code>pipreqs</code> with <code>sudo pip install pipreqs</code><ul> <li>We install it temporary within a Docker bash session in order to introduce   another dependency</li> <li>You need to re-install <code>pipreqs</code> everytime you create a new Docker bash   session</li> </ul> </li> <li>To run for a root dir do:     <code>pipreqs . --savepath ./tmp.requirements.txt</code><ul> <li>The command above will generate <code>./tmp.requirements.txt</code> with the list of   the imported packages, e.g.,   <code>amp==1.1.4   async_solipsism==0.3   beautifulsoup4==4.11.1   botocore==1.24.37   cvxopt==1.3.0   cvxpy==1.2.0   dill==0.3.4   environs==9.5.0   ...</code></li> <li>You can grep for a package name to see where it is used, e.g.,   ``` <p>jackpy \"dill\"   helpers/hpickle.py:108:       import dill   ...   ```</p> </li> </ul> </li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#how-to-build-a-local-image","title":"How to build a local image","text":"<ul> <li>The recipe to build a <code>local</code> image is in   <code>devops/docker_build/dev.Dockerfile</code>. This launches various scripts to   install:</li> <li>OS</li> <li>Python</li> <li>Venv + Python packages</li> <li>Jupyter extensions</li> <li>Application-specific packages (e.g., for the linter)</li> <li>To build a local image run:   ``` <p>i docker_build_local_image --version 1.0.0</p> </li> </ul> <p># Build from scratch and not incrementally.</p> <p>i docker_build_local_image --version 1.0.0 --no-cache</p> <p># Update poetry package list.</p> <p>i docker_build_local_image --version 1.0.0 --update-poetry</p> <p># Update poetry package list and build from scratch.</p> <p>i docker_build_local_image --version 1.0.0 --update-poetry --no-cache</p> <p># See more options:</p> <p>i docker_build_local_image -h   ```</p> <ul> <li>Once an image is built, it is tagged as <code>local-${user}-${version}</code>, e.g.,   <code>local-saggese-1.0.0</code>   ```   Successfully tagged 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp:local-gsaggese-1.0.9</li> </ul> <p>docker image ls 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp:local-gsaggese-1.0.9   REPOSITORY                                         TAG                    IMAGE ID            CREATED                  SIZE   665840871993.dkr.ecr.us-east-1.amazonaws.com/amp   local-gsaggese-1.0.9   cf16e3e3d1c7        Less than a second ago   2.75GB   ```</p> <ul> <li>A local image is a candidate for becoming a <code>dev</code> image.   ``` <p>i run_fast_tests --stage local --version 1.0.0   ```</p> </li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#testing-the-local-image","title":"Testing the local image","text":"<ul> <li>Testing the local image   ``` <p>i docker_bash pip list | tee pip_packages.dev.txt</p> </li> </ul> <p>i docker_cmd --cmd \"pip list | tee pip_packages.dev.txt\"</p> <p>i docker_bash --stage local --version 1.0.9 pip list | tee pip_packages.local.txt   ```</p> <ul> <li>Or in one command:   ``` <p>i docker_cmd --cmd \"pip list | tee pip_packages.dev.txt\"; i docker_cmd --stage=local --version=1.0.9 --cmd \"pip list | tee pip_packages.local.txt\"</p> </li> </ul> <p>vimdiff pip_packages.dev.txt pip_packages.local.txt   ```</p> <ul> <li>You can move the local image on different servers for testing by pushing it on   ECR:   ``` <p>i docker_login i docker push 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp:local-gsaggese-1.1.0   ```</p> </li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#pass-the-local-image-to-another-user-for-testing","title":"Pass the local image to another user for testing","text":"<ul> <li> <p>Push the local image built by a user to ECR registry. For e.g., if the image   is built by user <code>gsaggese</code>   ```</p> <p>i docker_login i docker push 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp:local-gsaggese-1.1.0   ```</p> </li> <li> <p>From user session who wants to test: pull the local image from ECR   ```</p> <p>i docker pull 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp:local-gsaggese-1.1.0   ```</p> </li> <li> <p>Tag the local image from user <code>gsaggese</code>, who built the image, as   <code>local-currentuser-1.1.0</code> for user <code>currentuser</code> who wants to test it   ```</p> <p>i docker tag 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp:local-gsaggese-1.1.0 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp:local-currentuser-1.1.0   ```</p> </li> <li> <p>Run any kind of test using the local image. For e.g., to run fast tests   ```</p> <p>i run_fast_tests --stage local --version 1.1.0   ```</p> </li> <li> <p>Check something inside the container   ```</p> <p>i docker_bash --stage local --version 1.1.0   docker &gt; pip freeze | grep pandas   ```</p> </li> <li>After testing and making sure the regressions are green, make sure to tag the   image built by the initial user as <code>dev</code> and not the one tagged for the   <code>current-user</code></li> <li>This will make sure image is tagged for both <code>arm</code> and <code>x86</code> architecture on   the remote registries</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#tag-local-image-as-dev","title":"Tag <code>local</code> image as <code>dev</code>","text":"<ul> <li>Docker tag is just a way of referring to an image. A good analogy is how Git   tags refer to a particular commit in your history.</li> <li>Basically, tagging is creating a reference from one image   (<code>local-saggese-1.0.0</code>) to another (<code>dev</code>)</li> <li>Once the <code>local</code> image is tagged as <code>dev</code>, your <code>dev</code> image becomes equal to   <code>local-saggese-1.0.0</code></li> <li><code>dev</code> image is also tagged with <code>dev-${version}</code>, e.g., <code>dev-1.0.0</code> to   preserve history and allow for quick rollback.</li> <li>Locally in git repository a git tag <code>${repo_name}-${version}</code>, e.g.   <code>cmamp-1.0.0</code> is created in order to properly control sync between code and   container.</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#push-image","title":"Push image","text":"<ul> <li>To push <code>dev</code> or <code>prod</code> image means to send it to the docker registry. It is   more like pushing a commit to the GitHub</li> <li>Once an image is pushed, it can be used by the team members by running   <code>i docker_pull</code></li> <li>Local git tag <code>${repo_name}-${version}</code>, e.g. <code>cmamp-1.0.0</code>, is pushed at this   stage to the remote repository to allow others to properly control sync   between code and container.</li> <li>To be able to push an image to the ECR one should have permissions to do so</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#end-to-end-flow-for-dev-image","title":"End-to-end flow for <code>dev</code> image","text":"<ul> <li>Conceptually the flow consists of the following phases:</li> <li>Build a local image of docker<ul> <li><code>i docker_build_local_image --version 1.0.0</code></li> </ul> </li> <li>Run fast tests to verify that nothing is broken<ul> <li><code>i run_fast_tests --stage local --version 1.0.0</code></li> </ul> </li> <li>Run end-to-end tests by, e.g., running linter on some file<ul> <li><code>i lint --files helpers/tasks.py --stage local --version 1.0.0</code></li> </ul> </li> <li>Tag <code>local</code> image as <code>dev</code><ul> <li><code>i docker_tag_local_image_as_dev --version 1.0.0</code></li> </ul> </li> <li>Push <code>dev</code> image to the docker registry<ul> <li><code>i docker_push_dev_image --version 1.0.0</code></li> </ul> </li> <li>The mentioned flow is executed by <code>Build dev image</code> GH action and that is a     preferred way to do an image release.</li> <li>For specific cases that can not be done via GH action see commands below:   ```   # To run the official flow end-to-end: <p>i docker_release_dev_image --version 1.0.0</p> </li> </ul> <p># To see the options:</p> <p>i docker_release_dev_image -h</p> <p># Run from scratch and not incrementally:</p> <p>i docker_release_dev_image --version 1.0.0 --no-cache</p> <p># Force an update to poetry to pick up new packages</p> <p>i docker_release_dev_image --version 1.0.0 --update-poetry</p> <p># Skip running the QA tests</p> <p>i docker_release_dev_image --version 1.0.0 --no-qa-tests</p> <p># Skip running the tests</p> <p>i docker_release_dev_image --version 1.0.0 --skip-tests</p> <p># Skip end-to-end tests</p> <p>i docker_release_dev_image --version 1.0.0 --no-run-end-to-end-tests   ```</p>"},{"location":"work_tools/all.docker.how_to_guide.html#build-prod-image","title":"Build prod image","text":"<ul> <li>The recipe to build a <code>prod</code> image is in   <code>dev_tools/devops/docker_build/prod.Dockerfile</code>.</li> <li>The main difference between <code>dev</code> image and <code>prod</code> image is that<ul> <li>Source code is accessed through a bind mount for <code>dev</code> image (so that it   can be easily modified) and copied inside the image for a <code>prod</code> image   (since we want to package the code)</li> <li>Requirements to be installed are different:</li> <li><code>dev</code> image requires packages to develop and run the code</li> <li><code>prod</code> image requires packages only to run the code</li> </ul> </li> <li>To build the <code>prod</code> image run:   ``` <p>i docker_build_prod_image --version 1.0.0</p> </li> </ul> <p># Check the options:</p> <p>i docker_build_prod_image -h</p> <p># To build from scratch and not incrementally:</p> <p>i docker_build_prod_image --version 1.0.0 --no-cache   ```</p> <ul> <li>To run a command inside the prod image   ``` <p>docker run --rm -t --user $(id -u):$(id -g) --workdir=/app   665840871993.dkr.ecr.us-east-1.amazonaws.com/cmamp:prod-1.0.3 \"ls -l /app\"   ```</p> </li> <li>Example of a complex command:   ``` <p>docker run --rm -t --workdir=/app 665840871993.dkr.ecr.us-east-1.amazonaws.com/cmamp:prod-1.0.3 \"python /app/im_v2/ccxt/data/extract/download_realtime.py --to_datetime '20211204-194432' --from_datetime '20211204-193932' --dst_dir 'test/ccxt_test' --data_type 'ohlcv' --api_keys 'API_keys.json' --universe 'v03'\"   ```</p> </li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#qa-for-prod-image","title":"QA for prod image","text":"<ul> <li>In dev_scripts repo test:   ``` <p>i lint --files \"linters/amp_black.py\"   ```</p> </li> <li>In amp repo make sure:   ``` <p>i lint -f \"helpers/dbg.py\"   ```</p> </li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#end-to-end-flow-for-prod-image","title":"End-to-end flow for <code>prod</code> image","text":"<ol> <li>Build docker <code>prod</code> image</li> <li><code>&gt; i docker_build_prod_image --version 1.0.0</code></li> <li>Run all the tests to verify that nothing is broken</li> <li><code>&gt; i run_fast_tests --version 1.0.0 --stage prod</code></li> <li><code>&gt; i run_slow_tests --version 1.0.0 --stage prod</code></li> <li><code>&gt; i run_superslow_tests --version 1.0.0 --stage prod</code></li> <li><code>&gt; i run_qa_tests --version 1.0.0 --stage prod</code></li> <li>Push <code>prod</code> image to the docker registry</li> <li> <p><code>&gt; i docker_push_prod_image --version 1.0.0</code></p> </li> <li> <p>To run the flow end-to-end do:   ```</p> <p>i docker_release_prod_image --version 1.0.0   ```</p> </li> <li>Same options are available as for <code>i docker_release_dev_image</code></li> <li>Check options <code>i docker_release_prod_image -h</code></li> </ol>"},{"location":"work_tools/all.docker.how_to_guide.html#flow-for-both-dev-and-prod-images","title":"Flow for both dev and prod images","text":"<ul> <li>To run both flows end-to-end do:</li> <li><code>i docker_release_all</code></li> <li>Alternatively, one can run the release stages step-by-step.</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#docker-in-docker-dind","title":"Docker-in-docker (dind)","text":"<ul> <li>It is possible to install a Docker engine inside a Docker container so that   one can run Docker container (e.g., OMS or IM) inside an isolated <code>amp</code>   container.</li> <li>The problems with this approach are:</li> <li>Dind requires to run the external container in privileged mode, which might     not be possible due to security concerns</li> <li>The Docker / build cache is not shared across parent and children     containers, so one needs to pull / build an image every time the outermost     container is restarted</li> <li>An alternative approach is the \"sibling container\" approach</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#sibling-container-approach","title":"Sibling container approach","text":"<ul> <li>Refs:</li> <li>Can I run Docker-in-Docker without using the --privileged flag - Stack Overflow</li> <li>https://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/</li> <li>Often what's really needed is the ability to build / run a container from   another container (e.g., CI or unit test). This can be achieved by mounting   the Docker socket <code>/var/run/docker.sock</code> to the container, so that a container   can talk to Docker Engine.</li> <li>This approach allows reuse of the build cache across the sibling containers.</li> <li>The downside is less isolation from the external container, e.g., spawned   containers can be left hanging or can collide.</li> <li>E.g.,   <code>``   # Run</code>docker ps` in a container, showing the containers running in the main   container <p>docker run -ti --rm \\         -v /var/run/docker.sock:/var/run/docker.sock \\         dindtest \\         docker ps</p> </li> </ul> <p># Start a sibling hello world container:</p> <p>docker run -it --rm \\         -v /var/run/docker.sock:/var/run/docker.sock \\         dindtest \\         docker run -ti --rm hello-world   ```</p>"},{"location":"work_tools/all.docker.how_to_guide.html#connecting-to-postgres-instance-using-sibling-containers","title":"Connecting to Postgres instance using sibling containers","text":"<ul> <li>We can start the Docker container with Postgres as a service from outside the   container.   ``` <p>(cd oms;  i oms_docker_up -s local)   INFO: &gt; cmd='/local/home/gsaggese/src/venv/amp.client_venv/bin/invoke oms_docker_up -s local'   report_memory_usage=False report_cpu_usage=False   docker-compose \\   --file /local/home/gsaggese/src/sasm-lime4/amp/oms/devops/compose/docker-compose.yml \\   --env-file /local/home/gsaggese/src/sasm-lime4/amp/oms/devops/env/local.oms_db_config.env \\   up \\   oms_postgres   Creating compose_oms_postgres_1 ... done   Attaching to compose_oms_postgres_1   oms_postgres_1  |   oms_postgres_1  | PostgreSQL Database directory appears to contain a database; Skipping initialization   oms_postgres_1  |   oms_postgres_1  | 2022-05-19 22:57:15.659 UTC [1] LOG:  starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit   oms_postgres_1  | 2022-05-19 22:57:15.659 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432   oms_postgres_1  | 2022-05-19 22:57:15.659 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432   oms_postgres_1  | 2022-05-19 22:57:15.663 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"   oms_postgres_1  | 2022-05-19 22:57:15.670 UTC [25] LOG:  database system was shut down at 2022-05-19 22:56:50 UTC   oms_postgres_1  | 2022-05-19 22:57:15.674 UTC [1] LOG:  database system is ready to accept connections   ```</p> </li> <li>Note that Postgres needs to be</li> <li>Start a container able to</li> <li>From inside a container I launch postgres through the /var/...   ``` <p>docker ps | grep postgres   CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES   83bba0818c74 postgres:13 \"docker-entrypoint.s...\" 6 minutes ago Up 6 minutes   0.0.0.0:5432-&gt;5432/tcp compose-oms_postgres-1   ```</p> </li> <li>Test connection to the DB from outside the container   ``` <p>psql --host=cf-spm-dev4 --port=5432 --user aljsdalsd -d oms_postgres_db_local   Password for user aljsdalsd:   psql (9.5.25, server 13.5 (Debian 13.5-1.pgdg110+1))   WARNING: psql major version 9.5, server major version 13.           Some psql features might not work.   Type \"help\" for help.   oms_postgres_db_local=#   ```</p> </li> <li>Test connection to the DB from inside the container   ``` <p>psql --host=cf-spm-dev4 --port=5432 --user aljsdalsd -d oms_postgres_db_local   ...   ```</p> </li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#release-flow","title":"Release flow","text":""},{"location":"work_tools/all.docker.how_to_guide.html#cmamp","title":"cmamp","text":"<ul> <li>File an Issue for the release (e.g., \"Add package foobar to cmamp image\")</li> <li>Create the corresponding branch with <code>i git_create_branch -i ${issue_number}</code></li> <li>Change the code</li> <li>Update the changelog, i.e. <code>//cmamp/changelog.txt</code></li> <li>Specify what was changed</li> <li>Pick the release version accordingly<ul> <li>We use semantic versioning convention</li> <li>For example for version <code>1.2.3</code>:<ul> <li>1 is major, 2 is minor, 3 is patch</li> </ul> </li> <li>We keep <code>dev</code> and <code>prod</code> image version major and minor versions in sync     <code>prod</code> gets patches -&gt; i.e. we go from <code>prod-1.1.0</code> to <code>prod-1.1.1</code> upon     a bug fix documented in the <code>changelog.txt</code>.<ul> <li>In this manner, it cannot happen we have <code>dev-1.1.0</code> and <code>prod-1.2.0</code>   at any point in time, but <code>dev-1.1.0</code> and <code>prod-1.1.2</code> are perfectly   fine.</li> </ul> </li> </ul> </li> <li>Test the change using the local release flow   <code>i docker_build_local_image -v ${version}</code></li> <li>If a new package is added run <code>docker_build_local_image</code> with     <code>--update-poetry</code> option and check in a <code>poetry.lock</code> file</li> <li>Make sure that the tests pass <code>i run_fast_slow_tests -s local -v ${version}</code>,   and that the goal of the Issue is achieved (e.g., a new package is visible,   the package version has been updated)</li> <li>Do a PR with the change including the updated <code>changelog.txt</code>, the poetry   files (both the specs <code>devops/docker_build/poetry.toml</code> and the package   version <code>devops/docker_build/poetry.lock</code>)</li> <li>Run the release flow manually (or rely on GH Action build workflow to create   the new image)   ```   # Release dev image <p>i docker_release_dev_image --version $version</p> </li> </ul> <p># Pick up the new image from ECR</p> <p>i docker_pull   ```</p> <ul> <li>Tag and push the latest <code>dev</code> to GHCR registry manually</li> <li>Perform a Docker login using your GitHub username and PAT (Personal Access     Token):     <code>bash     &gt; docker login ghcr.io -u &lt;username&gt;</code></li> <li>Tag the <code>dev</code> image to the GHCR namespace:     <code>bash     &gt; docker tag 623860924167.dkr.ecr.eu-north-1.amazonaws.com/cmamp:dev ghcr.io/cryptokaizen/cmamp:dev</code></li> <li> <p>Push the tagged image to the GHCR registry:     <code>bash     &gt; docker push ghcr.io/cryptokaizen/cmamp:dev</code></p> </li> <li> <p>Send a message on the <code>all@</code> chat telling people that a new version of the   <code>XYZ</code> container has been released</p> </li> <li>Users need to do a <code>i docker_pull</code> to get the new container</li> <li>Users that don't update should see a message telling them that the code and   container are not in sync any more, e.g.,:   ```</li> </ul> <p>This code is not in sync with the container:   code_version='1.0.3' != container_version='amp-1.0.3'</p> <p>You need to:   - merge origin/master into your branch with <code>invoke git_merge_master</code>   - pull the latest container with <code>invoke docker_pull</code>   ```</p>"},{"location":"work_tools/all.docker.how_to_guide.html#dev_tools","title":"dev_tools","text":"<ul> <li>File an Issue for the release</li> <li>Create the corresponding branch in dev_tools</li> <li>Change the code</li> <li>Run the release flow end-to-end   ``` <p>i docker_release_dev_image --version 1.1.0 i docker_release_prod_image --version 1.1.0   ```   TODO(Vlad): Add a command to run the push to Dockerhub and add it to the   single arch release flow</p> </li> <li>Push the image to Dockerhub manually</li> <li>Login to Dockerhub with the <code>sorrentum</code> account   ``` <p>docker login --username=sorrentum   ```</p> </li> <li>Tag the dev version image as <code>sorrentum/dev_tools:dev</code>   ``` <p>docker tag 665840871993.dkr.ecr.us-east-1.amazonaws.com/dev_tools:dev-1.1.0 sorrentum/dev_tools:dev   ```</p> </li> <li>Push the dev image to Dockerhub   ``` <p>docker push sorrentum/dev_tools:dev   ```</p> </li> <li>Tag the prod version image as <code>sorrentum/dev_tools:prod</code>   ``` <p>docker tag 665840871993.dkr.ecr.us-east-1.amazonaws.com/dev_tools:prod sorrentum/dev_tools:prod   ```</p> </li> <li>Push the prod image to Dockerhub   ``` <p>docker push sorrentum/dev_tools:prod   ```</p> </li> <li>Push the latest <code>prod</code> image to GHCR registry manually for GH actions to use   it</li> <li>Perform a Docker login using your GitHub username and PAT (Personal Access     Token):     <code>bash     &gt; docker login ghcr.io -u &lt;username&gt;</code></li> <li>Tag the <code>prod</code> image to the GHCR namespace:     <code>bash     &gt; docker tag 623860924167.dkr.ecr.eu-north-1.amazonaws.com/dev_tools:prod ghcr.io/cryptokaizen/dev_tools:prod</code></li> <li> <p>Push the tagged image to the GHCR registry:     <code>bash     &gt; docker push ghcr.io/cryptokaizen/dev_tools:prod</code></p> </li> <li> <p>Update the changelog, i.e. <code>//dev_tools/changelog.txt</code></p> </li> <li>The changelog should be updated only after the image is released; otherwise     the sanity checks will assert that the release's version is not higher than     the latest version recorded in the changelog.</li> <li>Specify what has changed</li> <li>Pick the release version accordingly<ul> <li>NB! The release version should consist of 3 digits, e.g. \"1.1.0\" instead   of \"1.1\"</li> <li>We use semantic versioning convention</li> <li>For example, adding a package to the image would mean bumping up version     1.0.0 to 1.0.1</li> </ul> </li> <li>Do a PR with the change including the updated <code>changelog.txt</code></li> <li>Send a message on the <code>all@</code> chat telling people that a new version of the   container has been released</li> <li>Users need to do<ul> <li><code>i docker_pull</code> from <code>dev_tools</code>,</li> <li><code>i docker_pull_dev_tools</code> from <code>cmamp</code></li> </ul> </li> <li>Users need to make sure to pull docker after the master is up-to-date     (including amp submodules)</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#design-release-flow-discussion","title":"Design release flow - discussion","text":"<p>TODO(gp, Vitalii): Turn this into a description of the release flow</p> <p>Let's assume that we want to release dev image with version 1.2.3:</p> <pre><code>&gt; i docker_build_local_image --tag-name 1.2.3\n</code></pre> <p>Initially we thought about using Git tags to mark releases points in the source repo for <code>dev</code> and <code>prod</code> releases (but not <code>local</code> since <code>local</code> is reserved to private use by a user).</p> <p>This approach is elegant, but it has some corner cases when used with containers for multiple repos that contain Git submodules.</p> <p>We decided to use an approach where a <code>changelog.txt</code> file contains the latest code version</p> <ul> <li>All test tasks now also use <code>hversion.get_code_version()</code> that calls   <code>hgit.git_describe()</code> to get latest tag in the repo (1.0.0 in this case)</li> </ul> <p>Agree. git_describe will need to accept a dir to find the tag of the releasable dir</p> <ul> <li>When we are satisfied with local image, we run   <code>i docker_tag_local_image_as_dev</code></li> </ul> <p>We will still need to pass --version 1.0.0</p> <ul> <li>Invoke internally tags <code>local-1.0.0</code> as <code>dev-1.0.0</code>, in addition to <code>dev</code></li> </ul> <p>Both for Git tags and docker tags</p> <ul> <li>Then we run <code>i docker_push_dev_image</code></li> </ul> <p>We will still need to pass --version 1.0.0</p> <ul> <li>Invoke internally pushes both <code>dev-1.0.0</code> and <code>dev</code> images to ECR **AND**   pushes local 1.0.0 git tag to remote git repo (github)</li> </ul> <p><code>docker_release_dev_image</code> will do basically the same (will require tag_name). Of course docker_release... is just a convenience wrapper running all the stages</p> <p>Now let's assume we want to promote dev image to prod:</p> <ul> <li>Then we run <code>i docker_build_prod_image</code></li> <li>Invoke internally checks with <code>hversion.get_code_version()</code> and builds   <code>prod-1.0.0</code> based on <code>dev-1.0.0</code>, also tagging <code>prod-1.0.0</code> as <code>prod</code></li> <li>Then we run <code>i docker_push_prod_image</code></li> <li>Invoke pushes <code>prod-1.0.0</code> and <code>prod</code> tags to ECR</li> </ul> <p><code>docker_release_prod_image</code> will do basically the same (will require tag_name).</p> <p>Q0: Is the flow ok?</p> <ul> <li>Yes</li> </ul> <p>Q1: The flow is the same for <code>dev_tools</code> and <code>cmamp</code>, but to update the version of image on which <code>dev_tools</code> is based -- we'll need to modify Dockerfile now. Is that ok?</p> <ul> <li>Maybe we should just create the dev_tools from scratch using the full-blown   flow instead of build on top of it</li> <li>The idea of building on top of it, was just a shortcut but it is creating more   problems that what it's worth it</li> <li>Then everything looks and behaves the same</li> <li>TODO(vitalii): File a bug, if we don't have it yet</li> </ul> <p>Q2: If the flow is run in the submodule, e.g. in <code>amp</code> dir, currently the behaviour is not well defined. Commands will try to build <code>cmamp</code> image in this case, but code version will be from <code>dev_tools</code> -- should we fix this?</p> <ul> <li>We are going towards the concept of \"releasable dirs\" (see im, optimizer). If   there is a dir with devops, then that dir runs inside a container</li> <li>The \"Git version\" should be associated to the dir we are releasing (e.g.,   cmamp, im, optimizer, dev_tools)</li> </ul> <p>Vitalii: If we will have monorepo with releasable dirs, then indeed git tags are not that comfortable to use, however I could argue that when one releases <code>im</code> image with version 1.0.0, he gets docker image <code>im:dev-1.0.0</code> , <code>im:prod-1.0.0</code>, <code>im:dev</code> and <code>im:prod</code> -- but how then one is able to find corresponding code that was used in that image?</p> <p>Perhaps instead, we could share namespace of git tags between all tags.</p> <p>E.g. in git repo (github) we will have:</p> <ul> <li>Im-dev-1.0.0</li> <li>Cmamp-dev-1.0.0</li> <li>Im-prod-1.0.0</li> </ul> <p>GP: Point taken. In fact the code in a releasable dir still needs code from other submodules (e.g., helpers). One approach is to put the Git hash in version.txt. The one you suggest (of tagging the entire repo) with also info on the dir makes sense.</p> <p>I think the Git tags are designed to do what we want, so let's use them.</p> <p>Q3: We don't need version.txt file in this flow. I will remove it, ok?</p> <ul> <li>Yes, we can remove version.txt and use a README or changelog in the releasable   dir</li> </ul> <p>The flow is similar to what I thought.</p> <p>Some observations / questions:</p> <p>INV: version becomes mandatory in the release flow</p> <ul> <li>This requires a lot of cosmetic changes to the code since now it's optional,   but it's worth make the changes</li> </ul> <p>We need to ensure that version can only be created going fwd.</p> <p>We can do a comparison of the current version with the new version as tuples (we could use semver but it feels not needed)</p> <ul> <li>The workflows are:</li> <li>Build a local image</li> <li>Release a dev image</li> <li>Release a prod image</li> <li>Rollback an image<ul> <li>We rarely move the dev / prod tag back, but rather users needs to docker   pull an older image and pass --basename --stage and --version to   docker{bash, cmd, jupyter}</li> <li>Then the image is fixed going forward</li> </ul> </li> </ul> <p>A releasable dir has a</p> <ul> <li>Repo_config</li> <li>Maybe we should call it component_config since now also dirs can be released</li> <li>README.md or changelog.md</li> <li>Devops</li> <li>Tasks.py (with the exposed Invoke tasks)</li> <li>Lib_tasks.py (with the custom invoke tasks)</li> </ul> <p>We want to try to move to helpers/lib_tasks all the \"common\" code without dependencies from the specific sw components. We pass function pointers for callbacks.</p> <p>What to do with:</p> <pre><code>CONTAINER_VERSION='amp-1.1.1'\nBUILD_TAG='amp-1.1.1-20211114_093142-AmpTask1845_Get_docker_in_docker_to_work-47fb46513f084b8f3c9008a2e623ec05040a10e9'\n</code></pre>"},{"location":"work_tools/all.docker.how_to_guide.html#qa-flow","title":"QA flow","text":"<ul> <li>The goal is to test that the container as a whole works</li> <li>We want to run the container as a user would do</li> </ul> <p>Usually we run tests inside a container to verify that the code is correct To test the container itself right now we test outside (in the thin client)</p> <pre><code>&gt; pytest -m qa test --image_stage dev\n</code></pre> <p>The problem is that now the thin client needs to have a bunch of deps (including pytest, pandas and so on) which defeats the purpose of the thin env</p> <p><code>dev_scripts_devto/client_setup/</code></p> <p>E.g., <code>//amp/dev_scripts/client_setup/requirements.txt</code></p> <p>A hack is to</p> <pre><code>vimdiff /Users/saggese/src/lemonade2/amp/dev_scripts/client_setup/requirements.txt dev_scripts_devto/client_setup/requirements.txt\n</code></pre> <pre><code>&gt; dev_scripts_devto/client_setup/build.sh\n</code></pre> <p>A possible solution is to use Docker-in-Docker</p> <ul> <li>In this way we don't have to pollute the thin env with a bunch of stuff</li> <li>Talk to Grisha and Vitalii</li> </ul> <p>This works in dev_tools because the code for the import detector is there and we are using a dev container which binds the src dir to the container</p> <pre><code>  &gt; i lint_detect_cycles --dir-name import_check/test/Test_detect_import_cycles.test1/input/ --stage dev\n</code></pre> <p>In all the other repos, one needs to use the prod of dev_tools container (that's what the user would do)</p> <p>Next steps:</p> <ul> <li>TODO(Sonya + Grisha): release the prod dev_toools container as it is</li> <li>TODO(Sonya + Grisha): document dev_tools, release procedure</li> <li>TODO(Sonya): pull prod dev_tools (i docker_pull_dev_tools) and test that now   in cmamp the tool works</li> <li>TODO(gp): figure out the QA workflow (and improve the thin client with dind)</li> <li>To break the circular dep we release a prod-candidate</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#dev_tools-container","title":"Dev_tools container","text":"<ul> <li>For specific dev_tools workflows see</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#optimizer-container","title":"Optimizer container","text":""},{"location":"work_tools/all.docker.how_to_guide.html#rationale","title":"Rationale","text":"<ul> <li>The high-level goal is to move towards containerized Python scripts running in   smaller containers instead of keep adding packages to <code>amp</code> / <code>cmamp</code>, which   makes the <code>amp</code> / <code>cmamp</code> container bloated and risky to build</li> <li>Along this design philosophy similar to microservices, we want to have a   Docker container, called <code>opt</code> with a Python script that uses some packages   that are not compatible with <code>amp</code> (specifically cvxopt, cvxpy)</li> <li>This is similar to what we do for the <code>dev_tools</code>, which is like a   containerized Python script for the linter</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#build-and-run-a-local-version-of-opt","title":"Build and run a local version of <code>opt</code>","text":"<ul> <li>You can build the container locally with:   ``` <p>cd optimizer i opt_docker_build_local_image --version 0.1.0   ```</p> </li> <li>This process takes around 5 mins and then you should have the container   <code>docker image ls 665840871993.dkr.ecr.us-east-1.amazonaws.com/opt:local-saggese-0.1.0   REPOSITORY                                         TAG                   IMAGE ID       CREATED         SIZE   665840871993.dkr.ecr.us-east-1.amazonaws.com/opt   local-saggese-0.1.0   bb7d60d6a7d0   7 seconds ago   1.23GB</code></li> <li>Run the container as:   ``` <p>i opt_docker_bash --stage local --version 0.1.0   ```</p> </li> <li>To run a Jupyter notebook in the <code>opt</code> container:</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#internals","title":"Internals","text":""},{"location":"work_tools/all.docker.how_to_guide.html#one-container-per-git-repo","title":"One container per Git repo","text":"<ul> <li>A simple approach is to have each deployable unit (i.e., container)   corresponding to a Git repo</li> <li> <p>The consequence would be:</p> <ul> <li>A multiplication of repos</li> <li>No implicit sharing of code across different containers</li> <li>Some mechanism to share code (e.g., <code>helpers</code>) across repos (e.g., using   bind mount)</li> <li>Not playing nice with Git subrepo mechanism since Docker needs to see the   entire repo</li> </ul> </li> <li> <p>So the code would be organized in 4 repos:   ```</p> </li> <li>lemonade / lime<ul> <li>helpers</li> <li>optimizer</li> <li>oms</li> <li>models in amp   ```</li> </ul> </li> <li>Where the dependency between containers are<ul> <li>Lemonade -&gt; amp</li> <li>Amp -&gt; optimizer, helpers</li> <li>Optimizer -&gt; helpers, core</li> </ul> </li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#multiple-containers-per-git-repo","title":"Multiple containers per Git repo","text":"<ul> <li>Another approach is to have <code>optimizer</code> as a directory inside <code>amp</code></li> <li>This keeps <code>amp</code> and <code>optimizer</code> in a single repo</li> <li>To build / run optimizer code in its container one needs to <code>cd</code> in the dir</li> <li>The problem then becomes how to share <code>helpers</code></li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#mounting-only-optimizer-dir-inside-docker","title":"Mounting only <code>optimizer</code> dir inside Docker","text":"<ul> <li>From <code>devops/compose/docker-compose.yml</code> <code>42 volumes:   43  # Move one dir up to include the entire git repo (see AmpTask1017).   44  - ../../:/app   45 # Move one dir down to include the entire git repo (see AmpTask1017).   46 working_dir: /app</code></li> <li>From <code>devops/docker_build/dev.Dockerfile</code></li> <li>ENTRYPOINT [\"devops/docker_run/entrypoint.sh\"]</li> <li>The problem is that Git repo doesn't work anymore   <code>git --version: git version 2.30.2   fatal: not a git repository (or any parent up to mount point /)   Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).</code></li> <li>A work around is to inject .git in /git of the container and then point git to   that   ```   environment:   ...</li> <li>GIT_DIR=/git</li> </ul> <p>volumes:     # Move one dir up to include the entire git repo (see AmpTask1017).     - ../../:/app     - ../../../../.git:/git     - ../../../../amp/helpers:/app/helpers   ```</p> <ul> <li>Git works but it gets confused with the paths   <code>modified: .dockerignore       deleted: .github/gh_requirements.txt       deleted: .github/workflows/build_image.yml.DISABLED       deleted: .github/workflows/fast_tests.yml       deleted: .github/workflows/linter.yml.DISABLED       deleted: .github/workflows/slow_tests.yml       deleted: .github/workflows/superslow_tests.yml.DISABLED       deleted: .gitignore</code></li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#mounting-the-supermodule-eg-lime-lemonade-amp-inside-docker","title":"Mounting the supermodule (e.g., lime, lemonade, amp) inside Docker","text":"<ul> <li>From <code>devops/compose/docker-compose.yml</code> <code>42 volumes:   43  # Move one dir up to include the entire git repo (see AmpTask1017).   44  - ../../../:/app   45 # Move one dir down to include the entire git repo (see AmpTask1017).   46 working_dir: /app/amp</code></li> <li>From <code>devops/docker_build/dev.Dockerfile</code></li> <li>ENTRYPOINT [\"optimizer/devops/docker_run/entrypoint.sh\"]</li> <li>This approach mounts 4 dirs up from devops/compose/docker-compose.yml, i.e.,   //lime</li> <li>The problem with this approach is that now repo_config.py is incorrect</li> <li><code>i opt_docker_build_local_image --version 0.4.0</code> <code>32 - ../../../helpers:/app/amp/optimizer/helpers   33   34 # Shared cache. This is specific of lime.   35 - /local/home/share/cache:/cache   36   37 # Mount `amp` when it is used as submodule. In this case we need to   38 # mount the super project in the container (to make git work with the   39 # supermodule) and then change dir to `amp`.   40 app:   41  extends:   42    base_app   43 volumes:   44  # Move one dir up to include the entire git repo (see AmpTask1017).   45  - ../../../../:/app   46 # Move one dir down to include the entire git repo (see AmpTask1017).   47 working_dir: /app/amp/optimizer   48 #entrypoint: /bin/bash -c \"ls helpers\"</code></li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#invariants","title":"Invariants","text":"<ul> <li>A deployable dir is a dir under a Git repo</li> <li>It corresponds to a software component (code + library = Docker container)</li> <li>Anything that has a devops dir is \"deployable\"</li> <li>Each Docker container is run from its corresponding dir, e.g.,</li> <li>Amp container from the amp dir</li> <li>Amp container from the lemonade dir (this is just a shortcut since lemonade     has the same deps right now as amp)</li> <li>Always mount the outermost Git repo under <code>/app</code></li> <li>Set the Docker working dir as the current dir</li> <li>Each deployable dir specifies all the needed information in <code>repo_config.py</code>   (which is the one in the current dir)</li> <li>What container to run</li> <li>What functionality is supported on different servers (e.g., privileged way)</li> <li>The <code>changelog.txt</code> file is in the deployable dir (e.g.,   optimizer/changelog.txt)</li> <li>Each</li> </ul> <p>One run the invoke commands from optimizer dir</p> <p>When the Docker container starts the current dir is optimizer</p> <p>helpers, core is mounted in the same dir</p> <p>You can't see code outside optimizer</p> <p>TODO(gp): running in amp under lemonade should use the local repo_config</p>"},{"location":"work_tools/all.docker.how_to_guide.html#release-and-ecr-flow","title":"Release and ECR flow","text":"<p>TODO(gp): Implement this</p>"},{"location":"work_tools/all.docker.how_to_guide.html#unit-testing-code-inside-opt-container","title":"Unit testing code inside <code>opt</code> container","text":"<ul> <li>Since we want to segregate the package dependencies in different containers,   tests that have a dependency from cvxopt /cvxpy can't be run inside the <code>amp</code>   container but need to be run inside <code>opt</code>.</li> <li>We want to:</li> <li>(as always) write and run unit tests for the optimizer code in isolation,      i.e., test the code in the directory <code>optimizer</code> by itself</li> <li>Run all the tests for the entire repo (relying on both containers <code>amp</code> and      <code>optimizer</code> with a single command invocation)</li> <li>Be able to run tests belonging to only one of the containers to shorten the      debugging cycle</li> <li>To achieve this we need to solve the 3 problems below.</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#avoid-compiling-code-depending-from-cvxopt-when-running-amp","title":"Avoid compiling code depending from cvxopt when running amp","text":"<ul> <li>We can't parse code (e.g., in <code>pytest</code>) that includes packages that are not   present in a container</li> <li> <p>E.g., <code>pytest</code> running in <code>amp</code> should not parse code in <code>//amp/optimizer</code>     since it contains imports that will fail</p> </li> <li> <p>Solution 1</p> </li> <li> <p>We use the pytest mechanism <code>cvx = pytest.importorskip(\"cvxpy\")</code> which is     conceptually equivalent to:     ```     try:       import cvxopt       has_cvxopt = True     except ImportError:       has_cvxopt = False</p> <p>if has_cvxopt:         def utils1():                  cvxopt\u2026 ```</p> </li> <li> <p>Solution 2</p> </li> <li> <p>Test in eachfile for the existence of the needed packages and enclose the     code in an <code>if _has_package</code></p> <ul> <li>Pros:</li> <li>We can skip code based dynamically on a <code>try ... except ImportModule</code> to     check what packages are present</li> <li>Cons:</li> <li>Repeat the same piece of <code>try ... except</code> in many places<ul> <li>Solution: we can factor it out in a function</li> </ul> </li> <li>We need to enclose the code in a <code>if ...</code> that screws up the indentation     and makes the code weird</li> </ul> </li> <li> <p>Solution 3</p> </li> <li> <p>Exclude certain directories (e.g., <code>//amp/optimizer</code>) from <code>pytest</code></p> <ul> <li>Pros:</li> <li>We don't have to spread the <code>try ... except</code> and <code>if \\_has_package</code> in     the code</li> <li>Cons:</li> <li>The directory is relative to the top directory<ul> <li>Solution: we can use a regex to specify the dir without the full path</li> </ul> </li> <li>Which directories are included and excluded depends on where <code>pytest</code> is     run<ul> <li>E.g., running <code>pytest</code> in an <code>amp</code> container we need to skip the   <code>optimizer</code> dir, while <code>pytest</code> in an <code>optimizer</code> container should   skip everything but the <code>optimizer</code> dir</li> </ul> </li> </ul> </li> <li> <p>Solution 4</p> </li> <li>Exclude certain directories or files based on which container we are running     in<ul> <li>Cons:</li> <li>We need to have a way to determine in which container we are running<ul> <li>Solution: we can use the env vars we use for versioning ``` <p>echo $AM_CONTAINER_VERSION amp-1.0.3- ```</p> </li> </ul> </li> </ul> </li> <li>Given the pros and cons, we decided to follow Solution 1 and Solution 3</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#run-optimizer-tests-in-a-stand-alone-opt-container","title":"Run optimizer tests in a stand-alone <code>opt</code> container","text":"<ul> <li>To run the optimizer tests, you can create an <code>opt</code> container and then run   <code>pytest</code>   ``` <p>cd optimizer i opt_docker_bash   docker&gt; pytest .   ```</p> </li> <li>We wrap this in an invoke target like <code>i opt_run_fast_tests</code></li> </ul> <p>Alternative solution</p> <ul> <li>We can use dind to run the <code>opt</code> container inside a <code>cmamp</code> one</li> <li>Cons:<ul> <li>Dind complicates the system</li> <li>Dind is not supported everywhere (one needs privileged containers)</li> <li>Dind is slower since there are 2 levels of (relatively fast)   virtualization</li> </ul> </li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#run-optimizer-tests-as-part-of-running-unit-tests-for-cmamp","title":"Run optimizer tests as part of running unit tests for <code>cmamp</code>","text":"<ul> <li>We use the same mechanism as <code>run_fast_slow_superslow_tests</code> to pull together   different test lists</li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#call-a-dockerized-executable-from-a-container","title":"Call a Dockerized executable from a container","text":"<ul> <li>From   https://github.com/cryptokaizen/cmamp/issues/1357</li> <li> <p>We need to call something from <code>amp</code> to <code>opt</code> Docker</p> </li> <li> <p>Solution 1</p> </li> <li> <p>Inside the code we build the command line     <code>cmd = 'docker run -it ... '; system(cmd)</code></p> <ul> <li>Cons:</li> <li>There is code replicated between here and the invoke task (e.g., the     info about the container, ...)</li> </ul> </li> <li> <p>Solution 2</p> </li> <li> <p>Call the Dockerized executable using the <code>docker_cmd</code> invoke target     <code>cmd = \"invoke opt_docker_cmd -cmd '...'\"     system(cmd)</code></p> <ul> <li>Pros:</li> <li>All the Docker commands go through the same interface inside invoke</li> <li>Cons</li> <li>Bash interpolation in the command</li> <li>Another level of indirection: do a system call to call <code>invoke</code>,     <code>invoke</code> calls docker, docker does the work</li> <li><code>invoke</code> needs to be installed inside the calling container</li> </ul> </li> <li> <p>Solution 3</p> </li> <li> <p>Call opt_lib_tasks.py <code>opt_docker_cmd(cmd, ...)</code></p> <ul> <li>Pros</li> <li>Avoid doing a call to invoke</li> <li>Can deal with bash interpolation in Python</li> </ul> </li> <li> <p>We should always use Solution 3, although in the code sometimes we use   Solution 1 and 2 (but we should replace in favor of Solution 3).</p> </li> </ul>"},{"location":"work_tools/all.docker.how_to_guide.html#_1","title":"Docker","text":"<ul> <li>The interface to the Dockerized optimizer is in <code>run_optimizer</code> in   <code>//amp/oms/call_optimizer.py</code></li> <li>To run the examples   ``` <p>cd //lime i docker_bash pytest ./amp/oms/test/test_call_optimizer.py::Test_run_dockerized_optimizer1   ```</p> </li> </ul>"},{"location":"work_tools/all.docker.tutorial.html","title":"Docker","text":""},{"location":"work_tools/all.docker.tutorial.html#introduction","title":"Introduction","text":"<ul> <li>Docker is an open-source tool designed to make our life typically easier   (although it takes energy and time to master) when creating, building,   deploying, and running software applications.</li> <li>Docker can package an application and its dependencies in a virtual container   that can run on any Linux, Windows, or macOS computer.</li> <li>Our Docker containers have everything required (e.g. OS packages, Python   packages) inside to run certain applications/code.</li> </ul>"},{"location":"work_tools/all.docker.tutorial.html#concepts","title":"Concepts","text":""},{"location":"work_tools/all.docker.tutorial.html#docker-image","title":"Docker image","text":"<ul> <li>A Docker image is a read-only template with instructions for creating a Docker   container</li> <li>Typically a Docker image includes needed libraries and packages and their   versions</li> </ul>"},{"location":"work_tools/all.docker.tutorial.html#dockerfile","title":"Dockerfile","text":"<ul> <li>A <code>Dockerfile</code> is a text document that contains all the commands to call on   the command line to assemble an image. E.g.   <code>//cmamp/devops/docker_build/dev.Dockerfile</code>.</li> </ul>"},{"location":"work_tools/all.docker.tutorial.html#docker-container","title":"Docker container","text":"<ul> <li>A Docker container is a runnable instance of an image. One can run code inside   a Docker container having all requirements installed.</li> </ul>"},{"location":"work_tools/all.docker.tutorial.html#docker-registry","title":"Docker registry","text":"<ul> <li>A Docker registry stores Docker images. In other words, Docker registry for   docker images is like GitHub for code.</li> </ul>"},{"location":"work_tools/all.docker.tutorial.html#poetry","title":"Poetry","text":"<ul> <li>Poetry is a tool for managing Python packages and dependencies and allows to:</li> <li>List packages you want to install with some constraints<ul> <li>E.g., <code>pandas</code> must be above 1.0 in <code>devops/docker_build/pyproject.toml</code></li> </ul> </li> <li>Given a list of packages you need to install to get the desired environment,     <code>poetry</code> \"optimizes\" the package versions and generate     <code>devops/docker_build/poetry.lock</code>, which contains the list of versions of     the packages to install</li> <li>If there is a new version of a package re-running <code>poetry</code> might give you an     updated list of packages to install</li> </ul>"},{"location":"work_tools/all.docker.tutorial.html#build-a-docker-image","title":"Build a Docker image","text":""},{"location":"work_tools/all.docker.tutorial.html#general","title":"General","text":"<ul> <li>A docker image is built from a <code>Dockerfile</code>. The image is then used to run a   Docker container.</li> </ul> <ul> <li>There is <code>/devops</code> dir under a project's dir that contains Docker-related   files, e.g. <code>cmamp/devops</code></li> </ul>"},{"location":"work_tools/all.docker.tutorial.html#base-image","title":"Base image","text":"<ul> <li>A <code>Dockerfile</code> should start with specifying a base image.</li> <li>The base image is an image that a new image is built on top of. A new Docker   image will have all the packages/dependencies that are installed in the base   image.</li> <li>Use <code>FROM</code> statement to specify a base image, e.g.   <code>FROM ubuntu:20.4</code></li> </ul>"},{"location":"work_tools/all.docker.tutorial.html#copy-files","title":"Copy files","text":"<ul> <li>Copy files that are required to build a Docker image to the Docker filesystem</li> <li>To copy a file from <code>/source_dir</code> (your filesystem) to <code>/dst_dir</code> (Docker   filesystem) do:   <code>COPY source_dir/file dst_dir</code></li> <li>E.g., the command below will copy <code>install_packages.sh</code> from   <code>devops/docker_build</code> to the Docker's root directory so that   <code>install_packages.sh</code> can be accessed by Docker   <code>COPY devops/docker_build/install_packages.sh .</code></li> </ul>"},{"location":"work_tools/all.docker.tutorial.html#install-os-packages","title":"Install OS packages","text":"<ul> <li>Install OS packages that are needed for a Docker app, but that are not   installed for a base image</li> <li>Use <code>RUN</code> instruction to install a package, e.g.   <code>RUN apt-get install postgresql-client</code></li> <li>Alternatively you can package all installation instructions in a <code>.sh</code> file   and run it. Do not forget to copy a <code>.sh</code> file to the Docker filesystem so   that Docker can see it. E.g.,   <code>COPY devops/docker_build/install_packages.sh .   RUN /bin/sh -c \"./install_packages.sh\"</code></li> </ul>"},{"location":"work_tools/all.docker.tutorial.html#install-python-packages","title":"Install Python packages","text":"<ul> <li>We prefer to install Python packages with <code>poetry</code></li> <li>Make sure that there is instruction to install <code>pip3</code> and <code>poetry</code>. You can   either put it in a <code>Dockerfile</code> or in a separate file like   <code>install_packages.sh</code>.   <code>RUN apt-get install python3-pip   RUN pip3 install poetry</code></li> <li>Copy poetry-related files to the Docker filesystem so that files can be   accessed by Docker   <code>COPY devops/docker_build/poetry.toml   COPY devops/docker_build/poetry.lock</code></li> <li>Install Python packages   <code>RUN poetry install</code></li> </ul>"},{"location":"work_tools/all.docker.tutorial.html#build-an-image-from-a-dockerfile","title":"Build an image from a Dockerfile","text":"<ul> <li>To build an image from a <code>Dockerfile</code> run:   ``` <p>docker build .   ```</p> </li> <li>The <code>Dockerfile</code> must be called <code>Dockerfile</code> and located in the root of the   build context</li> <li>You can point to any <code>Dockerfile</code> by using <code>-f</code>:   ``` <p>docker build -f /path/to/dockerfile   ```</p> </li> </ul>"},{"location":"work_tools/all.docker.tutorial.html#run-multi-container-docker-application","title":"Run multi-container Docker application","text":"<ul> <li>Docker Compose is a tool for defining and running multi-container Docker   applications</li> <li>With Docker Compose you use a <code>YAML</code> file to configure your application's   services</li> </ul>"},{"location":"work_tools/all.docker.tutorial.html#version","title":"Version","text":"<ul> <li>At the beginning of a <code>docker-compose.yaml</code> file specify the <code>docker-compose</code>   version. For more information see   the official documents <code>version: \"3.0\"</code></li> </ul>"},{"location":"work_tools/all.docker.tutorial.html#images","title":"Images","text":"<ul> <li>You can either re-use a public image or build a new one from a <code>Dockerfile</code></li> <li>The <code>app</code> service below uses the image that is built from the <code>dev.Dockerfile</code> <code>app:     build:       context: .       dockerfile: dev.Dockerfile</code></li> <li>The <code>im_postgres_local</code> service below uses the public <code>postgres</code> image pulled   from the Docker hub registry <code>im_postgres_local:     image: postgres: 13</code></li> </ul>"},{"location":"work_tools/all.docker.tutorial.html#bind-mount","title":"Bind mount","text":"<ul> <li>If you want to be able to share files between the host and a Docker container,   you should bind-mount a directory</li> <li>E.g. mount current directory to <code>/app</code> dir inside a Docker container:   ```   app:     volumes:<ul> <li>.:/app   ```</li> </ul> </li> </ul>"},{"location":"work_tools/all.docker.tutorial.html#environment-variables","title":"Environment variables","text":"<ul> <li>You can either use variables directly from the environment or pass them in a   <code>docker-compose.yaml</code> file</li> <li>It is supposed that <code>POSTGRES_VERSION</code> is already defined in the shell.   <code>db:     image: \"postgres:${POSTGRES_VERSION}\"</code></li> <li>Set environment variable in a service's container   ```   db:     environment:<ul> <li>POSTGRES_VERSION=13   image: \"postgres:${POSTGRES_VERSION}\"   ```</li> </ul> </li> <li>Set environment variable with <code>.env</code> file   ```   db:     env_file:<ul> <li>./postgres_env.env   image: \"postgres:${POSTGRES_VERSION}\"   ```</li> </ul> </li> <li>File <code>postgres_env.env</code>   ```bash <p>cat ./postgres_env.env   POSTGRES_VERSION=13   ```</p> </li> </ul>"},{"location":"work_tools/all.docker.tutorial.html#basic-commands","title":"Basic commands","text":"<ul> <li> <p>To check more advanced usage, please see   the official documentation.</p> </li> <li> <p>Let's assume that the <code>docker-compose.yaml</code> file is located in the current dir</p> </li> </ul> <p>```bash   # Build, (re)create, start, and attach to containers for a service.</p> <p>docker-compose up</p> <p># List containers</p> <p>docker-compose ps</p> <p># Stop containers created with <code>up</code></p> <p>docker-compose down   ```</p>"},{"location":"work_tools/all.docker_dev_tools_container.how_to_guide.html","title":"Dev tools container","text":""},{"location":"work_tools/all.docker_dev_tools_container.how_to_guide.html#dev_tools","title":"dev_tools","text":"<ul> <li>File an Issue for the release</li> <li>Create the corresponding branch in dev_tools</li> <li>Change the code</li> <li>Run the release flow end-to-end   ``` <p>i docker_release_dev_image --version 1.1.0 i docker_release_prod_image --version 1.1.0   ```   TODO(Vlad): Add a command to run the push to Dockerhub and add it to the   single arch release flow</p> </li> <li>Push the image to Dockerhub manually</li> <li>Login to Dockerhub with the <code>sorrentum</code> account   ``` <p>docker login --username=sorrentum   ```</p> </li> <li>Tag the dev version image as <code>sorrentum/dev_tools:dev</code>   ``` <p>docker tag 665840871993.dkr.ecr.us-east-1.amazonaws.com/dev_tools:dev-1.1.0 sorrentum/dev_tools:dev   ```</p> </li> <li>Push the dev image to Dockerhub   ``` <p>docker push sorrentum/dev_tools:dev   ```</p> </li> <li>Tag the prod version image as <code>sorrentum/dev_tools:prod</code>   ``` <p>docker tag 665840871993.dkr.ecr.us-east-1.amazonaws.com/dev_tools:prod sorrentum/dev_tools:prod   ```</p> </li> <li>Push the prod image to Dockerhub   ``` <p>docker push sorrentum/dev_tools:prod   ```</p> </li> <li>Push the latest <code>prod</code> image to GHCR registry manually for GH actions to use   it</li> <li>Perform a Docker login using your GitHub username and PAT (Personal Access     Token):     <code>bash     &gt; docker login ghcr.io -u &lt;username&gt;</code></li> <li>Tag the <code>prod</code> image to the GHCR namespace:     <code>bash     &gt; docker tag 623860924167.dkr.ecr.eu-north-1.amazonaws.com/dev_tools:prod ghcr.io/cryptokaizen/dev_tools:prod</code></li> <li> <p>Push the tagged image to the GHCR registry:     <code>bash     &gt; docker push ghcr.io/cryptokaizen/dev_tools:prod</code></p> </li> <li> <p>Update the changelog, i.e. <code>//dev_tools/changelog.txt</code></p> </li> <li>The changelog should be updated only after the image is released; otherwise     the sanity checks will assert that the release's version is not higher than     the latest version recorded in the changelog.</li> <li>Specify what has changed</li> <li>Pick the release version accordingly<ul> <li>NB! The release version should consist of 3 digits, e.g. \"1.1.0\" instead   of \"1.1\"</li> <li>We use semantic versioning convention</li> <li>For example, adding a package to the image would mean bumping up version     1.0.0 to 1.0.1</li> </ul> </li> <li>Do a PR with the change including the updated <code>changelog.txt</code></li> <li>Send a message on the <code>all@</code> chat telling people that a new version of the   container has been released</li> <li>Users need to do<ul> <li><code>i docker_pull</code> from <code>dev_tools</code>,</li> <li><code>i docker_pull_dev_tools</code> from <code>cmamp</code></li> </ul> </li> <li>Users need to make sure to pull docker after the master is up-to-date     (including amp submodules)</li> </ul>"},{"location":"work_tools/all.docker_optimizer_container.how_to_guide.html","title":"Optimizer container","text":""},{"location":"work_tools/all.docker_optimizer_container.how_to_guide.html#rationale","title":"Rationale","text":"<ul> <li>The high-level goal is to move towards containerized Python scripts running in   smaller containers instead of keep adding packages to <code>amp</code> / <code>cmamp</code>, which   makes the <code>amp</code> / <code>cmamp</code> container bloated and risky to build</li> <li>Along this design philosophy similar to microservices, we want to have a   Docker container, called <code>opt</code> with a Python script that uses some packages   that are not compatible with <code>amp</code> (specifically cvxopt, cvxpy)</li> <li>This is similar to what we do for the <code>dev_tools</code>, which is like a   containerized Python script for the linter</li> </ul>"},{"location":"work_tools/all.docker_optimizer_container.how_to_guide.html#build-and-run-a-local-version-of-opt","title":"Build and run a local version of <code>opt</code>","text":"<ul> <li>You can build the container locally with:   ``` <p>cd optimizer i opt_docker_build_local_image --version 0.1.0   ```</p> </li> <li>This process takes around 5 mins and then you should have the container   <code>docker image ls 665840871993.dkr.ecr.us-east-1.amazonaws.com/opt:local-saggese-0.1.0   REPOSITORY                                         TAG                   IMAGE ID       CREATED         SIZE   665840871993.dkr.ecr.us-east-1.amazonaws.com/opt   local-saggese-0.1.0   bb7d60d6a7d0   7 seconds ago   1.23GB</code></li> <li>Run the container as:   ``` <p>i opt_docker_bash --stage local --version 0.1.0   ```</p> </li> <li>To run a Jupyter notebook in the <code>opt</code> container:</li> </ul>"},{"location":"work_tools/all.docker_optimizer_container.how_to_guide.html#internals","title":"Internals","text":""},{"location":"work_tools/all.docker_optimizer_container.how_to_guide.html#one-container-per-git-repo","title":"One container per Git repo","text":"<ul> <li>A simple approach is to have each deployable unit (i.e., container)   corresponding to a Git repo</li> <li> <p>The consequence would be:</p> <ul> <li>A multiplication of repos</li> <li>No implicit sharing of code across different containers</li> <li>Some mechanism to share code (e.g., <code>helpers</code>) across repos (e.g., using   bind mount)</li> <li>Not playing nice with Git subrepo mechanism since Docker needs to see the   entire repo</li> </ul> </li> <li> <p>So the code would be organized in 4 repos:   ```</p> </li> <li>lemonade / lime<ul> <li>helpers</li> <li>optimizer</li> <li>oms</li> <li>models in amp   ```</li> </ul> </li> <li>Where the dependency between containers are<ul> <li>Lemonade -&gt; amp</li> <li>Amp -&gt; optimizer, helpers</li> <li>Optimizer -&gt; helpers, core</li> </ul> </li> </ul>"},{"location":"work_tools/all.docker_optimizer_container.how_to_guide.html#multiple-containers-per-git-repo","title":"Multiple containers per Git repo","text":"<ul> <li>Another approach is to have <code>optimizer</code> as a directory inside <code>amp</code></li> <li>This keeps <code>amp</code> and <code>optimizer</code> in a single repo</li> <li>To build / run optimizer code in its container one needs to <code>cd</code> in the dir</li> <li>The problem then becomes how to share <code>helpers</code></li> </ul>"},{"location":"work_tools/all.docker_optimizer_container.how_to_guide.html#mounting-only-optimizer-dir-inside-docker","title":"Mounting only <code>optimizer</code> dir inside Docker","text":"<ul> <li>From <code>devops/compose/docker-compose.yml</code> <code>42 volumes:   43  # Move one dir up to include the entire git repo (see AmpTask1017).   44  - ../../:/app   45 # Move one dir down to include the entire git repo (see AmpTask1017).   46 working_dir: /app</code></li> <li>From <code>devops/docker_build/dev.Dockerfile</code></li> <li>ENTRYPOINT [\"devops/docker_run/entrypoint.sh\"]</li> <li>The problem is that Git repo doesn't work anymore   <code>git --version: git version 2.30.2   fatal: not a git repository (or any parent up to mount point /)   Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).</code></li> <li>A work around is to inject .git in /git of the container and then point git to   that   ```   environment:   ...</li> <li>GIT_DIR=/git</li> </ul> <p>volumes:     # Move one dir up to include the entire git repo (see AmpTask1017).     - ../../:/app     - ../../../../.git:/git     - ../../../../amp/helpers:/app/helpers   ```</p> <ul> <li>Git works but it gets confused with the paths   <code>modified: .dockerignore       deleted: .github/gh_requirements.txt       deleted: .github/workflows/build_image.yml.DISABLED       deleted: .github/workflows/fast_tests.yml       deleted: .github/workflows/linter.yml.DISABLED       deleted: .github/workflows/slow_tests.yml       deleted: .github/workflows/superslow_tests.yml.DISABLED       deleted: .gitignore</code></li> </ul>"},{"location":"work_tools/all.docker_optimizer_container.how_to_guide.html#mounting-the-supermodule-eg-lime-lemonade-amp-inside-docker","title":"Mounting the supermodule (e.g., lime, lemonade, amp) inside Docker","text":"<ul> <li>From <code>devops/compose/docker-compose.yml</code> <code>42 volumes:   43  # Move one dir up to include the entire git repo (see AmpTask1017).   44  - ../../../:/app   45 # Move one dir down to include the entire git repo (see AmpTask1017).   46 working_dir: /app/amp</code></li> <li>From <code>devops/docker_build/dev.Dockerfile</code></li> <li>ENTRYPOINT [\"optimizer/devops/docker_run/entrypoint.sh\"]</li> <li>This approach mounts 4 dirs up from devops/compose/docker-compose.yml, i.e.,   //lime</li> <li>The problem with this approach is that now repo_config.py is incorrect</li> <li><code>i opt_docker_build_local_image --version 0.4.0</code> <code>32 - ../../../helpers:/app/amp/optimizer/helpers   33   34 # Shared cache. This is specific of lime.   35 - /local/home/share/cache:/cache   36   37 # Mount `amp` when it is used as submodule. In this case we need to   38 # mount the super project in the container (to make git work with the   39 # supermodule) and then change dir to `amp`.   40 app:   41  extends:   42    base_app   43 volumes:   44  # Move one dir up to include the entire git repo (see AmpTask1017).   45  - ../../../../:/app   46 # Move one dir down to include the entire git repo (see AmpTask1017).   47 working_dir: /app/amp/optimizer   48 #entrypoint: /bin/bash -c \"ls helpers\"</code></li> </ul>"},{"location":"work_tools/all.docker_optimizer_container.how_to_guide.html#invariants","title":"Invariants","text":"<ul> <li>A deployable dir is a dir under a Git repo</li> <li>It corresponds to a software component (code + library = Docker container)</li> <li>Anything that has a devops dir is \"deployable\"</li> <li>Each Docker container is run from its corresponding dir, e.g.,</li> <li>Amp container from the amp dir</li> <li>Amp container from the lemonade dir (this is just a shortcut since lemonade     has the same deps right now as amp)</li> <li>Always mount the outermost Git repo under <code>/app</code></li> <li>Set the Docker working dir as the current dir</li> <li>Each deployable dir specifies all the needed information in <code>repo_config.py</code>   (which is the one in the current dir)</li> <li>What container to run</li> <li>What functionality is supported on different servers (e.g., privileged way)</li> <li>The <code>changelog.txt</code> file is in the deployable dir (e.g.,   optimizer/changelog.txt)</li> <li>Each</li> </ul> <p>One run the invoke commands from optimizer dir</p> <p>When the Docker container starts the current dir is optimizer</p> <p>helpers, core is mounted in the same dir</p> <p>You can't see code outside optimizer</p> <p>TODO(gp): running in amp under lemonade should use the local repo_config</p>"},{"location":"work_tools/all.docker_optimizer_container.how_to_guide.html#release-and-ecr-flow","title":"Release and ECR flow","text":"<p>TODO(gp): Implement this</p>"},{"location":"work_tools/all.docker_optimizer_container.how_to_guide.html#unit-testing-code-inside-opt-container","title":"Unit testing code inside <code>opt</code> container","text":"<ul> <li>Since we want to segregate the package dependencies in different containers,   tests that have a dependency from cvxopt /cvxpy can't be run inside the <code>amp</code>   container but need to be run inside <code>opt</code>.</li> <li>We want to:</li> <li>(as always) write and run unit tests for the optimizer code in isolation,      i.e., test the code in the directory <code>optimizer</code> by itself</li> <li>Run all the tests for the entire repo (relying on both containers <code>amp</code> and      <code>optimizer</code> with a single command invocation)</li> <li>Be able to run tests belonging to only one of the containers to shorten the      debugging cycle</li> <li>To achieve this we need to solve the 3 problems below.</li> </ul>"},{"location":"work_tools/all.docker_optimizer_container.how_to_guide.html#avoid-compiling-code-depending-from-cvxopt-when-running-amp","title":"Avoid compiling code depending from cvxopt when running amp","text":"<ul> <li>We can't parse code (e.g., in <code>pytest</code>) that includes packages that are not   present in a container</li> <li> <p>E.g., <code>pytest</code> running in <code>amp</code> should not parse code in <code>//amp/optimizer</code>     since it contains imports that will fail</p> </li> <li> <p>Solution 1</p> </li> <li> <p>We use the pytest mechanism <code>cvx = pytest.importorskip(\"cvxpy\")</code> which is     conceptually equivalent to:     ```     try:       import cvxopt       has_cvxopt = True     except ImportError:       has_cvxopt = False</p> <p>if has_cvxopt:         def utils1():                  cvxopt\u2026 ```</p> </li> <li> <p>Solution 2</p> </li> <li> <p>Test in eachfile for the existence of the needed packages and enclose the     code in an <code>if _has_package</code></p> <ul> <li>Pros:</li> <li>We can skip code based dynamically on a <code>try ... except ImportModule</code> to     check what packages are present</li> <li>Cons:</li> <li>Repeat the same piece of <code>try ... except</code> in many places<ul> <li>Solution: we can factor it out in a function</li> </ul> </li> <li>We need to enclose the code in a <code>if ...</code> that screws up the indentation     and makes the code weird</li> </ul> </li> <li> <p>Solution 3</p> </li> <li> <p>Exclude certain directories (e.g., <code>//amp/optimizer</code>) from <code>pytest</code></p> <ul> <li>Pros:</li> <li>We don't have to spread the <code>try ... except</code> and <code>if \\_has_package</code> in     the code</li> <li>Cons:</li> <li>The directory is relative to the top directory<ul> <li>Solution: we can use a regex to specify the dir without the full path</li> </ul> </li> <li>Which directories are included and excluded depends on where <code>pytest</code> is     run<ul> <li>E.g., running <code>pytest</code> in an <code>amp</code> container we need to skip the   <code>optimizer</code> dir, while <code>pytest</code> in an <code>optimizer</code> container should   skip everything but the <code>optimizer</code> dir</li> </ul> </li> </ul> </li> <li> <p>Solution 4</p> </li> <li>Exclude certain directories or files based on which container we are running     in<ul> <li>Cons:</li> <li>We need to have a way to determine in which container we are running<ul> <li>Solution: we can use the env vars we use for versioning ``` <p>echo $AM_CONTAINER_VERSION amp-1.0.3- ```</p> </li> </ul> </li> </ul> </li> <li>Given the pros and cons, we decided to follow Solution 1 and Solution 3</li> </ul>"},{"location":"work_tools/all.docker_optimizer_container.how_to_guide.html#run-optimizer-tests-in-a-stand-alone-opt-container","title":"Run optimizer tests in a stand-alone <code>opt</code> container","text":"<ul> <li>To run the optimizer tests, you can create an <code>opt</code> container and then run   <code>pytest</code>   ``` <p>cd optimizer i opt_docker_bash   docker&gt; pytest .   ```</p> </li> <li>We wrap this in an invoke target like <code>i opt_run_fast_tests</code></li> </ul> <p>Alternative solution</p> <ul> <li>We can use dind to run the <code>opt</code> container inside a <code>cmamp</code> one</li> <li>Cons:<ul> <li>Dind complicates the system</li> <li>Dind is not supported everywhere (one needs privileged containers)</li> <li>Dind is slower since there are 2 levels of (relatively fast)   virtualization</li> </ul> </li> </ul>"},{"location":"work_tools/all.docker_optimizer_container.how_to_guide.html#run-optimizer-tests-as-part-of-running-unit-tests-for-cmamp","title":"Run optimizer tests as part of running unit tests for <code>cmamp</code>","text":"<ul> <li>We use the same mechanism as <code>run_fast_slow_superslow_tests</code> to pull together   different test lists</li> </ul>"},{"location":"work_tools/all.docker_optimizer_container.how_to_guide.html#call-a-dockerized-executable-from-a-container","title":"Call a Dockerized executable from a container","text":"<ul> <li>From   https://github.com/cryptokaizen/cmamp/issues/1357</li> <li> <p>We need to call something from <code>amp</code> to <code>opt</code> Docker</p> </li> <li> <p>Solution 1</p> </li> <li> <p>Inside the code we build the command line     <code>cmd = 'docker run -it ... '; system(cmd)</code></p> <ul> <li>Cons:</li> <li>There is code replicated between here and the invoke task (e.g., the     info about the container, ...)</li> </ul> </li> <li> <p>Solution 2</p> </li> <li> <p>Call the Dockerized executable using the <code>docker_cmd</code> invoke target     <code>cmd = \"invoke opt_docker_cmd -cmd '...'\"     system(cmd)</code></p> <ul> <li>Pros:</li> <li>All the Docker commands go through the same interface inside invoke</li> <li>Cons</li> <li>Bash interpolation in the command</li> <li>Another level of indirection: do a system call to call <code>invoke</code>,     <code>invoke</code> calls docker, docker does the work</li> <li><code>invoke</code> needs to be installed inside the calling container</li> </ul> </li> <li> <p>Solution 3</p> </li> <li> <p>Call opt_lib_tasks.py <code>opt_docker_cmd(cmd, ...)</code></p> <ul> <li>Pros</li> <li>Avoid doing a call to invoke</li> <li>Can deal with bash interpolation in Python</li> </ul> </li> <li> <p>We should always use Solution 3, although in the code sometimes we use   Solution 1 and 2 (but we should replace in favor of Solution 3).</p> </li> </ul>"},{"location":"work_tools/all.docker_optimizer_container.how_to_guide.html#_1","title":"Optimizer container","text":"<ul> <li>The interface to the Dockerized optimizer is in <code>run_optimizer</code> in   <code>//amp/oms/call_optimizer.py</code></li> <li>To run the examples   ``` <p>cd //lime i docker_bash pytest ./amp/oms/test/test_call_optimizer.py::Test_run_dockerized_optimizer1   ```</p> </li> </ul>"},{"location":"work_tools/all.dockerhub.how_to_guide.html","title":"Dockerhub","text":""},{"location":"work_tools/all.dockerhub.how_to_guide.html#login-dockerhub","title":"Login Dockerhub","text":"<p>https://hub.docker.com/</p> <p>Username: sorrentum Email: gp@crypto-kaizen.com</p> <p>There are several public images</p> <ul> <li>Sorrentum/cmamp</li> <li>Sorrentum/dev_tools</li> </ul> <p>Used in DATA605:</p> <ul> <li>Sorrentum/sorrentum</li> <li>Sorrentum/defi</li> <li>Sorrentum/jupyter</li> </ul> <p>The page corresponding to the Sorrentum repo is https://hub.docker.com/u/sorrentum</p>"},{"location":"work_tools/all.dockerhub.how_to_guide.html#login-through-cli","title":"Login through CLI","text":"<p>docker login --username sorrentum --password XYZ</p>"},{"location":"work_tools/all.dockerhub.how_to_guide.html#list-all-the-images","title":"List all the images","text":"<ul> <li>Without authentication</li> </ul> <pre><code>&gt; curl -s \"https://hub.docker.com/v2/repositories/sorrentum/?page_size=100\" | jq '.results|.[]|.name'\n\"sorrentum\"\n\"cmamp\"\n\"jupyter\"\n\"dev_tools\"\n\"defi\"\n</code></pre>"},{"location":"work_tools/all.dockerhub.how_to_guide.html#rename-an-image","title":"Rename an image","text":"<p>docker pull yourusername/oldimagename:tag docker tag yourusername/oldimagename:tag yourusername/newimagename:tag docker push yourusername/newimagename:tag</p> <ul> <li>To delete the old image you need to go through the GUI</li> </ul>"},{"location":"work_tools/all.gh_and_thin_env_requirements.reference.html","title":"Required Packages for the thin environment and GH Actions","text":""},{"location":"work_tools/all.gh_and_thin_env_requirements.reference.html#thin-environment","title":"Thin environment","text":"<p>File location:</p> <ul> <li>requirements.txt</li> </ul>"},{"location":"work_tools/all.gh_and_thin_env_requirements.reference.html#packages","title":"Packages","text":"<ul> <li><code>boto3</code></li> <li>Interacts with the AWS services:</li> <li><code>boto3</code> import in the <code>haws</code></li> <li> <p><code>haws</code> usage in the <code>lib_tasks_docker_release.py</code></p> </li> <li> <p><code>invoke</code></p> </li> <li>Need for running the invoke targets:</li> <li> <p>_run_tests</p> </li> <li> <p><code>poetry</code></p> </li> <li>Manage dependencies in the dev image:</li> <li> <p>docker_build_local_image</p> </li> <li> <p><code>pytest</code></p> </li> <li>To run <code>Docker image QA tests</code>:</li> <li> <p>_run_qa_tests</p> </li> <li> <p><code>tqdm</code></p> </li> <li>Widely used for showing the progress of the process for example:</li> <li> <p>_fix_invalid_owner</p> </li> <li> <p><code>s3fs</code></p> </li> <li>Needed for some invoke targets, for example:</li> <li> <p>docker_update_prod_task_definition</p> </li> <li> <p><code>requests</code></p> </li> <li>Dependency for the <code>docker</code>, for now pinned to the version <code>2.31.0</code> since     the versions &gt;=<code>2.32.1</code> is causing the issue with the <code>docker-compose</code>:     https://github.com/psf/requests/issues/6707</li> <li>See the https://github.com/cryptokaizen/cmamp/issues/8340 for details</li> </ul>"},{"location":"work_tools/all.gh_and_thin_env_requirements.reference.html#candidate-packages-to-remove","title":"Candidate Packages to remove","text":"<ul> <li><code>docker</code> and <code>docker-compose</code> should be moved to OS installation   https://github.com/cryptokaizen/cmamp/issues/6498</li> </ul>"},{"location":"work_tools/all.gh_and_thin_env_requirements.reference.html#gh-actions","title":"GH Actions","text":"<p>File location:</p> <ul> <li>gh_requirements.txt</li> </ul>"},{"location":"work_tools/all.gh_and_thin_env_requirements.reference.html#packages_1","title":"Packages","text":"<ul> <li><code>invoke</code></li> <li><code>poetry</code></li> <li><code>pytest</code></li> <li><code>tqdm</code></li> <li><code>s3fs</code></li> <li><code>requests</code></li> </ul> <p>For above packages, see descriptions in the Thin environment/Packages section.</p>"},{"location":"work_tools/all.gh_and_thin_env_requirements.reference.html#candidate-packages-to-remove_1","title":"Candidate Packages to remove","text":"<ul> <li><code>docker</code> and <code>docker-compose</code> see in the   Thin environment section</li> </ul>"},{"location":"work_tools/all.gh_thin_env_dependencies.how_to_guide.html","title":"Thin environment dependencies","text":""},{"location":"work_tools/all.gh_thin_env_dependencies.how_to_guide.html#description","title":"Description","text":"<ul> <li> <p>We have 3 sources of package requirements in the project:</p> </li> <li> <p>The thin environment to run <code>invoke</code> targets outside the container</p> <ul> <li>/dev_scripts/client_setup/requirements.txt</li> <li>This is managed with <code>pip</code></li> </ul> </li> <li>GitHub requirements used for GitHub Actions specifically<ul> <li>/.github/gh_requirements.txt</li> <li>This is managed with <code>pip</code></li> </ul> </li> <li> <p>Requirements necessary for the container:</p> <ul> <li>/devops/docker_build/pyproject.toml</li> <li>This is managed with <code>poetry</code></li> </ul> </li> <li> <p>We want to keep the thin environment as \"thin\" as possible (i.e., with fewer   dependencies)</p> </li> <li>The thin environment and GitHub requirements have to be in sync</li> <li>The only difference is that the GitHub requirements have some limitations     due to the GitHub Actions environment</li> <li> <p>TODO(Vlad): Still not clear what exact difference between the two     requirements files</p> </li> <li> <p>This document provides a step-by-step guide for adding or make any changes in   the requirements file of both the thin env and GitHub</p> </li> </ul>"},{"location":"work_tools/all.gh_thin_env_dependencies.how_to_guide.html#change-in-requirements-file","title":"Change in requirements file","text":"<ul> <li>Some reasons for updating/changing the <code>requirements.txt</code> file are:</li> <li>A new feature requires a new package outside the container, e.g., a new or     updated <code>invoke</code> target</li> <li>Upgrading the package version since the current one is outdated</li> <li>Removing a package since it is not used anymore</li> </ul>"},{"location":"work_tools/all.gh_thin_env_dependencies.how_to_guide.html#confirm-with-build-team","title":"Confirm with Build team","text":"<ul> <li>Changes in any of the requirement files should be confirmed with the Build   team before merging the PR</li> <li>Is the new dependencies really needed?</li> <li>If the new dependencies is really needed, can we limit the scope of the     dependency? E.g.,<ul> <li>Move the related imports to where it is strictly needed in the code</li> <li>Do a try-catch <code>ImportError</code></li> </ul> </li> </ul> <p>Example:</p> <ul> <li>The /helpers/lib_tasks_gh.py module has some   <code>invoke</code> targets that are executed only in the container</li> <li>If the new package is needed for the <code>invoke</code> target only in the container, we   should move the import to the function where it is strictly needed</li> <li>See the <code>gh_publish_buildmeister_dashboard_to_s3()</code> in the   /helpers/lib_tasks_gh.py   for reference.</li> </ul>"},{"location":"work_tools/all.gh_thin_env_dependencies.how_to_guide.html#update-requirements-file","title":"Update requirements file","text":"<ul> <li>Update both the requirements file if relevant   /dev_scripts/client_setup/requirements.txt   and /.github/gh_requirements.txt</li> <li>This file should be changed in every repository (e.g., <code>cmamp</code>,     <code>kaizenflow</code>, <code>orange</code>)</li> <li>After adding the new requirements the build team will run all the tests   locally as well as on GitHub</li> </ul>"},{"location":"work_tools/all.gh_thin_env_dependencies.how_to_guide.html#update-documentation","title":"Update Documentation","text":"<ul> <li>Update the   /docs/dev_tools/thin_env/all.gh_and_thin_env_requirements.reference.md</li> </ul>"},{"location":"work_tools/all.gh_thin_env_dependencies.how_to_guide.html#notify-team","title":"Notify Team","text":"<p>In the @all Telegram channel, notify the team about the new package and ask them to rebuild the thin env.</p> <p>Example:</p> <pre><code>Hi! In the PR: https://github.com/cryptokaizen/cmamp/pull/6800 we removed\nunused packages from the thin environment.\n\nYou need to update the thin environment by running:\n\n&gt; cd ~/src/cmamp1\n&gt; dev_scripts/client_setup/build.sh\n</code></pre> <p>Last review: GP on 2024-05-07</p>"},{"location":"work_tools/all.git.how_to_guide.html","title":"Git","text":""},{"location":"work_tools/all.git.how_to_guide.html#git-workflow-and-best-practices","title":"Git workflow and best practices","text":""},{"location":"work_tools/all.git.how_to_guide.html#before-you-start","title":"Before you start","text":"<ul> <li>GitHub is the place where we keep our code</li> <li><code>git</code> is the tool (program) for version control</li> <li>We interact with GitHub via <code>git</code></li> <li>Use public key for authorization</li> <li>You can add a new public key here     GH -&gt; Personal settings -&gt; SSH keys</li> <li>More details about what is public key you can find in     all.ssh.how_to_guide.md</li> </ul>"},{"location":"work_tools/all.git.how_to_guide.html#readings","title":"Readings","text":"<ul> <li>Read at least the first 3 chapters of   Git book</li> <li>Read about   Git Submodules</li> <li>We use Git submodules to compose and share code about repos</li> </ul>"},{"location":"work_tools/all.git.how_to_guide.html#workflow","title":"Workflow","text":"<ul> <li>Run <code>git fetch</code>   ```   # Fetch all the data from origin. <p>git fetch</p> </li> </ul> <p># List all the branches.</p> <p>git branch -r   origin/HEAD -&gt; origin/master   origin/PTask274   ...   ```</p> <ul> <li>Checkout master and pull</li> <li> <p>You want to branch from the latest version of master to avoid a merge:     <code>``     # Checkout the</code>master` branch.     &gt; git checkout master</p> </li> <li> <p>Name a branch after its corresponding issue</p> </li> <li> <p>The canonical name for a new feature branch is obtained by running     <code>i gh_issue_title</code>:     ```     &gt; i gh_issue_title -i 274     INFO: &gt; cmd='/Users/saggese/src/venv/amp.client_venv/bin/invoke gh_issue_title -i 274'     report_memory_usage=False report_cpu_usage=False     ## gh_issue_title: issue_id='274', repo_short_name='current'     ## gh_login:     07:35:54 - INFO  lib_tasks_gh.py gh_login:48                            account='sorrentum'     export GIT_SSH_COMMAND='ssh -i /Users/saggese/.ssh/id_rsa.sorrentum.github'     gh auth login --with-token &lt;/Users/saggese/.ssh/github_pat.sorrentum.txt</p> </li> <li> <p>Create and checkout the branch   ```</p> <p>git branch my_feature git checkout my_feature   ```</p> </li> <li>Alternatively, you can create and checkout in one command with     <code>&gt; git checkout -b my_feature</code></li> <li>From this point on, you commit only in the branch and changes to master will     not affect your branch</li> <li>If the branch already exists, check out the branch by executing     <code>&gt; git checkout my_feature</code></li> <li>Commit your work early and often</li> <li>Commits on your feature branch do not affect master. Checkpoint your work     regularly by committing:     <code>&gt; git status On branch my_feature     &gt; git add ...     &gt; git commit     [my_feature 820b296] My feature is awesome!</code></li> <li>Commits stay local (not seen on GitHub) until you explicitly tell git to     \"upload\" the commits through git push (see next)</li> <li>Push your feature branch changes upstream</li> <li>When you commit, commits are local (not seen on GitHub)</li> <li>When you want your code to be pushed to the server (e.g., to back up or to     share the changes with someone else), you need to push the branch upstream     <code>&gt; git push -u origin my_feature     ...     30194fc..820b296  my_feature -&gt; my_feature     Branch 'my_feature' set up to track remote branch 'my_feature' from 'origin'.</code></li> <li>Note that <code>-u</code> tells git to set the upstream of this branch to origin</li> <li>This operation is needed only the first time you create the branch and not     for each <code>git push</code></li> <li>Merge <code>master</code> into your feature branch regularly</li> <li>Merge <code>master</code> into your feature branch at least once a day, if the branch     stays around that long:     <code># Get the .git from the server     &gt; git fetch     # Make your master up to origin/master.     &gt; git checkout master     &gt; git pull     # Merge master into my_feature branch.     &gt; git checkout my_feature     &gt; git merge master</code></li> <li>A simpler flow which should be equivalent<ul> <li>TODO(gp): Verify that   ```   # Get the .git from the server <p>git fetch   # Merge master into my_feature branch. git checkout my_feature git merge origin/master   ```</p> </li> </ul> </li> <li>Repeat Steps 4-7 as needed</li> <li>Request a review of your work by making a pull request (PR)</li> <li>Verify that your work is ready for a review by going through this checklist:<ul> <li>The PR is self-contained</li> <li>The latest <code>master</code> has been merged into the feature branch</li> <li>All files in the PR have been linted with <code>linter.py</code></li> <li>All tests pass</li> </ul> </li> <li>If your work is ready for review, make a pull request<ul> <li>Use the GitHub UI (for now; we may replace with a script). Go to the   branch on the web interface and push \"Compare &amp; pull request\"</li> <li>Make sure that GP and Paul are assigned as reviewers, as well as anyone   else who may be interested</li> <li>Make sure that GP and Paul are assigned as assignees</li> </ul> </li> <li>Follow up on all comments and mark as resolved any requested changes that     you resolve</li> </ul>"},{"location":"work_tools/all.git.how_to_guide.html#make-sure-your-local-master-is-in-sync-with-the-remote","title":"Make sure your local <code>master</code> is in sync with the remote.","text":"<p>git pull --rebase <code>- Alternatively, and especially if you have local changes to move to a new branch, run</code> git checkout master i git_pull ```</p>"},{"location":"work_tools/all.git.how_to_guide.html#copied-to-system-clipboard","title":"Copied to system clipboard:","text":"<p>CmTask274_Update_names: https://github.com/sorrentum/sorrentum/pull/274 <code>- Before running verify that GitHub cli `gh` works</code></p> <p>gh --version   gh version 2.29.0 (2023-05-10)   https://github.com/cli/cli/releases/tag/v2.29.0   <code>``   - The name is</code>CmTask274_Update_names<code>- To use multiple branches for a given task, append a numeral to the name, e.g.,</code>CmTask274_Update_names_v02`</p>"},{"location":"work_tools/all.git.how_to_guide.html#best-practices","title":"Best Practices","text":""},{"location":"work_tools/all.git.how_to_guide.html#do-not-check-in-large-data-files","title":"Do not check in large data files","text":"<ul> <li>Avoid checking in large data files</li> <li>The reason is that large files bloat the repo</li> <li>Once a large file is checked in, it never goes away</li> <li>Therefore, DO NOT CHECK IN DATA FILES IN EXCESS OF 500K</li> <li>If in doubt (even on a branch), ask first!</li> <li>Sometimes is makes sense to check in some representative data for unit tests</li> <li>BUT, larger tests should obtain their data from s3 or MongoDB</li> </ul>"},{"location":"work_tools/all.git.how_to_guide.html#branch-workflow-best-practices","title":"Branch workflow best practices","text":""},{"location":"work_tools/all.git.how_to_guide.html#branches-are-cheap","title":"Branches are cheap","text":"<ul> <li>One of the advantages of working with Git is that branches are cheap</li> </ul>"},{"location":"work_tools/all.git.how_to_guide.html#master-is-sacred","title":"<code>master</code> is sacred","text":"<ul> <li>In an ideal world <code>master</code> branch is sacred (see Platinum rule of Git)</li> <li>Development should never be done directly on master</li> <li>Changes to master should only happen by pull-request or merge</li> <li>One should avoid working in master except in rare cases, e.g., a simple     urgent bug-fix needed to unblock people</li> <li><code>master</code> should be always never broken (all tests are passing and it is     deployable)</li> </ul>"},{"location":"work_tools/all.git.how_to_guide.html#always-work-in-a-branch","title":"Always work in a branch","text":"<ul> <li>Generally it is best to be the sole contributor to your branch</li> <li>If you need to collaborate with somebody on a branch, remember that the     golden rule of rebase still applies to this \"public\" branch: \"do not rebase     pushed commits\"</li> <li>It is ok to open multiple branches for a given task if needed</li> <li>E.g., if you have multiple chunks of work or multiple people are working on     orthogonal changes</li> <li>It might be that the task is too big and needs to be broken in smaller bugs</li> <li>All the rules that apply to <code>master</code> apply also to a branch</li> <li>E.g., commit often, use meaningful commit messages.</li> <li>We are ok with a little looser attitude in your branch</li> <li>E.g., it might be ok to not run unit tests before each commit, but be     careful!</li> <li>Use a branch even if working on a research notebook</li> <li>Try to avoid modifying notebooks in multiple branches simultaneously, since     notebook merges can be painful</li> <li>Working in a branch in this case facilitates review</li> <li>Working in a branch protects the codebase from accidental pushes of code     changes outside of the notebook (e.g., hacks to get the notebook working     that need to be cleaned up)</li> </ul>"},{"location":"work_tools/all.git.how_to_guide.html#keep-different-changes-in-separate-branches","title":"Keep different changes in separate branches","text":"<ul> <li>It is easier for you to keep work sane and separated</li> <li>Cons of multiple conceptual changes in the same branches</li> <li>You are testing / debugging all at once which might make your life more   difficult</li> <li>Reviewing unrelated changes slows down the review process</li> <li>Packaging unrelated changes together that means no change gets merged until   all of the changes are accepted</li> </ul>"},{"location":"work_tools/all.git.how_to_guide.html#pull-request-pr-best-practices","title":"Pull request (PR) best practices","text":"<ul> <li>Make sure your PR is coherent</li> <li>It may not need to do everything the Task requires, but the PR should be     self-contained and not break anything</li> <li>If you absolutely need changes under review to keep going, create the new   branch from the old branch rather than from master (less ideal)</li> <li>Try to avoid branching from branches<ul> <li>This creates also dependencies on the order of committing branches</li> <li>You end up with a spiderweb of branches</li> </ul> </li> <li>Frequent small PRs are easier to review</li> <li>You will also experience faster review turnaround</li> <li>Reviewers like working on smaller changes more than working on larger ones</li> <li>PR review time does not scale linearly with lines changed (may be more like     exponential)</li> <li>Merging changes frequently means other people can more easily see how the code   is progressing earlier on in the process, and give you feedback</li> <li>E.g., \"here it is a much simpler way of doing this\", or even better \"you     don't need to write any code, just do &lt;this_and_that&gt;\"</li> <li>Merged changes are tested in the Jenkins build</li> </ul>"},{"location":"work_tools/all.git.how_to_guide.html#workflow-diagram","title":"Workflow diagram","text":""},{"location":"work_tools/all.git.how_to_guide.html#deleting-a-branch","title":"Deleting a branch","text":"<ul> <li>You can run the script <code>dev_scripts/git/git_branch.sh</code> to get all the branches   together with some information, e.g., last commit and creator</li> <li>E.g., let's assume we believe that <code>PTask354_INFRA_Populate_S3_bucket</code> is   obsolete and we want to delete it:</li> <li>Get <code>master</code> up to date     <code>&gt; git checkout master     &gt; git fetch     &gt; git pull</code></li> <li>Merge <code>master</code> into the target branch</li> <li>Pull and merge     <code>&gt; git checkout PTask354_INFRA_Populate_S3_bucket     &gt; git pull     &gt; git merge master</code></li> <li>Resolve conflicts     <code>&gt; git commit     &gt; git pull</code></li> <li>Ask Git if the branch is merged<ul> <li>One approach is to ask Git if all the changes in master are also in the   branch   ``` <p>git branch <code>PTask354_INFRA_Populate_S3_bucket</code> --no-merged PTask354_INFRA_Populate_S3_bucket   ```</p> </li> <li>Note that Git is very strict here, e.g.,   <code>PTask354_INFRA_Populate_S3_bucket</code> is not completely merged since I've   moved code \"manually\" (not only through <code>git cherry-pick, git merge</code>)</li> <li>One approach is to just merge <code>PTask354_INFRA_Populate_S3_bucket</code> into   master and run <code>git branch</code> again</li> </ul> </li> <li>Manually check if there is any textual difference<ul> <li>Another approach is to check what the differences are between the branch   and <code>origin/master</code>   ``` <p>git log master..HEAD   6465b0c saggese, 25 seconds ago : Merge branch 'master' into PTask354_INFRA_Populate_S3_bucket  (HEAD -&gt; PTask354_INFRA_Populate_S3_bucket, origin/PTask354_INFRA_Populate_S3_bucket) git log HEAD..master   ```</p> </li> <li>Here we see that there are no textual differences</li> <li>So we can either merge the branch into <code>master</code> or just kill directly</li> </ul> </li> <li>Kill-kill-kill!<ul> <li>To delete both the local and remote branch you can do   ``` <p>git branch -d PTask354_INFRA_Populate_S3_bucket git push origin --delete PTask354_INFRA_Populate_S3_bucket   ```</p> </li> </ul> </li> </ul>"},{"location":"work_tools/all.git.how_to_guide.html#how-to-and-troubleshooting","title":"How-to and troubleshooting","text":""},{"location":"work_tools/all.git.how_to_guide.html#do-not-mess-up-your-branch","title":"Do not mess up your branch","text":"<ul> <li>If you are working in a branch, before doing <code>git push</code> make sure the branch   is not broken (e.g., from a mistake in merge / rebase mess)</li> <li>A way to check that the branch is sane is the following:</li> <li>Make sure that you don't have extra commits in your branch:<ul> <li>The difference between your branch and master   <code>bash &gt; git fetch &gt; git checkout &amp;lt;BRANCH&gt; &gt; git log origin/master..HEAD</code>   shows only commits made by you or, if you are not the only one working on   the branch, only commits belonging to the branch with the same <code>PTaskXYZ</code></li> <li>E.g., if George is working on <code>PTask275</code> and sees that something funny is   going on:   <code>a379826 Ringo, 3 weeks ago : LemTask54: finish dataflow through   cross-validation   33a46b2 George, 2 weeks ago : PTask275 Move class attributes docstrings to init, change logging</code></li> </ul> </li> <li>Make sure the files modified in your branch are only the file you expect to     be modified     <code>&gt; git fetch     &gt; git checkout &amp;lt;BRANCH&gt;     &gt; git diff --name-only master..HEAD</code></li> <li>If you see that there is a problem, don't push upstream (because the branch     will be broken for everybody) and ask a Git expert</li> </ul>"},{"location":"work_tools/all.git.how_to_guide.html#analyzing-commits","title":"Analyzing commits","text":""},{"location":"work_tools/all.git.how_to_guide.html#show-files-modified-in-a-commit","title":"Show files modified in a commit","text":"<ul> <li>You can see the files modified in a given commit hash with:   ``` <p>git show --name-only $HASH   ```</p> </li> <li>E.g.,   ``` <p>git show --name-only 39a9e335298a3fe604896fa19296d20829801cf2</p> </li> </ul> <p>commit 39a9e335298a3fe604896fa19296d20829801cf2   Author: Julia &lt;julia@...&gt;   Date:   Fri Sep 27 11:43:41 2019</p> <p>PTask274 lint</p> <p>vendors/cme/utils.py   vendors/first_rate/utils.py   ```</p>"},{"location":"work_tools/all.git.how_to_guide.html#conflicts","title":"Conflicts","text":""},{"location":"work_tools/all.git.how_to_guide.html#getting-the-conflicting-files","title":"Getting the conflicting files","text":"<ul> <li>To see the files in conflicts   <code>git diff --name-only --diff-filter=U</code></li> <li>This is what the script <code>git_conflict_files.sh</code> does</li> </ul>"},{"location":"work_tools/all.git.how_to_guide.html#accepting-theirs","title":"Accepting \"theirs\"","text":"<pre><code>&gt; git checkout --theirs $FILES\n&gt; git add $FILES\n</code></pre> <ul> <li>TODO(gp): Fix this ours and theirs. The first option represents the current   branch from which you executed the command before getting the conflicts, and   the second option refers to the branch where the changes are coming from.   ``` <p>git show :1:README git show :2:README git show :3:README   ```</p> </li> <li>Stage #1 is the common ancestor of the files, stage #2 is the target-branch   version, and stage #3 is the version you are merging from.</li> </ul>"},{"location":"work_tools/all.git.how_to_guide.html#how-to-get-out-of-a-messyun-mergeable-branch","title":"How to get out of a messy/un-mergeable branch","text":"<ul> <li>If one screws up a branch:</li> <li>Rebase to master</li> <li>Resolve the conflicts<ul> <li>E.g., pick the <code>master</code> version when needed:   <code>git checkout --theirs ...; git add ...</code></li> </ul> </li> <li>Diff the changes in the branch vs another client at <code>master</code> <code>&gt; diff_to_vimdiff.py --dir1 $DIR1/amp --dir2 $DIR2/amp --skip_vim     Saving log to file '/Users/saggese/src/...2/amp/dev_scripts/diff_to_vimdiff.py.log'     10-06_15:22 INFO : _parse_diff_output:36  : Reading '/tmp/tmp.diff_to_vimdiff.txt'     #       DIFF: README.md     #       DIFF: core/dataflow.py     #       DIFF: core/dataflow_core.py     #       DIFF: core/test/test_core.py     #       DIFF: dev_scripts/diff_to_vimdiff.py     #       ONLY: diff_to_vimdiff.py.log in $DIR1/dev_scripts     #       DIFF: dev_scripts/grc     #       ONLY: code_style.txt in $DIR2/docs/notes     ...     #       DIFF: vendors/test/test_vendors.py</code></li> <li>Diff / merge manually the files that are different     <code>&gt; diff_to_vimdiff.py --dir1 $DIR1/...2/amp --dir2 $DIR2/...3/amp --skip_vim     &gt; --only_diff_content     #       DIFF: README.md     #       DIFF: core/dataflow.py     #       DIFF: core/dataflow_core.py     #       DIFF: core/test/test_core.py     ...</code></li> </ul>"},{"location":"work_tools/all.git.how_to_guide.html#reverting","title":"Reverting","text":""},{"location":"work_tools/all.git.how_to_guide.html#reverting-the-last-local-commit","title":"Reverting the last local commit","text":"<pre><code>&gt; git reset --soft HEAD~\n</code></pre>"},{"location":"work_tools/all.git.how_to_guide.html#branching","title":"Branching","text":""},{"location":"work_tools/all.git.how_to_guide.html#checking-what-work-has-been-done-in-a-branch","title":"Checking what work has been done in a branch","text":"<ul> <li>Look at all the branches available:   ```   # Fetch all the data from origin. <p>git fetch   # List all the branches. git branch -r   origin/HEAD -&gt; origin/master   origin/PTask274   ...   ```</p> </li> <li>Go to the branch:   ``` <p>git checkout PTask274   ```</p> </li> <li>Check what are the commits that are in the current branch HEAD but not in   <code>master</code>:   ``` <p>gll master..HEAD git log master..HEAD   eb12233 Julia PTask274 verify dataset integrity ( 13 hours ago) Sat Sep 28 18:55:12 2019 (HEAD -&gt; PTask274, origin/PTask274) ...   a637594 saggese PTask274: Add tag for review ( 3 days ago) Thu Sep 26 17:13:33 2019   ```</p> </li> <li>To see the actual changes in a branch you can't do (Bad) \\   <code>&gt; git diff master..HEAD</code> since <code>git diff</code> compares two commits and not a range   of commits like <code>git log</code> (yes, Git has a horrible API)</li> <li>What you need to do is to get the first commit in the branch and the last from   <code>git log</code> and compare them:   ``` <p>git difftool a637594..eb12233 gd a637594..eb12233   ```</p> </li> </ul>"},{"location":"work_tools/all.git.how_to_guide.html#checking-if-you-need-to-merge-master-into-your-feature-branch","title":"Checking if you need to merge <code>master</code> into your feature branch","text":"<ul> <li>You can see what commits are in master but missing in your branch with:   ``` <p>gll ..master   de51a7c saggese Improve script to help diffing trees in case of difficult merges. Add notes from reviews ( 5 hours ago) Sat Oct 5 11:24:11 2019 (origin/master, origin/HEAD, master)   8acd60c saggese Add a basic end-to-end unit test for the linter ( 19 hours ago) Fri Oct 4 21:28:09 2019 \u2026   ```</p> </li> <li>You want to <code>rebase</code> your feature branch onto <code>master</code></li> </ul>"},{"location":"work_tools/all.git.how_to_guide.html#comparing-the-difference-of-a-directory-among-branches","title":"Comparing the difference of a directory among branches","text":"<ul> <li>This is useful if we want to focus on changes on a single dir   ``` <p>git ll master..PTask274 vendors/cme   39a9e33 Julia PTask274 lint ( 2 days ago) Fri Sep 27 11:43:41 2019   c8e7e1a Julia PTask268 modify according to review16 ( 2 days ago) Fri Sep 27   11:41:47 2019   a637594 saggese PTask274: Add tag for review ( 3 days ago) Thu Sep 26 17:13:33   2019    git diff --name-only a637594..33a46b2 -- vendors helpers   helpers/csv.py   vendors/cme/utils.py   vendors/first_rate/Task274_verify_datasets.ipynb   vendors/first_rate/Task274_verify_datasets.py   vendors/first_rate/reader.py   vendors/first_rate/utils.py   vendors/test/test_vendors.py   ```</p> </li> </ul>"},{"location":"work_tools/all.git.how_to_guide.html#merging-master","title":"Merging <code>master</code>","text":"<ul> <li>If your branch lives long, you want to apply changes made on master to show on   your branch</li> <li>Merge flow</li> <li>Assume your branch is clean</li> <li>E.g., everything is committed, or stashed</li> <li>Pull changes from <code>master</code> on the remote repo   ``` <p>git checkout master git pull   ```</p> </li> <li>Checkout your feature branch   ``` <p>git checkout my_feature   ```</p> </li> <li>Merge stuff from <code>master</code> to <code>my_feature</code>   ``` <p>git merge master --no-ff   ... editor will open a message for the merge commit ...   ```</p> </li> <li>In few informal words, the <code>--no-ff</code> option means that commits are not   \"inlined\" (similar to rebase) even if possible, but a merge commit is always   used</li> <li>The problem is that if the commits are \"inlined\" then you can't revert the     change in one shot like we would do for a merge commit, but you need to     revert all the inlined changes</li> </ul>"},{"location":"work_tools/all.git.how_to_guide.html#rebasing","title":"Rebasing","text":"<ul> <li>For now, we suggest avoiding the rebase flow</li> <li>The reason is that rebase makes things cleaner when used properly, but can get   you into deep trouble if not used properly</li> <li>You can rebase onto <code>master</code>, i.e., you re-apply your changes to <code>master</code></li> <li>Not the other way around: that would be a disaster!   ``` <p>git checkout my_feature   # See that you have that master doesn't have. git ll origin/master..   # See that master has that you don't have. git ll ..origin/master git rebase master git ll ..origin/master   # Now you see that there is nothing in master you don't have git ll origin/master..   # You can see that you are ahead of master   ```</p> </li> </ul>"},{"location":"work_tools/all.git.how_to_guide.html#merging-pull-requests","title":"Merging pull requests","text":"<ul> <li>The procedure for manual merges is as follows</li> <li>Do not merge yourself unless explicitly requested by a reviewer</li> <li>Pull changes from remote <code>master</code> branch   ``` <p>git checkout master git pull   ```</p> </li> <li>Merge your branch into <code>master</code> without fast-forward   ``` <p>git merge --no-ff my_feature   ```</p> </li> <li>Push the newly merged master   ``` <p>git push   ```</p> </li> <li>Delete the branch, if you are done with it:   ``` <p>git branch -d my_feature   ```</p> </li> </ul>"},{"location":"work_tools/all.git.how_to_guide.html#submodules","title":"Submodules","text":""},{"location":"work_tools/all.git.how_to_guide.html#adding-a-submodule","title":"Adding a submodule","text":"<ul> <li>Following the instructions in   https://git-scm.com/book/en/v2/Git-Tools-Submodules</li> </ul>"},{"location":"work_tools/all.git.how_to_guide.html#working-in-a-submodule","title":"Working in a submodule","text":"<ul> <li>When you work in a submodule, the flow should be like:</li> <li>Create a branch in a submodule</li> <li>Do your job</li> <li>Push the submodule branch</li> <li>Create a PR in the submodule when you are done</li> </ul>"},{"location":"work_tools/all.git.how_to_guide.html#updating-a-submodule-to-the-latest-commit","title":"Updating a submodule to the latest commit","text":"<ul> <li>After the submodule PR is merged:</li> <li>Checkout the submodule in the master branch and do <code>git pull</code></li> <li>In the main repo, create a branch like <code>PTask1234_update_submodule</code></li> <li>From the new branch do <code>git add &amp;lt;submodule_name&gt;</code>, e.g., <code>git add amp</code></li> <li>Commit changes, push</li> <li>Create a PR</li> </ul>"},{"location":"work_tools/all.git.how_to_guide.html#to-check-if-supermodule-and-amp-are-in-sync","title":"To check if supermodule and amp are in sync","text":"<ul> <li>Run the script:   ``` <p>dev_scripts/git/git_submodules_are_updated.sh   ```</p> </li> </ul>"},{"location":"work_tools/all.git.how_to_guide.html#roll-forward-git-submodules-pointers","title":"Roll forward git submodules pointers:","text":"<ul> <li>Run the script:   ``` <p>dev_scripts/git/git_submodules_roll_fwd.sh   ```</p> </li> </ul>"},{"location":"work_tools/all.git.how_to_guide.html#to-clean-all-the-repos","title":"To clean all the repos","text":"<pre><code>&gt; git submodule foreach git clean -fd\n</code></pre>"},{"location":"work_tools/all.git.how_to_guide.html#pull-a-branch-without-checkout","title":"Pull a branch without checkout","text":"<ul> <li>This is useful when merging <code>master</code> in a different branch and we don't want   to checkout master just to pull   ``` <p>git fetch origin master:master   ```</p> </li> </ul>"},{"location":"work_tools/all.git.how_to_guide.html#to-force-updating-all-the-submodules","title":"To force updating all the submodules","text":"<ul> <li>Run the script <code>&gt; dev_scripts/git/git_submodules_pull.sh</code> or   ``` <p>git submodule update --init --recursive` git submodule foreach git pull --autostash   ```</p> </li> </ul>"},{"location":"work_tools/all.invoke_workflows.how_to_guide.html","title":"Invoke Workflows","text":""},{"location":"work_tools/all.invoke_workflows.how_to_guide.html#introduction","title":"Introduction","text":"<ul> <li>We use <code>invoke</code> to implement workflows (aka \"tasks\") similar to Makefile   targets, but using Python</li> <li> <p>The official documentation for <code>invoke</code> is   here</p> </li> <li> <p>We use <code>invoke</code> to automate tasks and package workflows for:</p> </li> <li>Docker: <code>docker_*</code></li> <li>Git: <code>git_*</code></li> <li>GitHub (relying on <code>gh</code> integration): <code>gh_*</code></li> <li>Running tests: <code>run_*</code></li> <li>Branch integration: <code>integrate_*</code></li> <li>Releasing tools and Docker images: <code>docker_*</code></li> <li> <p>Lint: <code>lint_*</code></p> </li> <li> <p>Each set of commands starts with the name of the corresponding topic:</p> </li> <li>E.g., <code>docker_*</code> for all the tasks related to Docker</li> <li>The best approach to getting familiar with the tasks is to browse the list and   then check the output of the help</li> <li><code>i</code> is the shortcut for the <code>invoke</code> command</li> </ul> <p>```bash</p> <p>invoke --help command i -h gh_issue_title   Usage: inv[oke] [--core-opts] gh_issue_title [--options] [other tasks here ...]</p> <p>Docstring:   Print the title that corresponds to the given issue and repo_short_name.   E.g., AmpTask1251_Update_GH_actions_for_amp.</p> <p>:param pbcopy: save the result into the system clipboard (only on macOS)</p> <p>Options:   -i STRING, --issue-id=STRING   -p, --[no-]pbcopy   -r STRING, --repo-short-name=STRING   ```</p> <ul> <li> <p>We can guarantee you a 2x improvement in performance if you master the   workflows, but it takes some time and patience</p> </li> <li> <p><code>TAB</code> completion available for all the tasks, e.g.,</p> </li> </ul> <p>```bash</p> <p>i gh_   gh_create_pr      gh_issue_title    gh_login          gh_workflow_list  gh_workflow_run   ```   - Tabbing after typing a dash (-) or double dash (--) will display valid     options/flags for the current context."},{"location":"work_tools/all.invoke_workflows.how_to_guide.html#listing-all-the-tasks","title":"Listing all the tasks","text":"<ul> <li>New commands are always being added, but a list of valid tasks is below</li> </ul> <p>```bash</p> <p>invoke --list   INFO: &gt; cmd='/Users/saggese/src/venv/amp.client_venv/bin/invoke --list'   Available tasks:</p> <p>check_python_files Compile and execute Python files checking for errors.   docker_bash Start a bash shell inside the container corresponding to a stage.   docker_build_local_image Build a local image (i.e., a release candidate \"dev\"   image).   docker_build_prod_image (ONLY CI/CD) Build a prod image.   docker_cmd Execute the command <code>cmd</code> inside a container corresponding to a   stage.   docker_images_ls_repo List images in the logged in repo_short_name.   docker_jupyter Run jupyter notebook server.   docker_kill Kill the last Docker container started.   docker_login Log in the AM Docker repo_short_name on AWS.   docker_ps List all the running containers.   docker_pull Pull latest dev image corresponding to the current repo from the   registry.   docker_pull_dev_tools Pull latest prod image of <code>dev_tools</code> from the registry.   docker_push_dev_image (ONLY CI/CD) Push the \"dev\" image to ECR.   docker_push_prod_image (ONLY CI/CD) Push the \"prod\" image to ECR.   docker_release_all (ONLY CI/CD) Release both dev and prod image to ECR.   docker_release_dev_image (ONLY CI/CD) Build, test, and release to ECR the latest   \"dev\" image.   docker_release_prod_image (ONLY CI/CD) Build, test, and release to ECR the prod   image.   docker_rollback_dev_image Rollback the version of the dev image.   docker_rollback_prod_image Rollback the version of the prod image.   docker_stats Report last started Docker container stats, e.g., CPU, RAM.   docker_tag_local_image_as_dev (ONLY CI/CD) Mark the \"local\" image as \"dev\".   find_check_string_output Find output of check_string() in the test running   find_test_class Report test files containing <code>class_name</code> in a format compatible   with   find_test_decorator Report test files containing <code>class_name</code> in pytest format.   fix_perms :param action:   gh_create_pr Create a draft PR for the current branch in the corresponding   gh_issue_title Print the title that corresponds to the given issue and   repo_short_name.   gh_workflow_list Report the status of the GH workflows.   gh_workflow_run Run GH workflows in a branch.   git_add_all_untracked Add all untracked files to Git.   git_branch_copy Create a new branch with the same content of the current branch.   git_branch_diff_with_base Diff files of the current branch with master at the   branching point.   git_branch_files Report which files were added, changed, and modified in the   current branch   git_branch_next_name Return a name derived from the branch so that the branch   does not exist.   git_clean Clean the repo_short_name and its submodules from artifacts.   git_create_branch Create and push upstream branch <code>branch_name</code> or the one   corresponding to   git_create_patch Create a patch file for the entire repo_short_name client from   the base   git_delete_merged_branches Remove (both local and remote) branches that have   been merged into master.   git_files Report which files are changed in the current branch with respect to   git_last_commit_files Print the status of the files in the previous commit.   git_merge_master Merge <code>origin/master</code> into the current branch.   git_pull Pull all the repos.   git_fetch_master Pull master without changing branch.   git_rename_branch Rename current branch both locally and remotely.   integrate_compare_branch_with_base Compare the files modified in both the   branches in src_dir and dst_dir to   integrate_copy_dirs Copy dir <code>subdir</code> from dir <code>src_dir</code> to <code>dst_dir</code>.   integrate_create_branch Create the branch for integration in the current dir.   integrate_diff_dirs Integrate repos from dir <code>src_dir</code> to <code>dst_dir</code>.   lint Lint files.   lint_create_branch Create the branch for linting in the current dir.   print_setup Print some configuration variables.   print_tasks Print all the available tasks in <code>lib_tasks.py</code>.   pytest_clean Clean pytest artifacts.   pytest_compare Compare the output of two runs of <code>pytest -s --dbg</code> removing   irrelevant   pytest_failed Process the list of failed tests from a pytest run.   pytest_failed_freeze_test_list Copy last list of failed tests to not overwrite   with successive pytest   run_blank_tests (ONLY CI/CD) Test that pytest in the container works.   run_coverage_report   run_fast_slow_tests Run fast and slow tests independently.   run_fast_tests Run fast tests.   run_qa_tests Run QA tests independently.   run_slow_tests Run slow tests.   run_superslow_tests Run superslow tests.   traceback Parse the traceback from Pytest and navigate it with vim.   ```</p>"},{"location":"work_tools/all.invoke_workflows.how_to_guide.html#getting-help-for-a-specific-workflow","title":"Getting help for a specific workflow","text":"<ul> <li>You can get a more detailed help with</li> </ul> <p>```bash</p> <p>invoke --help run_fast_tests   Usage: inv[oke] [--core-opts] run_fast_tests [--options] [other tasks here ...]</p> <p>Docstring:   Run fast tests.</p> <p>:param stage: select a specific stage for the Docker image   :param pytest_opts: option for pytest   :param pytest_mark: test list to select as <code>@pytest.mark.XYZ</code>   :param dir_name: dir to start searching for tests   :param skip_submodules: ignore all the dir inside a submodule   :param coverage: enable coverage computation   :param collect_only: do not run tests but show what will be executed</p> <p>Options:   -c, --coverage   -d STRING, --dir-name=STRING   -k, --skip-submodules   -o, --collect-only   -p STRING, --pytest-opts=STRING   -s STRING, --stage=STRING   -y STRING, --pytest-mark=STRING   ```</p>"},{"location":"work_tools/all.invoke_workflows.how_to_guide.html#implementation-details","title":"Implementation details","text":"<ul> <li>By convention all invoke targets are in <code>*_lib_tasks.py</code>, e.g.,</li> <li><code>helpers/lib_tasks.py</code> - tasks to be run in <code>cmamp</code></li> <li><code>optimizer/opt_lib_tasks.py</code> - tasks to be run in <code>cmamp/optimizer</code></li> <li>All invoke tasks are functions with the <code>@task</code> decorator, e.g.,</li> </ul> <p>```python   from invoke import task</p> <p>@task   def invoke_task(...):     ...   ```</p> <ul> <li>To run a task we use <code>context.run(...)</code>, see   the official docs</li> <li>To be able to run a specified invoke task one should import it in <code>tasks.py</code></li> <li>E.g., see <code>cmamp/tasks.py</code></li> <li>A task can be run only in a dir where it is imported in a corresponding   <code>tasks.py</code>, e.g.,</li> <li><code>invoke_task1</code> is imported in <code>cmamp/tasks.py</code> so it can be run only from     <code>cmamp</code></li> <li><code>invoke_task2</code> is imported in <code>cmamp/optimizer/tasks.py</code> so it can be run     only from <code>cmamp/optimizer</code><ul> <li>In other words one should do <code>cd cmamp/optimizer</code> before doing   <code>i invoke_task2 ...</code></li> </ul> </li> </ul>"},{"location":"work_tools/all.invoke_workflows.how_to_guide.html#git","title":"Git","text":""},{"location":"work_tools/all.invoke_workflows.how_to_guide.html#merge-master-in-the-current-branch","title":"Merge master in the current branch","text":"<pre><code>&gt; i git_merge_master\n</code></pre>"},{"location":"work_tools/all.invoke_workflows.how_to_guide.html#github","title":"GitHub","text":"<ul> <li>Get the official branch name corresponding to an Issue</li> </ul> <p>```bash</p> <p>i gh_issue_title -i 256   ## gh_issue_title: issue_id='256', repo_short_name='current'</p> <p># Copied to system clipboard:   AmpTask256_Part_task2236_jenkins_cleanup_split_scripts:   https://github.com/alphamatic/amp/pull/256   ```</p>"},{"location":"work_tools/all.invoke_workflows.how_to_guide.html#create-a-pr","title":"Create a PR","text":"<p>TODO(gp): Describe</p>"},{"location":"work_tools/all.invoke_workflows.how_to_guide.html#extract-a-pr-from-a-larger-one","title":"Extract a PR from a larger one","text":"<ul> <li>When having a PR which is really big we prefer to brake it into smaller   mergeable PRs using <code>i git_branch_copy</code></li> </ul>"},{"location":"work_tools/all.invoke_workflows.how_to_guide.html#example","title":"Example","text":"<ul> <li>In my workflow there is a feature branch (e.g. <code>CmTask5874_Document_PR_flow</code>   that I am developing in.</li> <li>When a piece of code is ready to be merged rather than merging the main PR we   will create a child PR and merge that into <code>master</code>:</li> <li>Step 1: Make sure your branch is up to date with origin</li> </ul> <p>```bash   # First switch to your feature branch</p> <p>git checkout CmTask5874_Document_PR_flow</p> <p># Make sure that the branch is up-to-date with master</p> <p>i git_merge_master</p> <p># Commit and push the changes that you have made to the branch</p> <p>git commit -m \u201cInitial Changes\u201d git push origin CmTask5874_Document_PR_flow</p> <p># You can check the git diff between your branch and master using the following command:</p> <p>i git_branch_diff_with -t base --only-print-files   # Output:   INFO: &gt; cmd='/data/sameepp/src/venv/amp.client_venv/bin/invoke git_branch_diff_with -t base --only-print-files'   04:58:35 - INFO  lib_tasks_git.py _git_diff_with_branch:726   ###############################################################################   # files=3   ###############################################################################   04:58:35 - INFO  lib_tasks_git.py _git_diff_with_branch:727   ./figs/development/Fig1.png   ./figs/development/Fig2.png   docs/work_tools/all.development.how_to_guide.md   04:58:35 - WARN  lib_tasks_git.py _git_diff_with_branch:732             Exiting as per user request with --only-print-files   ```</p> <p>As we can see above I have made changes to 3 files. Lets say I just want to   partially merge this PR and still keep working on the main branch (e.g. merge   only the .png files).   - Step 2: create a new branch (e.g., <code>CmTask5874_Document_PR_flow_02</code>) derived     from our feature branch <code>CmTask5874_Document_PR_flow</code> using the command     <code>i git_branch_copy</code>.</p> <p>```bash   # Create a derived branch from the feature branch.</p> <p>i git_branch_copy</p> <p># Output:   INFO: &gt; cmd='/data/sameepp/src/venv/amp.client_venv/bin/invoke git_branch_copy'   git clean -fd   invoke git_merge_master --ff-only   From github.com:cryptokaizen/cmamp     e59affd79..d6e6ed8e4  master     -&gt; master   INFO: &gt; cmd='/data/sameepp/src/venv/amp.client_venv/bin/invoke git_merge_master --ff-only'   ## git_merge_master:   ## git_fetch_master:   git fetch origin master:master   git submodule foreach 'git fetch origin master:master'   git merge master --ff-only   Already up to date.   07:04:46 - INFO  lib_tasks_git.py git_branch_copy:599                   new_branch_name='CmTask5874_Document_PR_flow_2'   git checkout master &amp;&amp; invoke git_branch_create -b 'CmTask5874_Document_PR_flow_2'   Switched to branch 'master'   Your branch is up to date with 'origin/master'.   INFO: &gt; cmd='/data/sameepp/src/venv/amp.client_venv/bin/invoke git_branch_create -b CmTask5874_Document_PR_flow_2'   ## git_branch_create:   07:05:00 - INFO  lib_tasks_git.py git_branch_create:413                 branch_name='CmTask5874_Document_PR_flow_2'   git pull --autostash --rebase   Current branch master is up to date.   Switched to a new branch 'CmTask5874_Document_PR_flow_2'   remote:   remote: Create a pull request for 'CmTask5874_Document_PR_flow_2' on GitHub by visiting:   remote:      https://github.com/cryptokaizen/cmamp/pull/new/CmTask5874_Document_PR_flow_2   remote:   To github.com:cryptokaizen/cmamp.git   [new branch] CmTask5874_Document_PR_flow_2 -&gt;   CmTask5874_Document_PR_flow_2 git checkout -b CmTask5874_Document_PR_flow_2   git push --set-upstream origin CmTask5874_Document_PR_flow_2 Branch   'CmTask5874_Document_PR_flow_2' set up to track remote branch   'CmTask5874_Document_PR_flow_2' from 'origin'. git merge --squash --ff   CmTask5874_Document_PR_flow &amp;&amp; git reset HEAD Updating d6e6ed8e4..a264a6f30   Fast-forward Squash commit -- not updating HEAD   docs/work_tools/figs/development/Fig1.png | Bin 27415 -&gt; 0 bytes   docs/work_tools/figs/development/Fig2.png | Bin 35534 -&gt; 0 bytes 2 files   changed, 0 insertions(+), 0 deletions(-) delete mode 100644   docs/work_tools/figs/development/Fig1.png delete mode 100644   docs/work_tools/figs/development/Fig2.png Unstaged changes after reset: D   docs/work_tools/figs/development/Fig1.png D   docs/work_tools/figs/development/Fig2.png   <code>``   - Step 3: Once the command is completed you can see that there is a new branch</code>CmTask5874_Document_PR_flow_2`with the same changes from feature branch     ready to be staged. Hence finally you can just commit the desired files and     merge the changes to master.</p> <p>```bash</p> <p>git status   #Output:   On branch CmTask5874_Document_PR_flow_2   Your branch is up to date with 'origin/CmTask5874_Document_PR_flow_2'.</p> <p>Untracked files:   (use \"git add ...\" to include in what will be committed)         ./figs/development/Fig1.png         ./figs/development/Fig2.png         docs/work_tools/all.invoke_workflows.how_to_guide.md <p># Add, commit and push ont the required files.</p> <p>git add ./figs/development/Fig1.png ./figs/development/Fig2.png git commit -m \"Checkpoint\" git push origin CmTask5874_Document_PR_flow_2   ```</p> <ul> <li>Go to a fresh Git client (I have 2-3 Git clients separated from the one in   which I develop for this kind of operations) or go to master in the same Git   client</li> </ul> <p>```bash   # Go to master</p> <p>git checkout master</p> <p># Apply the patch from the run of <code>git_create_patch</code></p> <p>git apply /Users/saggese/src/lemonade1/amp/patch.amp.8f9cda97.20210609_080439.patch</p> <p># This patch should apply cleanly and with no errors from git, otherwise it means   that your feature branch does not have the latest master</p> <p># Remove what you don't want to commit.</p> <p># Do not change anything or run the linter otherwise your feature branch will not   merge easily.</p> <p>git diff git checkout master -- ...   ... git commit; git push</p> <p># Create a PR (non-draft so that GH can start running the tests)</p> <p>i gh_create_pr --no-draft</p> <p># Regress the branch</p> <p>i run_fast_tests ...</p> <p># Merge the PR into master</p> <p># Go back to your feature branch and merge master</p> <p>gco ${feature_branch} git pull</p> <p># Now one piece of your feature branch has been merged and you can repeat until   all the code is merged.   ```</p>"},{"location":"work_tools/all.invoke_workflows.how_to_guide.html#using-git","title":"Using git","text":"<pre><code>&gt; git checkout `dst_branch`\n&gt; git merge --squash --ff `src_branch`\n&gt; git reset HEAD\n</code></pre>"},{"location":"work_tools/all.invoke_workflows.how_to_guide.html#systematic-code-transformation","title":"Systematic code transformation","text":"<ul> <li>See the help of <code>amp/dev_scripts/replace_text.py</code></li> </ul>"},{"location":"work_tools/all.invoke_workflows.how_to_guide.html#generate-a-local-amp-docker-image","title":"Generate a local <code>amp</code> Docker image","text":"<ul> <li>This is a manual flow used to test and debug images before releasing them to   the team.</li> <li>The flow is similar to the dev image, but by default tests are not run and the   image is not released.</li> </ul> <p>```bash   # Build the local image (and update Poetry dependencies, if needed).</p> <p>i docker_build_local_image --update-poetry   ...   docker image ls 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp:local</p> <p>REPOSITORY TAG IMAGE ID CREATED SIZE   665840871993.dkr.ecr.us-east-1.amazonaws.com/amp local 9b3f8f103a2c 1 second ago 1.72GB</p> <p># Test the new \"local\" image</p> <p>i docker_bash --stage \"local\" python -c \"import async_solipsism\" python -c \"import async_solipsism; print(async_solipsism.version)\"</p> <p># Run the tests with local image   # Make sure the new image is used: e.g., add an import and trigger the tests.</p> <p>i run_fast_tests --stage \"local\" --pytest-opts core/dataflow/test/test_real_time.py i run_fast_slow_tests --stage \"local\"</p> <p># Promote a local image to dev.</p> <p>i docker_tag_local_image_as_dev i docker_push_dev_image   ```</p> <p>## Update the dev <code>amp</code> Docker image</p> <ul> <li>To implement the entire Docker QA process of a dev image</li> </ul> <p>```bash   # Clean all the Docker images locally, to make sure there is no hidden state.</p> <p>docker system prune --all</p> <p># Update the needed packages.</p> <p>devops/docker_build/pyproject.toml</p> <p># Visually inspect the updated packages.</p> <p>git diff devops/docker_build/poetry.lock</p> <p># Run entire release process.</p> <p>i docker_release_dev_image   ```</p> <p>## Experiment in a local image</p> <ul> <li>To install packages in an image, do <code>i docker_bash</code></li> </ul> <p>```bash   # Switch to root and install package.</p> <p>sudo su - source /venv/bin/activate pip install  <p># Switch back to user.</p> <p>exit   ```</p> <ul> <li>You should test that the package is installed for your user, e.g.,   ```bash <p>source /venv/bin/activate python -c \"import foobar; print(foobar);print(foobar.version)\"   ```</p> </li> <li>You can now use the package in this container. Note that if you exit the   container, the modified image is lost, so you need to install it again.</li> <li>You can save the modified image, tagging the new image as local, while the   container is still running.</li> <li>Copy your Container ID. You can find it</li> <li>In the docker bash session, e.g., if the command line in the container     starts with <code>user_1011@da8f3bb8f53b:/app$</code>, your Container ID is     <code>da8f3bb8f53b</code></li> <li>By listing running containers, e.g., run <code>docker ps</code> outside the container</li> <li>Commit image   ```bash <p>docker commit  /cmamp:local-$USER   ``` <li>E.g.     <code>docker commit da8f3bb8f53b 665840871993.dkr.ecr.us-east-1.amazonaws.com/cmamp:local-julias</code></li> <li>If you are running inside a notebook using <code>i docker_jupyter</code> you can install   packages using a one liner <code>! sudo su -; source ...;</code></li>"},{"location":"work_tools/all.invoke_workflows.how_to_guide.html#github-actions-ci","title":"GitHub Actions (CI)","text":"<pre><code>### Running a single test in GH Actions\n\nCreate a branch\n\nChange .github/workflows/fast_tests.yml\n\nrun: invoke run_fast_tests\n--pytest-opts=\"helpers/test/test_git.py::Test_git_modified_files1::test_get_modified_files_in_branch1\n-s --dbg\"\n## In the current implementation (where we try to not run for branches) to run in a branch\n</code></pre>"},{"location":"work_tools/all.invoke_workflows.how_to_guide.html#pytest","title":"pytest","text":"<ul> <li> <p>From https://gist.github.com/kwmiebach/3fd49612ef7a52b5ce3a</p> </li> <li> <p>More details on running unit tests with <code>invoke</code> is   /docs/coding/all.run_unit_tests.how_to_guide.md</p> </li> </ul>"},{"location":"work_tools/all.invoke_workflows.how_to_guide.html#run-with-coverage","title":"Run with coverage","text":"<pre><code>&gt; i run_fast_tests --pytest-opts=\"core/test/test_finance.py\" --coverage\n</code></pre>"},{"location":"work_tools/all.invoke_workflows.how_to_guide.html#capture-output-of-a-pytest","title":"Capture output of a pytest","text":"<ul> <li>Inside the <code>dev</code> container (i.e., docker bash)   <code>bash   docker&gt; pytest_log ...</code></li> </ul>"},{"location":"work_tools/all.invoke_workflows.how_to_guide.html#run-only-one-test-based-on-its-name","title":"Run only one test based on its name","text":"<ul> <li>Outside the <code>dev</code> container</li> </ul> <p>```bash</p> <p>i find_test_class Test_obj_to_str1   INFO: &gt; cmd='/Users/saggese/src/venv/amp.client_venv/bin/invoke find_test_class Test_obj_to_str1'   ## find_test_class: class_name abs_dir pbcopy   10:18:42 - INFO  lib_tasks_find.py _find_test_files:44                  Searching from '.'</p> <p># Copied to system clipboard:   ./helpers/test/test_hobject.py::Test_obj_to_str1   ```</p>"},{"location":"work_tools/all.invoke_workflows.how_to_guide.html#iterate-on-stacktrace-of-failing-test","title":"Iterate on stacktrace of failing test","text":"<ul> <li>Inside docker bash   <code>bash   docker&gt; pytest ...</code></li> <li>The test fails: switch to using <code>pytest_log</code> to save the stacktrace to a file</li> </ul> <p>```bash</p> <p>pytest_log dataflow/model/test/test_tiled_flows.py::Test_evaluate_weighted_forecasts::test_combine_two_signals   ...   =================================== FAILURES ===================================   _ Testevaluate_weighted_forecasts.test_combine_two_signals ___   Traceback (most recent call last):     File \"/app/dataflow/model/test/test_tiled_flows.py\", line 78, in test_combine_two_signals       bar_metrics = dtfmotiflo.evaluate_weighted_forecasts(     File \"/app/dataflow/model/tiled_flows.py\", line 265, in evaluate_weighted_forecasts       weighted_sum = hpandas.compute_weighted_sum(   TypeError: compute_weighted_sum() got an unexpected keyword argument 'index_mode'   ============================= slowest 3 durations ==============================   2.18s call     dataflow/model/test/test_tiled_flows.py::Test_evaluate_weighted_forecasts::test_combine_two_signals   0.01s setup    dataflow/model/test/test_tiled_flows.py::Test_evaluate_weighted_forecasts::test_combine_two_signals   0.00s teardown dataflow/model/test/test_tiled_flows.py::Test_evaluate_weighted_forecasts::test_combine_two_signals   ```</p> <ul> <li>Then from outside <code>dev</code> container launch <code>vim</code> in quickfix mode</li> </ul> <p>```bash</p> <p>invoke traceback   ```</p> <ul> <li>The short form is <code>it</code></li> </ul>"},{"location":"work_tools/all.invoke_workflows.how_to_guide.html#iterating-on-a-failing-regression-test","title":"Iterating on a failing regression test","text":"<ul> <li>The workflow is:</li> </ul> <p>```bash   # Run a lot of tests, e.g., the entire regression suite.</p> <p>pytest ...   # Some tests fail.</p> <p># Run the <code>pytest_repro</code> to summarize test failures and to generate commands to reproduce them.</p> <p>invoke pytest_repro   ```</p>"},{"location":"work_tools/all.invoke_workflows.how_to_guide.html#detect-mismatches-with-golden-test-outcomes","title":"Detect mismatches with golden test outcomes","text":"<ul> <li>The command is</li> </ul> <p>```bash</p> <p>i pytest_find_unused_goldens   ```</p> <ul> <li>The specific dir to check can be specified with the <code>dir_name</code> parameter.</li> <li>The invoke detects and logs mismatches between the tests and the golden   outcome files.</li> <li>When goldens are required by the tests but the corresponding files do not     exist<ul> <li>This usually happens if the tests are skipped or commented out.</li> <li>Sometimes it's a FP hit (e.g. the method doesn't actually call   <code>check_string</code> but instead has it in a string, or <code>check_string</code> is called   on a missing file on purpose to verify that an exception is raised).</li> </ul> </li> <li>When the existing golden files are not actually required by the     corresponding tests.<ul> <li>In most cases it means the files are outdated and can be deleted.</li> <li>Alternatively, it can be a FN hit: the test method A, which the golden   outcome corresponds to, doesn't call <code>check_string</code> directly, but the   test's class inherits from a different class, which in turn has a method B   that calls <code>check_string</code>, and this method B is called in the test method   A.</li> </ul> </li> <li>For more details see   CmTask528.</li> </ul>"},{"location":"work_tools/all.invoke_workflows.how_to_guide.html#lint","title":"Lint","text":""},{"location":"work_tools/all.invoke_workflows.how_to_guide.html#lint-everything","title":"Lint everything","text":"<pre><code>&gt; i lint --phases=\"amp_isort amp_class_method_order amp_normalize_import\namp_format_separating_line amp_black\" --files='$(find . -name \"\\*.py\")'\n</code></pre>"},{"location":"work_tools/all.jupytext.how_to_guide.html","title":"Jupytext","text":""},{"location":"work_tools/all.jupytext.how_to_guide.html#why-jupytext","title":"Why Jupytext?","text":"<ul> <li> <p>In few words Jupytext associates a   Python representation to a notebook, which is kept in sync with the notebook   in a sensible way</p> </li> <li> <p>Jupytext allows to:</p> </li> <li>Edit notebooks with your favorite editor (hopefully) vi or PyCharm</li> <li>Use a Python version of your notebook to run long computations from shell     instead of using a notebook</li> <li>Do a code review, diff changes, resolve conflicts using the Python code</li> </ul>"},{"location":"work_tools/all.jupytext.how_to_guide.html#reference-documentation","title":"Reference documentation","text":"<ul> <li>https://github.com/mwouts/jupytext</li> <li>https://jupytext.readthedocs.io/en/latest/</li> <li>https://jupytext.readthedocs.io/en/latest/faq.html</li> </ul>"},{"location":"work_tools/all.jupytext.how_to_guide.html#installation","title":"Installation","text":"<ul> <li>Check what version you have:</li> </ul> <p>```bash</p> <p>jupytext --version   1.2.1   ```</p> <ul> <li>Check if you have a Jupyter config:</li> </ul> <p>```bash</p> <p>ls ~/.jupyter/jupyter_notebook_config.py   ```</p> <ul> <li>If you don't have a config, generate it with:</li> </ul> <p>```bash</p> <p>jupyter notebook --generate-config   ```</p> <ul> <li>Edit <code>~/.jupyter/jupyter_notebook_config.py</code> and append the following:</li> </ul> <p><code>python   #------------------------------------------------------------------------------   # Jupytext   #------------------------------------------------------------------------------   c.NotebookApp.contents_manager_class = \"jupytext.TextFileContentsManager\"   # Always pair ipynb notebooks to py files   c.ContentsManager.default_jupytext_formats = \"ipynb,py\"   # Use the percent format when saving as py   c.ContentsManager.preferred_jupytext_formats_save = \"py:percent\"   c.ContentsManager.outdated_text_notebook_margin = float(\"inf\")</code></p> <ul> <li>Now you need to restart the notebook server to pick up Jupytext</li> <li>We use the \"percent\" format where cells are delimited by a <code>%%</code> comment</li> <li>Pycharm, black, and other tools understand / respect that this is a     delimiter for jupyter cells</li> </ul>"},{"location":"work_tools/all.jupytext.how_to_guide.html#using-jupytext","title":"Using Jupytext","text":"<ul> <li>Now when you <code>git add</code> a <code>.ipynb</code> file you always need to add also the paired   <code>.py</code> file</li> <li>Same thing if you rename with <code>git mv</code> or delete a notebook</li> <li>You need to explicitly take care of renaming and deleting also the <code>.py</code>     file</li> </ul>"},{"location":"work_tools/all.jupytext.how_to_guide.html#example-of-uses","title":"Example of uses","text":""},{"location":"work_tools/all.jupytext.how_to_guide.html#test-that-the-conversion-works-test-that-the-conversion-works","title":"Test that the conversion works {#test-that-the-conversion-works}","text":"<ul> <li>Jupytext keeps a notebook and the paired <code>.py</code> file in sync   ```bash <p>jupytext Task22.ipynb --test --to py:percent --stop jupytext Task22.ipynb --test-strict --to py:percent   ```</p> </li> </ul>"},{"location":"work_tools/all.jupytext.how_to_guide.html#manual-sync","title":"Manual sync","text":"<pre><code>```bash\n&gt; jupytext --sync --to py:percent XYZ.ipynb\n```\n</code></pre>"},{"location":"work_tools/all.jupytext.how_to_guide.html#automatic-syncing-when-using-the-jupyter-server","title":"Automatic syncing when using the Jupyter server","text":"<ul> <li>After you have installed the jupytext extension, open a notebook with your   Jupyter server</li> <li>You should see that there is a <code>.py</code> file close to the <code>.ipynb</code> file you     opened</li> <li>Open the <code>.py</code> file</li> <li> <p>You can see that there is the code in the cells separated by <code>%%</code></p> </li> <li> <p>Changes to the notebook are reflected in the file:</p> </li> <li>Modify the notebook and save the notebook</li> <li>Open the <code>.py</code></li> <li> <p>Note that the cell that you modified in the notebook has changed in the     <code>.py</code> file</p> </li> <li> <p>Changes to the file are reflected in the notebook:</p> </li> <li>Modify the <code>.py</code> file, e.g., changing one cell</li> <li>Go to the jupyter notebook and reload it</li> <li>The cell you modified has changed!</li> </ul>"},{"location":"work_tools/all.jupytext.how_to_guide.html#convert-a-notebook-to-script","title":"Convert a notebook to script","text":"<pre><code>```bash\n&gt; jupytext --to py:percent XYZ.ipynb\n```\n</code></pre>"},{"location":"work_tools/all.jupytext.how_to_guide.html#convert-a-script-into-a-notebook","title":"Convert a script into a notebook","text":"<pre><code>&gt; jupytext --to notebook XYZ.py\n</code></pre>"},{"location":"work_tools/all.jupytext.how_to_guide.html#remove-metadata-from-a-notebook","title":"Remove metadata from a notebook","text":"<ul> <li>This is equivalent to transforming the paired <code>.py</code> file in a notebook   ```bash <p>jupytext --to notebook XYZ.py   ```</p> </li> </ul>"},{"location":"work_tools/all.jupytext.how_to_guide.html#linter","title":"Linter","text":"<ul> <li>The linter automatically reformats the <code>.py</code> files and then updates the   <code>.ipynb</code> without losing the formatting   ```bash <p>linter.py --file XYZ.py jupytext --sync --to py:percent XYZ.py   ```</p> </li> </ul>"},{"location":"work_tools/all.jupytext.how_to_guide.html#refresh-all-the-scripts","title":"Refresh all the scripts","text":"<ul> <li> <p>The script <code>dev_scripts/notebooks/process_jupytext.py</code> automates some of the   workflow in Jupytext (see the help)</p> </li> <li> <p>The script <code>dev_scripts/notebooks/process_all_jupytext.sh</code> applies   <code>process_jupytext.py</code> to all the <code>ipynb</code> files   ```bash</p> <p>dev_scripts/notebooks/process_all_jupytext.sh    ```"},{"location":"work_tools/all.latex_toolchain.how_to_guide.html","title":"Latex Toolchain","text":""},{"location":"work_tools/all.latex_toolchain.how_to_guide.html#running-and-linting-latex-files","title":"Running and linting Latex files","text":"<p>We organize each project is in a directory (e.g., under <code>//papers</code>)</p> <p>Under each dir there are two scripts:</p> <ul> <li><code>run_latex.sh</code></li> <li><code>lint_latex.sh</code> that assign some variables and then call the main scripts to   perform the actual work:</li> <li><code>dev_scripts/latex/run_latex.sh</code></li> <li><code>dev_scripts/latex/lint_latex.sh</code></li> </ul> <p>Both main scripts are \"dockerized\" scripts, which build a Docker container with dependencies and then run use it to process the data</p> <p>To run the Latex flow we assume (as usual) that user runs from the top of the tree</p> <p>To create the PDF from the Latex files:</p> <pre><code>&gt; papers/DataFlow_stream_computing_framework/run_latex.sh\n...\n</code></pre> <p>To lint the Latex file:</p> <pre><code>&gt; papers/DataFlow_stream_computing_framework/lint_latex.sh\n...\n+ docker run --rm -it --workdir /Users/saggese/src/cmamp1 --mount type=bind,source=/Users/saggese/src/cmamp1,target=/Users/saggese/src/cmamp1 lint_latex:latest sh -c ''\\''./tmp.lint_latex.sh'\\''' papers/DataFlow_stream_computing_framework/DataFlow_stream_computing_framework.tex\npapers/DataFlow_stream_computing_framework/DataFlow_stream_computing_framework.tex 320ms (unchanged)\n</code></pre>"},{"location":"work_tools/all.latex_toolchain.how_to_guide.html#embedding-mermaid-and-planuml-figures","title":"Embedding Mermaid and PlanUML figures","text":"<p>Update ./dev_scripts/documentation/render_md.py</p> <ul> <li>Rename to render_figures.py</li> <li>It works on both Markdown and Latex files</li> <li>Find a mermaid/plantuml block and then add an image</li> </ul> <p>%<code>mermaid %flowchart %  Vendor Data --&gt; VendorDataReader --&gt; DataReader --&gt; User %</code></p>"},{"location":"work_tools/all.latex_toolchain.how_to_guide.html#finding-citations","title":"Finding citations","text":"<p>The simplest way is to use Google Scholar and then use the \"Cite\" option to get a Bibtex entry</p> <p>Some interesting links are https://tex.stackexchange.com/questions/143/what-are-good-sites-to-find-citations-in-bibtex-format</p>"},{"location":"work_tools/all.latex_toolchain.how_to_guide.html#todos","title":"TODOs","text":"<ul> <li> <p>Add a script to decorate the file with separators as part of the linting   <code>% ################################################################################   \\section{Adapters}   % ================================================================================   \\subsection{Adapters}   % --------------------------------------------------------------------------------   \\subsubsection{Adapters}</code></p> </li> <li> <p>Convert the Latex toolchain into Python code</p> </li> <li> <p>Add a script to run a ChatGPT prompt on a certain chunk of text</p> </li> <li> <p>Easily create a vimfile to navigate the TOC</p> </li> </ul>"},{"location":"work_tools/all.parquet.explanation.html","title":"Pyarrow Parquet management","text":""},{"location":"work_tools/all.parquet.explanation.html#introduction","title":"Introduction","text":"<ul> <li>What is Parquet?</li> <li> <p>Parquet is a columnar storage file format that provides efficient data     compression and encoding schemes with enhanced performance to handle complex     data in bulk. It is designed to support complex data structures and is ideal     for big data processing.</p> </li> <li> <p>Core features of Parquet:</p> </li> <li>Efficient storage and compression: Parquet uses efficient encoding and     compression techniques to store data in a columnar format, reducing the     storage space and improving the read performance</li> <li>Support for various compression algorithms: Parquet supports various     compression algorithms such as Snappy, Gzip, and LZO</li> <li>Support for complex data types: Parquet supports complex data types such as     nested fields, arrays, and maps, making it suitable for handling complex     data structures</li> <li> <p>Efficient encoding and decoding schemes: Parquet provides efficient encoding     and decoding schemes for complex data types, improving the read and write     performance</p> </li> <li> <p>Pyarrow</p> </li> <li>Pyarrow is a cross-language development platform for in-memory data that     provides efficient data interchange between Python and other languages. It     is designed to support complex data structures and is ideal for big data     processing.</li> <li>Pyarrow provides efficient data interchange between Python and other     languages, enabling seamless data exchange between different systems</li> <li>It supports various data types and complex data structures, making it     suitable for handling complex data processing tasks</li> </ul>"},{"location":"work_tools/all.parquet.explanation.html#implementation-details","title":"Implementation details","text":"<p>The <code>helpers.hparquet</code> module provides a set of helper functions to manage Parquet files using the pyarrow library.</p>"},{"location":"work_tools/all.parquet.explanation.html#writing-parquet","title":"Writing Parquet","text":"<ul> <li><code>to_parquet()</code></li> <li> <p>Writes a Pandas DataFrame to a Parquet file</p> </li> <li> <p><code>to_partitioned_parquet()</code></p> </li> <li>Writes a Pandas DataFrame to a partitioned Parquet files</li> </ul>"},{"location":"work_tools/all.parquet.explanation.html#reading-parquet","title":"Reading Parquet","text":"<ul> <li><code>from_parquet()</code></li> <li>Reads a Parquet file or partitional Parquet files into a Pandas DataFrame</li> </ul>"},{"location":"work_tools/all.parquet.explanation.html#change-log","title":"Change log","text":""},{"location":"work_tools/all.parquet.explanation.html#2024-02-26-cmamp-1140","title":"2024-02-26: cmamp-1.14.0","text":"<ul> <li>Upgraded pyarrow to 14.0.2 -&gt; 15.0.0</li> <li>Update outomes in the tests due to the new version of pyarrow changed the size   of the some Parquet files</li> <li>Delete <code>partition_filename</code> from <code>to_partitioned_parquet()</code> function</li> <li>Delete <code>partition_filename_cb</code> in the <code>pq.write_to_dataset()</code> call</li> <li>Delete <code>partition_filename</code> arguments from the all calls of   <code>to_partitioned_parquet()</code> function</li> <li>In the <code>list_and_merge_pq_files()</code></li> <li>In the <code>pq.ParquetDataset()</code> change <code>use_legacy_dataset=True</code> to     <code>partitioning=None</code></li> <li>Introduce <code>purify_parquet_file_names()</code> in the <code>helpers/hunit_test.py</code></li> </ul>"},{"location":"work_tools/all.parquet.explanation.html#rationale","title":"Rationale","text":"<ul> <li>The upgrade to pyarrow 15.0.0 is necessary to keep the library up-to-date and   benefit from the latest features and improvements</li> <li>Due the <code>partition_filename_cb</code> is deprecated in the new version of pyarrow,   it is necessary to remove it from the <code>to_partitioned_parquet()</code> function</li> <li>After discussion with the team, we decided to remove the     <code>partition_filename</code> from the <code>to_partitioned_parquet()</code> function</li> <li>The consequence of this change is that the Parquet files will be saved with     the default names like <code>&lt;guid&gt;-&lt;number&gt;.parquet</code> for example     <code>f3b3e3e33e3e3e3e3e3e3e3e3e3e3e3e-0.parquet</code></li> <li>In the pyarrow 1.15.0 the <code>use_legacy_dataset</code> is deprecated and the   <code>partitioning</code> should be used instead</li> <li>When we use the <code>partitioning=None</code> in <code>pq.ParquetDataset()</code> then we will     not use the partitioning and will not add the partitioned columns to the     dataset</li> <li>Some tests expect the Parquet files with the name <code>data.parquet</code>. The     <code>purify_parquet_file_names()</code> changes the names of the Parquet files to     <code>data.parquet</code> in the goldens</li> </ul>"},{"location":"work_tools/all.parquet.explanation.html#2024-03-11-cmamptask7331-remove-ns-vs-us-hacks-related-to-pyarrow-1402","title":"2024-03-11: CmampTask7331 Remove <code>ns vs us</code> hacks related to Pyarrow 14.0.2","text":"<ul> <li>Remove time unit casting to <code>us</code> in the <code>to_parquet()</code></li> <li>Keep time unit casting to <code>ns</code> in the <code>from_parquet()</code></li> </ul>"},{"location":"work_tools/all.parquet.explanation.html#time-unit-conversion-when-writing-to-parquet","title":"Time unit conversion when writing to Parquet","text":"<ul> <li>Context: Before the upgrade to pyarrow 15.0.0, casting the time unit to   <code>us</code> was necessary to avoid the <code>pyarrow.lib.ArrowInvalid</code> exception.</li> <li>Problem: In pyarrow 15.0.0, this exception is no longer raised, and the   time unit is preserved correctly.</li> <li>Insight: Casting the time unit to <code>us</code> in the <code>to_parquet()</code> function is   no longer necessary and can be removed. At the same time, casting to <code>us</code> in   the <code>to_parquet()</code> function does not make sense since the time unit will be   converted back to <code>ns</code> in the <code>from_parquet()</code> function.</li> <li>Solution: Remove the casting of the time unit to <code>us</code> in the   <code>to_parquet()</code> function.</li> </ul>"},{"location":"work_tools/all.parquet.explanation.html#time-unit-conversion-when-reading-from-parquet","title":"Time unit conversion when reading from Parquet","text":"<ul> <li>Context: The pyarrow version prior to 15.0.0 did not correctly preserve   the time unit information when reading data back from Parquet files. That's   why casting the time unit to <code>ns</code> was necessary in the <code>from_parquet()</code>   function.</li> <li>Problem: Since the upgrade to pyarrow 15.0.0, casting the time unit to   <code>ns</code> is no longer necessary, as the new version of pyarrow correctly preserves   the time unit. See the Pyarrow issue for details:   https://github.com/apache/arrow/issues/33321 When reading Parquet files with a   time unit that is not in ['us', 'ns'], the <code>pyarrow.lib.ArrowInvalid</code>   exception could be raised. This could occur when Pyarrow attempts to cast the   time unit to a lower resolution. This behavior is tested in the   <code>test_parquet_files_with_mixed_time_units_2</code> test. In this case, the   alphabetical order of the files is important. The data from the first file   will be cast to the time unit of the rest of the files.</li> <li>Insight: The general approach is to preserve the time unit information   after reading data back from Parquet files. Currently, resolving this issue is   challenging because Parquet data is mixed with data from CSV files, which   convert the time unit to <code>ns</code> by default. Refer to CmampTask7331 for details.   https://github.com/cryptokaizen/cmamp/issues/7331</li> <li>Solution: Retain the casting of the time unit to <code>ns</code> in the   <code>from_parquet()</code> function.</li> </ul>"},{"location":"work_tools/all.pycharm.how_to_guide.html","title":"Pycharm","text":""},{"location":"work_tools/all.pycharm.how_to_guide.html#pycharm_1","title":"PyCharm","text":""},{"location":"work_tools/all.pycharm.how_to_guide.html#current-situation","title":"Current situation","text":"<p>There are multiple ways to develop on a remote server using PyCharm</p> <ol> <li> <p>VNC approach</p> <ul> <li>PyCharm runs locally on the server using a \"virtual screen\"</li> <li>Your laptop interacts with a VNC server to get the GUI locally</li> <li>Pros:</li> <li>Everything works</li> <li>You can run anything like you are local on the server, since you are in     practice just using a virtual screen</li> <li>Cons:</li> <li>Without enough bandwidth it's slow and not snappy enough</li> </ul> </li> <li> <p>X11 approach</p> <ul> <li>Same as VNC, but instead of sending bitmaps through VNC, a \"compressed\"   version of the GUI is sent to the local computer directly</li> <li>Pros:</li> <li>Maybe faster than VNC</li> <li>PyCharm window is like a native window on your laptop</li> <li>Cons:</li> <li>X11 is old crap developed long time again and not really supported any     more</li> <li>One needs to tunnel X11 traffic, set things up, and so on</li> </ul> </li> <li> <p>PyCharm Gateway</p> <ul> <li>New client-server architecture for PyCharm</li> <li>A \"headless\" PyCharm runs on the server</li> <li>A GUI client PyCharm runs on your laptop</li> <li>Pros</li> <li>It's as fast as possible, probably as fast as running locally</li> <li>Cons</li> <li>Need a PyCharm pro license (not a problem, we have money)</li> <li>It's not super polished: kind of beta, but it will get better and better</li> </ul> </li> <li> <p>PyCharm Remote Set-up</p> <ul> <li>This is described below in   PyCharm - Advanced tip and tricks</li> <li>Edit locally and then PyCharm moves the files back and forth</li> <li>Pros</li> <li>Only requires ssh</li> <li>Cons</li> <li>You can't run / debug remotely</li> </ul> </li> </ol>"},{"location":"work_tools/all.pycharm.how_to_guide.html#current-situation_1","title":"Current situation","text":"<ul> <li> <p>Approach 1) seems to require lots of memory and CPU and it's not really fast.</p> </li> <li> <p>Approach 2) works but it's a pain to set-up and slow.</p> </li> <li> <p>We want to try with 3)</p> </li> <li>TODO(gp): @Juraj pls a short tutorial on how to install</li> <li>TODO(gp): @Juraj understand if it works, if it's fast, and if it requires     less memory</li> </ul>"},{"location":"work_tools/all.pycharm.how_to_guide.html#how-to-run-our-cmamp-container-directly-from-pycharm","title":"How to run our cmamp container directly from PyCharm","text":"<ul> <li>PyCharm allows to run commands directly inside a container</li> <li>See     https://www.jetbrains.com/help/pycharm/using-docker-as-a-remote-interpreter.html</li> <li> <p>In fact when we do <code>i docker_bash</code> we launch a container and run bash inside   it, but PyCharm can do the same thing</p> </li> <li> <p>TODO(gp): @Juraj Let's both try this. There are some notes below about it</p> </li> </ul>"},{"location":"work_tools/all.pycharm.how_to_guide.html#how-to-review-a-pr-inside-pycharm","title":"How to review a PR inside Pycharm","text":"<ul> <li> <p>CTRL + SHIFT + A -&gt; View Pull Request</p> </li> <li> <p></p> </li> </ul>"},{"location":"work_tools/all.pycharm.how_to_guide.html#how-to-edit-remote-code","title":"How to edit remote code","text":"<ul> <li> <p>You need to use a certain local directory (e.g.,   /Users/saggese/src/commodity_research1) and a remote directory (e.g.,   /wd/saggese/src/commodity_research1)</p> </li> <li> <p>They need to be synced at the same git branch (e.g., master or   AmpTask1112_Audit_amp_Docker_system_03)</p> </li> <li> <p>Set-up Deployment</p> </li> </ul> <p></p> <p></p> <ul> <li>The deployment options are</li> </ul> <p></p> <ul> <li>You can see what file is changed in the file transfer window:</li> </ul> <p></p> <p>pycharm</p> <ul> <li>Develop on one node, sync, run on the server</li> <li> <p>Run local application with venv</p> </li> <li> <p>Database</p> </li> <li> <p>Run application inside Docker</p> </li> <li> <p>Run application remotely inside Docker</p> </li> </ul>"},{"location":"work_tools/all.pycharm.how_to_guide.html#general-ssh-config","title":"General ssh config","text":"<ul> <li>File | Settings | Tools | SSH Configurations</li> <li> </li> <li> <p>Once setup, ssh config can be used for all tools in PyCharm.</p> </li> <li>Remote Interpreter</li> <li>DataGrip</li> <li>Deployment</li> <li>Etc.</li> </ul>"},{"location":"work_tools/all.pycharm.how_to_guide.html#db-connection-via-ssh","title":"DB connection via ssh","text":"<p>Note: PyCharm Professional DataGrip is used as an example. There are numerous open source alternatives such as Beaver. Config below should apply to them also.</p> <ul> <li>To add a new data source in DataGrip, go to the database section in the lower   left corner.</li> <li> <p></p> </li> <li> <p>Then pick your desired data source from the dropdown in the upper right   corner.</p> </li> <li> <p></p> </li> <li> <p>You will be presented with a dummy config that needs to be replaced with   proper data as shown below.</p> </li> <li> <p></p> </li> <li> <p>Before that is done, be sure that proper ssh info is added in SSH/SSL section.</p> </li> <li></li> </ul>"},{"location":"work_tools/all.pycharm.how_to_guide.html#deployment-with-remote-repository-through-sync","title":"Deployment with remote repository (through sync)","text":"<p>Note: Before setting up deployment, pull the cmamp repo on EC2 instance and use the same name as on your local machine (example: cmamp1). Always try to keep both repos in sync via git. For more subtle and simpler changes use File | Reload All From Disk . This will upload changes to the remote repo.</p> <ul> <li>Tools | Deployment | Configuration</li> <li></li> <li> <p></p> </li> <li> <p>Tools | Deployment | Options</p> </li> <li> <p></p> <ul> <li>Uncheck \"Skip external changes\" and check \"Delete remote files\"</li> </ul> </li> <li> <p>Tools | Deployment | Automatic Upload</p> </li> <li> <p>Check it</p> </li> <li> <p>Tools | Deployment | Browse Remote Host</p> </li> <li></li> </ul>"},{"location":"work_tools/all.pycharm.how_to_guide.html#pudb-remote-debugging-todo","title":"PUDB - remote debugging - ToDo","text":""},{"location":"work_tools/all.pycharm.how_to_guide.html#how-to-run-tests-inside-a-container","title":"How to run tests inside a container","text":"<ul> <li> <p>https://www.jetbrains.com/help/pycharm/using-docker-compose-as-a-remote-interpreter.html#docker-compose-remote</p> </li> <li> <p>Note that the \"start SSH session...\" action is available only in PyCharm   Professional Edition, while the terminal itself is available in both   Professional and Community editions.</p> </li> </ul>"},{"location":"work_tools/all.pycharm.how_to_guide.html#installing-pycharm-professional","title":"Installing PyCharm Professional","text":""},{"location":"work_tools/all.pycharm.how_to_guide.html#windows","title":"Windows","text":"<ol> <li>Download the installer using this    link</li> <li>Run the installer and follow the wizard steps.</li> <li>To run PyCharm, find it in the Windows Start menu or use the desktop    shortcut.</li> </ol>"},{"location":"work_tools/all.pycharm.how_to_guide.html#macos","title":"macOS","text":"<p>There are separate disk images for Intel and Apple Silicon processors.</p> <ol> <li> <p>Download the image, based on your processor using this    link</p> </li> <li> <p>Mount the image and drag the PyCharm app to the Applications folder.</p> </li> <li> <p>Run the PyCharm app from the Applications directory, Launchpad, or Spotlight.</p> </li> </ol>"},{"location":"work_tools/all.pycharm.how_to_guide.html#linux","title":"Linux","text":"<p>**Using tar archives **</p> <ol> <li> <p>Download the tar archive using this    link</p> </li> <li> <p>Unpack the pycharm-*.tar.gz file to a different folder, if your current    Download folder doesn't support file execution:    ```</p> <p>tar xzf pycharm-*.tar.gz -C     ``    ``` <p>The recommended installation location according to the filesystem hierarchy standard (FHS) is <code>/opt</code>. To install PyCharm into this directory, enter the following command:</p> <pre><code>&gt; sudo tar xzf pycharm-\\*.tar.gz -C /opt/\n</code></pre> <ol> <li> <p>Switch to the bin subdirectory:    ```</p> <p>cd /pycharm-*/bin    # E.g., cd /opt/pycharm-*/bin    ``` <li> <p>Run pycharm.sh from the bin subdirectory    ```</p> <p>sh pycharm.sh    ```</p> </li> <p>Using snap packages</p> <ol> <li> <p>For Ubuntu 16.04 and later, you can use snap packages to install PyCharm.    ```</p> <p>sudo snap install pycharm-professional --classic    # or sudo snap install pycharm-community --classic    ```</p> </li> <li> <p>Run in the Terminalu    ```</p> <p>pycharm-professional    # or pycharm-community    # or pycharm-educational    ```</p> </li> </ol>"},{"location":"work_tools/all.pycharm.how_to_guide.html#connecting-via-pycharm-gateway-ssh","title":"Connecting via PyCharm gateway (SSH)","text":"<p>The first thing you need to do is sign up for a free trial license or use it if it already have</p> <p>Then make sure you have a VPN connection to our VPC</p> <ol> <li>Click on Connect via SSH</li> <li>Into Username: write &lt;&gt; Example: richard <li>Into Host: write &lt;&gt; Example: 172.30.2.136 <li>Mark the Specify private key check box and locate the private key from the     zip which was sent in the onboarding process. Example: crypto.pub</li> <li>Leave Port: 22 as it is.</li> <li>Click on Check Connection and Continue.</li> <li>Select IDE version: PyCharm Py 213.6777.x</li> <li>Locate your directory. Example: /data/richard</li> <li>Click on Download and Start IDE.</li>"},{"location":"work_tools/all.pycharm.how_to_guide.html#connecting-via-vnc","title":"Connecting via VNC","text":"<ul> <li>Make sure you have a VPN connection.</li> </ul> <p>Installing VNC</p> <ul> <li>Install VNC using this link:</li> <li>https://www.realvnc.com/en/connect/download/viewer/windows/</li> </ul> <p>Sysadmin has sent you:</p> <ul> <li><code>os_password.txt</code></li> <li>Your username <code>$USER</code></li> <li>A key <code>crypto.pub</code> that looks like:   <code>-----BEGIN OPENSSH PRIVATE KEY-----   b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAABlwAAAAdzc2gtcn   NhAAAAAwEAAQAAAYEA0IQsLy1lL3bhPT+43sht2/m9tqZm8sEQrXMAVtfm4ji/LXMr7094 ...   hakqVTlQ2sr0YTAAAAHnNhZ2dlc2VAZ3BtYWMuZmlvcy1yb3V0ZXIuaG9tZQECAwQ=   -----END OPENSSH PRIVATE KEY-----</code></li> </ul> <p>Let's say you are connected via VNC.</p> <ol> <li>Login into the OS.</li> <li>Run pycharm.sh using terminal (should be there)    ``` <p>bash /opt/pycharm-community-2021.2.3/bin/pycharm.sh    ```</p> </li> </ol>"},{"location":"work_tools/all.pycharm.how_to_guide.html#configuration","title":"Configuration","text":""},{"location":"work_tools/all.pycharm.how_to_guide.html#reflow","title":"Reflow","text":"<ul> <li>Set the reflow to reindent</li> <li></li> </ul>"},{"location":"work_tools/all.pycharm.how_to_guide.html#some-recommended-plug-ins","title":"Some recommended plug-ins","text":"<ul> <li>Vim</li> <li>Grazie</li> <li>Wrap-to-column</li> <li>GitHub Copilot</li> </ul>"},{"location":"work_tools/all.python_package_upgrade_and_troubleshooting.how_to_guide.html","title":"Python package upgrade &amp; troubleshooting","text":""},{"location":"work_tools/all.python_package_upgrade_and_troubleshooting.how_to_guide.html#description","title":"Description","text":"<ul> <li>Upgrading Python or its packages is a complex process requiring significant   attention and time</li> <li>This document aims to provide a step-by-step guide for upgrading Python or its   packages</li> <li>Several steps are outlined to be followed to avoid any potential issues</li> <li>The primary objective is to streamline the upgrade process, ensuring it   unfolds seamlessly while mitigating any potential issues that might arise   along the way</li> </ul>"},{"location":"work_tools/all.python_package_upgrade_and_troubleshooting.how_to_guide.html#building-the-local-image","title":"Building the local image","text":"<ul> <li>Upgrade the packages versions in the   /devops/docker_build/pyproject.toml</li> <li>After the upgrade, the first step is to build the local image</li> <li>The build command is described in the   /docs/work_tools/all.docker.how_to_guide.md#multi-architecture-build</li> </ul>"},{"location":"work_tools/all.python_package_upgrade_and_troubleshooting.how_to_guide.html#run-the-tests","title":"Run the tests","text":"<ul> <li>Create a new task-specific gdoc, e.g., CmTask7256_Upgrade_Pandas.docx</li> <li>Run all the tests from the orange repo against the new dev image</li> <li>Fast</li> <li>Slow</li> <li>Superslow</li> <li>QA</li> <li>Example: <code>i run_fast_tests --stage local --version {new version}</code></li> </ul>"},{"location":"work_tools/all.python_package_upgrade_and_troubleshooting.how_to_guide.html#document-errors","title":"Document Errors","text":"<ul> <li>Collect all the failures in the GDOC:</li> <li>Test name</li> <li>Traceback</li> <li>Group errors by type, e.g.,   <code># Error type 1   ## Test name 1   traceback   ## Test name 2   traceback   # Error type 2   ## Test name 1   traceback   ## Test name 2   traceback   # Error type N ...</code></li> </ul>"},{"location":"work_tools/all.python_package_upgrade_and_troubleshooting.how_to_guide.html#identify-forward-compatible-fixes","title":"Identify forward-compatible fixes","text":"<p>Forward-compatible fix is a fix that works on both versions of a Python package, i.e. on the current one and the target one.</p> <ul> <li>Identify forward-compatible changes</li> <li>Move them to a separate GDOC section to isolate from the other changes</li> <li>For each forward-compatible fix file a separate GH issue and a separate PR to   master</li> </ul>"},{"location":"work_tools/all.python_package_upgrade_and_troubleshooting.how_to_guide.html#handle-non-forward-compatible-fixes","title":"Handle non-forward compatible fixes","text":"<ul> <li>File an issue for the error</li> <li>Describe the solution in the GDOC</li> <li>Apply changes to the main PR</li> <li>Run the tests for this error type and make sure they do not fail</li> <li>Repeat this process for all the types of errors</li> <li>Once resolved, run all the regressions locally on the new local image from the   orange repo and make sure the regressions are green</li> </ul>"},{"location":"work_tools/all.python_package_upgrade_and_troubleshooting.how_to_guide.html#image-release","title":"Image Release","text":"<ul> <li>Run release command described in the   /docs/work_tools/all.docker.how_to_guide.md#command-to-run-the-release-flow</li> <li>Follow the post-release check-list in the   /docs/work_tools/all.docker.how_to_guide.md#post-release-check-list</li> </ul>"},{"location":"work_tools/all.python_package_upgrade_and_troubleshooting.how_to_guide.html#merge-base-pr","title":"Merge Base PR","text":"<ul> <li>Run the regressions from GitHub on the main PR</li> <li>Check that the GitHub Actions is picking up new image and all regressions   (fast, slow, superslow) are green on <code>cmamp</code>, <code>orange</code> and <code>kaizenflow</code></li> <li>Merge the main PR on all the repos (<code>cmamp</code>, <code>orange</code>, <code>kaizenflow</code>)</li> </ul>"},{"location":"work_tools/all.ssh.how_to_guide.html","title":"Ssh","text":""},{"location":"work_tools/all.ssh.how_to_guide.html#what-is-ssh","title":"What is SSH?","text":"<p>From the Wikipedia</p> <p>Secure Shell (SSH) is a cryptographic network protocol for operating network services securely over an unsecured network.[1] Typical applications include remote command-line, login, and remote command execution, but any network service can be secured with SSH.</p> <p>More details here</p>"},{"location":"work_tools/all.ssh.how_to_guide.html#how-we-use-ssh-in-our-company","title":"How we use ssh in our company?","text":"<ul> <li>We use it to connect to any of our servers.</li> <li>Sometimes we use <code>scp</code> to copy files between hosts via <code>ssh</code>.</li> <li>Don't know what is <code>scp</code>? read     here</li> </ul>"},{"location":"work_tools/all.ssh.how_to_guide.html#public-key-for-authorization","title":"Public key for authorization?","text":"<ul> <li>We use <code>public key</code> authorization. This is the common way of secure   authorization for SSH connection.</li> <li>GitHub also can authorize you with <code>public key</code>, if you setup it in your GH   account. This is mean that you don't have to type your <code>login</code> and <code>password</code>   when you interact with GitHub via <code>git</code> e.g. <code>git clone</code>, <code>git pull</code>, etc.</li> </ul> <p>More details about <code>public key</code> here</p>"},{"location":"work_tools/all.telegram_notify_bot.how_to_guide.html","title":"telegram-notify","text":"<p>Send notifications via Telegram.</p>"},{"location":"work_tools/all.telegram_notify_bot.how_to_guide.html#configuring-the-bot","title":"Configuring the Bot","text":""},{"location":"work_tools/all.telegram_notify_bot.how_to_guide.html#getting-token-and-chat-id","title":"Getting token and chat id","text":"<ul> <li> <p>Start messaging with either <code>https://t.me/NotifyJupyterBot</code> (or with a custom   bot by sending it <code>/start</code> message)</p> </li> <li> <p>Run:   ```bash</p> <p>python helpers/telegram_notify/get_chat_id.py --username    ``` <li> <p>Specify <code>--token &lt;your bot token&gt;</code> if you are using a custom bot</p> </li> <li> <p>This will display a message in your terminal with your chat id and also send a   message with it through the bot   <code>bash   User `saggese` is not in the config.py   Your chat id is: 967103049</code></p> </li>"},{"location":"work_tools/all.telegram_notify_bot.how_to_guide.html#modifying-config-for-your-token-and-chat-id","title":"Modifying config for your token and chat id","text":"<ul> <li>Go to <code>config.py</code> and insert the following code   <code>python   elif user == &lt;your local user name&gt;:       TELEGRAM_TOKEN = &lt;bot token&gt;       TELEGRAM_CHAT_ID = &lt;chat id&gt;</code></li> <li>You should push the modified file to the repo</li> </ul>"},{"location":"work_tools/all.telegram_notify_bot.how_to_guide.html#how-to-use","title":"How to use","text":"<ul> <li>There are 3 ways of getting notification from the bot:</li> <li>Using command line</li> <li>Through <code>TelegramNotify</code> class (e.g., from Python code or a Jupyter     notebook)</li> <li> <p>Using logging</p> </li> <li> <p>In any case the bot will send you something like this:   <code>&lt;program name&gt;: &lt;your_message&gt;</code></p> </li> </ul>"},{"location":"work_tools/all.telegram_notify_bot.how_to_guide.html#command-line","title":"Command line","text":"<ul> <li>You can use a command line wrapper to signal the end of a command line:</li> </ul> <pre><code>&gt; cmd_to_check; tg.py -m \"error=$?\"\n&gt; ls; tg.py -m \"error=$?\"\n&gt; ls /I_do_not_exist; tg.py -m \"error=$?\"\n</code></pre>"},{"location":"work_tools/all.telegram_notify_bot.how_to_guide.html#telegramnotify","title":"TelegramNotify","text":"<pre><code>import helpers.telegram_notify.telegram_notify as tg\n\ntgn = tg.TelegramNotify()\ntgn.notify('test message')\n</code></pre>"},{"location":"work_tools/all.telegram_notify_bot.how_to_guide.html#logging","title":"Logging","text":"<pre><code>import logging\nimport helpers.telegram_notify.telegram_notify as tg\n\n_TG_LOG = logging.getLogger('telegram_notify')\n_TG_LOG.setLevel(logging.INFO)\ntg.init_tglogger()\n\n_TG_LOG.info('test message')\n</code></pre>"},{"location":"work_tools/all.visual_studio_code.how_to_guide.html","title":"Visual Studio Code","text":""},{"location":"work_tools/all.visual_studio_code.how_to_guide.html#visual-studio-code_1","title":"Visual Studio Code","text":""},{"location":"work_tools/all.visual_studio_code.how_to_guide.html#connecting-via-vnc","title":"Connecting via VNC","text":"<ul> <li>Make sure you have a VPN connection.</li> </ul>"},{"location":"work_tools/all.visual_studio_code.how_to_guide.html#installing-vnc","title":"Installing VNC","text":"<ul> <li>Install VNC using this link: https://www.realvnc.com/en/connect/download/viewer/windows/</li> <li>Sysadmin has sent you:</li> <li><code>os_password.txt</code></li> <li>Your username <code>$USER</code></li> <li> <p>A key <code>crypto.pub</code> that looks like:     <code>-----BEGIN OPENSSH PRIVATE KEY-----     b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAABlwAAAAdzc2gtcn     NhAAAAAwEAAQAAAYEA0IQsLy1lL3bhPT+43sht2/m9tqZm8sEQrXMAVtfm4ji/LXMr7094     \u2026     hakqVTlQ2sr0YTAAAAHnNhZ2dlc2VAZ3BtYWMuZmlvcy1yb3V0ZXIuaG9tZQECAwQ=     -----END OPENSSH PRIVATE KEY-----</code></p> </li> <li> <p>Let's say you are connected via VNC.</p> </li> <li>Login into the OS.</li> <li>Run <code>pycharm.sh</code> using terminal (should be there):     <code>&gt; bash /opt/pycharm-community-2021.2.3/bin/pycharm.sh</code></li> </ul>"},{"location":"work_tools/all.visual_studio_code.how_to_guide.html#installation-of-vs-code","title":"Installation of VS Code","text":""},{"location":"work_tools/all.visual_studio_code.how_to_guide.html#windows-linux-mac","title":"Windows, Linux, Mac","text":"<ul> <li>Download the installer using this link:   Download Visual Studio Code - Mac, Linux, Windows.</li> <li>Run the installer and follow the wizard steps.</li> <li>To run VS Code, find it in the Start menu or use the desktop shortcut.</li> <li>In the left navigation bar search for extensions ( or use <code>Ctrl+Shift+X</code> ) and   search for \"ms-vscode-remote.remote-ssh\" and then click on the install button.   </li> <li>Connect to the VPN.</li> <li>In bottom left corner click on this green button:   </li> <li>Then you will see these options on top of the screen, click on \"Open SSH   Configuration File\u2026\" and then click on the <code>user\\.ssh\\config</code> or   <code>user/.ssh/config</code>.   </li> <li>The config should look like this:   </li> <li><code>HostName</code>: dev1 (or dev2) server IP.</li> <li><code>User</code>: your linux user name on the dev server.</li> <li><code>IdentityFile</code>: private key that you use to <code>SSH</code> to the dev server.</li> <li>Save and close the config file and press the green button again, then for   connection click on \"Connect to Host...\". You should see the IP address of the   server, so just click on it and it will connect you in a new window.</li> <li>Open a preferred repo directory.</li> <li>Click on the \"Source control\" button on the left:     </li> <li>Choose \"Open Folder\":     </li> <li>Choose the desired repo directory from the drop-down menu, e.g., <code>cmamp1</code>:     </li> </ul>"},{"location":"work_tools/all.visual_studio_code.how_to_guide.html#how-to-run-a-vscode-debugger-within-a-remote-container","title":"How to run a VSCode debugger within a remote container","text":"<p>The goal is to successfully run a Visual Studio Code (VSCode) debugger on code that runs within a docker container located on a remote server.</p>"},{"location":"work_tools/all.visual_studio_code.how_to_guide.html#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Mac OS or Linux-based OS</p> </li> <li> <p>Visual Studio Code installed on the local machine you are working from</p> </li> <li> <p>VSCode extensions installed:</p> </li> <li>Python     (Installed on the remote machine)<ul> <li>VS Code installs some files inside <code>.vscode-*</code> directories on the remote   host to ensure full functionality of the extensions</li> </ul> </li> <li>Remote SSH</li> <li>Remote Explorer</li> <li>Remote Development     (Installed on the remote machine)</li> <li> <p>Dev Containers</p> </li> <li> <p>Ability to access the remote server where the container will be located using   SSH</p> </li> <li> <p>Kaizen dev environment set-up on the remote machine to allow running <code>invoke</code>   targets</p> </li> <li> <p>A running Kaizen dev docker container created on the remote machine using   <code>invoke docker_bash</code></p> </li> </ul>"},{"location":"work_tools/all.visual_studio_code.how_to_guide.html#tips","title":"Tips","text":"<ul> <li>To open the command palette in VSCode use a keyboard shortcut Cmd + Shift + P</li> </ul>"},{"location":"work_tools/all.visual_studio_code.how_to_guide.html#steps","title":"Steps","text":"<ol> <li> <p>Open the command palette and search for <code>Remote-SSH: Connect to host</code> action</p> </li> <li> <p>Click <code>+ Add new SSH Host</code>. The prompt will ask you to provide a full     command to log in to the server, e.g. \"ssh ubuntu@10.11.12.13\"</p> <ol> <li>Note: if you use <code>~/.ssh/config</code> to specify frequently used hosts,     VSCode will automatically fetch the list for you, this way you simply     choose the name of the host and the connection is established     automatically</li> </ol> </li> <li> <p>Make sure you have a container running using <code>invoke docker_bash</code> on the     remote machine</p> </li> <li> <p>VSCode will establish a remote connection to the server and open a new     window. Make sure that is your active window.</p> </li> <li> <p>Open the command palette and search for     <code>Dev Containers: Attach To Running Container</code>. A list of all running     containers on your server will appear, choose your container.</p> </li> <li> <p>Choose the debugging panel from the sidebar and click on <code>Run and Debug</code></p> </li> </ol> <p></p> <ol> <li>Click \"Add Configuration\" an editor for \"launch.json\" should open</li> </ol> <p></p> <ol> <li>Paste the following JSON into the file and save it.</li> </ol> <pre><code>{\n    // Use IntelliSense to learn about possible attributes.\n    // Hover to view descriptions of existing attributes.\n    // For more information, visit:\n    https://go.microsoft.com/fwlink/?linkid=830387\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n    {\n        \"name\": \"Python: Current File\",\n        \"type\": \"python\",\n        \"request\": \"launch\",\n        \"program\": \"${file}\",\n        \"console\": \"integratedTerminal\",\n        \"justMyCode\": true,\n        \"python\":\"/venv/bin/python\",\n        \"env\": {\n            \"PATH\":\n            \"/app/amp/documentation/scripts:/app/amp/dev_scripts/testing:/app/amp/dev_scripts/notebooks:/app/amp/dev_scripts/install:/app/amp/dev_scripts/infra:/app/amp/dev_scripts/git:/app/amp/dev_scripts/aws:/app/amp/dev_scripts:/app/amp:/app/dev_script_p1:/app:.:/venv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\n            \"PYTHONPATH\":\"/app/amp:/app:\"\n        }\n    }\n    ]\n}\n</code></pre> <ol> <li>Now you can open any Python file within the environment. You can for example     set a breakpoint, hit the green \"play\" button and the debugger will run the     currently opened file</li> </ol> <p></p> <ol> <li>To run a script with a specific set of command line arguments, append     <code>\"args\"</code> key to the launch.json file (the location should be     <code>.vscode/launch.json</code>). The value is a list of command line arguments and     values. Example below:</li> </ol> <pre><code>\"args\": [\n    \"--file\",\n    \"/app/im_v2/ccxt/notebooks/Master_universe.ipynb\",\n    \"--action\",\n    \"convert\"\n]\n</code></pre>"},{"location":"work_tools/all.visual_studio_code.how_to_guide.html#how-to-access-the-jupyter-server-running-on-the-remote-server-through-your-local-machine","title":"How to access the Jupyter server running on the remote server through your local machine","text":"<p>1.<code>i docker_jupyter</code></p> <pre><code>###&gt; devops/docker_run/run_jupyter_server.sh\n&gt; cmd=jupyter notebook --ip=* --port=10421 --allow-root --NotebookApp.token=''\n...\n</code></pre> <ol> <li>In VSCode, add port forwarding from port <code>10421</code> on the server to the same    port on your local machine. You can then access the Jupyter notebook running    on the remote machine through <code>http://localhost:10421</code> on your local machine.</li> </ol> <p></p>"}]}