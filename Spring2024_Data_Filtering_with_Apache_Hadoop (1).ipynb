{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Download and extract Hadoop\n",
        "!wget -q https://dlcdn.apache.org/hadoop/common/hadoop-3.4.0/hadoop-3.4.0.tar.gz\n",
        "!tar -xzf hadoop-3.4.0.tar.gz\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"HADOOP_HOME\"] = \"/content/hadoop-3.4.0\"\n",
        "os.environ[\"PATH\"] += f\":{os.environ['HADOOP_HOME']}/bin\"\n",
        "\n",
        "# Verify Java and Hadoop installation\n",
        "!echo $JAVA_HOME\n",
        "!hadoop version\n"
      ],
      "metadata": {
        "id": "cVGH9wWCYGis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the Dataset\n",
        "The dataset involves lines containing either a name, email address, or both.\n",
        "\n",
        "Example would be:\n",
        "\n",
        "Tom Jones - tom.jones@gmail.com\n",
        "\n",
        "Ethan Smith\n",
        "\n",
        "Chandler Johnson\n",
        "\n",
        "Brian Flaunders: bflaunders@yahoo.com\n",
        "\n",
        "Samantha Lipson"
      ],
      "metadata": {
        "id": "_lS6Fsc4YuAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The python mapper script I crafted will read each line separately, check if it contains an email address using a regex pattern, and output that."
      ],
      "metadata": {
        "id": "BOBaJ_J0Yyyy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0VidNO4CO7y"
      },
      "outputs": [],
      "source": [
        "%%writefile email_mapper.py\n",
        "#!/usr/bin/env python3\n",
        "import sys\n",
        "import re\n",
        "\n",
        "def email_mapper(line):\n",
        "    pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
        "    if re.search(pattern, line):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "for line in sys.stdin:\n",
        "    line = line.strip()\n",
        "    if email_mapper(line):\n",
        "        print(line)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x email_mapper.py\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q4USHdB3CTk8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo -e \"Lebron James - LebronJamess@NBA.com\\nLos Angeles Lakers\\nEmpty Line\\nLawyer: JaneJohnson@hotmail.com\\nDallas Cowboys\" | python email_mapper.py\n"
      ],
      "metadata": {
        "id": "igCyzi94QA80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo -e \"Lebron James - LebronJamess@NBA.com\\nLos Angeles Lakers\\nEmpty Line\\nLawyer: JaneJohnson@hotmail.com\\nDallas Cowboys\" > input.txt\n"
      ],
      "metadata": {
        "id": "QuZ-e3GQTa9g"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar \\\n",
        "    -files email_mapper.py \\\n",
        "    -input input.txt \\\n",
        "    -output output \\\n",
        "    -mapper email_mapper.py \\\n",
        "    -reducer NONE\n",
        "\n"
      ],
      "metadata": {
        "id": "crWs6QFEYjtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat output/*\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1G5Dka6SfkR",
        "outputId": "c236da1a-7691-4966-a2eb-a9f776c7b655"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lebron James - LebronJamess@NBA.com\t\n",
            "Lawyer: JaneJohnson@hotmail.com\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Markdown Report\n",
        "###short description of the technology used:\n",
        "\n",
        "Docker is a platform that allows developers to package applications into containersâ€”standardized executable components combining application source code with the operating system libraries and dependencies required to run that code in any environment. Docker's container technology offers a lighter-weight form of virtualization, providing almost the same isolation as virtual machines but without the overhead of having to include a full OS in each application's files. Docker is efficient for resource use, rapid deployment, and consistent operation across systems. A con is that containers still share the host OS kernel, which can lead to security vulnerabilities if not managed correctly. The principle of \"build once, run anywhere\" can be applied using Docker, demonstrating modern deployment methodologies.\n",
        "\n",
        "Redis is a memory key value data storer, used as a database, cache, and message broker. It can use strings, lists, maps, sets, and much more. Unlike most databases that store data on disk, Redis stores data in memory, which allows for much efficient and timely data retrieval. Redis is exceptionally fast and efficient, supports rich data types, and can be easily scaled in distributed environments using Redis Cluster. Unfortunatley though, its data size is limited by memory, and persistence configuration can compromise performance.\n",
        "\n",
        "Google Colab is a cloud service that supports Python and Jupyter scripts for machine learning applications. Colab removes the necessity for complex hardware setups and software configurations by providing a fully prepared execution environment, which is particularly beneficial for students and researchers. It requires no previous setup, free access to hardware accelerators, and integration with Google Drive; but it does have imited session durations. Google Colab can be used for programming, data analysis, and machine learning easily.\n",
        "\n",
        "Java is object oriented programming language designed to have as few implementation dependents as possible. Its enables applications to be written once and run anywhere again. Java achieves platform independence through the use of the Java Virtual Machine, which abstracts the application from hardware-specific details. Benefits of Java include platform independence, strong memory management, extensive standard libraries. But, compared to other languages, Java may require more memory and has a slower runtime.\n",
        "Java's concept of write once, run anywhere is its most significant feature, illustrating all platform compatibility.\n",
        "\n",
        "Hadoop is a open source framework that processes large data sets using simple programming algorithms or models. Hadoop is designed to scale up from a single server to several, efficiently. Hadoop has high scalability, is cost efficient, flexible data processing, fault tolerance. There is complexity in setup and management, and slow in small data operations. Hadoop and MapReduce are often discussed in courses on big data technologies, showcasing how large data sets can be managed and processed efficiently.\n",
        "\n",
        "\n",
        "Sources:\n",
        "\n",
        "docs.docker.com/\n",
        "\n",
        "redis.io\n",
        "\n",
        "research.google.com/colaboratory/faq.html\n",
        "\n",
        "docs.oracle.com/en/java/\n",
        "\n",
        "hadoop.apache.org/\n",
        "\n",
        "svn.apache.org/repos/asf/hadoop/common/site/main/publish/index.html\n",
        "\n",
        "..."
      ],
      "metadata": {
        "id": "6aFP3rgQHEdt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###describe and explain the logic of the Docker system that you have built:\n",
        "\n",
        "Explain the decision you have made\n",
        "What are the containers involved\n",
        "How they communicate\n",
        "\n",
        "The choice of openjdk:8-jdk as a base image is because of  Hadoop's requirement for Java. This image provides a Java environment, which simplifies setting up. Installing tools like wget for downloading necessary files, vim for editing configurations, and ssh and pdsh for cluster management; emphasizes versaitlity, a manageable environment, when dealing with distributed systems like Hadoop. Hadoop is downloaded directly from its offical website. Copying custom configuration files into the Hadoop directory in the container is crucial for tailoring Hadoop's behavior to the needs of your project, such as setting up correct networking and storage options. Exposing ports like 9870 (NameNode web UI) and 8088 (ResourceManager web UI) is essential for accessing Hadoop's management interfaces from outside the container, facilitating monitoring and management. If all services, or Hadoop nodes, run within a single container, the communication happens internally, which simplifies the network complexity but deviates from production-like environments where each node would typically be in its own container. In a more scalable setup, each Hadoop component would be housed in separate containers. These containers would communicate over a Docker defined network, which isolates traffic and secures communication channels.\n"
      ],
      "metadata": {
        "id": "-6t8WQSNezAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#explain how to run the system by starting the container system:\n",
        "Open a terminal and then run:\n",
        "\n",
        "bash\n",
        "\n",
        "docker build -t my-hadoop-system .\n",
        "\n",
        "This command tells Docker to build a new image,\n",
        "-t my-hadoop-system: This tags the created image with the name \"my-hadoop-system,\" making it easier to reference.\n",
        ".: This specifies the build context as the current directory. Docker will look for the Dockerfile here.\n",
        "Then the dockerfile begins pulling the base image, running commands to install software, and copying files. When the process is completed without error, you will see a message saying so. \"docker run\" creates a container from the image, \"-d\" runs the container in the background. \"-p 9870:9870 -p 8088:8088\" reports the container's ports to the host, which is needed for accessing the NameNode and ResourceManager web UIs from the browser. \"--name hadoop-instance\": assigns a name to the container. \"my-hadoop-system\" is the name chosen of the image to run.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CiSO2EDZh5Er"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#describe exactly what you have done:\n",
        "\n",
        "Java Installation updates the package lists and installs the headless version of OpenJDK 8. Hadoop Setup downloads and extracts Hadoop 3.4.0 silently using wget with the quiet option -q. Then I set JAVA_HOME to the directory where Java is installed. Also set HADOOP_HOME to the directory where Hadoop is extracted. Appends the Hadoop binary directory to the PATH environment variable, enabling execution of Hadoop commands from the shell. Print the Java installation path to verify JAVA_HOME is set correctly. Checks the installed Hadoop version to ensure it's correctly installed and ready to work.\n",
        "The email_mapper script extracts lines containing email addresses from the input and prints them out. The regex pattern utilized identifies normal email formats within the text. The code reads from standard input (sys.stdin), which is typical in Hadoop streaming tasks where data is piped into the script. I use echo-e to test the code locally,it does this using sample text directly into the Python script to test email extraction functionality and regex efficiency.Then it configures and runs a Hadoop streaming job using email_mapper.py as the mapper. The outputs lines from the input that contain email addresses. The output after is: Lebron James - LebronJames@NBA.com\n",
        "Lawyer: JaneJohnson@hotmail.com\n",
        "so I know the code works correctly and efficiently."
      ],
      "metadata": {
        "id": "KFl9LnIRl11y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AY8qsPN2qrL1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}